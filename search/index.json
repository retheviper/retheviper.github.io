[{"content":"今年のKotlinConfが開催されました。数日かけてのイベントなので、全てのセッションを見ることはできませんが、まずはキーノートの方で重要な情報をまとめてみました。全体のスケージュールはこちらから確認できて、YouTubeでのライブ配信も行われているので、興味がある方はぜひチェックしてみてください。\nまずはKotlinConf'24の公式アプリの紹介がありました。公式アプリでは今回行われるカンファレンスとセッションを確認できるもので、GitHubにてソースコードを公開しています。Kotlin Multiplatformで作成されていて、iOS、Android、Webで動作するものなので、いいサンプルとして使えるかもしれません。\nKotlinの今 まずはKotlinの現状から。常にKotlinを使っているエンジニアの数が、200万以上になっているとのことです。\nまた、Kotlinを導入している企業もどんどん増えているとのことです。代表的な企業は以下の画像通りだそうです。\nKotlin 2.0 and K2 Compiler Kotlin 2.0からはK2コンパイラーの導入により、全般的にコンパイルの速度が向上したという話が主なテーマです。ただ一部プロジェクトでは、遅くなるケースもあるとのことでした。また、IntellijでもK2コンパイラーモードがあり、コードハイライトが1.8倍速くなるとのことです。\nK2モードは、Intellij 2024.1以上から以下の設定画面で設定できます。2024.2からは性能の改善を含め、Beta版として提供される予定です。\n2.0へのマイグレーションは1000万行のコード、18000人のエンジニア、80000のプロジェクトでテストされていて、1.9からのマイグレーションはスムーズに行えるとのことです。\nMetaの場合 Metaの場合、Kotlin firstでの開発を積極的に進めているとのことでした。IDEからコードの最適化までの全てにおいてKotlinを採用して、既存のJavaで作成されたFacebook, Instagram, Facebook Messenger, WhatsAppなどのアプリにおいて、Kotlinに自動変換できるツールを作成して移行を自動化しているとのことです。\nそしてMetaはKotlin 1.8の時点から一部のプロジェクトにおいてすでにK2コンパイラーを採用していて、今は95%のプロジェクトがK2コンパイラーを使っているとのことです。その効果としてはAndroidプロジェクトにおいては最大20%のビルド時間の短縮ができたらしいです。\nGoogleの場合 K2コンパイラーにはGoogle側も協力していて、Androidのツーリングに関してLintやParcelize、KSPなどにもコントリビュートしているそうです。また、Jetpack Composeのコンパイラーの改善もあり、従来はKotlinとバージョンと相違があったのですが、2.0からは同じバージョンで指定できるようになったとのことです。\nまたAndroid StudioでもKotlin 2.0のサポートを予定しているとのことでした。代表的な機能は以下の画像の通りです。\nJetpack Composeにおいても新しい機能を追加される予定だそうです。主に以下の画像で挙げている機能が、7月から提供される予定だそうです。\nコンパイラーそのものの改善もあり、安定性やパフォーマンスが向上しているとのことです。\n他にも、Google内部ではサーバサイドKotlinの採用も進んでいたり、KMPによる開発も進んでいるとのことでした。あとはJetpack ComposeのライブラリもMultiplatformに対応していて、ViewModelやRoomなどのライブラリもKotlin Multiplatformで使えるようになったとのことです。\nKotlin Multiplatform K2コンパイラーの導入により、Kotlinのコードを直接Swiftのコードに変換することができるようになったとのことです。これにより、iOSアプリの開発においてもKotlin Multiplatformを使って開発することができるようになりました。\nそしてFleetで、iOSアプリも開発できるという紹介もありました。FleetだとCompose MultiplatformによるiOSとAndroidのアプリ開発が同時に可能で、リファクタからデバッグまで一貫して開発できるとのことです。AppCodeのサポートが2023年に終了となっていたので、これは嬉しいニュースですね。\nまた、新しいビルドツールであるAmperも紹介されました。まだ発表されてから間もないのですが、すでにJetBrainsのIDEでサポートされていて、YAMLファイルだけでビルド設定を行うことができるので、新しいプロジェクトで使ってみるのもいいかもしれません。\nCompose Multiplatformにおいても、以下の新しい機能が追加されたらしいです。どれも期待していた機能だったので、嬉しいですね。個人的にはデスクトップアプリでファイルの選択ダイアログを実装した時に、対応する機能がなくJavaのAWTを使わざるを得なかったので、このようなAPIもあるといいなと思っています。\nUpcoming 次に、Kotlin 2.1のベータ版から導入予定の機能について紹介や、新しいライブラリ、AIモデルの発表がありました。以下の画像の通りです。\nGuard whenの分岐で、変数の重複を防ぐための機能です。既存のコードなら、以下のようなコードでstatusが重複する場合があっても、どうしようもなかったですね。\nfun render(status: Status): String = when { status == Status.Loading -\u0026gt; \u0026#34;Loading\u0026#34; status is Status.OK \u0026amp;\u0026amp; status.data.isEmpty() -\u0026gt; \u0026#34;No data\u0026#34; status is Status.OK -\u0026gt; status.data.joinToString() status is Status.Error \u0026amp;\u0026amp; status.isCritical -\u0026gt; \u0026#34;Critical problem\u0026#34; else -\u0026gt; \u0026#34;Unknown problem\u0026#34; } なぜなら、以下のようにコードを変えた場合にandでコンパイルエラーが発生してしまうからです。\nfun render(status: Status): String = when (status) { Status.Loading -\u0026gt; \u0026#34;Loading\u0026#34; is Status.OK \u0026amp;\u0026amp; status.data.isEmpty() -\u0026gt; \u0026#34;No data\u0026#34; // Error: expecting \u0026#39;-\u0026gt;\u0026#39; is Status.OK -\u0026gt; status.data.joinToString() is Status.Error \u0026amp;\u0026amp; status.isCritical -\u0026gt; \u0026#34;Critical problem\u0026#34; // Error: expecting \u0026#39;-\u0026gt;\u0026#39; else -\u0026gt; \u0026#34;Unknown problem\u0026#34; } これを改善するため、以下のようにGuarded Conditionを導入する予定だそうです。\n$-escaping problem Kotlinで$は、文字列の中で変数を埋め込むために使われています。ということは、$を文字列として使いたい場合にエスケープの問題が発生するとのことでもあります。特にMulti-line Stringがそうですね。例えば、以下の例があります。\nval jsonSchema: String = \u0026#34;\u0026#34;\u0026#34; \u0026#34;$schema\u0026#34;: \u0026#34;https://json-schema.org/draft/2020-12/schema\u0026#34;, \u0026#34;$id\u0026#34;: \u0026#34;https://example.com/product.schema.json\u0026#34;, \u0026#34;$dynamicAnchor\u0026#34;: \u0026#34;meta\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Product\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }\u0026#34;\u0026#34;\u0026#34; ここでschemaやidなどは変数ではなく、文字列として使いたい場合がありますが、Multi-line Stringの場合にはエスケープができません。なので、以下のようなコードになるケースもありました。\nval jsonSchema: String = \u0026#34;\u0026#34;\u0026#34; \u0026#34;${\u0026#39;$\u0026#39;}schema\u0026#34;: \u0026#34;https://json-schema.org/draft/2020-12/schema\u0026#34;, \u0026#34;${\u0026#39;$\u0026#39;}id\u0026#34;: \u0026#34;https://example.com/product.schema.json\u0026#34;, \u0026#34;${\u0026#39;$\u0026#39;}dynamicAnchor\u0026#34;: \u0026#34;meta\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Product\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }\u0026#34;\u0026#34;\u0026#34; これを解決するために、文字列リテラルで$を2回入れることでinterpolationできるようにする、という機能が導入される予定だそうです。\nNon-local break/continue 今までは、コンパイラーがLambdaが実行される場所を特定できないため、breakやcontinueが使えなかったです。なので、以下のようなコードはエラーになってしまいます。\nfor (i in 0..n) { val date = state[i]?.let { when (it) { -1 -\u0026gt; break // Error: \u0026#39;break\u0026#39; is not allowed here 0 -\u0026gt; continue // Error: \u0026#39;continue\u0026#39; is not allowed here else -\u0026gt; process(it) } } } これもまた、Kotlin 2.1からはletのようなinline関数を正しく解析できるようになるため、breakやcontinueが使えるようになるとのことです。\nContexts 去年発表されたContextについても発表がありました。これはすでにプレビューとして導入されていて、Kotlin 2.2からはベータ版として提供される予定だそうです。これでDIと似たようなことをしたり、セッションやトランザクションなど色々な関数で使い回す必要があるものは、関数の引数に渡すことなく、Contextを使って共有することができるようになります。\nCore Libraries Kotlinのコアライブラリも改善される予定だそうです。すでに発表されているkotlinx.ioのようなものだけでなく、kotlinx.rpcのような新しいライブラリの発表もありました。これらのコアライブラリは、Multiplatformでの開発をサポートするために提供されるもので、どのプラットフォームでも使えるものです。\nAWS SDK for Kotlin Kotlin用のAWS SDKも提供される予定という発表もありました。今まではJavaのSDKを使うことが多かったのですが、CoroutineやNull SafetyのようなKotlinの特徴を活かせつつ、Multiplatformで使えるSDKが提供されるとのことです。\nKotlin Language Model Fleetではすでに利用可能で、Intellijでは2024.2から利用可能となるKotlinの言語モデルが開発中のことです。既存のいくつかのモデルと比べ、Kotlinに特化されたからか、比較的パラメータの数が少ないことに比べ、ベンチマークでは高い精度を示しているとのことです。ただ、比較で使っているLlamaの場合はすでにバージョン3がリリースされているので、最新のモデルと比べる場合どの程度の精度があるのかは気になるところです。\n最後に KotlinConf'24のキーノートでの発表内容を以上でまとめてみました。他にも多くのセッションがあ理、Kotlin 2.0もChangelogを見ると、多くの変更があるので、また色々とチェックしていきたいところですね。\nComposeの発展も良かったのですが、Flutterのような他のフレームワークもあるのでこれからどれだけのシェアを取れるかが楽しみです。言語としても他の言語の発展も早いので、Kotlinも引けを取らないように頑張ってほしいですね。\nでは、また！\n","date":"2024-05-25T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-conf-2024/","title":"KotlinConf'24を要約してみた"},{"content":"この度は、動的画像リサイズのAPIを作りましたのでその紹介です。ここでおいう動的画像リサイズAPIとは、元の画像のURLとリサイズしたいサイズを指定すると、そのサイズにリサイズした画像を返すAPIです。\n目的 そもそも動的画像リサイズAPIを作る理由はなにか。それから説明しないとですね。今までは画像の配信において、エンドユーザが画像をアップロードする場合、あらかじめサムネイルの画像も作成してアップロードするようにしていました。\nただ、その方法だと全ての画面比と解像度に対応したサムネイルを作れないという問題があります。そのため代案として、フロントエンドから画面に最適化したサイズを指定して、APIからリサイズした画像を取得するようにしたいというのが目的です。\n設計・技術選定 APIはマイクロサービスとして、なるべくシンプルに作ることにしました。Getのエンドポイントを一つ持っていて、そこにクエリパラメータで元の画像のURLとリサイズしたいサイズを指定すると、リサイズした画像を返すというものです。フロントエンドではそのままimgタグに使えるようにしたいので、返すのは画像のデータそのまま（ヘッダーにはContent-typeを指定）にします。また、リサイズだけでなく、画像の形式を変換することもできるようにします。\nまた、APIはCloud Run上で動かすことにしました。既存のAPIもそうなので使い方を合わせるためでもあり、ローカルでの開発でもdocekr composeを使って楽に開発できるためです。他にもコンテンツの配信にはCloud Storageを使っているので、それとの連携もしやすいためです。そして最終的に処理された画像は、Cloud CDNに保存され、2回目以降の呼び出しではキャッシュを返すようにします。\n会社ではすでにウェブフレームワークとしてKtorを採用しているので、それと揃えるためにこちらもKtorを採用。Ktorの以前ににNode.jsとRustによる実装を試みたことがありますが、前者は性能の問題から、後者はメンテが難しくなる問題から（社内にRustができる人が少ないので）採用しない方になっています。当時作成していたRustバージョンに近いサンプルコードはGitHubにて公開していますので、こちらから確認できます。\n画像の変換とリサイズのためにはScrimageを採用することにしました。他の候補としてはJavaのImageIOなども検討しました。ただ、リサイズ対象の画像のフォーマットと、返すデータのフォーマットとしてWebPを処理する必要があったのですがそれに対応していないものが多かったです。\n他に考えたものとしては、GraphicsMagickのように画像の変換やリサイズを行うツールを使う方法もあります。こちらの場合はJavaから取得したデータを一度ファイルに書き出して、それをコマンドラインで実行するという方法になりますので、その分のI/Oコストがかかるため今回はScrimageを使うことにしました。\nリサイズ処理 では、実際のAPIを書いていきます。Ktorは使い慣れているのもあり、今回はKtorでのAPI構築というよりはScrimageを使った画像のリサイズ処理が重要なので、その部分に焦点を当てていきます。\nAPI全体で処理のフローは大まかに以下の通りです。\nクエリパラメータからurlとリサイズ後の大きさを取得 画像の取得 画像のリサイズ 画像の形式変換 ここでScrimageを使った画像の処理は3〜4の部分ですが、実際のリサイズを行う前に取得した画像の形式をまず判定したり、画像のサイズを確認する必要もあります。理由としては処理の効率化のためですね。\n今回はPNG, JPEG, WEBP, GIFの4つの形式に対して、リサイズ後にWEBPに変換するという処理を行うことにしています。ここで元の画像がWEBPだった場合、あえてWEBPに変換する必要はないです。また、リサイズが必要ない場合もあります。そのため、まずは画像の形式を判定して、リサイズが必要な場合のみリサイズ処理を行うようにします。\n画像の形式判定 URLもしくはローカルストレージ（今回はCloud RunにCloud Storageをマウントする形で使っている）から画像を取得する際、その画像の形式を判定する必要があります。Scrimageでは画像の形式を判定するためのFormatDetectorというクラスを提供しています。\n使い方は簡単で、以下のように読み込んだ画像のデータをByteの配列で渡すだけです。API上では想定してないフォーマットが来た場合はエラーを返すようにしていて、ここで返すFormatはPNG, GIF, JPEG, WEBPでありScrimageのものをそのまま使っています。\nfun detectImageFormat(data: ByteArray): Format { return FormatDetector.detect(data).orElseThrow { IllegalArgumentException(\u0026#34;Unsupported format\u0026#34;) } } PNG, JPEGの処理 まず一番簡単なPNG, JPEGの場合です。これらの形式の場合、ScrimageのImmutableImageとして扱うことになります。\nここで画像のデータをImmutableImageに変換するにはImageReaderのインタフェースを実装したクラスを使います。公式サイトではImmutableImage.loader()で形式に関係なく画像を読み込むことができると書いてありますが、実際にAPIをビルドする際はAWT関連のエラーが出るので、形式に応じて読み込むクラスを変える必要があります。\nfun asImmutableImage(rawData: ByteArray): ImmutableImage { return ImageIOReader().read(rawData) } ImmutableImageに変換したら、リサイズ処理を行います。Scrimageではリサイズのためのメソッドが用意されているので、それを使ってリサイズを行います。\nここで注意すべきは、resize()やresizeTo()のようなメソッドがあるのですが、前者の場合はパーセントでのリサイズ、後者の場合は指定したサイズにリサイズするという違いがあります。これらの場合、元の画像のアスペクト比が保持されないため、scaleTo()やscaleToWidth()などのメソッドを使う必要があります。\n今回はwidthのみを指定してアスペクト比を保持したままリサイズするため、scaleToWidth()を使います。\nfun resizeImmutableImage(image: ImmutableImage, width: Int): ImmutableImage { return image.scaleToWidth(width) } 最後に、リサイズした結果をByteArrayとして返すためのメソッドを用意します。どの形式に変換するかによってImageWriterを実装するクラスを選ぶ必要があります。今回はWEBPにしたいので、WebpWriterを使います。\nfun encodeImage(image: ImmutableImage): ByteArray { return image.bytes(WebpWriter.DEFAULT) } 最後にRouterでは、Content-Typeを指定してByteArrayを返すようにします。\ncall.respondBytes( bytes = resizedImage, contentType = ContentType(\u0026#34;image\u0026#34;, \u0026#34;webp\u0026#34;) ) WEBPの場合 WEBPの場合は、上記データ読み込む時点でWebpImageReaderを使う必要があります。その後のリサイズ処理はPNG, JPEGの場合と同じです。\nただ、形式がWEBPの場合にはscrimage-webpが提供しているWebpImageReaderを使う必要があります。なので、形式に応じて読み込むクラスを変える必要があります。先ほどのasImmutableImageの引数に形式を追加して、形式に応じて読み込むクラスを変えるようにします。\nfun asImmutableImage(rawData: ByteArray, format: Format): ImmutableImage { return when (format) { Format.WEBP -\u0026gt; WebpImageReader().read(rawData) Format.GIF, Format.PNG, Format.JPEG -\u0026gt; ImageIOReader().read(rawData) } } 他の処理はPNG, JPEGの場合と同じです。\nGIFの場合 GIFの場合は、AnimatedGifというクラスを使ってリサイズを行います。ImmutableImageと同じようにリサイズメソッドが用意されているので、それを使ってリサイズを行います。処理で使われるクラスが違うので、GIFの場合は別途メソッドを用意します。\nfun asAnimatedGif(rawData: ByteArray): AnimatedGif { return AnimatedGifReader.read(ImageSource.of(rawData)) } また、AnimatedGifの場合はframesというプロパティで各フレームのデータを保持していて、これらはImmutableImageとして扱うことができます。そのため、リサイズ処理は各フレームに対して行い、それをAnimatedGifに戻すという処理を行います。若干複雑ですが、以下のように書くことができます。\nsuspend fun resizeAnimatedGif(gif: AnimatedGif, width: Int): AnimatedGif { val resizedData = ByteArrayOutputStream().use { StreamingGifWriter().prepareStream(it, BufferedImage.TYPE_INT_ARGB).use { stream -\u0026gt; gif.frames.mapIndexed { index, image -\u0026gt; stream.writeFrame(image.scaleToWidth(width), gif.getDelay(index)) } it.toByteArray() } return AnimatedGifReader.read(ImageSource.of(resizedData)) } 最後に、リサイズした結果をByteArrayとして返すためのメソッドを用意します。基本的にはImmutableImageと同じですが、GIFをWEBPに変換する場合はGif2WebpWriterを使います。これでWEBPに変換後も、GIFのアニメーションが保持されたままリサイズすることができます。\nfun encodeGif(gif: AnimatedGif): ByteArray { return gif.bytes(Gif2WebpWriter.DEFAULT) } 処理を共通化する ここまででPNG, JPEG, WEBP, GIFの4つの形式に対してリサイズ処理を行うことができました。ただ、それぞれの形式に対して処理を書いていると、処理が重複してしまうため、共通化する必要があります。特にImmutableImageとAnimatedGifの処理は似ているため、それらを共通化することにします。\n共通Interfaceを作る ScrimageではImmutableImageとAnimatedGifは別のクラスであるだけでなく、共通のInterfaceを持っていないため、まずはそれを作成する必要があります。ここでは、ImageというInterfaceを作成し、それを実装するクラスを作成します。それぞれのクラスはWrapperとして作成し、それぞれのクラスのプロパティをInterfaceのプロパティとして持つようにします。\nsealed interface Image { val width: Int } class AnimatedGifWrapper( val animatedGif: AnimatedGif ) : Image { override val width: Int get() = animatedGif.frames.first().width } class ImmutableImageWrapper( val immutableImage: ImmutableImage ) : Image { override val width: Int get() = immutableImage.width } 画像取得の共通化 あとは外部に公開するAPIとして、asImageというメソッドを作成し、それぞれの形式に応じてWrapperを返すようにします。ここで、形式の判定は先ほど作成したdetectImageFormatを使います。\nfun asImage(rawData: ByteArray): Image { return when (val format = detectImageFormat(rawData)) { Format.GIF -\u0026gt; asAnimatedGifWrapper(rawData) Format.WEBP, Format.PNG, Format.JPEG -\u0026gt; asImmutableImageWrapper(rawData, format) } } private fun asAnimatedGifWrapper(rawData: ByteArray): AnimatedGifWrapper { val gif = AnimatedGifReader.read(ImageSource.of(rawData)) return AnimatedGifWrapper(gif) } private fun asImmutableImageWrapper(rawData: ByteArray, format: Format): ImmutableImageWrapper { val image = when (format) { Format.WEBP -\u0026gt; WebpImageReader().read(rawData) Format.GIF, Format.PNG, Format.JPEG -\u0026gt; ImageIOReader().read(rawData) } return ImmutableImageWrapper(image) } リサイズ処理の共通化 同じく、リサイズ処理も共通化します。ここでは、resizeImageというメソッドを作成し、それぞれの形式に応じてリサイズ処理を行うようにします。ここで、リサイズ処理は先ほど作成したresizeImmutableImageとresizeAnimatedGifを使います。AnimatedGifのリサイズ処理はまた別途writeAnimatedGifというメソッドを作成して分けています。\nここでImageはsealed interfaceとして作成しているため、分岐処理はwhen式を使って網羅することができます。\nsuspend fun resizeImage(image: Image, width: Int): Image { return when (image) { is AnimatedGifWrapper -\u0026gt; resizeAnimatedGif(image, width) is ImmutableImageWrapper -\u0026gt; resizeImmutableImage(image, width) } } private suspend fun resizeAnimatedGif(gifWrapper: AnimatedGifWrapper, width: Int): AnimatedGifWrapper { val gif = gifWrapper.animatedGif val resizedData = writeAnimatedGif { stream -\u0026gt; gif.frames.mapIndexed { index, image -\u0026gt; stream.writeFrame(image.scaleToWidth(width), gif.getDelay(index)) } } val resizedGif = AnimatedGifReader.read(ImageSource.of(resizedData)) return AnimatedGifWrapper(resizedGif, resizedData) } private suspend fun writeAnimatedGif(writeFunction: (StreamingGifWriter.GifStream) -\u0026gt; Unit): ByteArray { return withContext(Dispatchers.IO) { StreamingGifWriter().use { streamingGifWriter.prepareStream(it, BufferedImage.TYPE_INT_ARGB).use { stream -\u0026gt; writeFunction(stream) } it.toByteArray() } } } private fun resizeImmutableImage(imageWrapper: ImmutableImageWrapper, width: Int): ImmutableImageWrapper { val resizedImage = imageWrapper.immutableImage.scaleToWidth(width) return ImmutableImageWrapper(resizedImage, imageWrapper.format) } 画像のエンコード処理の共通化 最後に、画像のエンコード処理も共通化します。ここでは、encodeImageというメソッドを作成し、それぞれの形式に応じてエンコード処理を行うようにします。\nsuspend fun encodeImage(image: Image): ByteArray { return when (image) { is AnimatedGifWrapper -\u0026gt; image.animatedGif.bytes(Gif2WebpWriter.DEFAULT) is ImmutableImageWrapper -\u0026gt; image.immutableImage.bytes(WebpWriter.DEFAULT) } } これで共通化も終わり、呼び出す側はasImage, resizeImage, encodeImageの3つのメソッドを使うだけでリサイズ処理を行うことができます。サイズのチェックもImageの方でプロパティ化しているため、それを使ってリサイズが必要かどうかの判定も行うことができます。\nあとはImageWriterなど、毎回インスタンスを生成する必要のないクラスは、シングルトンとして作成しておくとよいでしょう。\n最後に だいぶ長くなってしまいましたが、ここまでで動的画像リサイズAPIの作成方法を紹介しました。Scrimageを使うことで、PNG, JPEG, WEBP, GIFの4つの形式に対してリサイズ処理を行うことができ、共通化することで処理の重複を防ぐことができました。\nあとは、Cloud Run上で動かすためのDockerfileを作成し、Cloud Storageとの連携を行うことで、画像の配信を行うことができます。また、Cloud CDNを使うことで、2回目以降の呼び出しではキャッシュを返すようにすることで、負荷を軽減することができます。なかなか面白いプロジェクトでした。\nでは、また！\n","date":"2024-03-30T00:00:00Z","image":"https://retheviper.github.io/images/ktor.jpg","permalink":"https://retheviper.github.io/posts/image-resize-api/","title":"動的画像リサイズAPIを作る"},{"content":"CI/CDは一度インフラとアプリケーションを構築すれば、あまり変わることはありません。極端な話、期待通り動いてくれれば問題はないといえることです。しかし、場合によってはCI/CDのフローを見直すことも必要になります。アプリケーションの機能追加やインフラの変更などにはいつ変化があるかわからないし、緊急対応が必要な場合は迅速なデプロイが必要とされる場合もあるからです。そのため、CI/CDの高速化が非常に重要な課題となるケースもあります。\n実は去年転職をしましたが、転職先の会社ではCI/CDの高速化が課題となっていました。前職では2週間に一回というリリースのサイクルがあったのですが、今回は一日でも数回のリリースがあるため何よりもそのため、CI/CDの高速化を行うために、いくつかの方法を試してみました。今回はその中で、最も効果があった方法を紹介します。\nインフラ構成 基本的には、以下のような構成になっています。\nGitHubにソースコードおよびCI/CDの構成ファイルを配置 Cloud Buildを利用してDockerコンテナをビルド Cloud Runにデプロイ この構成はFrontend, Backendともに同じで、データベースマイグレーションも同じ環境となっています。マイグレーションの時にはCloud RunでFlywayを実行してAlloyDBにマイグレーションを行っているようになっています。\nキャッシュで高速化 CI/CDの高速化にはいくつかの方法がありますが、今回はキャッシュを利用することで高速化を行いました。\n高速化のために考えられるのはいくつかの方法がありますが、まずは簡単な方法としてキャッシュを利用することにしました。今回利用したキャッシュは二つで、一つはGradleのキャッシュ、もう一つはDockerイメージのキャッシュです。\nGradleのキャッシュ まずGradleにはBuild CacheとConfiguration Cacheというものがあり、これらを有効にした場合、次回のビルドからはそれぞれ前回のビルドの成果物とビルド時の設定をキャッシュとして使うことになるのでビルドの時間をかなり短縮することができます。\nこれらのキャッシュを有効にするには、gradle.propertiesファイルに以下のように設定を行います。\n// build cache org.gradle.caching=true // configuration cache org.gradle.configuration-cache=true Dockerイメージのキャッシュ Gradleのキャッシュを有効にしても、CI/CDの環境で実際ビルドが走る時に利用できるキャッシュが存在しなければなりません。そのために利用したのがDockerのキャッシュです。Cloud Buildのフローが実行されるたびに前回のビルドの成果物をどこかにプッシュしておき、ビルドが走る時にそれを利用するとGradleのキャッシュを利用することができるようになります。\nこれを実現するためにはまず最初にCloud buildの構成ファイルは以前は以下のようになっていました。(一部抜粋)\nsteps: - name: gcr.io/cloud-builders/docker args: - build - \u0026#39;-t\u0026#39; - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA\u0026#39; - api - \u0026#39;-f\u0026#39; - api/Dockerfile id: Build - name: gcr.io/cloud-builders/docker args: - push - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA\u0026#39; id: Push - name: \u0026#39;gcr.io/google.com/cloudsdktool/cloud-sdk:slim\u0026#39; args: - run - services - update - $_SERVICE_NAME - \u0026#39;--platform=managed\u0026#39; - \u0026#39;--image=$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:COMMIT_SHA\u0026#39; - \u0026gt;- --labels=managed-by=gcp-cloud-build-deploy-cloud-run,commit-sha=$COMMIT_SHA,gcb-build-id=$BUILD_ID,gcb-trigger-id=$_TRIGGER_ID,$_LABELS - \u0026#39;--region=$_DEPLOY_REGION\u0026#39; - \u0026#39;--quiet\u0026#39; id: Deploy entrypoint: gcloud images: - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:COMMIT_SHA\u0026#39; 上記のファイルでは、最初にコミットのハッシュでDockerイメージをビルドし、それをプッシュしています。その後、Cloud Runにデプロイするために、そのイメージを利用しています。この時、イメージのタグにはコミットのハッシュを利用しています。\nここでキャッシュを挟むようにすると、以下のようになります。\nsteps: - name: gcr.io/cloud-builders/docker args: - \u0026#39;-c\u0026#39; - \u0026gt;- docker pull $_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:latest || exit 0 id: Pull entrypoint: bash - name: gcr.io/cloud-builders/docker args: - build - \u0026#39;--cache-from\u0026#39; - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:latest\u0026#39; - \u0026#39;-t\u0026#39; - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA\u0026#39; - \u0026#39;-t\u0026#39; - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:latest\u0026#39; - api - \u0026#39;-f\u0026#39; - api/Dockerfile id: Build - name: gcr.io/cloud-builders/docker args: - push - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA\u0026#39; id: Push (Cache) - name: gcr.io/cloud-builders/docker args: - push - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:latest\u0026#39; id: Push (Latest) - name: \u0026#39;gcr.io/google.com/cloudsdktool/cloud-sdk:slim\u0026#39; args: - run - services - update - $_SERVICE_NAME - \u0026#39;--platform=managed\u0026#39; - \u0026#39;--image=$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:latest\u0026#39; - \u0026gt;- --labels=managed-by=gcp-cloud-build-deploy-cloud-run,commit-sha=$COMMIT_SHA,gcb-build-id=$BUILD_ID,gcb-trigger-id=$_TRIGGER_ID,$_LABELS - \u0026#39;--region=$_DEPLOY_REGION\u0026#39; - \u0026#39;--quiet\u0026#39; id: Deploy entrypoint: gcloud images: - \u0026#39;$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:latest\u0026#39; 上記のファイルでは、まず最初に前回のビルドの成果物をプルしています。これにより、前回のビルドの成果物を利用することができます。その後、ビルドでは--cache-fromオプションを利用して、前回のビルドの成果物をキャッシュとして利用しています。また、イメージのタグにはlatestとコミットのハッシュを利用しています。そして最後にContainer Registryにlatestとコミット8種の両方をプッシュします。こうすると、Container Registryでは一つのイメージに対して二つのタグが付与され、次回のビルドでもlatestを利用できるようになります。\nDockerfileの修正 Dockerイメージをビルドする際にDockerfileを使っていましたが、キャッシュをより効率的に利用するためにDockerfileを修正しました。修正前のDockerfileは以下のようになっていました。(一部抜粋)\nFROM eclipse-temurin:17 RUN mkdir /api WORKDIR /api RUN ./gradlew build -x test -x compileTestKotlin 上記のDockerfileでは、まずGradleのビルドを行っていて、成果物はGradleの設定によりjarファイルとしてビルドされます。そしてCloud RunではリポジトリないのEntrypointを実行するように設定しているため、jarファイルを実行するようになっています。\n一般的にはjarファイルだけを残すとコンテナのサイズを小さくすることができますが、今回はキャッシュを利用するためにjarファイル以外の成果物も残す必要があるので、ここは修正していません。ただ、Gradleのビルドが走る時に、毎回依存関係の解決を行うのは非常に時間がかかるので、依存関係の解決を行う部分だけをキャッシュするようにしました。結果は以下のようになります。\nFROM eclipse-temurin:17 WORKDIR /api # dependency cache COPY build.gradle.kts settings.gradle.kts gradlew gradle.properties /api/ COPY gradle /api/gradle COPY docker /api/docker COPY detekt /api/detekt RUN ./gradlew build -x test -x compileTestKotlin -x detekt || return 0 # build app COPY src /api/src COPY resources /api/resources RUN ./gradlew build -x test -x compileTestKotlin デプロイではほとんどの場合、ソースコードの変更が行われます。なので、あまり変わることのないGradle関係のファイルだけを先にコピーして、ソースコードのない状態でビルドを行います。これにより、実際のアプリはビルドされずに依存関係の解決だけが行われます。それからソースコードをコピーしてビルドを行います。\nなぜこのような構造にしたかというと、Dockerイメージのビルド時にはキャッシュが存在していても、コピーしたファイルに変更があった場合にキャッシュが無効になるためです。依存関係の更新があった場合は仕方なくキャッシュは効かなくなりますが、ソースコードの変更がない場合は依存関係の解決まではキャッシュが有効になるため、ビルドの時間を短縮することができます。\nここでは主にBackendの設定を紹介しましたが、Frontendの設定もDockerfileを同じように修正することでキャッシュを利用できるようにしています。backendと多少は設定が違いますが、基本的にはyarnもGradleと仕組みとしては同じく先に依存関係の解決を行うようにするだけです。\nFROM node:18.17.1-slim AS base WORKDIR /app # dependency cache COPY package.json yarn.lock ./ RUN yarn --frozen-lockfile COPY ./tsconfig.json ./ COPY ./packages/app/package.json ./packages/app/package.json RUN cd ./packages/app \u0026amp;\u0026amp; yarn || return 0 # build app FROM base AS builder WORKDIR /app COPY ./packages/app/ ./packages/app/ COPY --from=base /app/packages/app ./packages/app RUN yarn build:app:production データベースマイグレーションの場合、Cloud Buildで直接DBに接続してマイグレーションを行うのが難しかったのと、Backendと同様の設定をすることでDBアクセスは簡単にできたので基本的にはBackendと同じくDockerイメージを作成し、Cloud Run上でEntrypointのスクリプトを利用しFlywayを実行するようにしていました。\nただ、ここでBackendと同じDockerfileを使って同じくアプリをビルドしたあと、実際のアプリをCloud Runで実行するようになっていましたが、ここは必要なかったので（Cloud Runでは指定したポートからコンテナが応答するようにすればいいだけなので）アプリのビルドは行わなく、依存関係の解決だけを行うようにしました。ポートからの応答に関してはnginxを起動して応答するようにしています。\n最後に 今回はCI/CDの高速化について紹介しました。キャッシュを利用することで、ビルドの時間が大幅に短縮できて、既存だと14分ほどかかっていたのが今は6分ほどになっているので、一日中でも数回のリリースが行われる現状はかなり大きい成果だと言えます。ただ、データベースのマイグレーションではCloud Runを使わないようにするなど依然として改善の余地はありますね。\nでは、また！\n","date":"2024-01-21T00:00:00Z","image":"https://retheviper.github.io/images/gcp.jpg","permalink":"https://retheviper.github.io/posts/application-build-cache/","title":"Cloud BuildのCI/CDを高速化してみた"},{"content":"Java 17がリリースされてから約2年、今月にはJava 21がリリースされます。まだ案件によっては1.7など古いバージョンを使っている場合も多いと思いますが、21は新しいLTSなので、今後新しいプロジェクトを始めるときは採用を検討するのも良いかもしれません。そこで今回はJava 21で何が変わったのかをざっくりとまとめてみました。\n今回の記事はJava 17からの変化について述べているので、Java 11から17までの変化については前回のポストを参照してください。\n言語スペック String Templates (Preview) Kotlinのような言語にはいわゆるString Interpolationという機能があります。これは文字列の中に変数を埋め込むことができる機能ですね。例えばxとyという変数があったとして、それをStringに埋め込むときにKotlinだと以下のように書くことができます。\nval s = \u0026#34;$x plus $y equals ${x + y}\u0026#34; これをJavaで実現するためには以下のように書くことになります。\n// String concatenation String s = x + \u0026#34; plus \u0026#34; + y + \u0026#34; equals \u0026#34; + (x + y); // StringBuilder String s = new StringBuilder() .append(x) .append(\u0026#34; plus \u0026#34;) .append(y) .append(\u0026#34; equals \u0026#34;) .append(x + y) .toString(); // String.format String s = String.format(\u0026#34;%2$d plus %1$d equals %3$d\u0026#34;, x, y, x + y); String t = \u0026#34;%2$d plus %1$d equals %3$d\u0026#34;.formatted(x, y, x + y); // MessageFormat MessageFormat mf = new MessageFormat(\u0026#34;{0} plus {1} equals {2}\u0026#34;); String s = mf.format(x, y, x + y); これらの方法はどれも冗長で、可読性が低いです。そこでJava 21ではString Templatesという機能が追加されました。これは文字列の中に変数を埋め込むことができる機能です。なので、Javaでももっと簡単な方法でStringを作成することができるようになりました。\nただ、String InterpolationにはSQL Injectionのような問題があるので、Javaでは別のアプローチを取りました。これは文字列の中に変数を埋め込むのではなく、文字列の中に変数を埋め込むためのテンプレートをまず作成して、それを使って文字列を作成するという方法になっています。なのでコード以下のようになります。\n// STRを使う場合 String name = \u0026#34;Joan\u0026#34;; String info = STR.\u0026#34;My name is \\{name}\u0026#34;; // My name is Joan // RAWを使う場合 String name = \u0026#34;Joan\u0026#34;; StringTemplate st = RAW.\u0026#34;My name is \\{name}\u0026#34;; String info = STR.process(st); // My name is Joan STRやRAWはStringTemplateのインスタンスをまず作るようになっていますが、このStringTemplateのインスタンスにはfragmentsというフィールドとvaluesという配列があります。fragmentsは文字列の中に変数があるところを空文字列に置き換えたものの配列で、valuesは変数の値の配列です。なので、変数を埋め込んだ結果の文字列だけでなく実際与えられた変数の値も取得することができます。\nint x = 10, y = 20; StringTemplate st = RAW.\u0026#34;\\{x} plus \\{y} equals \\{x + y}\u0026#34;; String s = st.toString(); // StringTemplate{ fragments = [ \u0026#34;\u0026#34;, \u0026#34; plus \u0026#34;, \u0026#34; equals \u0026#34;, \u0026#34;\u0026#34; ], values = [10, 20, 30] } また、StringTemplateにはProcessorというInterfaceがあり、Functional Interfaceとして独自の実装をすることも可能です。\n// Processor Interface public interface StringTemplate { @FunctionalInterface public interface Processor\u0026lt;R, E extends Throwable\u0026gt; { R process(StringTemplate st) throws E; } } var INTER = StringTemplate.Processor.of(StringTemplate::interpolate); String s = INTER.\u0026#34;\\{x} plus \\{y} equals \\{x + y}\u0026#34;; まだPreviewなので、このような使い方は今後変わる可能性がありますが、かなり面白いアプローチなので今後の動向に注目したい機能でした。\nSequenced Collections JavaのCollectionの場合、種類によって最後の要素を取るためには色々書き方が変わったり、冗長になったりしますね。例えば、最初の要素と最後の要素を取る場合Collecitonの種類によって以下のようになります。\n// List var firstOnList = list.get(0); var lastOnList = list.get(list.size() - 1); // Deque var firstOnDeque = deque.getFirst(); var lastOnDeque = deque.getLast(); // SortedSet var firstOnSortedSet = sortedSet.first(); var lastOnSortedSet = sortedSet.last(); // LinkedHashSet var firstOnLinkedHashSet = linkedHashSet.iterator().next(); var lastOnLinkedHashSet = linkedHashSet.stream().reduce((first, second) -\u0026gt; second).orElse(null); また、ループを逆順にする場合もコードは冗長になったり、使い勝手が悪く感じられる場合もあります。例えば以下のコードを見ると、やろうとしていることは一緒なのに、コードが全然違うということがわかります。\n// NavigableSet with descendingSet for (var e: navigableSet.descendingSet()) { process(e); } // Deque with reverse Iterator for (var it = deque.descendingIterator(); it.hasNext(); ) { var e = it.next(); process(e); } // List with reverse ListIterator for (var it = list.listIterator(list.size()); it.hasPrevious(); ) { var e = it.previous(); process(e); } また実装クラスによっては要素の順番が保持されるCollectionからそうでないものにダウングレードされるケースもあります。例えばLinkedHashSetをCollections::unmodifiableSetでラップすると、LinkedHashSetの順番が失われることになります。\nそこでJava 21ではSequencedCollectionおよびSequencedSetというInterfaceを追加して、上記の問題を解決します。これらInterfaceは以下のようなメソッドを提供します。\ninterface SequencedCollection\u0026lt;E\u0026gt; extends Collection\u0026lt;E\u0026gt; { // new method SequencedCollection\u0026lt;E\u0026gt; reversed(); // methods promoted from Deque void addFirst(E); void addLast(E); E getFirst(); E getLast(); E removeFirst(); E removeLast(); } interface SequencedSet\u0026lt;E\u0026gt; extends Set\u0026lt;E\u0026gt;, SequencedCollection\u0026lt;E\u0026gt; { SequencedSet\u0026lt;E\u0026gt; reversed(); // covariant override } またMapにおいてもSequencedMapというInterfaceが追加されていて、以下のようなメソッドを提供します。\ninterface SequencedMap\u0026lt;K,V\u0026gt; extends Map\u0026lt;K,V\u0026gt; { // new methods SequencedMap\u0026lt;K,V\u0026gt; reversed(); SequencedSet\u0026lt;K\u0026gt; sequencedKeySet(); SequencedCollection\u0026lt;V\u0026gt; sequencedValues(); SequencedSet\u0026lt;Entry\u0026lt;K,V\u0026gt;\u0026gt; sequencedEntrySet(); V putFirst(K, V); V putLast(K, V); // methods promoted from NavigableMap Entry\u0026lt;K, V\u0026gt; firstEntry(); Entry\u0026lt;K, V\u0026gt; lastEntry(); Entry\u0026lt;K, V\u0026gt; pollFirstEntry(); Entry\u0026lt;K, V\u0026gt; pollLastEntry(); } これらの新しいInterfaceが追加されることで、Collection全体の継承関係が以下のように変更されました。\n出典：OpenJDK - JEP 431: Sequenced Collections\n継承関係によってダウンキャストが発生する場合もあるかなと思いますが、Listの場合はSequencedCollectionを継承しているのでそのまま新しいメソッドを使うことができます。\nGenerational ZGC ZGCはJava 11で導入されたGarbage Collectorですが、Java 21ではGenerational ZGCという機能が追加されました。これはZGCの性能を向上させるために、ZGCのヒープをYoung GenerationとOld Generationに分けることになります。これによって、Young GenerationのGCをより頻繁に行うことができるようになり、Young GenerationのGCの時間を短縮することができ、メモリやCPUのオーバーヘッドを減らすことができるらしいです。\nGenerational ZGCを使うには以下のように起動オプションを指定します。\njava -XX:+UseZGC -XX:+ZGenerational ただ、新しいGCに関しては公式のドキュメントを参照すると色々と設計や実装について述べていますが、アプリケーションエンジニアの立場としてはそれを使った場合の実際の性能上の利点がどれくらいあるのかが気になるところですね。なので、実際にGenerational ZGCを使った場合の性能についてはこちらの記事が参考になるかと思います。結論としてはまだParallelGCがもっとも性能が良いということになっています。もちろんこれはマシンスペック（特にメモリ）によっても変わってくると思うので、自分の環境で試してみるのが良いかと思います。\nRecord Patterns Java 16ではPattern Matchingという機能が導入され、instanceOfで型チェックをした後にキャストするというコードを簡潔に書くことができるようになりました。例えば以下のようなものです。\n// Prior to Java 16 if (obj instanceof String) { String s = (String)obj; ... use s ... } // As of Java 16 if (obj instanceof String s) { ... use s ... } Java 21ではこのPattern Matchingを、同じくJava 16で導入されたRecordにも適用することができるようになりました。例えば以下のようなコードです。\n// As of Java 16 record Point(int x, int y) {} static void printSum(Object obj) { if (obj instanceof Point p) { int x = p.x(); int y = p.y(); System.out.println(x+y); } } // As of Java 21 static void printSum(Object obj) { if (obj instanceof Point(int x, int y)) { System.out.println(x+y); } } さらにネストしたRecordにも適用することができます。例えば以下のようなコードも可能です。\nstatic void printXCoordOfUpperLeftPointWithPatterns(Rectangle r) { if (r instanceof Rectangle(ColoredPoint(Point(var x, var y), var c), var lr)) { System.out.println(\u0026#34;Upper-left corner: \u0026#34; + x); } } Pattern Matching for switch Pattern Matchingの改善はswitchにも適用されています。例えば以下のように、switchのcaseにnullを指定することができたり、スマートキャストが使えたり、さらにwhenを使った条件分岐ができるようになりました。\nstatic void testStringEnhanced(String response) { switch (response) { case null -\u0026gt; { } case \u0026#34;y\u0026#34;, \u0026#34;Y\u0026#34; -\u0026gt; { System.out.println(\u0026#34;You got it\u0026#34;); } case \u0026#34;n\u0026#34;, \u0026#34;N\u0026#34; -\u0026gt; { System.out.println(\u0026#34;Shame\u0026#34;); } case String s when s.equalsIgnoreCase(\u0026#34;YES\u0026#34;) -\u0026gt; { System.out.println(\u0026#34;You got it\u0026#34;); } case String s when s.equalsIgnoreCase(\u0026#34;NO\u0026#34;) -\u0026gt; { System.out.println(\u0026#34;Shame\u0026#34;); } case String s -\u0026gt; { System.out.println(\u0026#34;Sorry?\u0026#34;); } } } この改善はEnumにも適用されています。例えば以下のようなコードが可能になりました。\nstatic void exhaustiveSwitchWithBetterEnumSupport(CardClassification c) { switch (c) { case Suit.CLUBS -\u0026gt; { System.out.println(\u0026#34;It\u0026#39;s clubs\u0026#34;); } case Suit.DIAMONDS -\u0026gt; { System.out.println(\u0026#34;It\u0026#39;s diamonds\u0026#34;); } case Suit.HEARTS -\u0026gt; { System.out.println(\u0026#34;It\u0026#39;s hearts\u0026#34;); } case Suit.SPADES -\u0026gt; { System.out.println(\u0026#34;It\u0026#39;s spades\u0026#34;); } case Tarot t -\u0026gt; { System.out.println(\u0026#34;It\u0026#39;s a tarot\u0026#34;); } } } まだprimitive typeには適用されないのですが、これは今後改善する予定らしいので、また次のバージョンで期待したいところです。\nForeign Fuctions and Memory Access API (Third Preview) Java 19から導入された機能で、Java runtime外のコードやデータにアクセスできるようなAPIが追加されます。これはJava 21ではThird Previewとして提供されていて、JavaからCやC++のコードを呼び出すことができるようになります。例えば以下のようなコードでCのライブラリを呼び出すことができます。\n// 1. Find foreign function on the C library path Linker linker = Linker.nativeLinker(); SymbolLookup stdlib = linker.defaultLookup(); MethodHandle radixsort = linker.downcallHandle(stdlib.find(\u0026#34;radixsort\u0026#34;), ...); // 2. Allocate on-heap memory to store four strings String[] javaStrings = { \u0026#34;mouse\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;car\u0026#34; }; // 3. Use try-with-resources to manage the lifetime of off-heap memory try (Arena offHeap = Arena.ofConfined()) { // 4. Allocate a region of off-heap memory to store four pointers MemorySegment pointers = offHeap.allocateArray(ValueLayout.ADDRESS, javaStrings.length); // 5. Copy the strings from on-heap to off-heap for (int i = 0; i \u0026lt; javaStrings.length; i++) { MemorySegment cString = offHeap.allocateUtf8String(javaStrings[i]); pointers.setAtIndex(ValueLayout.ADDRESS, i, cString); } // 6. Sort the off-heap data by calling the foreign function radixsort.invoke(pointers, javaStrings.length, MemorySegment.NULL, \u0026#39;\\0\u0026#39;); // 7. Copy the (reordered) strings from off-heap to on-heap for (int i = 0; i \u0026lt; javaStrings.length; i++) { MemorySegment cString = pointers.getAtIndex(ValueLayout.ADDRESS, i); javaStrings[i] = cString.getUtf8String(0); } } // 8. All off-heap memory is deallocated here assert Arrays.equals(javaStrings, new String[] {\u0026#34;car\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;mouse\u0026#34;}); // true 今はJavaを使って他の言語で作られたライブラリやアプリケーションを参照する場合、WrapperやRuntimeを利用した形が多いかなと思いますが、これを使うことでより簡単に他の言語のライブラリを呼び出すことができ、アプリケーションのサイズを減らしたり、パフォーマンスを向上させることができるようになるかなと思います。ただ、まだPreviewなので今後変わる可能性があるのと、直接メモリにアクセスするのでメモリリークの可能性があるかなと思いますので、使用時には注意が必要かと思います。\nUnnamed Patterns and Variables (Preview) 処理の中で使われてない変数を_で表現することができるようになりました。なので、以下のようなコードが書けるようになります。\n// Loop int acc = 0; for (Order _ : orders) { if (acc \u0026lt; LIMIT) { ... acc++ ... } } // Multiple assignment Queue\u0026lt;Integer\u0026gt; q = ... // x1, y1, z1, x2, y2, z2, ... while (q.size() \u0026gt;= 3) { var x = q.remove(); var _ = q.remove(); var _ = q.remove(); ... new Point(x, 0) ... } // Catch block String s = ... try { int i = Integer.parseInt(s); ... i ... } catch (NumberFormatException _) { System.out.println(\u0026#34;Bad number: \u0026#34; + s); } // try-with-resources try (var _ = ScopedContext.acquire()) { ... no use of acquired resource ... } // Lambda stream.collect(Collectors.toMap(String::toUpperCase, _ -\u0026gt; \u0026#34;NODATA\u0026#34;)) Virtual threads Project Loomという名で長い間開発されていた機能です。個人的な意見ですが、Java 21においてもっとも注目されている機能ではないかと思います。既存のマルチスレッドプログラミングでは生成できるスレッドの数において物理的な制約があったのですが、今回の導入される仮想スレッドはそのOSのスレッドをさらに細かく分けて使うことになるので、より多くのスレッドを同時に扱うことができるのが特徴です。\n使い方としては既存の物理スレッドと大きく変わるわけではないので、以下のようなコードで使うことができます。\ntry (var executor = Executors.newVirtualThreadPerTaskExecutor()) { IntStream.range(0, 10_000).forEach(i -\u0026gt; { executor.submit(() -\u0026gt; { Thread.sleep(Duration.ofSeconds(1)); return i; }); }); } // executor.close() is called implicitly, and waits 仮想スレッドは実際のOSのスレッドに1:1対応しないので、既存のようにThreadPoolを作成してスレッドの数を制限する必要はほとんどありません。公式でもPoolを使うことをおすすめしないと言っているくらいです。\n実際KotlinでJavaの仮想スレッドを利用するようにDispatcherを実装して実験した記事によると、30スレッドのマシンでも100万の 仮想スレッドを作成して処理を行うことができるのがわかります。JVM言語で作成されたサーバーサイドアプリケーションの場合、従来のスレッドモデルではスレッドの数を制限する必要があったので、この仮想スレッドの導入によって、より多くのリクエストを同時に処理することができるようになるかなと思います。\nUnnamed Classes and Instance Main Methods (Preview) 関数をトップレベルに定義することができるようになりました。なので、伝統のHello Worldのサンプルは以下のようなコードが書けるようになります。\n// Prior to Java 21 public class HelloWorld { public static void main(String[] args) { System.out.println(\u0026#34;Hello, World!\u0026#34;); } } // As of Java 21 void main() { System.out.println(\u0026#34;Hello, World!\u0026#34;); } トップレベルの関数やフィールドもUnnamed Classのメンバー扱いとなるので、以下のようなコードも問題なく動きます。\n// Method String greeting() { return \u0026#34;Hello, World!\u0026#34;; } void main() { System.out.println(greeting()); } // Field String greeting = \u0026#34;Hello, World!\u0026#34;; void main() { System.out.println(greeting); } また、main関数を持つUnnamed Classは以下のように実行することができます。\nnew Object() { // the unnamed class\u0026#39;s body }.main(); Scoped Values (Preview) Webアプリケーションの場合、一つのリクエストに対してはスレッドが割り当てられ、一貫したコンテキストの中で実行されるようにするのが一般的です。ただ、そこでコンテキストをオブジェクトとして扱う場合、既存だと実行される関数の引数として渡す必要があります。例えば以下のようなコードです。\n@Override void handle(Request request, Response response, FrameworkContext context) { ... var userInfo = readUserInfo(context); ... } private UserInfo readUserInfo(FrameworkContext context) { return (UserInfo)framework.readKey(\u0026#34;userInfo\u0026#34;, context); } または、ThreadLocalを使用して以下のように書くこともできます。\npublic class Framework { private final Application application; public Framework(Application app) { this.application = app; } private final static ThreadLocal\u0026lt;FrameworkContext\u0026gt; CONTEXT = new ThreadLocal\u0026lt;\u0026gt;(); void serve(Request request, Response response) { var context = createContext(request); CONTEXT.set(context); Application.handle(request, response); } public PersistedObject readKey(String key) { var context = CONTEXT.get(); var db = getDBConnection(context); db.readKey(key); } } しかし、ThreadLocalを使う場合は色々と問題があります。まずThreadLocalの値そのものが変更されるということです。そして不要になったThreadLocalの値を適宜削除する必要があったり、オーバーヘッドが発生するということです。\nそこでJava 21ではScopedValueというクラスが追加されました。これを使うと以下のようにスレッドあたりの値を設定することができます。\nclass Framework { private final static ScopedValue\u0026lt;FrameworkContext\u0026gt; CONTEXT = ScopedValue.newInstance(); void serve(Request request, Response response) { var context = createContext(request); ScopedValue.where(CONTEXT, context) .run(() -\u0026gt; Application.handle(request, response)); } public PersistedObject readKey(String key) { var context = CONTEXT.get(); var db = getDBConnection(context); db.readKey(key); } } ScopedVlaueにはsetterがないですが、だからと言って他の値を与えられないわけではないです。ThreadLocalとは別のアプローチで、特定の値を渡してrun()関数を実行することができるようになっています。\nprivate static final ScopedValue\u0026lt;String\u0026gt; X = ScopedValue.newInstance(); void foo() { ScopedValue.where(X, \u0026#34;hello\u0026#34;).run(() -\u0026gt; bar()); } void bar() { System.out.println(X.get()); // prints hello ScopedValue.where(X, \u0026#34;goodbye\u0026#34;).run(() -\u0026gt; baz()); System.out.println(X.get()); // prints hello } void baz() { System.out.println(X.get()); // prints goodbye } このScopedValueはスレッドの実行中にだけ値が保持されないので、ThreadLocalより安全な使い方ができるようになっています。\nVector API (Sixth Incubator) Java 1.0の時代の配列を扱うVectorとは違って、数値（行列）の計算のためのVector APIが追加されました。基本的に以下のようなことができるようになります。\n// Prior to Java 21 void scalarComputation(float[] a, float[] b, float[] c) { for (int i = 0; i \u0026lt; a.length; i++) { c[i] = (a[i] * a[i] + b[i] * b[i]) * -1.0f; } } // As of Java 21 static final VectorSpecies\u0026lt;Float\u0026gt; SPECIES = FloatVector.SPECIES_PREFERRED; void vectorComputation(float[] a, float[] b, float[] c) { int i = 0; int upperBound = SPECIES.loopBound(a.length); for (; i \u0026lt; upperBound; i += SPECIES.length()) { // FloatVector va, vb, vc; var va = FloatVector.fromArray(SPECIES, a, i); var vb = FloatVector.fromArray(SPECIES, b, i); var vc = va.mul(va) .add(vb.mul(vb)) .neg(); vc.intoArray(c, i); } for (; i \u0026lt; a.length; i++) { c[i] = (a[i] * a[i] + b[i] * b[i]) * -1.0f; } } 一般的なWebアプリケーションではあまり使われることはないかと思いますが、もしこのような計算が必要とされる処理を書く場合、従来のコードよりも高速で並列かもできるということなので、使う場面があるかもしれません。\nDeprecate the Windows 32-bit x86 Port for Removal Windows x86-32のポートがいずれ終わるので、まずはDeprecatedにしてするということです。Virtual Threadが該当のOSだと期待通りの性能向上がなく、32bitに対応する最後のWindowsであるWindows 10が2025年10月にサポート終了となるための対応とされています。\nPrepare to Disallow the Dynamic Loading of Agents Java agentによる動的ロードは、実行中のアプリケーションを変更することも可能です。しかしこのよう機能はアプリケーションの整合性を保証できなくする可能性もあります。そのような問題を防ぐために、将来は動的ロードを禁止し、Java 21ではまず警告を出力します。以下のようなメッセージが出力されることがあります。\nWARNING: A {Java,JVM TI} agent has been loaded dynamically (file:/u/bob/agent.jar) WARNING: If a serviceability tool is in use, please run with -XX:+EnableDynamicAgentLoading to hide this warning WARNING: If a serviceability tool is not in use, please run with -Djdk.instrument.traceUsage for more information WARNING: Dynamic loading of agents will be disallowed by default in a future release このような警告を回避するためにはアプリケーションの実行時に-XX:+EnableDynamicAgentLoadingというオプションを指定する必要があります。\nDatadogやJMXなどアプリケーションをモニタリングするためのツールがこのような機能に依存している場合があるので、今後のバージョンを使う際には何か実装の方法が変わるかもしれませんね。\nKey Encapsulation Mechanism API 最新の暗号化のアルゴリズムに対応するものです。量子コンピュータでは既存の暗号化アルゴリズムが通用しなくなるという話もあるので、その対応として導入されたものかと思われます（公式でも、 Post-Quantum Cryptography standardization processと述べています）。\n新しいアルゴリズムを用いた公開鍵・秘密鍵のペアの生成、カプセル化、カプセルの解除などの機能に対応しています。以下のような使い方となります。\n// Receiver side KeyPairGenerator g = KeyPairGenerator.getInstance(\u0026#34;ABC\u0026#34;); KeyPair kp = g.generateKeyPair(); publishKey(kp.getPublic()); // Sender side KEM kemS = KEM.getInstance(\u0026#34;ABC-KEM\u0026#34;); PublicKey pkR = retrieveKey(); ABCKEMParameterSpec specS = new ABCKEMParameterSpec(...); KEM.Encapsulator e = kemS.newEncapsulator(pkR, specS, null); KEM.Encapsulated enc = e.encapsulate(); SecretKey secS = enc.key(); sendBytes(enc.encapsulation()); sendBytes(enc.params()); // Receiver side byte[] em = receiveBytes(); byte[] params = receiveBytes(); KEM kemR = KEM.getInstance(\u0026#34;ABC-KEM\u0026#34;); AlgorithmParameters algParams = AlgorithmParameters.getInstance(\u0026#34;ABC-KEM\u0026#34;); algParams.init(params); ABCKEMParameterSpec specR = algParams.getParameterSpec(ABCKEMParameterSpec.class); KEM.Decapsulator d = kemR.newDecapsulator(kp.getPrivate(), specR); SecretKey secR = d.decapsulate(em); Structured Concurrency (Preview) 並列処理をより簡単にするためのAPIです。複数のスレッドで実行される作業の単位を一つのタスクとして扱うことができます。\n例えば以下のようなコードがあったとします。userとorderのデータをそれぞれ違うスレッドで取得して、その結果を返す関数です。\nResponse handle() throws ExecutionException, InterruptedException { Future\u0026lt;String\u0026gt; user = esvc.submit(() -\u0026gt; findUser()); Future\u0026lt;Integer\u0026gt; order = esvc.submit(() -\u0026gt; fetchOrder()); String theUser = user.get(); // Join findUser int theOrder = order.get(); // Join fetchOrder return new Response(theUser, theOrder); } 上記のコードだと、以下のような問題が考えられます。\nfindUser()で例外が発生してもfetchOrder()は実行されてリソースの無駄になる handle()を実行しているスレッドがインタラプトされた場合、findUser()とfetchOrder()は実行されたままになる findUser()の実行が長すぎる場合、fetchOrder()が失敗してもそれを待つことになる（結果的に失敗） これらの問題が挙げられてということは、新しいAPIではそれを解決できるということですね。新しいAPIでは上記の問題を、以下のようなコードで解決します。\nResponse handle() throws ExecutionException, InterruptedException { try (var scope = new StructuredTaskScope.ShutdownOnFailure()) { Supplier\u0026lt;String\u0026gt; user = scope.fork(() -\u0026gt; findUser()); Supplier\u0026lt;Integer\u0026gt; order = scope.fork(() -\u0026gt; fetchOrder()); scope.join() // Join both subtasks .throwIfFailed(); // ... and propagate errors // Here, both subtasks have succeeded, so compose their results return new Response(user.get(), order.get()); } } StructuredTaskScopeを利用して処理を行う場合、以下のメリットがあります。\nfindUser()やfecthOrder()のどちらかが失敗したら、残りの処理はキャンセルされる handle()を実行しているスレッドがインタラプトされた場合、findUser()とfetchOrder()はキャンセルされる 処理が明確に理解できる API 今回の新しいAPIに関しては、言語スペックでよく説明されており、新しいJavadocの方でそれぞれのバージョン別にどんなものが追加されたかフィルタしながら確認ができるので、ここではJavadocのリンクだけを貼っておきます。\n最後に いかがだったでしょうか。私はもうJavaでアプリを書くことはほとんどなく、主にKotlinを書いていて、新しいAPIもそこまでコードに影響を与えることはないのですが、それでもJavaの新しいバージョンがリリースされると、なんだか嬉しくなりますね。特にVirtual ThreadのようなAPIはKotlinでも使えるし、Javaで作成されたTomcatやNettyのようなミドルウェアの性能もこれを活用することでさらに性能が上がると思うとありがたいです。他にも追加されるAPIはKotlinとはまた違うアプローチをしているので大変勉強になるなと思いました。\n今は仕事でJava 17を使っているのですが、Java 21になったらすぐにでも使いたいと思います。特に来年はKotlinも2.0がリリースされるので、Javaの新機能を活かしてKotlinのビルドもパフォーマンスもさらに向上させていきたいなと思います。\nでは、また！\n","date":"2023-09-18T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-enter-to-21/","title":"Java 21は何が変わったか"},{"content":"しばらくKotlinで書いてみたシリーズを書いていなかったので、今回はその四です。\nListをListでソート データ構造として、ListをListでソートしたい場合がありますね。例えばとあるテーブルに対して、別のテーブルのIDを持たせている場合などです。このテーブルのIDを配列にして、並び順を表現しているとしたら、この配列を元にソートしたいことになります。\nもちろんクエリでソートすることもできますが、クエリでソートすると、ソートした結果をキャッシュできないというデメリットがあります。そこで、ListをListでソートする方法を考えてみました。\nまずは以下のようなデータがあるとします。イメージとしては、複数の画像を持つImageBoxというデータがあり、そのImageBoxのimageOrderにはImageのIDが配列として入っているとします。\ndata class ImageBox( val id: Int, val title: String, val imageOrder: List\u0026lt;Int\u0026gt; // imageのIDの配列 ) data class Image( val id: Int, val url: String ) ここでDBとしてはImageBoxテーブルとImageテーブルがあり、ImageBoxテーブルのimageOrderにはImageテーブルのIDが配列として入っているとします。そしてアプリではImageBoxのimageOrderの順番にImageを表示したいとします。\nval imageBox = ImageBox( id = 1, title = \u0026#34;title\u0026#34;, imageOrder = listOf(3, 1, 2, 5, 4) ) val images = listOf( Image(id = 1, url = \u0026#34;url1\u0026#34;), Image(id = 2, url = \u0026#34;url2\u0026#34;), Image(id = 3, url = \u0026#34;url3\u0026#34;), Image(id = 4, url = \u0026#34;url4\u0026#34;), Image(id = 5, url = \u0026#34;url5\u0026#34;) ) この場合、以下のようにソートすることができます。\n// imageBoxのimageOrderのをIndexをkeyとするMapに変換 val indexMap = imageBox.imageOrder.mapIndexed { index, value -\u0026gt; value to index }.toMap() // imagesをimageOrderを元にソート val sortedImages = images.sortedBy { indexMap[it.id] } 原理は簡単で、imageOrderの配列をIndexとvalueのMapに変換して、その値を元にソートしているだけです。この方法であれば、クエリでソートすることなく、imageOrderの順番にソートすることができます。\nまた、こういったソートは以下のように拡張関数として定義すると、他の場所でも使い回すことができます。anotherには並び順を表現する配列を渡して、keySelectorにはソートしたい配列の要素から並び順の基準となる値を取り出す関数を渡します。\nfun \u0026lt;T, U\u0026gt; List\u0026lt;T\u0026gt;.sortedBy(another: List\u0026lt;U\u0026gt;, keySelector: (T) -\u0026gt; U): List\u0026lt;T\u0026gt; { val indexMap = another.mapIndexed { index, value -\u0026gt; value to index }.toMap() return this.sortedBy { indexMap[keySelector(it)] } } この場合、以下のように使えます。\nval sortedImages = images.sortedBy(imageBox.imageOrder) { it.id } keySelectorでitすら省略したい場合は、以下のような方法もあります。\nfun \u0026lt;T, U\u0026gt; List\u0026lt;T\u0026gt;.sortedBy(another: List\u0026lt;U\u0026gt;, keySelector: T.() -\u0026gt; U): List\u0026lt;T\u0026gt; { val indexMap = another.mapIndexed { index, value -\u0026gt; value to index }.toMap() return this.sortedBy { indexMap[keySelector(it)] } } この場合、以下のように使えます。\nval sortedImages = images.sortedBy(imageBox.imageOrder) { id } Listのページング ページングはよくDBのクエリで行いますが、クエリでなくコードのみで処理したい場合もありますね。例えば、外部APIに対してリクエストを送る場合などです。この場合、ページングをどう実装するかが課題になります。\nこの場合は以下のように拡張関数を定義することで、Listのページングを実装することができます。\nfun \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt;.paginated(offset: Int, limit: Int): List\u0026lt;T\u0026gt; { // ページングが必要ない場合はそのまま返す if (offest == 0 \u0026amp;\u0026amp; limit \u0026lt; size) { return this } // ページングが必要な場合はoffsetとlimitを元にsubListを取得する val fromIndex = (offset - 1) * limit return if (size \u0026lt;= fromIndex) { // ページングの範囲外の場合は空のリストを返す emptyList() } else { // ページングの範囲内の場合はsubListを返す val toIndex = size.coerceAtMost(fromIndex + limit) subList(fromIndex, toIndex) } } 細かいものとしてはcoerceAtMost()を使って、sizeより大きい値を渡された場合はsizeを返すようにしています。これはもちろんIndexOutOfBoundsExceptionを防ぐためです。\nExposed 厳密にいってExposedはORMなのですが、Kotlinの書き方によってさらに便利になる部分もあるので今回はこちらも紹介したいと思います。\nInsertとUpdateの共通化 ExposedでInsertやUpdateを書くのはかなり直感的なのですが、多くの場合にInsertとUpdateで同じ処理を書くことになります。そこで、InsertとUpdateの共通化を考えてみました。例えば以下のようなコードがあるとします。\nfun insert(image: Image): Image { ImageTable.insert { this[id] = image.id this[url] = image.url this[title] = image.title this[description] = image.description } return image } fun update(image: Image): Image { ImageTable.update({ ImageTable.id eq image.id }) { this[url] = image.url this[title] = image.title this[description] = image.description } return image } この場合、InsertとUpdateではID以外だとurlやtitleのような値をセットするという面では同じ処理を書いているだけですね。テーブルが増えれば増えるほど、このようなコードも増えていきます。幸いExposedではUpdate時のUpdateBuilderをInsert時のInsertStatementが継承している形の実装となっているので、以下のように共通化することができます。\nprivate fun UpdateBuilder\u0026lt;Int\u0026gt;.setParametersFrom(image: Image) { this[url] = image.url this[title] = image.title this[description] = image.description } このような関数を定義すると、InsertとUpdateの処理は以下のように書くことができます。\nfun insert(image: Image): Image { ImageTable.insert { this[id] = image.id it.setParametersFrom(image) } return image } fun update(image: Image): Image { ImageTable.update({ ImageTable.id eq image.id }) { it.setParametersFrom(image) } return image } これはbatchInsertの場合でも同じです。BatchInsertStatementもまたInsertStatementを継承しているので、以下のように共通化することができます。これでだいぶすっきりしたコードが書けるようになります。\nfun batchInsert(images: List\u0026lt;Image\u0026gt;): List\u0026lt;Image\u0026gt; { ImageTable.batchInsert(images) { this[id] = it.id this.setParametersFrom(it) } return images } BatchUpsertの実装 ExposedにbatchInsertはありますが、batchUpsertはありません。ただ、クエリの書き方とアプリ側の実装によってbatchUpsertを実現することはできます。もちろんExposedPowerUtilsのようなライブラリもあるので、それを導入するのも良いでしょう。\nただ、あのライブラリを使う場合は少し問題がありました。まずこのライブラリの実装では特定のカラムを指定して、それを基準にデータがすでに存在する場合はUpdate、存在しない場合はInsertという処理を行っています。最終的に作られるコードは、Insertをまず実行して、指定したカラムが重複している場合はON CONFLICTでUpdateを実行するというものです。\nここでテーブルのIDはアプリ側で生成するUUIDであり、ON CONFLICTの基準となるカラムがFKの場合にはIDまでUPDATEされるという問題がありました。なので、先ほどのライブラリの実装を元に、PKを除いてUpdateするように実装を変更しました。それが以下です。（あと一部使ってない引数は省略してます）\nfun \u0026lt;T : Table, E\u0026gt; T.batchUpsert( data: Collection\u0026lt;E\u0026gt;, vararg keys: Column\u0026lt;*\u0026gt; = (primaryKey ?: throw IllegalArgumentException(\u0026#34;primary key is missing\u0026#34;)).columns, body: T.(BatchInsertStatement, E) -\u0026gt; Unit ): BatchInsertOrUpdate { return BatchInsertOrUpdate(table = this, keys = keys).apply { data.forEach { addBatch() body(this, it) } execute(TransactionManager.current()) } } class BatchInsertOrUpdate( table: Table, private vararg val keys: Column\u0026lt;*\u0026gt;, ) : BatchInsertStatement(table, false, false) { override fun prepareSQL(transaction: Transaction): String { val transactionManager = TransactionManager.current() val primaryKey = table.primaryKey?.columns?.toSet() // ここでprimaryKeyを取得 val updateSetter = ((table.columns - keys.toSet()) .let { columns -\u0026gt; primaryKey?.let { columns - primaryKey } ?: columns }) // ここでprimaryKeyを除外 .joinToString { \u0026#34;${transactionManager.identity(it)} = EXCLUDED.${transactionManager.identity(it)}\u0026#34; } val onConflict = \u0026#34;ON CONFLICT (${keys.joinToString { transactionManager.identity(it) }}) DO UPDATE SET $updateSetter\u0026#34; return \u0026#34;${super.prepareSQL(transaction)} $onConflict\u0026#34; } } トランザクションを分ける この度はリードレプリカを導入することになり、GET系のAPIにはとりあえずリードレプリカを使うことになりました。ただ、リードレプリカは更新系のクエリを受け付けないので、更新系のクエリはプライマリに送る必要があります。\nそして、APIの実装としては例外を投げる前に明示的にロールバックを行うため別途の関数を定義していて、これと同じような使い方としてリードレプリカに接続するものを作りたいと思いました。まず、既存で使っていた関数は以下のようなものです。\nfun \u0026lt;T\u0026gt; transactionWrapper(statement: Transaction.() -\u0026gt; T): T { return transaction { try { statement() } catch (ex: Exception) { TransactionManager.current().rollback() throw ex } } } Exposedではtransactionの引数にDatabaseを渡すことで、簡単にどのDBに接続するかを指定することができます。なので、これを使ってリードレプリカに接続する関数を以下のように定義することができます。\nfun \u0026lt;T\u0026gt; readReplicaTransactionWrapper(statement: Transaction.() -\u0026gt; T): T { val readReplicaDatabase: Database = getKoinModule(named(\u0026#34;readReplicaDatabase\u0026#34;))) return transaction(readReplicaDatabase) { statement() } } ここでのDatabaseをKoinを使用してinjectしているので、追加で以下のような設定を加えてます。\n// デフォルトのDBはプライマリに固定 val defaultDatabase: Database = getKoinModule(named(\u0026#34;primaryDatabase\u0026#34;)) TransactionManager.defaultDatabase = defaultDatabase // Top level functionでKoinのinjectを使うための設定 fun \u0026lt;T\u0026gt; getKoinModule(qualifier: Qualifier? = null): T { return object : KoinComponent { val value: T by inject(qualifier) }.value } これでtransactionを使うたびに関数名だけを変えるだけで、プライマリとリードレプリカに接続することができます。\n最後に いかがでしたでしょうか。今回もKotlinの小技的なものを紹介してみました。最近はあまり複雑なロジックを書いてない上、プライベートでの開発もあまりしてないので、なかなかブログのネタが思いつかないですね。たまにこちらのブログの記事を参考にしていただいている方がいるようなので、もう少し頻繁に更新できるように頑張ります。\nしばらくブログを更新してませんでしたが、今回はCompose Multiplatformも1.5.0になり、Java 21のリリースもまもなくなので、近いうちにまたそれらについての記事も更新していきたいと思います。\nでは、また！\n","date":"2023-09-03T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-code-in-my-style-4/","title":"Kotlinで書いてみた〜その四〜"},{"content":"今年もKotlinConfが開催されました。毎年興味を持って参加やキーノートを視聴しているわけでもありませんが、最近はK2 CompilerにやKMMなどJetbrainsのプロダクトがかなり盛んな感じなので視聴することにしました。そこで思ったより興味深い発表が多かったので、今回はその内容について簡単にまとめてみようかなと思います。\nでは、セッション別にどんな話があったのかを紹介していきます。\nK2 Compiler まずはKotlin 2.0で採用予定のK2 Compilerの話です。2021年から発表されていて、コンパイラの性能向上のみでなくプラグイン対応などの機能も提供する予定となっていますね。今現在開発が進んでいて、Kotlin 1.8がリリースされた今もかなり多い部分が完成されている状態です。\nここではKotlin 1.8と、2.0になった場合のコンパイラのパフォーマンスの違いをまずグラフとして提示していました。当たり前ながら、2.0でかなり速度が上がったように見えます。同じ環境で20秒かかるものが、10秒に短縮されたということですね。\nAndoridの公式言語としてKotlinが採用され、JavaからKotlinに移行した開発者の間でやはりビルドが遅くなったという話を聞いた覚えがあるのでこれはかなり嬉しい結果となっていますね。またコンパイラのパフォーマンスが上がるとintellijでのコンパイルも早くなるわけなのでより快適な開発環境になると思います。\nまた、今年の後半にはKotlin 1.9のリリースが予定されていて、その次に1.10のようなバージョンは予定にないとのことでした。つまり、1.9の後はすぐに2.0となるとのことですね。そして2.0は1.9の後方互換性を保つので、1.9でコンパイルができるのであれば2.0でもコンパイルができるとのことです。\nもちろん言語のバージョンアップを急ぐ企業さんはそこまではないと思いますし、コンパイラ自体が変わることでどんな問題が起こるかわからないので、実際の採用まではかなり時間がかるでしょう。ただ個人的にはサイドプロジェクトで色々試してみたいので、来年からはK2 Compilerに触れられるということが楽しみです。\n要望の多い機能 YoutrackというサイトからJetbrainsのプロダクトに関して議論できるのですが、ここで要望の多い機能に対して今後どのような形で対応していくかを説明するセッションです。ここは一つ一つ、どのような要望があるかを見ていきます。\nStatic Extensions KT-11968に上がっているもので、JavaクラスにもCompanion objectのように、staticなメソッドやプロパティを追加できるようにしてほしいということです。\n例えば、今までは以下のようにJavaのクラスにCompanion objectを定義してインスタンスの生成なしで使うことはできませんでした。\nFile.open(\u0026#34;data.txt\u0026#34;) fun File.Companion.open(name: String) これを、staticというキーワードを使って以下のように書けるようになるとのことです。\nfun File.static.open(name: String) 個人的にはMySQLとJVMの日付のMAX_VALUEの閾値が違ってバグが起きたことがあって、LocalDateに別途プロパティを追加したかったのにJavaのクラスにはstaticなプロパティの追加ができなくて諦めた経験があるのでこれはとても嬉しい変化です。\nCollection Literals KT-43871に上がっているもので、その名の通りコレクションのリテラルを作成できるようにしてほしいということです。\n例えば、今までは言語レベルでコレクションのリテラルに対応していなかったので、以下のような書き方が多かったかなと思います。\ncmdArgs = listOf(\u0026#34;-language-version\u0026#34;, \u0026#34;2.0\u0026#34;) それが、以下のようにコレクションのリテラルを書けるようになるとのことです。\nval skip: PersistentSet\u0026lt;Int\u0026gt; = [0, 1] val skip2 = PersistentSet [0, 1] 個人的にはconstキーワードの適用範囲をもっと広げてほしいと思っているところですが、これもまた悪くない変化ですね。特にアノテーションなどで使われる配列は実際リテラルになるケースが多いと思いますので、そこでまた色々と活用できる余地ができるかもしれません。\nName-Based Destructing KT-19627に上がっているもので、分解宣言時に変数名とフィールド名が一致するようにしてほしいということです。\n例えば以下のように分解宣言をしたとしましょう。\ndata class Person( val firstName: String, val lastName: String ) val (firstName, lastName) = Person(\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;) 上記のコードの場合、firstNameはJohn, lastNameはDoeとなります。data classの実際のフィールド名とも一致しているので、なんの問題もありません。ただ、もし間違って以下のようにしたらどうでしょう。\nval (lastName, firstName) = Person(\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;) この場合、意図とは違ってfirstNameはDoe, lastNameはJohnとなってしまいます。このようなミスを回避するためにinline classなどを導入してフィールドごとに型を定義したり、分解宣言そのものを使わないようにするなどの対応をしていることも少なくはないかと思いますが、今後はこのようなミスを回避するために、分解宣言時に変数名とフィールド名が一致するかどうかコンパイラが判断して値の代入を行うことになるとのことです。\n個人的にはかなりすごいと思いますが、心配も多いですね。単純に変数名とフィールドが一致する時のみ動作してくれるのかどうかわからないので、実際の動作を見ておきたいものです。\nContext Recivers KT-10468に上がっているもので、関数にコンテキストが必要な場合はパラメータとしてではなく、別途のキーワードを使ってコンテキストを渡すことができるようにしてほしいということです。\n例えば、以下のような関数があるとします。\nfun processRequest(context: ServiceContext, request: ServiceRequest) { val data = request.loadData(context) } 上記の関数ではコンテキストを引数としてもらい、さらにそのコンテキストを違う関数の方に渡しています。当然、呼ばれる関数の方も引数にコンテキストが必要になります。以下のようにですね。\nfun ServiceRequest.loadData(context: ServiceContext): Data { /** ... */ } この場合、関数の中で呼ばれる他の関数が多くなればなるほど、その関数にもコンテキストを渡す必要が出てきます。そこで、以下のように別途のキーワードを使って、引数の追加なしでコンテキストを渡すことができるようになるということです。\ncontext(ServiceContext) fun processRequest(request: ServiceRequest) { val data = request.loadData() } context(ServiceContext) fun ServiceRequest.loadData(): Data { /** ... */ } このcontextのキーワードに渡せるコンテキストの基準が何か、そしてどのように関数の中でコンテキストを呼び出せるかどうかはまだわかりませんが、より綺麗な感覚になっているかなと思います。\nExplicit Fields KT-14663に上がっているもので、privateなプロパティに対してpublicなプロパティを定義しなくても良いようにしてほしいということです。\n例えば、privateのプロパティに対して、外部から参照する場合は以下のように書くケースがあるかなと思います。privateなプロパティを維持しつつ、外部で参照のみできる別のプロパティがその値を提供するということですね。\nprivate val _applicationState = MutableStateFlow(State()) val applicationState: StateFlow\u0026lt;State\u0026gt; get() = _applicationState これを、以下のように書けるようになるらしいです。\nval applicationState: StateFlow\u0026lt;State\u0026gt; field = MutableStateFlow(State()) 行数をかなり減らせられ、似たようなプロパティを定義する必要もなくなるのでかなり便利そうな感覚です。特にComposeのようにプロパティで状態を管理する場合はこのようなパターンが多いかなと思いますので、有効活用できるかなと思いました。\nKotlin Notebooks このセッションで新しくKotlin Notebooksというものの発表がありました。今はJupyter NotebookでもKotlinは使えるのですが、似たような機能をKotlin専用として開発したような感覚ですね。Jupyterそのものも有名で、動画の中ですでに多くの機能を紹介しているので説明よりはそのキャプチャを載せておきます。以下のような活用例が紹介されています。\nその他、自動完成やオンラインでのコードシェア対応、テーブルのソートとカラムの順番変更などができるという点が紹介で明らかになっています。\nGoogle@KotlinConf こちらのセッションはGoogleの人が出て発表をしていましたが、主に指標の話で、Androidのアプリの中でKotlinやComposeを採用しているものがどれほど多いかの話などが多かったですね。また、GoogleでもKotlinを積極活用していて、Google WorkspaceでもKotlin Multiplatformを利用してビジネスロジックを書いているという話がありました。\n他に注目したいところは、Androidの開発においてGradleのデフォルト設定がKotlin DSLになったというところです。Kotlinで開発をしているならもちろんGroovyよりはKotlinの方が便利だと思っていたので、このような変化はありがたいですね。\nただ、GoogleでもKotlinを利用しているとはいえ、FlutterやGoという競合もあるので、今後どのような方向性を取っていくのかが気になりました。もちろんKotlinのカンファレンスなのでそのような話には触れなかったのですが、今後のGoogleの方針やCompose Multiplatformのシェアについてはもっと注目したいと思いました。\nCrossplatform このセッションではComposeのiOS対応の発表と、Mutiplatformの現在の紹介などがありました。ComposeのiOSはAlpha、MultiplatformはBetaの段階で多くのライブラリが対応しているなどの話が続きましたね。Kotlinが初めからJVM以外の領域でも活用できることを目標として開発されたのは確かですが、登場から10年以上も経ってやっとそのロードマップが現実になっているなという感覚でした。\n個人的にはXcodeやSwiftUIよりはintellijとComposeの方が合っていると感じていたので、iOSの開発ができるようになったのは大変嬉しく思っています。特に、AppCodeの開発が今年て終了するとのことだったのでMacやiOSの開発のためには致し方なくSwiftUIを勉強する必要があるかなと思っていたところなのでタイミング的にも最適ですね。直近でちょうどKotlinのみでウェブとモバイル、デスクトップのアプリを開発するというサイドプロジェクトをやっているので、早速試してみようかなと思っています。\n最後に 純粋にK2 Compilerの現在が知りたくて試聴した動画で、思わぬ発表が続いて嬉しい気持ちになりました。個人的には一つの言語で必要なことができるようにしたいと思っていたので、Kotlinという言語を選択したのは正解だったという気持ちになりました。\nやはりまだサーバサイドでもシェアはそこまで高くないかなと思いますが、これからもいろいろな分野でKotlinを使える環境が整っていくと言語として伸びるポテンシャルは十分など思いますので、また今後に期待ですね。\n取り急ぎ、KotlinConfのキーノートの動画を紹介しましたが、他にもいろいろな動画が公式のチャンネルにて配信中なので、興味がある方はぜひ見てみてください。\nでは、また！\n","date":"2023-04-14T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-conf-2023/","title":"KotlinConf'23を要約してみた"},{"content":"最近はサイドプロジェクトとしてシンプルなWebアプリケーションを作っていて、サーバサイドのフレームワークではKtorを採用しています。今まではずっとSpringを触ってきたので、たまにはこうやって違うフレームワークで何か作ってみるのも楽しいですね。\nさて、そこで今回の記事のテーマとなるのが、KtorでPath Parameterを受け取りInline Classとして扱う方法についてです。高度な技術を要するものではないのですが、より型安全で便利に使えるコードが書ける方法ではないかと思い試したことの紹介となります。\nInline Classとは Kotlin 1.6からInline Classという機能が追加されました。Inline Classは、型のラッパーとして使うことができる機能です。例えば、以下のようなコードがあるとします。プレイヤーがいて、そのプレイヤーごとの成績を表現したものです。\n// プレイヤー data class Player(val id: Int, val playerRecordId: Int) // プレイヤーの成績 data class PlayerRecord(val id: Int, val score: Int) そして以下のようなメソッドがあるとします。\n// プレイヤーの成績を記録する fun createPlayerRecord(val playerId: Int, val score: Int) { // ... } このメソッドを呼び出すときに、プレイヤーのIDとスコアを渡す必要があります。しかし、このコードでは、プレイヤーのIDとスコアの両方ともがInt型になっているため、プレイヤーのIDをスコアとして渡してしまうというようなミスが起こりうるのです。そこで、以下のように、プレイヤーのIDとスコアをラップした型を定義して、それを使うようにすると、より安全にコードを書くことができます。\n// プレイヤーのID @JvmInline value class PlayerId(val value: Int) // スコア @JvmInline value class Score(val value: Int) 二つのInline Classを定義することで、先ほどの関数は以下のように修正できます。こうなると、パラメータを間違えて指定したらコンパイラーがエラーを吐いてくれるので、より安全にコードを書くことができますね。\n// プレイヤーの成績を記録する fun createPlayerRecord(val playerId: PlayerId, val score: Score) { // ... } ここだけみると、Inline ClassがJavaのラッパークラスや普通のdata classと何が違うんだ？という疑問が湧いてくるかもしれません。また、typealiasのような既存の機能もありますね。ただ、Inline Classはコンパイル時にprimitive型の扱いでありながら、実行時にはラッパークラスのように振る舞うという特徴があります。そのため、Inline Classだと型安全を担保できつつパフォーマンスへの影響も少ないというメリットがあります。\nKtorでPath Parameterを扱う Ktorでは、Path Parameterを受け取るためには、以下のように書く必要があります。\nrouting { get(\u0026#34;/{id}\u0026#34;) { val id = call.parameters[\u0026#34;id\u0026#34;]?.toInt() } } Path Parameterを取得するために、ApplicationCallからParametersで{id}に指定された値をまずStringで読み込むようになります。そして、さらにtoInt()でIntに変換しています。これでPath ParameterをIntで受け取り、処理の中で使うことができるようになります。\nPath Parameterを受け取る処理を改善する call.parametersを利用したサンプルは一瞬見てシンプルなコードなのであまり改善の余地はないかなと思うかもしれませんが、実はこのコードにはいくつか問題があります。例えば、Int変換時のエラーを考慮する必要がありますね。toInt()でIntに変換するときに、nullや\u0026quot;abc\u0026quot;といった文字列が渡された場合には、NumberFormatExceptionが発生してしまいます。また、toInt()でIntに変換するときに、Int.MAX_VALUEを超える値が渡された場合には、NumberFormatExceptionが発生してしまいます。このように、Path Parameterを受け取るときには、必ずnullチェックやNumberFormatExceptionのチェックを行う必要があります。\nまた、ApplicationCallはget()やpost()のような関数の中でしか呼び出せないです。ということは、エンドポイントごとに同じような処理（エラーハンドリングなど）を書く必要があるということです。ルータ内で何回も同じような処理があるのは望ましくないので、このコードを共通化したいですね。なので、以下のようにApplicationCallに拡張関数を定義して、Path Parameterを取得する処理を共通化すると良いはずです。\nfun ApplicationCall.getIdFromPathParameter(name: String): Int { val parameter = parameters[name] ?: throw IllegalArgumentException(\u0026#34;id is required\u0026#34;) val id = parameter.toIntOrNull() ?: throw IllegalArgumentException(\u0026#34;id must be integer\u0026#34;) return idInt } このようにすると、以下のようにエラーハンドリングを共通化できます。try-catchで例外を処理していますが、ここは必要に応じてStatus Pagesによるエラーハンドリングを追加すると良いでしょう。\nrouting { get(\u0026#34;/{id}\u0026#34;) { try { val id = call.getIdFromPathParameter(\u0026#34;id\u0026#34;) } catch (e: IllegalArgumentException) { call.respond(HttpStatusCode.BadRequest, e.message) } } } Path ParameterをInline Classでラップする さて、Path ParameterからInt型で取得する処理を共通化できたので、次はPath ParameterをInline Classでラップすることで、より安全にコードを書くことを考えてみましょう。まず、もっとも簡単な方法は以下のように拡張関数が返す値をInline Classにすることです。先ほどの関数だと、以下のようになります。\nrouting { get(\u0026#34;/{playerId}\u0026#34;) { val id = PlayerId(call.getIdFromPathParameter(\u0026#34;playerId\u0026#34;)) } } ただ、Inline Classもとりあえずコード上ではクラスの扱いなので、ジェネリックを使うこともできます。なので、先ほどの関数をジェネリックを使ったものにする方法も考えられます。イメージ的には、以下のようになります。\nrouting { get(\u0026#34;/{playerId}\u0026#34;) { val id = call.getIdFromPathParameter\u0026lt;PlayerId\u0026gt;(\u0026#34;playerId\u0026#34;) } } このようにすると、getIdFromPathParameter()の戻り値をPlayerIdに変換する処理をgetIdFromPathParameter()の中で行うことができます。また、ジェネリックであるため、使える型を特定のInterfaceに制限し、IDに関するInline Classがそれを実装するという形にしたらより安全なコードになるでしょう。なので、まずは以下のようにID系の共通のInterfaceを定義します。\n// ID系の共通のInterface interface Id(val value: Int) // プレイヤーのID @JvmInline value class PlayerId(val value: Int) : Id // 監督のID @JvmInline value class DirectorId(val value: Int) : Id そして、以下のようにgetIdFromPathParameter()をジェネリックにして、Idを実装したクラスのみを受け取れるようにします。\n// IDを取得する拡張関数 inline fun \u0026lt;reified T: Id\u0026gt; ApplicationCall.getIdFromPathParameter(name: String): T { val parameter = parameters[name] ?: throw IllegalArgumentException(\u0026#34;id is required\u0026#34;) val id = id.toIntOrNull() ?: throw IllegalArgumentException(\u0026#34;id must be integer\u0026#34;) return T::class.java.getDeclaredConstructor(Int::class.java).apply { isAccessible = true }.newInstance(id) } 修正は簡単で、指定されたIdタイプのインスタンスを作成して、Path Parameterから取得したInt値をラップして返すだけですね。これでIdを実装するInline Classのみ対応するという制限もかけながら、型の安全性も確保できるようになります。\nただ一つ、Path Parameterとしての変数名をInline Classの方にcompanion objectとして持たせて共通化できるといいのですが、残念ながらそれは難しいようです。interfaceのcompanion objectはoverrideできなく、Inline Classはabstract classを実装することができないからです。なので、他の方法でPath Parameter名の指定ができるようにすれば拡張関数の引数を減らしよりシンプルなものになるという改善の余地がまだありそうです。\n最後に 久々にKtorを触り、ブログの記事にしてみましたが、どうだったでしょうか。ずっとRest APIの実装ばかりしていたので、プライベートで何か新しいチャレンジをしてみないと、なかなか発見がない状態となっているのではないかという気がしています。ブログを始めてかれこれ5年目になり、まだ勉強不足だと感じるところが多いと感じつつもなかなかそれを言語化することは簡単ではないとも感じています。\nブログの更新の頻度が減ることになっても、次からはより良い記事になるように頑張りたいと思います。では、また！\n","date":"2023-03-26T00:00:00Z","image":"https://retheviper.github.io/images/ktor.jpg","permalink":"https://retheviper.github.io/posts/ktor-path-parameter-as-inline/","title":"Path ParameterをInline classで受け取る"},{"content":"Spring Boot 2 (Spring Framework 5) から Spring Boot 3 (Spring Framework 6) にアップデートしてみました。\nJava 17 仕事でJavaを使っていたごろは、Java 17をずっと待っていました。以前こちらのブログの記事として整理したこともありましたがテキストブロックやSwitchを式で使えるなど便利な機能がたくさん追加されたためです。しかし、Kotlinを使っている今は機能や言語仕の観点からJavaのバージョンを機にする必要はありません。なので、既存のアプリケーションがJava 11で動いているのであれば、Java 17に上げるモチベーションはあまりないように見えるかもしれません。\nそれでもあえてバージョンを上げようとした理由は、まずサポート期間が今年の9月で終わるからということになります。正確にはJava 11のPremier Supportが2023年9月で終わり、それに準するExtended Supportは2026年9月までということになります。ただ、「準する」という表現が曖昧で、11は今の時点ではもうだいぶ古く、17もリリースから1年以上の期間の間に十分検証されていると判断しました。\nサポート期間の話だけだと、まだ終了まで半年以上の時間が残っているのですが、このタイミングで行うのは会社で「リファクタ期間」というものを設けているためです。この期間中は主に技術負債の解消や依存関係のバージョンアップなどを重点的に行うので、新しい機能の開発とバージョンアップが重なり問題が起こるような事項は避けたいと思いました。\nそのほかでも、新しいJavaのバージョンを採用することでパフォーマンスの向上を図ることができる点があります。例えば、Java 11よりGCの性能が改善されていたり、状況によってZGCの導入を考慮できるオプションができたりです。Javaのバージョンが上がるということは、JVMの改善を含むということになるので、Kotlinでも十分その恩義を受けられることになるでしょう。\nなので、Java 17を採用することで決め、以降時の経験に関して共有したいと思います。\nRecord 社内ではORMとしてjOOQを採用していて、これから自動生成されたテーブル定義のコードをKotlinのコード(Repository)で呼び出す構造となっています。ただ、場合によってはこの自動生成のコードによりJava 17でのコンパイルが失敗することがあります。今回の場合は、以下のようなエラーメッセージと共にコンパイルが失敗するのを確認できました。\nエラー: Recordの参照はあいまいです public \u0026lt;O extends Record\u0026gt; SomeTable(Table\u0026lt;O\u0026gt; child, ForeignKey\u0026lt;O, SomeRecord\u0026gt; key) { ^ org.jooqのインタフェース org.jooq.Recordとjava.langのクラス java.lang.Recordの両方が一致します これはjOOQにRecordというクラスが存在していて、自動生成のコードでそれを利用しているためです。Java 14以降Lombokの@Valueと似た機能を持つRecord Classが登場していて、recordキーワードを使って定義したクラスは全てjava.lang.Recordを実装する形になっています。というわけで、「jOOQのRecordを意味するのか、JavaのRecordを意味するのかコンパイラが判断できない」とエラーが発生してしまうのです。\nこれは、既存の自動生成コードのインポートを修正するだけで回避できます。既存のコードだと、自動生成のコードでは以下のようにjOOQのクラスをインポートしています。\nimport org.jooq.*; これを、明確にjOOQのRecordを指定するように修正します。\nimport org.jooq.Record; import org.jooq.*; 以上で問題なくコンパイルが通るようになりました。もし自作のライブラリなどでRecordというクラスを定義している場合はここと同じエラーになる可能性があるので、なるべくクラスの名前を変えた方が良いかもです。\nDocker Image 現在開発中のサービスはJibを使ってコンテナ化しています。ここでベースとなるイメージの指定が必要なのですが、既存で使っていたイメージはOpenJDKの11-jreでした。これが17からはJREのみのイメージはなく、17-jdkのみとなったので、バージョンをあげる際は注意する必要があります。\nただ、OpenJDK以外のイメージ意外を使っている場合は状況が違うかもしれませんので確認が必要です。例えば、TemurinやZuluは17-jreを提供していて、Libericaの場合は17とバージョンだけになっているなど使っているJDKの種類によってタグ名が違うので、JDKのバージョンアップの際には使っているイメージのタグはチェックしておく方が良いでしょう。\n依存関係 自分の担当しているアプリで発生していた問題ではないのですが(マイクロサービスとして、Kotlinのサービスは複数あります)、一部でJavaのバージョンを11から17に上げた際にJasperreportsを使った帳票の出力で報告された問題がありました。このライブラリはPDFの出力のために利用しているのですが、レイアウトに問題はなかったものの、表の中の表示文字数が少し減ったという問題がありました。幸い、これは大きい問題ではなかったのでまずは対応なしとなりそうですが場合によっては致命的かもしれません。\nおそらくこのような問題が発生したら、依存しているライブラリのバージョンをJava 17に対応したものにあげれば解消できるのではないかと思いますが、まだ17に対応していないライブラリがある可能性もあるので、事前に依存関係の方をチェックしておいた方が胃良いでしょう。\nSpring Boot 3 Java 17は今年に11のサポートが終了するということで必須としていましたが、Spring Boot 3(Spring Framework 6)の場合は去年の12月にリリースされたばかりなので今回はあえて移行を試す必要はありませんでした。ただ、Spring Boot 3からちょうどJava 17が最低バージョンになり、以前からサーバレスのGAE上で起動しているプロジェクトもあったので、起動時間を減らすためにSpring Nativeを試してみたいと思っていたためでもあります。Spring NativeはSpring Boot 3から正式にサポートされることになったので。\nまた、Spring Boot 3で依存関係が変わったり、プロパティの記述方法が変わったりするケースがあるとしても、自分が扱っているアプリケーションはマイクロサービスなので比較的に影響が少ないと予測できたからです。おそらくモノリシックで、Springの様々な機能に依存している場合は移行が難しい場合もあるかなと思います。なので、もし移行を考えている場合はなるべくリリースノートなどを確認しておいた方が良いでしょう。\nSpring Boot 2系から3系の移行は、基本的に公式のマイグレーションガイドを参照すると良いのですが、マイグレーションガイドだけではわからないことや、Spring Frameworkの依存関係の変化により他のライブラリに影響が出ることがあります。なので自分の場合はどのように対応したかを少し紹介したいと思います。\n@ConstructorBinding @ConstructorBindingは、@ConfigurationPropertiesを付けたクラスにアプリケーションプロパティファイルから読み込んだ値をコンストラクタインジェクションするためのアノテーションです。現在のプロジェクトではAWSのS3のバケット名やメールのテンプレート、他のAPIを呼び出すためのエンドポイントなどをapplication.yamlに記載して読み込むようにしています。\nここで、Spring Boot 3だと@ConstructorBindingなしでもプロパティを読み込むようになって、そもそもこのアノテーション自体がDeprecatedになっています。なので、最初はコンパイルエラーとなりますが、@ConstructorBindingを削除するだけで問題なく動作するようになりました。最も簡単な対応でした。\nTracingの変更 現在のアプリケーションでは、他のマイクロサービスのAPIを呼び出す際にBraveを使ってヘッダにTrace Contextを載せています。この場合、同じTrace IDが連携されるのでログの確認がやりやすくなりますね。今まではSpring Cloud Sleuthを通じてBraveをDIしていましたが、これがSpring Boot 3になくなり(Spring Bootに統合）、Micrometerの新しいAPIを利用することになったらしいです。\nつまり、既存のSpring Cloud SleuthによるDIは使えなくなり、依存関係も変化があるということです。まずは、DIを担当していたBraveAutoConfigurationがSpring Boot Actuatorに移りました。なので、もうSpring Cloud Sleuthはいらなくなります。\nただ、Spring Boot ActuatorだけだとBraveAutoConfigurationによるDIはできません。なので以下のようにブリッジの依存関係を追加する必要があります。(バージョンはSpring Bootのものが使われる)\nimplementation(\u0026#34;io.micrometer:micrometer-tracing-bridge-brave\u0026#34;) 依存関係が変わるので、application.yamlの設定も変更が必要になります。既存では、以下のように記載していました。\nspring: sleuth: enabled: true propagation: type: - w3c Micrometerに移行したので、このように変更します。実際は、以下の設定はデフォルトなので記載しなくても問題はありませんが、メンテナンス時に現在の設定がわかりやすいように記載しています。\nmanagement: tracing: enabled: true propagation: type: w3c ただ、最もつまづいた部分はユニットテストでした。テストを実行してみると、なぜか以下のようなエラーメッセージが出ていました。\nCaused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type \u0026#39;brave.Tracing\u0026#39; available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {} Spring Cloud Sleuthを使っていた頃はユニットテスト時もDIができていたのですが、なぜかBraveAutoConfigurationによるDIが聞かない状態です。ローカルで起動した場合は同じエラーにならないので、おそらくテスト時にだけAutoConfigurationが呼ばれてないように思われます。\n@TestConfigurationなどを使って、直接Bean登録をする方法もあるかと思いますが、調査してみるとMicrometerにはテスト用のライブラリが用意されているので、それを使うことにしました。このライブラリがリリースされたのは2022年の12月なので、なかなか情報を見つけるのが難しいです。とにかく、以下のようにライブラリを追加することでDIの問題は解消できます。\ntestImplementation(\u0026#34;io.micrometer:micrometer-tracing-test\u0026#34;) 今回はライブラリの追加だけで解消できたのですが、新しいライブラリの情報はこちらで調べないとなかなかわからないというのが問題でしたね。他のライブラリの場合も同じ問題があるかもしれないので、もしAutoConfigurationによりDIされるはずのBeanが見つからないとのエラーが出る場合は、テスト用のライブラリが追加されてないか確認するのが良いかもしれません。\n追加で、Datadogの場合もMicrometerの設定の記載方法が全体的に変わったためか、一部プロパティの記載方法が変わっています。以前の場合は以下のように記載していました。\nmanagement: metrics: export: datadog: enabled: true apiKey: ${API_KEY} step: 1m これが以下のように変わります。\nmanagement: datadog: metrics: export: enabled: true api-key: ${API_KEY} step: 1m 他にもMicrometerの機能を使っている場合、既存のSpring Boot 2と比べプロパティの設定方法が変わってないか確認した方がいいでしょう。\nJakarta EE Spring Framework 6はOracleのJava EEからEclipseのJakarta EEに移行しているので、コンパイルが通らない場合があります。主にHttpServletRequestなどsevlet系や@Validなどvalidation系パッケージがよく使われているかと思います。これらはimport時のパッケージ名を変更することで対応できます。例えば、以下のようなパッケージを使っている場合だとします。\njavax.servlet javax.validation これらは以下のように変更します。\njakarta.servlet jakarta.validation パッケージ名が変わっただけで、中身は変わっていないので、簡単な対応となります。\nspringdoc springdoc-openapiを使ってAPIドキュメントを生成している場合、既存のバージョンだとSpring Boot 3と互換性がないようです。CIでは/v3/api-docsから取得したJSONを解析してS3にドキュメントをアップロードするようになっていますが、Spring Boot 3だと404エラーとなっていました。\n幸い、これも依存関係を変更することで対応ができます。以前は以下のように依存関係を追加していました。\nimplementation(\u0026#34;org.springdoc:springdoc-openapi-ui:1.6.14\u0026#34;) implementation(\u0026#34;org.springdoc:springdoc-openapi-kotlin:1.6.14\u0026#34;) これを、springdoc-openapiのv2に変更するだけで良いです。Kotlinのサポートも含まれるので、以下のみで問題なく動きます。\nimplementation(\u0026#34;org.springdoc:springdoc-openapi-starter-webmvc-ui:2.0.2\u0026#34;) これで404エラーは発生することなく、S3へのドキュメントのアップロードもできます。\nLiquibase Spring Frameworkとの直接的な関係がある訳ではないのですが、Spring Bootのバージョンを上げたことにより影響を受けたのでこちらのケースも紹介します。DBのマイグレーションのためにGradleのプラグインとしてLiquibaseを使っている場合、バージョンによっては以下のようなエラーが出る場合があります。\nSLF4J: No SLF4J providers were found. SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details. SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier. SLF4J: Ignoring binding found at [jar:file:/root/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.11/4741689214e9d1e8408b206506cbe76d1c6a7d60/logback-classic-1.2.11.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation. Exception in thread \u0026#34;main\u0026#34; java.lang.ClassCastException: class org.slf4j.helpers.NOPLogger cannot be cast to class ch.qos.logback.classic.Logger (org.slf4j.helpers.NOPLogger and ch.qos.logback.classic.Logger are in unnamed module of loader \u0026#39;app\u0026#39;) at liquibase.integration.commandline.Main.setupLogging(Main.java:233) at liquibase.integration.commandline.Main.run(Main.java:145) at liquibase.integration.commandline.Main.main(Main.java:129) これはおそらく、Spring Boot 3からSLF4Jのバージョンが2系になったためですね。Liquibaseのプラグインでは今までLogbackを使ってマイグレーションの状況を出力していたのですが、このLogbackのバージョンが1.2だったのでSLF4Jの2系と互換性がなかったのです。Logbackの1.3から互換性があるらしいので、それ以前のバージョンを使っている場合はこちらも合わせてバージョンを上げておく必要があるでしょう。\n最後に 何とかJavaとSpring Bootのバージョンアップには成功していますが、前述した通り、これはあくまで自分が開発しているアプリケーションがマイクロサービスで、依存関係が比較的少なかったからできたことなのではないかと思います。モノリシックなアプリケーションだったり、より複雑な依存関係を持っているアプリケーションならここに記載したこと以外の部分でも問題が発生する可能性は高いでしょう。\nただ、Springの場合はいずれSpring Boot 2のサポートが切れ、3系に移行するしかない状況が来るかもしれませんが、その時は十分マイグレーションの実例も出てくるのではないかと思います。Javaの場合は11から17に上げる場合なら大きく問題はないかと思いますが、8から移行する場合はJava 9で導入されたModuleで問題が起こる可能性もあるかと思いますので、慎重に行うべきかなと思います。\n嬉しいことに、まだNative化までは試してないのですが、JVM上でもアプリケーションの起動時間が減少していることを確認できました。ローカルで起動した場合、Java 11 \u0026amp; Spring Boot 2だと31秒がかかり、Java 17 \u0026amp; Spring Boot 3だと23秒がかかっていたので、断定はできないものの全体的な性能の向上もある程度は期待できるのではないかという気がします。正確なデータはまだ取れていないので、今後の課題として残しておきますが、ありがたいことですね。\nでは、また！\n","date":"2023-01-29T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-boot-2-to-3/","title":"Spring Boot 3を導入してみた"},{"content":"自分がJavaからKotlinに転向してからもう2年ほどが経ちます。しかし、いまだにKotlinでできることは無限にあって、新しい発見は終わることがないと感じています。Kotlinという言語自体のバージョンアップが早く、色々と機能が追加されて続けているのでまだしばらくこの発見も続きそうですね。\nそんな中で思うことなのですが、Intellijを使っていると自動的にJavaのコードを変換してくれたり、Javaの書き方をそのまま流用しても問題になることは少ないものの、やはりKotlinならではのコードを書きたいという欲求も湧いてきます。つまり、「Kotlinらしき書き方」をしたいと思ってしまうのです。\nKorlinらしき書き方って？ 「Kotlinらしき書き方」とは一体どういうものなのでしょうか。まずはその定義が必要ですね。いろいろな捉え方があるかと思いますが、私は基本的に「Kotlinの仕様や機能を最大限に活かすこと」なのではと思っています。つまり、スタンダードライブラリのAPI、Scope Function, Extensionなどを積極的にコードに取り入れることです。そうすることでコードを書く時間は短くなり、より効率が上がるだろうと私は思っています。\nただ、そういう概念を言葉で述べるだけでは曖昧なところがあるので、コードを持って例を挙げた方がいいでしょう。例えば、以下のような関数を実装する必要があるとします。\nfun numbers(): List\u0026lt;String\u0026gt; { // TODO } この関数を通じて行いたい処理は、「0〜10の数字をStringに変換してListとして返す」ことだとしましょう。その場合、実装の一例として以下のようなコードを提示できるかなと思います。\nfun numbers(): List\u0026lt;String\u0026gt; { val list = mutableListOf\u0026lt;String\u0026gt;() var i = 0 while (i \u0026lt; 10) { i++ list.add(i.toString()) } return list.toList() } ここでkotlin.collections.map()を使ったら、同じコードを以下のように変換することができます。\nfun numbers(): List\u0026lt;String\u0026gt; = (0..10).map { it.toString() } map()のような関数が初めて登場した際は、あまり直感的にその処理の意味を把握できないということから「可読性に欠ける」という評価もあったようです。上記の二つのコードを比べると、場合によってはwhileループでの処理がわかりやすいと感じる方もいらっしゃるかなと思います。今はmap()が多くのプログラミング言語で採用している関数であり、期待できる関数の実行結果も常識化していますが、古典的なコードに慣れている人からしたら「ループで何かを行う」というコードの方がわかりやすいかもです。\n「可読性」として しかし、果たしてより高度な処理を行う場合も「長いコードの方が可読性は良い場合もある」と断定できるのでしょうか。ここでまた、違う例を挙げてみましょう。例えば、Listの数字を全て掛け算した結果を返す関数を実装する必要があるとします。例えば以下のようなコードを書くことができるでしょう。\nfun multiply(numbers: List\u0026lt;Int\u0026gt;): Int { if (numbers.isEmpty()) { throw IllegalAccessException(\u0026#34;List is empty\u0026#34;) } var acc = numbers[0] if (numbers.size == 1) return acc for ((index, value) in numbers.withIndex()) { if (index != 0) { acc *= value } } return acc } 上記のコードでは当初の目的は果たしていますが、「可読性」という観点からするとどうでしょうか。関数のシグニチャーだけでなく、関数で行なっている処理全体に目を通さないと、何が起こっているかわからないのではないでしょうか？\nこの関数で行なっている処理を一つ一つ読んでみると、「リストが空の場合はエラーを返す」、「リストの要素が一つの場合はそれを返す」、「リストが空でなく、要素が一つでもない場合は後続の処理を行う」、「最初の要素をとり、リストの要素をループしながら最初の要素以外を全て掛け算する」という情報が込められています。\nここでExtensionとkotlin.collectionsの関数を使って同じ処理を行う関数を実装してみると、コードは大きく変わります。以下がそのサンプルコードです。\nfun List\u0026lt;Int\u0026gt;.multiply(): Int = reduce { acc, e -\u0026gt; acc * e } map()の場合と同じく、reduce()という関数が何をするかがわからない場合もあるかと思います。しかし、その関数が「要素を一つに減らす」ことであり、その中で行なっている「減らし方は掛け算」ということを理解すれば良いだけなので、こちらの方がその意図を把握しやすいのではないでしょうか。先ほどの関数は、処理全体をいくつかの単位で分けて理解する必要があったということから考えると、可読性の面ではこちらの方がより優秀だと言えるのではないかと私は思っています。\nまた、Listに拡張関数として定義することで、まるで元からついていたメソッドのように使えるのも良い点でしょう（IDEから自動補完に出てくるはずなので）。\n「工数」として コードを書くのは工数がかかる行為です。なので、同じ機能をする関数を書くとしても、毎回全ての処理を書くより共通化できる部分は分離し、再利用するのが常識のようなものです。そのため多くのライブラリやフレームワークが存在していますね。\nプログラミング言語の使用や機能をよく理解し、それらを活用するということも本質的にはライブラリやフレームワークを使うことと変わらないものです。先のように、「Listの要素を一つにまとめる」処理を毎回自前で書くとしたら、かなりの時間が必要となるでしょう。\nここで他の例をまた挙げます。例えば以下のようなデータがあるとします。\ndata class Data(val id: Int, val version: Int) val list = listOf( Data(1, 1), Data(1, 2), Data(2, 1), Data(2, 2), Data(2, 3) ) DBの中に履歴を残したい場合はversionや枝番など名称の列を持たせ、同じIDのデータをいくつか挿入する場合があるかと思います。そしてその場合、最新のデータのみを処理したいケースもありますね。上記の例だと、Data(1, 2)とData(2, 3)のみを取得したいということです。\n最初からクエリで最新のデータのみを取得できるといいのですが、外部APIのレスポンスの場合はそのようにフィルタされたデータでない場合もあります。なのでこちらでversionがmaxのデータをフィルタする処理を書くとします。例えば以下のようなコードを考えられます。\nfun maxVersions(list: List\u0026lt;Data\u0026gt;) { val result = mutableListOf\u0026lt;Data\u0026gt;() for ((index, value) in list.withIndex()) { if (index == 0) { result.add(value) } else { val last = result[result.size - 1] if (value.id == last.id) { if (value.version \u0026gt; last.version) { result.remove(last) result.add(value) } } else { result.add(value) } } } return result.toList() } 可読性の問題は一旦置いといて、このようなコードを書くときの工数はどうでしょうか。慣れてしまえば簡単なのかもしれませんが、初めてこの処理を書く人の立場からしたらかなりの工数がかかり、関数が期待通りに動作するかの検証を含めるとさらに工数が必要となりそうだと思います。慣れている場合でも、常にこのようなコードを的確に書けるかどうかが疑問です。\nそれに対して、スタンダードライブラリを使った例を考えてみます。以下のように、メソッドチェーンによって簡単に同じことができます。\nval maxVersions = list.groupBy { it.id }.map { it.value.maxBy { it.version } } ここで使われているgroupBy()、map()、maxBy()はそれぞれ「ValueがListのMapを作る」、「要素を違う形にマッピングする」、「Listの要素のうちmaxの値を探す」という関数なので、ここだけでなく色々な場面で活用できる関数となっています。このように便利な関数を使いこなし、さらに組み合わせることでより複雑な処理でも簡単に書くことができるという点をみると、スタンダードライブラリの機能を理解するのは工数の面でもかなり効率を上げられることになるのではないかと思います。\n注意点 ここまではKotlinの仕様や機能を活かすと可読性と工数という二つの観点から、メリットがあるという話をしてきました。しかし、どんなことでもメリットがあればデメリットもあるものですね。\n当たり前ながら、どんな機能でも単純に「それができる」という理由で乱用するとむしろ逆効果が出るケースがあります。例えば以下のような例を考えられます。二つのデータクラスがあって、DataからRequestにマッピングする必要があるとしましょう。\ndata class Data( val id: Int?, val version: Int? ) data class Request( val id: Int, val version: Int ) Dataのフィールドはnullableとなっていますが、Requestの場合はそうではないです。このようにビジネスロジック上はnullになることはなくても、実装上の都合によってnullableにするケースもありますね。その場合、どうやってマッピングしたらいいでしょうか。例えば以下のような例があるとします。\ndata class Request( val id: Int, val version: Int ) { companion object { fun from(data: Data): Request { return Request( id = requireNotNull(data.id) { \u0026#34;${Data::id.name} should not be null.\u0026#34; }, version = requireNotNull(data.version) { \u0026#34;${Data::version.name} should not be null.\u0026#34; } ) } } } ここではcompanion objectを利用してRequestのインスタンスを生成しながら、データのマッピングを行うようにしています。そこで、requireNotNull()を利用したバリデーションを行い、nullだった場合はエラーメッセージを出力するようにしていますね。\nここまでは良い実装だとして、問題はエラーメッセージの方です。どのパラメータがnullだったかわかるようにしていますが、そこでDataのフィールドをRefelctionで取得し、そのフィールド名をStringに埋め込むようにしています。エラーが発生した時には意図的に動作するとしても、果たして性能の劣るRefelctionを使うまでのことだったでしょうか。\nこのように、言語が提供する機能を活用する場合は適切な場面を判断する必要があります。ここで紹介したmap()やreduce()、groupBy()なども便利で簡単な実装ができるようにしてくれる優秀な関数ですが、これらの関数の実装をみるとそれぞれが一つのループを必ず行い新しいCollectionを生成するという処理を行なっているということを理解すれば、メソッドチェーンで複数の関数を利用する場合は性能に影響を及ぼす可能性もあるということがわかるでしょう。他にも可読性の面でもわかりにくくなったり、別の関数として分離した方が良いコードが一つの関数内でいくつも繰り広げられることになる可能性もあるかと思います。\nなので、「曖昧な理解」や「慣性」としてこれらの機能を利用するという行為にはリスクがあるということを理解し、どのような実装をするかは常に悩むべきではないかと思います。\n最後に すでに何回か、「Kotlinで書いてみた」というタイトルで、Kotlinならこういう書き方ができるというポストを載せていて、今回の記事もまたそれらと大きく離れた題ではないです。また、ある意味ここで挙げたことはプログラマなら基本的に熟知しておくべき常識のようなもので、今更な感があるのかもしれません。\nそれでもあえて記事として書いている理由は、基本こそ大事だという自分の考えゆえでもあります。経験を積んでいくと発展する部分もありますが、逆に変な習慣がついてしまいなかなか直せないところも出てくるものですので。自分はそうでないかという警戒を兼ねて、一度は基本に戻り今の自分と照らし合わせてみるのもまた一つの勉強になるのではないかと思います。ちょうど年末ということもありますが。\n少し遅れてしまいましたが、2022年のポストはこれにて終わりとなります。来年は自分も、ここに訪れるみなさんにも成長のある一年になることをお祈りします。\nでは、また！\n","date":"2022-12-30T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-how-to-kotlin-like/","title":"Kotlinはどう書いたらいいか"},{"content":"ほとんどの言語で、特定の条件に合致した場合にのみ実行する処理を書くには、ifのように分岐処理のための構文を使うのが当たり前のように考えられています。言語によっては構文ではなく式として扱われたり、switchや三項演算子のような他の選択肢も存在しますが、基本的に「条件とそれに従う処理」を書く機能として本質は変わらないものですね。\n分岐処理に限った話ではありませんが、便利な道具というのは時に危険性を伴うこともあります。ifを使う場合、最初の実装ではわかりやすく簡単に目的を達成できますが、維持保守の観点からするとあまり良くない選択になるケースもありますね。たとえば条件が増えたり変わるなどコードに変更が必要となった場合は修正がすべてのケースを網羅しているかどうかがわからなくなったり、ユニットテストが困難になったりするなどが考えられます。\nなので少なくともifの処理をよりシンプルにしたり、もしくはデザインパターンなどで分岐の構文を使わず同じ処理ができるように改善する必要が出てくることもあるかなと思います。そうすることで、コードリーディングはより難しくなるとしても、維持保守の観点からはより良いコードになる可能性もあるでしょう。\nもちろん、完全にifをなくするというのは不可能に近い話で、そこまでする必要もありません。道具自体に罪はなく、あくまで使い方が問題になるだけですので。ここではあくまで、ifを使ったコードをどういう風にリファクタできるか、それだけに集中したいと思います。（初心者向けな感じで）\nif文の例 まずは、以下の関数をご覧ください。コードと価格を渡したら、内部ではコードに合わせて元の価格から割引の値を返すというものです。極めて単純化してはいますが、ECサイトのプロモーションなどでこのような処理が存在することもあるかなと思います。\nfun getDiscountAmount(code: String, amount: Int): Int { return if (code == \u0026#34;Facebook\u0026#34;) { (amount * 0.1).roundToInt() } else if (code == \u0026#34;Twitter\u0026#34;) { (amount * 0.15).roundToInt() } else if (code == \u0026#34;Instagram\u0026#34;) { if (amount \u0026lt; 1000) { amount } else { 1000 } } else { 0 } } すでにどのように処理をリファクタすれば良いか、一目でわかる方もいらっしゃるかなと思いますが、ここでは色々な観点でどうコードを変えられるかを述べたいので、一つ一つ項目を挙げて説明したいと思います。\n関数のリファクタ まずは関数内の処理をどう変えられるかを考えてみます。コードをより単純化して可読性を向上したり、処理の漏れをなくしたり、共通化できたり処理の単位が曖昧な場合は分離するなどいろいろな方法が考えられるはずです。\n標準ライブラリ 上記の関数ではifの中にさらにifが入っている構造となっているのがわかります。このようなネストは深くなれば深くなるほど良いコードとは言えませんね。なのでまずはここから直していきましょう。\nifのネストを一つ減らす方法として、標準ライブラリを用いた方法を考えられます。標準ライブラリでなくでも関数として分離するという選択もありますが、標準ライブラリに処理を委任することでこの関数の負担がまず減るかなと思います。\nKotlinにはcoerceAtLeast()という関数があり、パラメータとして渡された値をミニマムとして返すという働きをします。なので、amountが1000以下の場合はamount自身を、それ以外は1000を返すという処理はこの関数を使うことで単純化できます。以下のようにですね。\nfun getDiscountAmount(code: String, amount: Int): Int { return if (code == \u0026#34;Facebook\u0026#34;) { (amount * 0.1).roundToInt() } else if (code == \u0026#34;Twitter\u0026#34;) { (amount * 0.15).roundToInt() } else if (code == \u0026#34;Instagram\u0026#34;) { amount.coerceAtLeast(1000) // 閾値を超えない値となる } else { 0 } } 単純に標準ライブラリを使用しただけですが、ネストが一つなくなりより単純なコードになりました。また、ここでの修正により閾値の修正が必要になった場合でも、1箇所だけを修正すると良いというメリットもありますね。\n他にも標準ライブラリで処理ができそうな場所は積極的に利用したり、似たような処理が繰り返されるところがあったら切り出して自前のライブラリとして分離しておくことも良い選択になるでしょう。\nwhen もう一つのリファクタとしては、ifをwhen(他の言語のswitch)に入れ替えるという方法を考えられます。whenは結果的に同じ機能をするので、常にifの代替として良いというわけではありません。しかし、ifの条件が一律であれば、whenを選ぶのは良い選択になる場合があります。\n先ほどのifで分岐する条件は、あくまでcodeという文字列がどのような値となっているか比較することだけですね。他には特に条件がないので、whenを用いた方がブランチを文字列だけで収めるのでより単純化つ明瞭なコードになります。たとえば以下のようにです。\nfun getDiscountAmount(code: String, amount: Int): Int { return when (code) { // codeの値を比較するだけの分岐 \u0026#34;Facebook\u0026#34; -\u0026gt; (amount * 0.1).roundToInt() \u0026#34;Twitter\u0026#34; -\u0026gt; (amount * 0.15).roundToInt() \u0026#34;Instagram\u0026#34; -\u0026gt; amount.coerceAtLeast(1000) else -\u0026gt; 0 } } ifをwhenに変えただけで、全体的に短くなってより読みやすいコードになっているかと思います。このように、ifの条件文がどのようなものかをみて、whenに変えるのも場合によっては良い選択の一つになり得るかなと思います。\n拡張関数 Kotlinのような言語には、拡張関数で既存のクラスにメソッドを追加する機能がありますね。同じような処理が2箇所以上繰り返されているなら関数として分離を考慮すべきで、その関数を拡張関数として定義することも場合によっては考えられます。\nここではcodeがFacebookかTwitterかによる分岐がありますが、やりたいことはamountに特定のパーセンテージをかけて返すことですね。なので、パーセンテージを求める関数を定義しておいた方が良いでしょう。\nパーセンテージを求めるのはここでしか使わないとしたらprivateな関数として定義しても良いのですが、より汎用的な使い方ができるようにしたいなら、以下のような拡張関数を定義するのもありでしょう。\n// パーセントを求める拡張関数 infix fun Int.percentOf(amount: Int): Int = (amount * this / 100) fun getDiscountAmount(code: String, amount: Int): Int { return when (code) { \u0026#34;Facebook\u0026#34; -\u0026gt; 10 percentOf amount // 10パーセントの値を返す \u0026#34;Twitter\u0026#34; -\u0026gt; 15 percentOf amount // 15パーセントの値を返す \u0026#34;Instagram\u0026#34; -\u0026gt; amount.coerceAtLeast(1000) else -\u0026gt; 0 } } 先のほどのコードと比べ、拡張関数を定義することで処理の共通化ができたことと共に、「パーセントを計算する」という意図がコードでより明確に表れているようになっているのではないかと思います。\nMap ifやwhenを使わない場合でも分岐ができる場合はあります。たとえばMapを活用する方法ですね。codeによって違う値を掛け算したいので、codeをKeyに、掛けたい値をValueとするMapを定義しておくことです。たとえば以下のようなものです。\n// コードと割引率 val discountPercentages = mapOf( \u0026#34;Facebook\u0026#34; to 10, \u0026#34;Twitter\u0026#34; to 15 ) fun getDiscountAmount(code: String, amount: Int): Int { // 割引率が定義してあったら掛け算 discountPercentages[code]?.let { return it percentOf amount } // Mapにないcodeの場合 return when (code) { \u0026#34;Instagram\u0026#34; -\u0026gt; amount.coerceAtLeast(1000) else -\u0026gt; 0 } } ただ、この方法では全ての分岐を網羅することはできなくなりますね。codeの値ががMapのKeyに含まれてない場合の処理が必要となるからです。ここでクロージャを用いるとcodeの値がInstagramの場合の処理もMapに含めることができます。たとえば以下のようにですね。\n// Valueを(Int) -\u0026gt; Intに変える val discountRules = mapOf( \u0026#34;Facebook\u0026#34; to { amount: Int -\u0026gt; 10 percentOf amount }, \u0026#34;Twitter\u0026#34; to { amount: Int -\u0026gt; 15 percentOf amount }, \u0026#34;Instagram\u0026#34; to { amount: Int -\u0026gt; amount.coerceAtLeast(1000) } ) fun getDiscountAmount(code: String, amount: Int): Int { return discountRules[code]?.let { it(amount) } ?: 0 } Mapを利用する方法が条件分岐より良い方法だとは言い切れないのですが、コード別の割引率を他の関数でも参照する必要があるなど、複数の関数やクラスを跨いで共通の値を保持しておきたい場合は考えられる方法の一つになるかなと思います。この場合は、Map一つを修正するだけで全体の処理で整合性が保証されるコードになるという効果がありますね。\nOOP的な考え方 今までは単純に関数内部の処理をどう変えていくかについて述べましたが、より高度な方法ももちろんあります。OOPの考え方として捉えると、先ほどの関数は「割引額を求める」責任がありますが、その中で「割引」の定義そのものと、その計算式まで持っていることになります。なので、責任を分離していく必要がありますね。\nこの修正に処理は一見より複雑なものになっていくと感じる場合もあるかと思いますが、これはOOPに原則であるSOLIDを考慮したものでもあります。長期的な観点からすると、このような方法をとった方がより維持保守には向いていることになるでしょう。\ninterface抽出 まずは「割引ポリシー」をinsterfaceとして分離します。この割引ポリシーを実装するクラスで実際のポリシーに従う割引額を計算するイメージです。\ninterface DiscountPolicy { fun calculate(amount: Int): Int companion object { val NONE: DiscountPolicy = object : DiscountPolicy { override fun calculate(amount: Int): Int = 0 } } } あとはこのinterfaceを実装するクラスを、コード別に定義しておきます。\nclass FacebookDiscountPolicy : DiscountPolicy { override fun calculate(amount: Int): Int = 10 percentOf amount } class TwitterDiscountPolicy : DiscountPolicy { override fun calculate(amount: Int): Int = 15 percentOf amount } class InstagramDiscountPolicy : DiscountPolicy { override fun calculate(amount: Int): Int = amount.coerceAtLeast(1000) } こうやって割引ポリシーを定義しておくと、getDiscountAmount()は以下のように変えられるでしょう。\nfun getDiscountAmount(code: String, amount: Int): Int { val discountPolicy = when (code) { \u0026#34;Facebook\u0026#34; -\u0026gt; FacebookDiscountPolicy() \u0026#34;Twitter\u0026#34; -\u0026gt; TwitterDiscountPolicy() \u0026#34;Instagram\u0026#34; -\u0026gt; InstagramDiscountPolicy() else -\u0026gt; DiscountPolicy.NONE } return discountPolicy.calculate(amount) } Factory 先ほどのinterface抽出で割引ポリシー自体は分離できたものの、getDiscountAmount()ではまだ「割引ポリシーを生成する」という責任を持っています。これもまた別の役割として分離ができるでしょう。ここは以下のように割引ポリシーを生成するFactoryを定義しておくと良いでしょう。\nobject DiscountPolicyFactory { fun getDiscountPolicy(code: String): DiscountPolicy { return when (code) { \u0026#34;Facebook\u0026#34; -\u0026gt; FacebookDiscountPolicy() \u0026#34;Twitter\u0026#34; -\u0026gt; TwitterDiscountPolicy() \u0026#34;Instagram\u0026#34; -\u0026gt; InstagramDiscountPolicy() else -\u0026gt; DiscountPolicy.NONE } } } 最終的に、getDiscountAmount()は以下のように修正できます。interfaceの抽出やFactoryの作成でコードの量は増えましたが、この関数の責任はより軽くなり、割引ポリシーの追加や修正が必要な場合でも柔軟な対応ができるようになりました。\nfun getDiscountAmount(code: String, amount: Int): Int { val discountPolicy = DiscountFactory.getDiscountPolicy(code) return discountPolicy.calculate(amount) } Enum 割引ポリシーを生成するためにFactoryを使う代わりに、Enumを使うこともできます。先ほどのDiscountPolicyを継承して、クラスではなく列挙定数として扱う方法です。たとえば以下のようなものが定義できます。\nenum class DiscountPolicies(private val code: String) : DiscountPolicy { FACEBOOK(\u0026#34;Facebook\u0026#34;) { override fun calculate(amount: Int): Int = 10 percentOf amount }, TWITTER(\u0026#34;Twitter\u0026#34;) { override fun calculate(amount: Int): Int = 15 percentOf amount }, INSTAGRAM(\u0026#34;Instagram\u0026#34;) { override fun calculate(amount: Int): Int = amount.coerceAtLeast(1000) }; companion object { fun fromCode(code: String): DiscountPolicy { return values().find { it.code == code } ?: DiscountPolicy.NONE } } } 上記のEnumを利用する場合、getDiscountAmount()は以下のようになるでしょう。\nfun getDiscountAmount(code: String, amount: Int): Int { val discountPolicy = DiscountPolicies.fromCode(code) return discountPolicy.calculate(amount) } Enumの場合、fromCode()でcodeによる分岐そのものが必要なくなり、割引ポリシを追加したい場合でも列挙定数を追加することで拡張も容易にできるのでFactoryよりも良い方法ではないかと思います。\n最後に 最初に断っておいたように、全てのifをなくすのは不可能に近い話で、そうする必要もありません。しかしそのifが行っている処理の本質、責任、可読性のような要素は注意深く観察する必要があり、最初はifでとりあえず動くコードを作ったあとは他の方法で改善できるかどうか振り返ってみる必要はあるかなと思います。\nこういう自分も常に綺麗なコードを書いているわけではないのですが、たまにはこうやって初心者の気持ちで自分の書いたコードを振り返ってみるという姿勢は常に必要なのではという気持ちにもなります。良いコードを書くのは常に難しいものですね。でも、難しいことを最初にやっておいた方が後に後悔しないことにもつながるだろうと思います。\nでは、また！\n","date":"2022-11-20T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-if-to-non-if/","title":"ifでの分岐を考える"},{"content":"世の中にはさまざまなプログラミング言語があり、それぞれの特徴も明確で、言語ごとにできる・できないことも違うケースが多いですね。企業ならエンジニアの採用や費用など現実的な観点から技術選定をするので、プロジェクトにおいてどの言語を使うかは明確かつ一般的な基準があるかと思います。しかし、個人のレベルだとチームでの作業を考慮すべきでもなく、その人の好みや慣れというものから言語を選ぶ傾向があるのではないかと思います。なので、割とマイナーな言語やフレームワークを使うケースもあるでしょう。\n自分がまさにそうであって、個人的に使うために実装するアプリや自動化のスクリプトなどは、なるべくKotlinやPythonで作成しています。特にKotlinの場合、仕事でも使っているので最も慣れているからでもありますが、さまざまなフレームワークや言語自体の特徴によりサーバサイドというジャンルやJVMという環境に限らずいろいろなことにチャレンジできるのが魅力的で好きです。\nというわけで、今回もプライベートでちょっと変わった形でウェブアプリを一つ作ってみた、という話です。どこが変わっているかというと、表題にも書いてある通りですが、「Kotlin」だけでファイルサーバのアプリを実装した話となります。\n背景 まず、どんなアプリをなぜ作ったかから述べないとですね。私の実家には、以前から使っていたWindowsのパソコンがあります。組み立てたのはおよそ8年ほど前のことで、最近は自分が実家に帰ることも少ないのであまり使われてないです。ただ、今のPC(Mac)からファイルを送ったりもらったりして使うことがあります。\nここでファイルのやりとりには、今までMicrosoft社のOneDriveを使っていました。片方で必要なファイルをOneDriveのフォルダにコピーしておくと、そのファイルがクラウドにアップロードされ、自動的に同期される方式ですね。これでも問題は全然なく安定的に使ってはいましたが、ふと思うとクラウドを経由するというステップが無駄だという気がしました。また、同期の前後でファイルをコピーしたり移動したりすることもめんどい作業になっています。\nここで、自分でインターネット越しでファイルのやり取りができるアプリを作ってみたらどうかと思ったわけです。すでに自分が思っているような機能を提供している何らかのサービスはあるかもしれませんが、そこまで複雑なものでもないので、数日で作れるような気がしましたのでとりあえずチャレンジしてみることにしました。（SFTPというオプションもありましたが、GUIで楽にしたかったので却下です）\n要件 さて、作りたいものがあったらやることは決まっています。いつものことですが、アプリを作る前に簡単に要件を決めておきます。まず、機能的には以下のようなことができれば良いかなと思いました。\nサーバアプリを起動すると、クライアントからサーバのストレージにアクセスできる サーバのパスを指定したらその中身（ファイルとフォルダ）が見える フォルダをクリックすると表示中のパスが変わる ファイルをクリックするとダウンロードできる パスにファイルをアップロードできる 機能が決まったらそれを実現するための技術の番ですね。ここでは、何よりもKotlinで全て解決したい！という考えで、技術選定は全てKotlinを中心にしています。\nまずFrontendでは、ちょうどこないだCompose for Desktopで簡単なアプリを作ったとこのブログに書いたことがありましたが、Compose for Webというものもあったので、今回はこれを使ってみるとどうかなと思いました。これに関しては言語を統一したいという理由が最も大きいのですが、他にはFrontendの経験や知識があんまりないので少しでも触れてみた技術を使いたかったという理由もあります。\nBackendのフレームワークはKtorにすることとしました。普段はSpringをやっているのでこちらの場合もあまり本格的な経験があるわけではありませんが、以前触れてみた感覚だとアプリの起動がはやく、実装も簡単だったので採用。また同じく、最も大きい理由はKotlin用ということです。\n大きくはこの二つで、他にも当然色々とライブラリなどが必要となるわけですが、ここは実装を進めながら必要なものがKotlin製かJetBrainsのものかを基準に選んで実装することにしました。もしくは実装において参考となるだろう公式のドキュメントに出てくるものを採用するという方針です。\nFrontend Frontendでは、先に述べた通りCompose for Webを使いました。やはり初めてということもあったのですが、まだ新しい技術だったり、そもそも自分がFrontendに対してあまりわかってないということもあったので最も工数がかかった部分です。ここについては、肌で感じたことを良かった点・思ったことと違った点・問題だった点という三つの軸で分けて述べていきたいと思います。\n良かった点 良かった点としては、やはりComposeでデスクトップアプリを作ってみた経験を活かした実装ができたというところです。ComposeではrememberとMutableState\u0026lt;T\u0026gt;を組み合わせて状態を管理したり、@Composableをつけた関数の単位で画面の構成要素を分けて実装することができますが、ここでもそれは同じでした。\nなので、「指定したパスをブラウズ」する機能を実装した時、「一つ前のパスに戻る機能を追加したいな」と思ったときはそのパスを保持するために状態にパスを持たせたり、サーバから取得したパスの中身のオブジェクト（ファイルやフォルダなど）を画面に描画するためのコンポーネントを一つの@Composable関数として定義して使ったりなどが思ったよりも簡単にできたわけです。\n他にもKotlinなのでCoroutineが簡単に使えたり、サーバサイドと同じリポジトリにソースコードを作成できるというところも良いところでした。特に後者の場合、GradleでKotlinのプラグインをmultiplatformにすることでFrontendではJavaScriptにコンパイルされ、サーバサイドではいつも通りJVMのバイトコードにコンパイルされるようにできるという点がお気に入りです。\n思ったことと違った点 自分の考えが甘かったのですが、Desktopとはかなり違うところがありました。何かというと、言語としてはKotlinを使うとしても、HTMLやCSSを排除することはできないという点です。ここでもやはりdivやformのようなタグを使ったり、タグにマウスオーバ時のカーソルを変えるためにタグのattrを変更する必要がありました。例えば、以下はファイルアップロードのコンポーネントですが、Kotlinで書いているだけで実際はHTMLをそのまま書いているような感覚です。\n@Composable private fun FileUploadForm(currentPath: String) { Div { Form( action = \u0026#34;$API_URL$ENPOINT_UPLOAD\u0026#34;, attrs = { method(FormMethod.Post) encType(FormEncType.MultipartFormData) target(FormTarget.Blank) } ) { HiddenInput { name(\u0026#34;target\u0026#34;) value(currentPath) } Input(InputType.File) { name(\u0026#34;file\u0026#34;) } Input(InputType.Submit) } } } ここは完全に他のプラットフォームでのComposeを使うというよりは、Kotlinようにラップされたクラスを提供するだけという印象が強く、やはりある程度Frontendの知識が必要となる部分ではないかと思っています。なので、ReactやVue.jsなどメジャーなFrontendのフレームワークの知識がある場合にはあまりComposeを選ぶ理由はなさそうな気がしています。\n他には、いつもとは違ってKotlin/JSとKotlin/JVMが共存するプロジェクトとなっているためか、intellij上の自動補完やビルド時の挙動が少し違う感覚があります。例えば、Gradleで依存関係を変更してもすぐに反映されなかったり…\n問題だった点 意外と問題になったのは、プロジェクトのビルドでした。Compose for Webではindex.htmlファイルとWebpackなどを使ってビルドされたjsファイルを使うことになり、ビルド自体はGradleのコマンドひとつで簡単にできるものですが、どうやら内部的にyarnなどを使っているようですが、intellijで生成したプロジェクトのデフォルト設定ではビルド時にエラーが出ることが多かったです。\n調べてみると自分のようなエラーが出る場合、ビルドできない場合はKotlinのバージョンがv1.6.20以降だと解消されるらしいのですが、問題はComposeのバージョンでした。このアプリを実装した時点の最新はv1.1.1なのですが、これだと対応しているKotlinのバージョンがv1.6.10までです。なので、自分の場合はv1.2.0のベータ版を使ってKotlinのバージョンをv1.7.10にしてから解消できました。これはマイナーなプロジェクトのハマりどころと言えるものかもしれないですね。\nまた、HTTPクライアントとしてはKtor Clientを使っていますが、大容量のファイルをアップロードする場合を想定してformタグでMultipartのデータを直接送るよりHTTPクライアントを使う方法を取ろうとするとうまくいかなかったです。Ktor ClientはMultiplatform対応のものなので、クライアントの宣言時にどのEngineを使うかを選択できるのですが、Kotlin/JSで使えるEngineだと公式で紹介している内容通りに実装してもFileオブジェクトを直接扱えないので送信ができませんでした。ここは今後の改善に期待するか、Websocketなどを使うなど他の方法を取る必要がありそうです。\nBackend 次にBackendですが、こちらは自分の慣れている分野で、Ktor自体については他のポストでも述べたことがあり、技術的な面の話よりはロジック面で試行錯誤をしたことを中心に述べていきたいと思います。\nファイルツリーのブラウズ このアプリにはまずファイルをブラウズする機能があるので、クライアントで指定したパスを探索して、その中にあるコンテンツ（ファイルとフォルダ）を返す必要があります。問題は、JSONの構造をどうするかですね。ここではまず一つの方法を試してみてから判断することにしました。\n全取得する 最初は、以下のような形で実装をしようと思いました。パスを指定したら、その配下にある全てのフォルダをたどり、親子関係をネストで表現する形です。\n{ { \u0026#34;name\u0026#34;: \u0026#34;Documents\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;directory\u0026#34;, \u0026#34;children\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SomeDocument.pdf\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;1391482\u0026#34;, \u0026#34;mimeType\u0026#34;: \u0026#34;application/pdf\u0026#34; } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Images\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;directory\u0026#34;, \u0026#34;children\u0026#34;: [] } } このようなファイルツリー返すために、サーバ側のコードは以下のようなものを使いました。\n// ルートとなるパスを指定すると、子要素（ファイルとフォルダ）を全て取得する val files = Files.list(root) .filter { !it.isHidden() } .map { it.toFileTree() } .toList() // PathをJSONオブジェクトとして加工する fun Path.toFileTree(): FileTree { return FileTree( name = this.fileName.toString(), size = if (this.isDirectory()) null else this.fileSize(), type = if (this.isDirectory()) FileType.DIRECTORY else FileType.FILE, children = if (this.isDirectory()) { Files.list(this) .filter { !it.isHidden() } .map { it.toFileTree() } .toList() } else { null } ) } Files.walk()を使うと、指定したパスを基準にネストされているファイルツリーを全てStream\u0026lt;Path\u0026gt;として取得してくれますが、それだと上記のJSONの形として加工するのが簡単ではないです。一度取得した結果をもとに、親子関係を追跡しながらJSONオブジェクトとしてまとめるにはかなり複雑な処理になるっでしょう。\nなので、ここではFiles.list()を使って指定したパスに含まれた要素を取得し、その要素がディレクトリの場合はさらに子要素として取得するように再帰を使って子要素を再度取得するという形としてまとめています。単純な処理ですが、効率的になりましたね。\nただ、この方法で思った通りのファイルツリーをJSONとして返すことはできましたが、問題がありました。まず、指定したパスがルートに近くなればなるほど探索にかかる時間が長くなり、レスポンスも遅く慣ればJSONのサイズも大きくなるという問題がありました。また、JSONを受け取ったところで、Frontendで描画をするにも難点がありそうな気がしました。というわけで、この案はまず廃棄して、二つ目の方法を取ることにしました。\nネストさせない 次に試した方法は、指定したパスのみに処理を制限することでした。何かというと、JSONオブジェクトのネストを無くして、指定したパスにどんなファイルとフォルダが含まれているかだけをリストとして返すということです。つまり、以下のような形になります。\n[ { \u0026#34;name\u0026#34;: \u0026#34;Documents\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;directory\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Images\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;directory\u0026#34; } ] こうなると、中のフォルダを辿る必要がなくなるのでレルポンスも早く、軽くなるわけです。最初からちゃんと考えるべきだったのですが、こちらの方がFrontendとしても実装が楽であって、さらにサブパスのフォルダにアクセスしたい場合はそのパスを再度Backendに送ればいいだけですね。コードとしては再帰を使わなくなったくらいです。\nval files = Files.list(root) .filter { !it.isHidden() } .map { it.toFileTree() } .toList() // PathをJSONオブジェクトとして加工する fun Path.toFileTree(): FileTree { return FileTree( name = this.fileName.toString(), size = if (this.isDirectory()) null else this.fileSize(), type = if (this.isDirectory()) FileType.DIRECTORY else FileType.FILE ) } ファイルアップロード ファイルアップロードについては、Mutlipartとして送られているデータをどう扱うかですが、これはKtorらしく簡単な処理で対応できました。以下のコードが実際の実装となっています。\n// router post(ENPOINT_UPLOAD) { // Multipartデータを受信 val multipart = call.receiveMultipart() // ファイル保存先のパス var path = Path.of(ROOT_DIRECTORY) multipart.forEachPart { part -\u0026gt; when (part) { // ルートでないパスを指定した場合は保存先を更新 is PartData.FormItem -\u0026gt; { if (part.name == \u0026#34;target\u0026#34;) { path = path.resolve(part.value) } } // ファイルデータを保存 is PartData.FileItem -\u0026gt; { withContext(Dispatchers.IO) { val file = Files.createFile(path.resolve(part.originalFileName!!)) part.streamProvider().use { input -\u0026gt; Files.newOutputStream(file).use { output -\u0026gt; input.copyTo(output) } } } } // どちらでもない場合は一旦出力 else -\u0026gt; { println(\u0026#34;Unknown part: $part\u0026#34;) } } // 処理の終わったデータはdispose part.dispose() } } ただ、個人的にはストレージアクセスのある処理に対してはNIOを使いたいので、はじめはFiles.copy()を使おうと思ったのですが、なぜかファイルの保存処理を以下のような作成するとうまくいかなったです。Coroutineとの相性に何か問題があるのかもしれないですので、ここは注意ですね。\nval file = Files.createFile(path.resolve(part.originalFileName!!)) Files.copy(part.streamProvider(), file) // ファイルが保存されない ファイルダウンロード ファイルダウンロードの場合も、ロジックは特にないので、ほとんどKtorのみのコードとなっています。自分の好みでPathを使っているくらいですのでコードだけを紹介します。ひとつ注意すべきところは、アップロードする時もそうですが、ファイル名を返すときにURLパスとしてエンコードすることですね。\nget(ENDPOINT_DOWNLOAD) {try { val filepath = call.request.queryParameters[\u0026#34;filepath\u0026#34;] ?: \u0026#34;\u0026#34; val path = FileService.getFullPath(filepath) // ルートディレクトリからのフルパスを取得 if (Files.notExists(path)) { call.respond(HttpStatusCode.BadRequest, \u0026#34;File not found\u0026#34;) } else { call.response.header( name = HttpHeaders.ContentDisposition, value = ContentDisposition.Attachment.withParameter( key = ContentDisposition.Parameters.FileName, value = path.fileName.toString().encodeURLPath() ).toString() ) call.respondFile(path.toFile()) } } 注意すべきところ まず、一つのプロジェクトにKotlin/JSとKotlin/JVMを両立する場合、dependenciesとして記述するものに対してはbuild.gradle.ktsファイルで以下のように指定することができます。\nkotlin { sourceSets { // Kotlin/JSの依存関係 val jsMain by getting { dependencies { implementation(compose.web.core) implementation(compose.runtime) // ...省略 } } // Kotlin/JVMの依存関係 val jvmMain by getting { dependencies { implementation(\u0026#34;io.ktor:ktor-server-core-jvm:$ktor_version\u0026#34;) implementation(\u0026#34;io.ktor:ktor-server-auth-jvm:$ktor_version\u0026#34;) // ...省略 } } } } しかし、Composeを使うためにはpluginとして指定する必要があり、これがプロジェクト全体の依存関係に追加されることになっていました。なので、アプリの作りとしてはまずFrontendのComposeをビルドし、サーバを起動したらビルドしたファイルをstaticとして提供する構造になっていますが、Backendの起動にもComposeのランタイムが必要になります。このランタイムを追加してくれないと、エラーが吐き出され、Ktorが起動できなくなっています。何かKotlin/JSのみの依存関係にpluginを追加する他の方法があるかもしれませんが、とりあえずはJVMの依存関係に以下のようにランタイムを追加することで問題は解消できました。\nval jvmMain by getting { dependencies { implementation(\u0026#34;io.ktor:ktor-server-core-jvm:$ktor_version\u0026#34;) implementation(\u0026#34;io.ktor:ktor-server-auth-jvm:$ktor_version\u0026#34;) // ...省略 implementation(compose.runtime) // Composeランタイム } } その他 Kotlin/JSとKotlin/JVMを一つのプロジェクトとして扱う場合に、commonというパッケージを設けることで、コードの共有ができるのが何より嬉しかったところです。例えば、JSONオブジェクトをdata classとして定義してcommonパッケージに置くことで、FrontendとBackendの両方で同じオブジェクトを使うことができます。他にももちろんEnumやconstを共有できたりするので、実装がかなり楽でした。\nまた、今回は採用しなかったのですが、Ktor Serverの場合Type-safe Routingというものに対応しているので、うまく活用できたらかなり良さそうな気がしました。これはKtor ClientでもType-safe Requestとして対応しているので、FrontendとBackend両方で使える機能です。またKtorを使う機会があったら、ぜひ使ってみたいと思っています。\n最後に ファイルアップロードが思った通り改善できなかったので、アプリの完成はまだ少し先のことになりそうですが、かなり面白い経験となりました。Kotlinでできることは色々とあるので、また何か作ってみたいものがあればチャレンジしてみたくなります。ただ、やはりまだ成熟してない技術なので、思ってもなかったところで問題が発生したりリファレンスがあまりないという点ではまだプロダクションレベルでは使えないものかなという気がします。\nアプリ全体のコードはGitHubにて公開していますので、こちらから参照できます。\nでは、また！\n","date":"2022-10-10T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-compose-web/","title":"Kotlinだけでファイルサーバを作ってみた"},{"content":"バックエンドの開発をしていると、テストの自動化では対応できない場合もありますね。理想的なシナリオとしては、ユニットテストから全てのシナリオを想定したインテグレーションテストまでを全部作成でき、開発・企画に属するものがそれらを理解しきっていることだろうとは思いますが、現実ではなかなか難しいものです。特に、サービスが成長していきながら技術的負債を解消しようとしたり、新しい機能を足したり、昔は対応できなかった改修が必要となったり、運用上のイレギュラー対応が必要となったりなどで最初の仕様は変わり続けていき、改修を行うエンジニアや運用する側でも今の状態がどうで、どう変わるべきかを判断するのは難しくなりがちなのだから、と自分は理解しています。\nなので、少なくとも今のアプリがちゃんと想定通りに動くかどうかを人の手で検証する必要が出てくるケースも十分にあり得るものです。そしてそうなった場合はどうやってテストを行うかを考える必要もありますね。テストの方法も色々あり、小さい機能単位でユニットテストを行い、最終的にはインテグレーションテストやシナリオテストまで上がっていくと良いはずですが、その全部を自動化するのが難しいケースもあるかと思います。例えばテストするためのデータのパターンを色々と用意する必要があったり、エンジニアが完全に仕様を把握してなかったりなどの場合もありますね。なので、人の手によるテスト（モンキーテスト的な）が必要となる場面も存在すると思います。\n今回はその「人の手によるテスト」を手伝うために、テストツールを作った話です。自分の扱っているシステムはマイクロサービスの一つであり、業務仕様が複雑でさまざまなパターンで機能をテストする必要がありました。なのでエンジニアとしては実装を進めながら、同時に業務仕様に詳しい人にさまざまなパターンのデータを使ってテストができるツールを作ることになったわけです。\n目標と設計、技術選定 実は以前から、リポジトリにはすでにテストツールが存在していました。しかし調べたところ、作られて2年以上放置されていて、Ruby on railsという自分が全く触れたことも（興味を持ったことも）ないフレームワークで作られているという問題がありました。これだと、自分がRubyを勉強して既存のツールを改修するという手もあったかもしれませんが、以下の理由から一から作り直そうと思いました。\nKotlinエンジニアがRubyアプリをメンテするのは良くない ドキュメント化が進んでなく、使い方が不便 そう決めてからは、テストを行いたい側（企画）からの要請を受け、ツールに要求される仕様としての機能をまとめることに。テストが行えるツールという確実な目標があったので、仕様は極めて単純です。要求事項としてツールに揃うべき機能は以下のようなものでした。\nテストデータのファイルを読み込ませる バックエンドのアプリのAPIを呼び出す APIの実行結果をファイルに書き込む テストツールとしては上記に挙げている要求事項を満たしているならテストツールとしては合格というわけです。しかし、実際のテストを行いたい側がまずエンジニアではなく、今後もエンジニアではない人がツールを触る可能性があります。そこまでを考慮して、以下の追加的な目標を立てました。\n環境構築をしなくても使えるようにする 手順書がなくても使えるくらい簡単なものに仕上げる ここまで決めたら、次に要求されている機能の細部を掘り下げていきます。設計書を書くほどでもないですが、土台となる設計のようなものです。\nデータの読み込みと書き込み ツールを使える人はSQLが使える=テーブル（表）が読める テーブルの形でデータの入出力ができた方がわかりやすい テストデータはCSVで読み込む API実行結果もCSVに書き込む APIコールができる HTTPクライアントでGET・POSTする APIコールにはトークンが必要 トークンはセキュリティ問題でソースコードに埋め込むのはNG しかし毎回入力するのはめんどくさい アプリを実行して最初はトークンを入力し、次回からはそのトークンを使い回すようにする 本番以外の環境が対象 複数の環境があるのでどれかを選択できるようにする これも毎回入力はめんどくさい 最初に一回だけ選択できるようにしたい そして自分の立てた目標を、上記の要求事項を達成できるかどうかを考えながら振り返ってみます。環境構築をしなくても使えるなら、実行可能な一つのバイナリとして提供した方が良いでしょう。また、使い方が簡単な方だと、やはりGUIを含めた方が良いですね。特にGUIを採用したら要求事項に対してもかなり良い感じで機能を完成できると思いました。\n例えばファイルの読み込みや書き込みにはパスの指定が必要で、トークンと環境の選択も入力が必要な項目で、CLIだとやはり不便です。エンジニアではない人が触るとしたら尚更ですね。Windowsユーザなら、コマンドラインも考慮しなければならないかもしれません。その反面、GUIだとファイルパスならダイアログ、トークンの入力もテキストポックス、環境の選択ならプールダウンメニューで対応できます。なので、「バイナリの実行で起動できるGUIのアプリを作る」という結論を出しています。\nCompose for Desktop テストツールの仕様と技術的な要件が決まったら次は技術選定になりますがね。まずどの言語を使うかについてですが、自分以外でも同じチーム、つまりKotlinエンジニアがこれからもメンテを行うことになる可能性が高いのでKotlinにしました。Kotlinを使うことで、機能の実現で必要なライブラリの選定も楽になりますね。すでにテスト対象のバックエンドアプリで使っているHTTPクライアントがあるので、一部のコードはそのまま移植しても良いはずです。また、同じライブラリを使うことでメンテもより簡単になるでしょう。\nあとはGUIですが、今回はCompose for Desktopを使うことにしました。KotlinはJavaと互換性があるので、当然SwingやJavaFXなどJavaのGUIツールキットをそのまま使うという選択肢もあります。他にもTornadoFXという選択肢があったりもしますが、今回あえてComposeを選んだのはいくつかの理由があります。\nまずは個人的にモバイルに興味があって以前から興味を持っていたので、今回本格的にこれでアプリを作ってみたいという願望もありましたが、今後もKotlinエンジニアの手でメンテが行われるとすると、やはりモバイルの経験があるか、少なくとも興味を持つ方が多いだろうという点です。Composeはまだ正式リリースされて1年ほどしか経ってない新しいものですが、最近流行っているいわゆる「宣言型」のフレームワークなので、少なくともAndroidアプリの開発ではメインストリームになる可能性が高いだろうという判断からでした。\nまた、Composeはモバイルのみでなく、そもそもマルチプラットフォーム向けに開発されたものなので、Windows/Mac/Linuxの環境を問わず実行可能なバイナリをビルドできるという点でも魅力的だったです。これならテスターがどんなOSを使っていても同じ感覚でツールを使えて、\nただ、やはり今まであまり接したことのない技術なので勉強はもちろん試行錯誤などもあったので、テストツールを作りながらこれは覚えておいた方が良いなと思ったところをいくつか挙げてみようと思います。\n状態管理 SwiftUIのポストの時も触れた状態管理ですが、Composeでも同じくGUIを扱うことになるので、状態管理が大事となります。今回はアプリとしての画面がひとつしかないので、複数の画面にまたがって状態を管理する必要はないかなと思いましたが、それでもやはり処理を行うためにはアプリ全体で共有する状態として管理が必要なものがいくつかありました。\nただ、上記ポストでも述べた通り、SwiftUIとComposeとは状態管理の方式が少し違います。SwiftUIでは状態がどこで使われるかによって明確に使われるアノテーションやクラスなどが変わっていたなら、Composeでは大体remember()とMutableState\u0026lt;T\u0026gt;の組み合わせで事足りることになります。画面の構成要素の最小単位をComposeではWidgetでも使い方が同じだということは、SwiftUIと比べると定義するのは簡単ですが、使い方には少し注意が必要だなという感覚でした。\nまず、Composeでの状態は、以下のような三つの方法で定義することができます。\n// Delegateで定義する var isOn: Boolean by remember { mutableStateOf(false) } // 直接値を書き換えできる isOn = false // 分解宣言で定義する val (isOff: Boolean, setIsOff: (Boolean) -\u0026gt; Unit) = remember { mutableStateOf(true) } // 参照と更新が分離される if (isOff) { setIsOff(!isOff) // toggle } // MubtableState\u0026lt;T\u0026gt;として扱う val isNotOff: MutableState\u0026lt;Boolean\u0026gt; = remember { mutableStateOf(false) } // ラッパーになっているので、値を更新するためにはvalueにアクセスする必要がある isNotOff.value = !isNotOff.value ここでDelegateでvarとして定義した場合は最も使いやすくなりますが、Intellij上ではコンパイルエラーになりがちです。なぜかというと、Delegateを使うためにはandroidx.compose.runtime.setValueとandroidx.compose.runtime.getValueをimportする必要がありますが、これが自動で行われないからです。最初このエラーの理由がわからなかったり、忙しい場合にいちいちimport文を書いていくが面倒だったりでかなり使わなくなるケースも多いかなという気がします。ただこれは、まだIntellijでのCompose対応が完璧ではないのが原因なので、これはいずれ解消されると期待できるでしょう。\n分解宣言で値の参照と更新を別々で使うのは、どこで使うか悩ましいケースもあるかなと思いますが、Composeの一部Widgetに状態を渡す場合に使われる場面があります。代表的なものがTextFieldで、これはコードを見るとすぐにその目的がわかります。実際のコードで、以下のように使われます。\nval (text: String, updateText: (String) -\u0026gt; Unit) = remember { mutableStateOf(\u0026#34;\u0026#34;) } TextField( onValueChange = setValue, // TextFieldに文字を入力するとその値でtextを更新する value = text // textの値をTextFieldに表示する ) 最後にMutableState\u0026lt;T\u0026gt;として定義するケースですが、直接的な値の更新ができないので最も使い方としては不便なのですが、実際は最も多く使われるのではないかと思います。なぜかというと、アプリ全体で状態を共有するなど複数のWidgetをまたがって使う場合は、以下のようにclassの中にフィールドとしてMutableState\u0026lt;T\u0026gt;を定義することになるからです。\n// アプリ全体で共有するためにクラスに状態を定義する class AppState( val isOn: MutableState\u0026lt;Boolean\u0026gt; ) これはもちろん別途getter/setterをclassに定義しておくと、中のvalueにアクセスしなくても直接プロパティにアクセスする感覚で使えます。イメージ的には以下のようなものですね。これだと状態として管理したい項目が増えれば増えるほどコードの量が増えてしまう面倒さがあるというのが短所かなと思います。\nclass AppState( private val _isOn: MutableState\u0026lt;Boolean\u0026gt; ) { var isOn: Boolean get() = _isOn.value set(value) { _isOn.value = value } } このように、Composeでの状態には定義する方法が色々あり、それぞれの特徴があるわけなのでどの場面で使うかによって適切な定義の方法を考えるのが何よりも大事だという印象です。\nSwing/AWT Compose for Desktopの特徴の一つは、SwingやAWTに対する互換性があるという点です。最初はMacのトレイ、メニューバー、通知にも対応していたので基本的な機能は全て揃っているのではないかと思いましたが、実はそうでもなく、一部の機能はSwingやAWTの機能を借りて実装することになるケースもありました。実際、私の作ったテストツールでも一部SwingとAWTの機能を使っているところがあります。\n例えばファイル選択機能がそうです。CSVを読み込むためにファイル選択のダイアログを表示したかったのですが、ComposeのWigdetではまだ対応できてないので、やむを得なくAWTのFileDialogを使う必要がありました。以下がその実装の例です。\n// 選択したファイル名を状態として保持する var fileName by remember { mutableStateOf(\u0026#34;\u0026#34;) } // AWTのファイル選択ダイアログを使う FileDialog(ComposeWindow()).apply { // 選択できるのはCSVのみにしたい setFilenameFilter { _, name -\u0026gt; name.endsWith(\u0026#34;.csv\u0026#34;, ignoreCase = true) } isVisible = true // ファイルが選択された場合は状態を更新する if (!file.isNullOrBlank()) { fileName = file } } しかし、これでも十分ではない場合もありました。フォルダのみを選択できるようにしたい場合にFileDialogはあまり良い選択ではなかったのです。名前からそうですが、あくまでファイルの選択を想定したものであるため、フォルダのみを選択できるようにはできなかったのです。なので、フォルダのみを選択できるようにするためには、Swingの力も借りる必要があります。その場合は、以下のように実装ができます。\n// 選択したフォルダのパスを状態として保持する var selectedPath by remember { mutableStateOf(\u0026#34;\u0026#34;) } // Swingのファイル選択ダイアログをディレクトリのみ選択できるように設定して定義する val fileChooser = JFileChooser().apply { dialogTitle = \u0026#34;Choose Directory\u0026#34; fileSelectionMode = JFileChooser.DIRECTORIES_ONLY } // ダイアログを表示する if (fileChooser.showOpenDialog(ComposeWindow()) == JFileChooser.APPROVE_OPTION) { // ダイアログで選択したパスが状態として保持しているパスと違う場合、選択したディレクトリの絶対パスを状態として更新する val path = fileChooser.selectedFile.absolutePath if (selectedPath != path) { selectedPath = path } } 今回はこの二つユースケースのみSwingやAWTが登場していませんでしたが、どんなアプリを実装するかによって他のAPIも色々と使う必要性が出てくるかも知れないという良い一例になっている気がします。まだComposeはリリースされて1年ほどしか経っていないので、今後のバージョンアップでより多彩なWidgetが追加されることに期待ですね。\nビルド Composeを選んだ理由の一つのバイナリのビルドができるという点ですが、これはかなり満足度が高かったです。gradleを使って、コマンドひとつで実行可能なバイナリが生成されます。Macでビルドして見ると、他のアプリと同じくパッケージが生成されます。中を見ると、実行に必要なJREと依存関係のJarが含まれていて、ネイティブではなくJVM上で起動される構造になっていました。\nバイナリをビルドするときのオプションには色々なものがあり、OSの種類(Windows, Mac, Linux)によって違うアイコンを使ったり、基本的には含まれないモジュールを含むように指定したりすることができました。以下が実際のビルド時のオプションのサンプルです。\ncompose.desktop { application { mainClass = \u0026#34;com.testtool.MainKt\u0026#34; // 実行時のメインクラスを指定 nativeDistributions { packageName = \u0026#34;Test Tool\u0026#34; packageVersion = \u0026#34;1.0.0\u0026#34; modules(\u0026#34;java.sql\u0026#34;, \u0026#34;java.naming\u0026#34;) // デフォルトでは含まれないパッケージを追加 macOS { iconFile.set(project.file(\u0026#34;misc/appicon/icon-mac.icns\u0026#34;)) } windows { iconFile.set(project.file(\u0026#34;misc/appicon/icon-win.ico\u0026#34;)) } } } } ただ、ビルド時は注意が必要です。ビルドするとき、Composeでは内部的にjpackageを使うので、まずJava 15以上が必要となります。また、CPUのアーキテクチャによって違うJDKをインストールするようになっているため、ビルドするマシンと違うのアーキテクチャのCPUを使っているマシンをターゲットにすることはできません。\nつまり、自分の使用のMacだとApple Silicon用のバイナリが生成され、IntelチップのMacだとx64用のバイナリが生成されるということです。実際ComposeでApp Storeにアプリを提出した人もいるらしいのですが、Rosettaで起動できるということでIntelチップのMacを使ってビルドしているとのことでした。Universal Binaryを作りたい場合は、JDKそのものがまずUniversal Binaryとして提供されることを待つしかなさそうです。\n最後に 今回はデスクトップアプリの中でもかなり制限された機能と単純なロジックしかないシンプルなものを作ったので、もしこれからまたComposeを使ってさまざまな機能を持つように実装するとしたら(マルチウィンドウやダークモード対応、ナビゲーションなど)また色々と発見があるかも知れない気がしています。個人的にはかなりためになる経験で、思ったより実装もそこまで難しくなかったので、ツールの機能を拡張するか新しいツールを作ってみるチャンスがあるとしたら再度Composeを使ってみたいなと思いました。\nまだリリースされてからそう長くもなく、足りない機能や情報も多かったり競合のフレームワークが色々とあるので未来はどうなるかわからないものですが、自分のようにKotlinをメインとしているエンジニアで、GUIに興味がある方なら一度はComposeを使って見ることをお勧めしたいですね。\nでは、また！\n","date":"2022-09-09T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-compose-desktop/","title":"Kotlinでデスクトップアプリを作ってみた"},{"content":"アプリを書いていると、DBに保存した形とは違う形でデータを読み込んだり出力するケースが多いです。代表的に、多くのバックエンドアプリで採用しているREST APIがそうですね。APIの入力値と戻り値は、DBに保存されている形とは一致しないケースが多く、必要(リクエストを送信してきた側の都合)に合わせて適切にパラメータを受け取り、レスポンスを返すようになっています。そして時と場合によっては、人間が楽に読める形としてデータをまとめる必要もあります。そういった場合はExcelファイルやCSV、PDFといったいろいろなものを想定できますね。\n今回のポストも場合も人が読める形のファイルとして、特にCSVを扱う場合にKotlin側の実装をどうやっていくかに関するものです。\nKotlinでCSVを扱う KotlinのCSV読み込み/書き込みのライブラリとしてkotlin-csvがあり、JVMだけでなくKoltin/JSの場合でもこのライブラリを使って簡単にCSVを扱えます。更にkotlin-grassというライブラリもあり、kotlin-csvとの組み合わせででCSVのデータを簡単にdata classのListとしてまとめることもできますね。読み込みの際に指定できるデータのフォーマットやカスタムマッピングオプションなどの機能も豊富にあり、かなり使いやすく良いライブラリとなっています。\nしかし、実はkotlin-csvを使うときに問題が一つあります。先に述べた通りCSVにデータの出力そのものは可能なものとなっているのですが、読み込みの時にdata classへのマッピングには別のライブラリが必要であったように、data classのリストを書き込むには追加の処理が必要となります。これは、kotlin-csvの書き込み用のメソッドが以下のようになっているからです。\nfun writeAll(rows: List\u0026lt;List\u0026lt;Any?\u0026gt;\u0026gt;, targetFile: File, append: Boolean = false) { open(targetFile, append) { writeRows(rows) } } ここでrowsが書き込みで使うデータとなりますが、型がList\u0026lt;List\u0026lt;Any?\u0026gt;\u0026gt;になっているので、列のデータを一つの行としてListに定義し、それをさらにListに格納することでCSVのデータ全体を定義する必要があります。これはつまり、data classのリストを書き込むためには、フィールド一つ一つを列として定義し、それらをListとしてまとめる必要があるということです。また、CSVには一般的にヘッダが含まれますが、List\u0026lt;List\u0026lt;Any?\u0026gt;\u0026gt;の形だと最初の行にヘッダのみを定義した行は必要となることでもありますね。\n一見複雑に見えますが、reflectionを利用すると、data classのフィールド名とその値を得ることができますので、それを利用してdata classのListをこのメソッドに適した形に変えられます。これをヘッダを作る方法と、data classの値を行に変更する二つの段階で分けて説明していきます。\ndata classからヘッダを作る まずはヘッダを作ります。ヘッダを作るには、data classからフィールドを取得し、そのフィールドの名前のみを取得するといいでしょう。idというフィールドがあるとしたら、ヘッダもそのままidになるということです。フィールド名とは別の名前をつけたい場合はアノテーションを活用する方法を考えられますが、まずはフィールド名をそのまま使う方法から述べたいと思います。\nKotlinのdata classから、フィールドを取得する方法が3つがあります。まずは、KClass.membersがあります。ただ、これだとメソッドを含め、全てのメンバーを持ってくることになります。以下のようにですね。\ndata class Data(val id: Int, val name: String) val members = Data::class.members println(members) // [val Line_2.Data.id: kotlin.Int, val Line_2.Data.name: kotlin.String, fun Line_2.Data.component1(): kotlin.Int, fun Line_2.Data.component2(): kotlin.String, fun Line_2.Data.copy(kotlin.Int, kotlin.String): Line_2.Data, fun Line_2.Data.equals(kotlin.Any?): kotlin.Boolean, fun Line_2.Data.hashCode(): kotlin.Int, fun Line_2.Data.toString(): kotlin.String] フィールド名が現れているのでこれも使える方法の一つではありますが、やはりdata classだと基本的にequals(), hashCode(), copy(), toString(), componentN()のようなメソッドが できてしまうので、これらをフィルタする必要があります。例えば、以下のようにですね。\nval memberProperties = Data::class.members.filterNot { it.name.contains(\u0026#34;component\u0026#34;) || it.name == \u0026#34;copy\u0026#34; || it.name == \u0026#34;equals\u0026#34; || it.name == \u0026#34;hashCode\u0026#34; || it.name == \u0026#34;toString\u0026#34; } println(memberProperties) // [val Line_2.Data.id: kotlin.Int, val Line_2.Data.name: kotlin.String] しかし、フィルタをしなくてももっと簡単にフィールドのみを抽出できる方法もあります。memberPropertiesを使うことです。\nval memberProperties = Data::class.memberProperties println(memberProperties) // [val Line_2.Data.id: kotlin.Int, val Line_2.Data.name: kotlin.String] ただ、この方法にも問題はあります。取得したフィールドの順番が、data classに定義した通りにならなく、アルファベット順になるということです。以下の例を見ると、nameとageの順で定義したフィールドが、ageとnameの順になっているのがわかります。\ndata class Person(val name: String, val age: Int) val memberProperties = Person::class.memberProperties println(memberProperties) // name, age順にならない // [val Line_11.Person.age: kotlin.Int, val Line_11.Person.name: kotlin.String] ここでどうしてもフィールドを定義した順に取得したい場合、data classのコンストラクタを使う方法があります。まずはコンストラクタを使った方法ですが、これはdata classに、自動的にコンストラクタがフィールドを定義した順番通りのパラメータを持つように生成されるということを利用した方法です。以下のようになります。\nval parameters = Person::class.primaryConstructor!!.parameters.mapNotNull { it.name } println(parameters) // [name, age] 多少強引な方法ではありますが、これでフィールドとして使うフィールド名は取得できました。では、次にアノテーションを使う場合を見ていきましょう。\nアノテーションを使う場合 フィールド名をそのままヘッダとして利用したくない場合は、アノテーションを活用できます。フィールドとしてStringを持つアノテーションを定義して、ヘッダを読み取るときにそのフィールドにつけたアノテーションを取得するという方法です。例えば、以下のようなアノテーションを定義したとしましょう。\n@Target(AnnotationTarget.PROPERTY) annotation class CsvHeaderName(val value: String) アノテーションは、以下のようにdata classで使います。\ndata class Person( @CsvHeaderName(\u0026#34;名前\u0026#34;) val name: String, @CsvHeaderName(\u0026#34;年齢\u0026#34;) val age: Int ) そしてこのdata classのアノテーションを取得していきます。memberPropertiesでフィールドを取得した場合、そのフィールドからアノテーションの一覧(アノテーションは複数存在できるので)を取得し、そこから先に定義したCsvHeaderNameのみをフィルタします。あとはアノテーションがあるかどうかをみて、どちらの値を使うかを決めればいいですね。以下のコードが、そのサンプルです。\nval datas = listOf(Person(\u0026#34;John\u0026#34;, 20)) val headers = datas.first()!!::class.memberProperties.map { property -\u0026gt; val name = property.annotations.filterIsInstance\u0026lt;CsvHeaderName\u0026gt;().firstOrNull() // アノテーションはないケースもある name?.value ?: property.name // アノテーションがnullの場合は、フィールド名を使う } println(headers) // [年齢, 名前] primaryConstructorのパラメータで取得した場合でも、やり方は大きく変わりません。この場合は、コンストラクタのパラメータを基準にループしながら一致するフィールドを探すという処理が追加されるだけです。例えば以下のようにです。\nval fieldNames = seeds.first()::class.primaryConstructor!!.parameters.mapNotNull { it.name } val headers = fieldNames.mapNotNull { name -\u0026gt; // パラメータと一致するフィールドを対象に処理を行う datas.first()::class.memberProperties.find { it.name == name }?.let { property -\u0026gt; val headerName = property.annotations.filterIsInstance\u0026lt;CsvHeaderName\u0026gt;().firstOrNull() headerName?.value ?: property.name } } これで、ヘッダのデータはできました。次は、このヘッダに合わせて下に出力する行としてdata classをListに変換するのみですね。\ndata classをListに変える ヘッダの処理で既にやっていたように、data classをListに変換する場合でも処理は大きく変わりません。違う点は、フィールドから実際のデータを取得するだけですね。ここでは、コンストラクタからパラメータを取得した場合を想定してコードを書きましょう。\nKotlinのreflectionでフィールドの値を取得する場合はJavaと変わらなくて、取得したフィールドにdata classのインスタンスを渡すだけとなります。ただ、フィールドがnullの場合は考慮する必要があります。nullになってしまうと、その列自体がスキップされ、最終的に出力されたCSVのデータで列がずれる場合があるからです。なので、空白のStringを指定するなどで、行ごとの長さ(Listのサイズ)を合わせる必要があります。以下のようにですね。\ndatas.map { d -\u0026gt; fieldNames.mapNotNull { name -\u0026gt; d::class.memberProperties.find { it.name == name }?.let { field -\u0026gt; field.call(d) ?: \u0026#34;\u0026#34; // フィールドの値を取得し、nullのばいは空白にする } } } ただ、ここで時間や日付を扱う場合、フォーマッタを利用したいケースがあるかと思います。例えば、アプリの中ではLocalTimeとして扱っているが、CSVとしてはHH:mmのような形で出力したい場合や、LocalDateをyy/MM/ddにしたい場合などですね。ここでフォーマット自体は、DateTimeFormatterを使うだけですが、問題は取得したフィールドがどの型であるかの判定です。\nKotlinのreflectionで取得したフィールドは、KProperty1という型になっています。ここでどうやって元の型を取得するかが問題ですね。このクラスはKCallableというインタフェースを実装していて、ここにはreturnTypeというプロパティがあります。これでKTypeというインタフェースが取得できるようになるので、これを持って判定をおこ泣くことになります。\nしかし、名前から分かるように、KTypeはKotlinの型に関するインタフェースとなっています。比較したいLocalDateやLocalTimeなどのクラスはJavaのものなので、直接的な比較ができないですね。幸い、JavaのクラスでもKotlinで参照できるKtypeとして変換することはできます。以下のようにです。\nval localDateKType: KType = LocalDate::class.createType() なので、あとは分岐によって適切なフォーマッタを使うだけですね。以下のようにです。\ndatas.map { d -\u0026gt; fieldNames.mapNotNull { name -\u0026gt; d::class.memberProperties.find { it.name == name }?.let { field -\u0026gt; field.call(d)?.let { when (field.returnType) { // タイプによる分岐 LocalDate::class.createType() -\u0026gt; dateFormatter.format(it as LocalDate) LocalTime::class.createType() -\u0026gt; timeFormatter.format(it as LocalTime) LocalDateTime::class.createType() -\u0026gt; dateTimeFormatter.format(it as LocalDateTime) else -\u0026gt; it } } ?: \u0026#34;\u0026#34; } } } ただ、ここでもう一つ注意しなければならないのは、nullableなフィールドのKTypeは別のものになるということです。つまり、上記のコードでは、以下のようなdata classのフィールドは分岐処理でフォーマッタが働かなくなるということです。\n// どのフィールドもフォーマットされない data class DateTimes( val date: LocalDate?, val time: LocalTime?, val dateTime: LocalDateTime? ) この場合は、KTypeを作るときにnullableであることを指定することで簡単に解決できます。あとは分岐で、両方チェックするようにするだけです。以下のようにですね。\ndatas.map { d -\u0026gt; fieldNames.mapNotNull { name -\u0026gt; d::class.memberProperties.find { it.name == name }?.let { field -\u0026gt; field.call(d)?.let { when (field.returnType) { // nullableでもnullableではない場合でも分岐でフォーマットする LocalDate::class.createType(), LocalDate::class.createType(nullable = true) -\u0026gt; dateFormatter.format(it as LocalDate) LocalTime::class.createType(), LocalTime::class.createType(nullable = true) -\u0026gt; timeFormatter.format(it as LocalTime) LocalDateTime::class.createType(), LocalDateTime::class.createType(nullable = true) -\u0026gt; dateTimeFormatter.format(it as LocalDateTime) else -\u0026gt; it } } ?: \u0026#34;\u0026#34; } } } あとは、ヘッダと、data classから取得した値のリストを結合してkotlin-csvのwriteAll()に渡すだけですね。一つ、値は上記のコードで既にList\u0026lt;List\u0026lt;Any\u0026gt;\u0026gt;の形となっていますが、ヘッダはList\u0026lt;String\u0026gt;なので、ヘッダは更にListに入れる必要があります。\n// ヘッダ val header = // 省略 // 実際のデータ val rows = datas.map { /** 省略 */ } csvWriter().writeAll( rows = listOf(headers) + rows, targetFile = targetFile ) これでヘッダが先に行に書き込まれ、次の行からはdata classのフィールドに格納した実際の値が出力されることになりました。\n最後に この度軽く「KotlinなんだからKotlin制のライブラリを使おう」と、軽い気持ちで採用したライブラリが想定していたものと違ったので困っていましたが、幸いJavaを使っていた時にApache POIを使って似たような機能をするライブラリを作ってみた経験があったのでその知識を活かせたと言えます。当時はまだ駆け出しのエンジニア(今もそうと思っていますが)だったので大変苦労した思い出でもありますが、今はその経験があってこそ対処できたようなものなので大変ありがたい経験だったなと思いました。\n上記のコードに対してはちょっとしたライブラリを作ってみたので、またどこかで活用してみたいものですね。色々と改善して、のちにMaven Repositoryのようなところでも公開できるようになったらなと思います。\nでは、また！\n","date":"2022-08-27T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-write-csv/","title":"data classのListをCSVにする"},{"content":"前回に続いて、今回はSwiftUIを触りながら感じたことについて述べたいと思います。自分のように、今までバックエンドの実績しかないエンジニアがGUIを作るとしたら、画面のレイアウトや色合い、画面間の繊維など「今までなかった概念」で混乱することも多いかなと思います。そしてその中でも特に難しい、うまく扱えない機能などもあるかなと思います。\n自分の場合、プロダクションレベルのコードを書いたことはないものの、以前からReact Native, Flutter, Jetpack Composeには少し触れたことがあったためSwiftUIで「画面を構成する方式」については少し理解しているつもりでした。しかし、やはりバックエンドでは存在しない機能がありました。\n今回はそのSwiftUIの話となりますが、中でも私が注目した機能もしくは概念について述べたいと思います。\nSwiftUI まずは、SwiftUIそのものに対して簡単に紹介しましょう。SwiftUIはいわゆる宣言型(Declarative)UIのフレームワークで、ReactやVueなどのフロントエンドからの影響が見えます。要するに、画面を構成する要素(Widget, Component, Materialなどフレームワークやライブラリによって呼び方は様々ですが)を一つのオブジェクトとして「宣言」し、それらの要素の組み合わせによって一つの画面を完成するという形になっているということです。このような宣言型UIはSwiftUIだけでなく、モバイルに限定してもReact NativeやFlutter、Jetpack Composeなど様々なフレームワークやライブラリで採択しているものでもあります。\nそして、要素の実装についてもフレームワークやライブラリによって色々と方法が分かれますが、SwiftUIでは、個別の要素はViewといい、Viewというprotocolをstructとして実装していきます。なので、一覧を表示する画面であれば、一つの行としてデータを表示するView、その行をリストとして表示するためのView、さらに一覧の上や下にメニューを表示するViewなどが一つ一つのstructとして定義されていく形ということです。\nこのような画面の作り方は、フレームワークのパラダイム・コンセプトとして決まってあるものなので、それに従って実装を進めることには私のようにバックグラウンドが全く違うエンジニアでも特に問題はないかなと思います。\nただ、実際のアプリを作るとしましょう。画面に対してはフレームワークで提示する通り要素を実装して作ったとしたら、アプリを実行してなんらかの処理を行うためにはバックエンドを繋げるか、アプリ内でなんらかの処理を行うかなど「ロジック」とつなげる必要があるはずです。ここで常にとは言い難いのですが、バックエンドでは見慣れない概念が出てくる場合があります。「状態」というものです。\n状態 バックエンドのアプリを実装している場合は、リクエストがあり、それに対してのレスポンス（HTTPステータスのみだとしても）がある、という明確なプロセスがありますね。この一連の処理には、「途中で変化する」という概念はありません。この場合のデータは永続化されるか、処理が終了するまでの一時的な物が多いです。\nしかし、画面の世界だと話は変わってきます。多くのモバイルアプリではスライダ、ボタン、テキストボックスなどいろいろな要素で構成されてあり、それらの要素に関する状態が常に変わってくるケースもあるのです。ファイルをダウンロードしている状態を表示するためのプログレスバーがあるとしたら、単純に進行状況を見せるとしたら一つのスレッドを割り当てて処理するだけで良いでしょう。\nここで一つ、もしダウンロードに「一時停止」のような機能があったらどうでしょうか。プロセスが進行中にボタンを押下したら止まり、再度押下したら再開されるような機能です。他にも色々と考慮すべきものはあるかと思いますが、この「停止している」「再開した」という概念をどこかに記憶しておく必要はあるでしょう。つまり、画面においてのユーザの入力に対して、なんらかの処理を実際に行うまで記憶しておくための機構が必要ということですね。\nSwiftUIでももちろん、状態を管理するためのものがあります。ただ、ユースケースで考えると画面の要素(View)一つに限って必要なものか、それとも複数の要素において必要なものか、複数の画面（アプリ全体）にかけて必要なものかという、スコープによって必要な状態は違うものになります。View一つで必要なものをアプリ全体で管理する必要もなければ、複雑になりがちだからですね。なので、ここではそのスコープ別に状態を管理するためにどんなものがあるのかについて述べていきたいと思います。\n個別Viewの状態 まずは最も小さい単位、Viewの場合です。先に述べた、一覧の画面が良い例になりそうですね。Appleのチュートリアルでは、以下のような一覧の画面を作ることになります。\nこの画面では右上のボタンを押すことで、一覧に表示するアイテムをフィルタリングできる機能があります。たとえば一部の行にハートマークがついてありますが、これは「お気に入り」を意味していて、そのお気に入りとして登録したものだけをフィルタするか、アイテムのカテゴリでフィルタするかなどの機能が入っています。\nそしてこの画面では、「フィルタするカテゴリ」や「お気に入りだけを表示する」を状態として扱っていますが、この状態は他の画面では知る必要がないデータとなっています。この場合に使えるのが、@Stateです。コードとしては、以下のようになっています。(一部省略)\n// 一覧画面 struct LandmarkList: View { // お気に入りだけを表示するかどうか @State private var showFavoritesOnly = false // フィルタするカテゴリ @State private var filter = FilterCategory.all // カテゴリの種類 enum FilterCategory: String, CaseIterable, Identifiable { case all = \u0026#34;ALL\u0026#34; case lakes = \u0026#34;Lakes\u0026#34; case rivers = \u0026#34;Rivers\u0026#34; case mountains = \u0026#34;Mountains\u0026#34; var id: FilterCategory { self } } // アイテムにフィルタを適用 var filteredLandmarks: [Landmark] { modelData.landmarks.filter { landmark in (!showFavoritesOnly || landmark.isFavorite) \u0026amp;\u0026amp; (filter == .all || filter.rawValue == landmark.category.rawValue ) } } var body: some View { NavigationView { // アイテムの表示部 List(selection: $selectedLandmark) { ForEach(filteredLandmarks) { landmark in NavigationLink { LandmarkDetail(landmark: landmark) } label: { LandmarkRow(landmark: landmark) } } } .toolbar { ToolbarItem { // ツールバーにフィルタを適用するためのボタンを追加 Menu { // 選択できるカテゴリ Picker(\u0026#34;Category\u0026#34;, selection: $filter) { ForEach(FilterCategory.allCases) { category in Text(category.rawValue).tag(category) } } .pickerStyle(.inline) // お気に入りだけを表示するかどうかのトグル Toggle(isOn: $showFavoritesOnly) { Label(\u0026#34;Favorites only\u0026#34;, systemImage: \u0026#34;heart.fill\u0026#34;) } } label: { Label(\u0026#34;Filter\u0026#34;, systemImage: \u0026#34;slider.horizontal.3\u0026#34;) } } } } } } このように、一つのViewにおいて状態を管理するためには、@Stateを使うことになります。\n親子関係のViewが共有する状態 さて、一つのViewでの状態を管理することはできましたが、次に気になるのは複数のView、特に親子関係になるView間でどうやって状態を共有できるかです。たとえば先ほどの一覧画面だと、一つ一つの行がViewになっているものなのですが、\n実は、親のViewから子Viewに状態を渡すというのは、すでに先ほどのコードに表れています。「お気に入りだけを表示するかどうか」のトグルがあるのですが、ここでisOnに親の状態を渡していますね。ただ、トグル時の挙動を@Stateで定義したBooleanと結びつけるためにisOnとして渡す際、$をつけることに注目する必要があります。$をつけることで、Booleanではなく、Binding\u0026lt;Boolean\u0026gt;という形でプロパティを渡すことになります。こうしてラッパーを渡すことで、Toggleの中でも親の状態を変更できるようになります。Toggleは確かに一覧の画面とは別のViewになりますが、押下するたびに親の状態であるshowFavoritesOnlyの値が変わるということです。\n後でまた関連したポストを書きたいと思いますが、Jetpack Composeでも何となく似たような形で状態の管理ができる方法があります。例えば、@Stateのように簡単な状態を管理するためには以下のような書き方ができます。\n// true/falseの状態 var toggle: Boolean by remember { mutableStateOf(false) } if toggle { println(\u0026#34;On!\u0026#34;) toggle = false } else { println(\u0026#34;Off!\u0026#34;) toggle = true } 上記のような書き方はいわゆるDelegationによるもので、mutableStateOf\u0026lt;T\u0026gt;が返すのはMutableState\u0026lt;T\u0026gt;ですが、byを使うことで実際はBooleanそのものを扱うようになります。\nそしてMutableState\u0026lt;T\u0026gt;を分解し、以下のように状態と、状態を変化させる処理の処理を指定することもできます。これもまた、先ほどのBindingのような役割をするようなものですね。\n// テキストの状態と値の変更 val (content: String, onValueChange: (String) -\u0026gt; Unit) = remember { mutableStateOf(\u0026#34;\u0026#34;) } // テキストフィールドに状態を表示し、変化があった場合は状態を変更させる TextField(value = content, onValueChange = onValueChange) こうやって他の要素に状態の変更を可能にするためには、基本的にその状態をラップしているオブジェクトを利用するという考えは、SwiftUIでもJetpack Composeでも一緒のようです。ただ、$というキーワードを使ってラッパーにアクセスできるSwiftと、状態のオブジェクトをどうやって宣言するかを考える必要のあるKotlinの違いがまた面白いポイントです。\nアプリ全体で共有する状態 さて、次はもっと大きい単位としての状態について述べたいと思います。アプリの画面は色々とあり、仲には親子の関係ではない場合もあります。Appleのチュートリアルのアプリをまたの例としてあげますと、以下のように、タブが分かれている場合が代表的なものと言えるかもしれません。\nここで、下にある「Featured」や「List」をタッチすることにより表示される画面が変わるわけですが、この二つの画面は親子といえる関係ではありません。以下のコードをご覧ください。\nstruct ContentView: View { @State private var selection: Tab = .featured enum Tab { case featured case list } var body: some View { // タブメニュー TabView(selection: $selection) { // Featured画面 CategoryHome() .tabItem { Label(\u0026#34;Featured\u0026#34;, systemImage: \u0026#34;star\u0026#34;) } .tag(Tab.featured) // LandmarkList画面 LandmarkList() .tabItem { Label(\u0026#34;List\u0026#34;, systemImage: \u0026#34;list.bullet\u0026#34;) } .tag(Tab.list) } } } 要件によっては、この対等な関係にある画面間でも状態を共有する必要はあります。たとえば、ショッピングのアプリを作るとしたら、自分のアカウントの画面を開いている場合でもカートに入れている商品の情報は維持される必要がありますね。このように、現在表示している画面とは関係なく、アプリ全体で共有する状態が必要な場面があります。\nこの場合は、どうしたらいいでしょうか？もちろん、起点となるViewがあるので、そこに@Stateを定義しておくというのも一つの方法かもしれません。ただ、状態として管理したい項目が増えれば増えるほど複雑になりがちですね。起点のViewには複数の@Stateが必要となり、画面ごとにそれらを渡す必要があります。だとすると、やはり一つのオブジェクトに状態をまとめて置いて使いまわしたいものです。\nAppleのチュートリアルでもそれを提示していて、画面間で共有するデータとしてLandmarkのデータと、ユーザのプロフィールをまとめて共有できるオブジェクトとして提供する方法があります。まずは、以下がその状態のオブジェクトのコードとなります。（一部省略）\nfinal class ModelData: ObservableObject { @Published var landmarks: [Landmark] = load(\u0026#34;landmarkData.json\u0026#34;) @Published var profile = Profile.default } ここでアプリ内でLandmarkの情報はjsonから読み取ったものとなっていて、プロフィールはEnumとなっています。そしてObservableObjectというプロトコールが使われているのが見えますね。ObservableObjectを使うことで、このModelDataというクラスは変更される状態を保持し、値の変更があった場合はそれを参照する画面に通知するPublisherとして機能することになります。そしてそれぞれの状態となるプロパティについては、@Publishedをつけることで状態として指定することになります。\nこうして状態を保持するためのオブジェクトを定義したら、次はアプリ内でどうやって使用するかですね。チュートリアルでは、以下のようにアプリのメインとなるstructに以下のように定義しています。（一部省略）\n@main struct LandmarksApp: App { // ModelDataを状態のオブジェクトとして宣言 @StateObject private var modelData = ModelData() var body: some Scene { WindowGroup { ContentView() .environmentObject(modelData) // メイン画面に状態オブジェクトを渡す } } } コードでわかるように、アプリでは定義しておいた状態オブジェクトを@StateObjectをつけて、メイン画面に渡すことで使うようになります。ちなみにこうやってViewとして定義された画面にならどんなものでもenvironmentObject()に状態オブジェクトを渡すことができるので、画面の一部だけをプレビューとして表示する場合でも、プレビューに表示する画面に状態オブジェクトを渡すことで動作を試すこともできます。例えば、先ほどのLandmarkの一覧画面のプレビューだと、以下のようになっています。\nstruct LandmarkList_Previews: PreviewProvider { static var previews: some View { LandmarkList() .environmentObject(ModelData()) // 状態オブジェクトを渡す } } こうして状態オブジェクトを画面に渡したら、中では使うだけです。@EnvironmentObjectというアノテーションを使って、状態を持つオブジェクトを宣言するだけで自動的にオブジェクトはDIされ、その画面で使えるようになります。また一覧の画面の話となりますが、以下のコードを見てください。landmarksのデータを使って一覧を表示しているのがわかります。（一部省略）\nstruct LandmarkList: View { // 状態オブジェクト @EnvironmentObject var modelData: ModelData // 状態オブジェクトからデータを取り出しフィルタを適用 var filteredLandmarks: [Landmark] { modelData.landmarks.filter { landmark in (!showFavoritesOnly || landmark.isFavorite) \u0026amp;\u0026amp; (filter == .all || filter.rawValue == landmark.category.rawValue ) } } ちなみに、Jetpack Composeの場合はこのような方法ではなく、オブジェクトそのものをrememberするように使うようです。例えば以下のような形になります。そもそも要素を作成するアプローチが違うのが理由の一つなのかもしれませんね。\n// アプリ全体で共有する状態 class ApplicationState( val environment: MutableState\u0026lt;Environment\u0026gt;, val hash: MutableState\u0026lt;String\u0026gt; ) // 状態の初期化 @Composable fun rememberApplicationState( environment: MutableState\u0026lt;Environment\u0026gt; = mutableStateOf(Environment.PRODUCTION), hash: MutableState\u0026lt;String\u0026gt; = mutableStateOf(\u0026#34;\u0026#34;) ): ApplicationState { return remember(environment, hash) { ApplicationState(environment, hash) } } // 状態の定義 val appState = rememberApplicationState() 永続化できる状態 今までの状態は、アプリが実行中のみ有効なものでした。それだけでも十分な場合もありますが、場合によっては状態を永続化したい場合もあるでしょう。例えば、学習用のアプリだとしたらどこまで進行したかなどを保存しておきたいかなと思います。このような種のデータは、アプリを再起動しても変わらないことを期待しますね。\nもちろん、この場合のためにDBがあり、ネットワーク機能のあるアプリならサーバ側にデータを保存したりするでしょう。ただ、この場合は「状態」とは言えないものですね。なぜなら、状態は画面の更新を伴うものだからです。画面をタッチした回数を、画面に表示しながら同時に保存しておきたい場合は毎回DBの更新と参照で実装したくない場合もあるかなと思います。そこで使えるのが、@AppStorageです。これを使うことで、画面の更新と永続化を同時に行うことができます。\nまた、チュートリアルによると、いくつかデフォルトとして保存されているデータに関してもこのアノテーションでアクセスできるようにになっています。一覧画面に表示される「お気に入り」ボタンの例ですが、ボタンのアイコンを@AppStorageから参照するようになっています。コードは以下です。（一部省略）\nstruct FavoriteButton: View { // ボタンはハートのアイコンとして表示する @AppStorage(\u0026#34;Favorite.iconType\u0026#34;) private var iconType: IconType = .heart } 最後に いかがでしたか。自分の感想としては、やはり今まで自分が触れてきた分野とは全く違う概念やアプローチが多かったので、大変興味深いと思いつつ、「これであっているのか」という疑問もかなり湧いてくるような経験となっています。全く経験のない分野に足を踏み入れるのは常にそういうものなのかもしれませんが。\nただこれで、ある程度アプリを作る場合にどうやって処理を行うべきかについての疑問の一つは解けたような気がします。本当は画面を作る分、他にも色々と新しい概念が出てきたり、画面のレイアウトやUXの観点で色々と難しいものが出てくるかもしれませんが、とにかく「動く」ものを作れそうな気はしますね。何卒自分のようにバックエンドのみを経験してきた形にとって参考できるような記事となっていればと思います。\nでは、また！\n","date":"2022-08-20T00:00:00Z","image":"https://retheviper.github.io/images/swift.jpg","permalink":"https://retheviper.github.io/posts/swift-ui-first-impression-2/","title":"SwiftUIを触ってみた〜その2〜"},{"content":"今までの自分のキャリアを振り返ってみると、仕事としての経験はバックエンドばかりで、画面側の実装にはあまり関わったことがありません。しかし、スタンドアロンのアプリを作るためには、ウェブ・モバイル・デスクトップを問わず画面が必要となるので、いつかは画面側の実装もできるようになる必要があるかなと常に思っているところです。\n画面を作るといっても、どんな分野のエンジニアとしてキャリアパスを考えているか、どのような企業で働きたいか、慣れている言語は何であるかなど色々と考慮すべき要素は多いのですが、自分の場合はKotlinに慣れているのもあり、ウェブ・モバイル・デスクトップアプリに全部対応できるという点からJectpack Composeを、また普段からMacとiPhone、iPadといったApple社の製品をよく使っている上、KotlinからのSwift入門が比較的簡単ということでSwiftUIを勉強したいと思っています。\nさて、言語とフレームワークを決めてからは実践ですね。公式のチュートリアルが充実していたので、まずはこちらの方をすすめながら感じたSwiftやSwiftUIで印象的だった部分についてまとめてみたいと思います。もちろん、自分は仕事としてモバイルアプリの実装に関わったことがないのでコンテンツとしては粗末なものとなるかなと思いますが、もし自分のようにKotlinのバックグラウンドからSwiftに触れてみようと思っている方や、バックエンドのみのキャリアからGUIに初めて触れる方、もしくはKotlinとSwiftのどちらかに興味を持っている方には参考になる内容となればと思います。\nSwift まずは言語そのものから。KotlinとSwiftはよく似ているという話を聞くことがありますが、正確に「どこが」というのはやはり触れてみる前はわからないものです。似ているという表現は共通点があるという意味なので、何に基準を置くかによって挙げられる共通点は色々と変わってくるものだからです。\n例えば、言語デザインの観点でOOP志向的で、関数型的な要素があり、GCが存在する、ということでも共通点は発見できます。もしくは、言語の使用としてキーワードや書き方の印象が似ているという意味にもなれますね。細かくは、セミコロンを使わなくて良いという点も挙げられますね。\nなので、まずは上記のチュートリアルを進行しながら、肌で感じた感覚から、Kotlinに比べたSwiftはどのようなものだったかを述べていきたいと思います。\nKotlinに似ているもの では、まずKotlinに似ているなと感じたところから述べていきます。似ているとしても、あくまで「肌の感触」なものなので、厳密には違う仕様になっているものも多いのですが、ここでの基準は「Kotlinでできたことをどれほど近い感覚で再現できるか」となっていますので（といっても個人的な感想ですが）、参考までに。\nComputed properties まず、プロパティの話からです。Kotlinではdata classを定義するとき、プロパティを以下のような二つの方法で定義することができます。\ndata class Student( val age: Int ) { val isAdult: Boolean get() = age \u0026gt;= 18 } ここでageはインスタンスを作成するときに固定される単純な値となりますが、isAdultはgetterとして定義した処理(ageが18以上かどうかという)の結果を返すように定義する形ですね。このような処理を伴うプロパティは、Swiftでも同じくComputed Propertiesを通じて定義することができました。同じような処理を行う場合、以下のように定義できます。\nstruct Student { var age: Int var isAdult: Bool { get { return age \u0026gt;= 18 } } } まだ一つしたあげてないのですが、これだけでもなんとなく「KotlinとSwiftが似ている」の意味が少しは見えてきた気がしますね。処理を伴うプロパティを扱える、という仕様がそうですが、型の定義もそうで、キーワードは少し違うけど大体似たような感覚でコードが読めるというところがそうです。\nただ、やはり違う部分もありますね。data classに対して、SwiftはGoやRustのようにstructを使えるというところがそうかなと思います。もちろんSwiftにもClassはあるので、目的によってどれかを選ぶようになるらしいです。という面では、またなんとなくKotlinでdata classとclassを分けて使うという点と似ているような気もしますね。\nExtension 次は、拡張です。Kotlinではオブジェクトについて、そのオブジェクトの外にメソッドやプロパティを定義することができますね。これらを拡張関数や拡張プロパティと呼び、以下のように定義することができます。\nval Student.isUnderAge: Boolean get() = age \u0026lt; 18 以前このブログでも述べたことのあるEffective Kotlinで提示されている活用方法ですが、ユースケースやドメインによって違う処理が必要となった場合は、class内に全てのメソッドやプロパティをを定義するよりはこのような拡張を使って、パッケージごとに定義することでアクセス制限を設ける方法があります。\n同じようなことがSwiftでもできますが、書き方はやはり少し違いました。上記のようなプロパティをSwiftで同じ方法で実装する場合、以下のようになります。\nextension Student { var isUnderAge: Bool { get { return age \u0026lt; 18 } } } 上記のようにSwiftにはExtensionが別途キーワードとして存在していて、新しくclassやstructを定義するかのような感覚で関数やプロパティを付け加えることができます。個人的な感想としてはRustのメソッドと似ている形で、一つのextensionの中にまとめられるところがむしろKotlinより整頓された感覚なので良さげですね。Kotlinの場合、一つのオブジェクトに対しての拡張が複数あると少し汚くも見えるので…\nString Interpolation Javaの場合でもそうで、多くの言語では文字列と、違う変数として格納してある値を一つの文字列にまとめる場合はformat()を使うか、文字列に変換して結合するケースが多いかなと思います。Kotlinでもそのような使い方はもちろんできますが、String templateがあるので、簡単に文字列の中で違う値を埋め込むことができます。例えば以下のようなものですね。\nval world = \u0026#34;World\u0026#34; println(\u0026#34;Hello, $world\u0026#34;) SwiftでもString Interpolationがあるので、同じことができます。少し書き方が変わっているのですが、機能的にはほぼ一緒です。\nlet world = \u0026#34;World\u0026#34; print(\u0026#34;Hello, \\(world)!\u0026#34;) Arguments Kotlinでは、関数のパラメータにデフォルト値を設定することで、簡単にオーバーロードを実現でき、そのパラメータが渡されてない場合の処理にも対応できます。\n// timesに指定した数値分、stringを標準出力する fun printHello(string: String, times: Int = 1) { repeat(times) { println(\u0026#34;Hello, $string\u0026#34;) } } printHello(\u0026#34;world\u0026#34;) // timesに値を指定しなくても関数を呼び出せる また、どのパラメータに値を指定したいかを明確にするときや、関数に定義されたパラメータの順番に関係なく値を指定したい場合など色々な場面でNamed Argumentsを使うことができますね。例えばjoinToString()にはseparator、limit、truncatedなど6つのパラメータがあるのですが、デフォルト値が指定されていて、Named Argumentsにより以下のような使い方が可能です。\nlistOf(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;).joinToString(prefix = \u0026#34;[\u0026#34;, postfix = \u0026#34;]\u0026#34;) Named ArgumentはKotlinにおいてはオプションで、基本的にはJava同様、関数に定義されてあるパラメータの順番に合わせて値を渡すだけでも問題ありません。しかし、Swiftではこれが逆になっていて、sturctのインスタンスを作る場合や関数を呼び出す場合は基本的にパラメータは基本的にNamed Argumentsのような形で渡す必要があります。これをArgument Labelと読んでいるそうです。\nfunc printHello(string: String) { print(\u0026#34;Hello \\(string)!\u0026#34;) } printHello(string: \u0026#34;world\u0026#34;) // Function Argument Labelでstringを指定 ただ、これもKotlinと同様、デフォルト値を指定することができ、その場合はパラメータを省略することができます。\nfunc printHello(string: String, times: Int = 1) { var count = 1 repeat { // Kotlinのdo-whileループ的なもの print(\u0026#34;Hello \\(string)!\u0026#34;) count += 1 } while (count \u0026lt;= times) } printHello(string: \u0026#34;world\u0026#34;) // timesを省略している 他にも、アンダースコアを使うことでArgument Labelを省略できるようにもなります。\nfunc printHello(_ string: String, times: Int = 1) { var count = 1 repeat { print(\u0026#34;Hello \\(string)!\u0026#34;) count += 1 } while (count \u0026lt;= times) } printHello(\u0026#34;world\u0026#34;) // stringを省略 関数を定義する側からしたらあまり似ていないような気もするのですが、呼び出す側としてはかなり似たような形でコードが書けるのが特徴的かなと思います。\nRange KotlinではrangeTo()を使って、簡単に数値の範囲を定義することができます。この関数はoperatorとして定義されているので、..で簡単に使えます。こうやって定義したRangeでは、最小値と最大値の取得や、Listに変換するなど色々なことができます。\n// Rangeの定義 val min = 10 val max = 20 val range = min..max // 最小値と最大値の取得 println(range.start) // 10 println(range.endInclusive) // 20 // RangeをListにする val intList = range.toList() // [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] SwiftでもRange Operatorを使って範囲を定義することができます。こちらも形は似ていて、...となります。ドットの数がKotlinより一つ多いという点を除くと全く同じ感覚で、最小値と最大値もまた名前が違うだけでプロパティとして取得できるという点もまた一緒です。\n// Rangeの定義 let min = 10 let max = 20 let range = min...max // 最小値と最大値の取得 print(range.lowerBound) // 10 print(range.upperBound) // 20 // RangeをArrayにする let array = Array(range) // [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] ただ、上記のコードを見ると気付きにくいところですが、Range実装については両言語での扱いが少し違うところがあります。KotlinではrangeTo()の戻り値が、元の値の型に合わせてInteRangeやLongRangeのようなものとなっていて、最小値と最大値をプロパティで取得する場合もrangeToに渡された二つの値の型と一緒です。\nしかし、SwiftのRangeはRange\u0026lt;Bound\u0026gt;という型で、当然Rangeから取得できる最小値や最大値もBoundの型となっています。IntやLongとはまた別の型になるので、場合によっては注意して使う必要があるかもしれません。\nSwiftだけのもの 今まではKotlinユーザの観点から、Kotlinとどれだけ同じ感覚でコードを書けるか、ということを述べていましたが、ここからは少し間隔が違うなと思ったところを少しまとめてみようと思います。\nメソッド・プロパティコールでの省略 Kotlinでは、apply()のように自分自身を指しているのが明確な場合、thisを省略することができます。以下のようにですね。\ndata class Student( val name: String, var age: Int = 0 ) val studentA = Student(name = \u0026#34;A\u0026#34;).apply { age = 18 } // Student(name=A, age=18) このように、thisを使う場合か、明確に対象importしているなど特定のケースを除くとKotlinでは基本的にClass.method()のような形でどのクラスのメンバを呼び出しているかを表記するのが原則ですね。\nしかし、Swiftの場合は少し状況が違います。もっとゆるい感じで、コンパイラを基準に対象が明確であれば、.method()のような形で省略できるような感覚です。以下はSwiftUIのチュートリアルで提示しているコードの一部を抜粋したものですが、filterがFilterCategoryというenumであるため、.allという形で三項演算子の中で使われていることを確認できます。\nstruct LandmarkList: View { @State private var filter = FilterCategory.all enum FilterCategory: String, CaseIterable, Identifiable { case all = \u0026#34;ALL\u0026#34; case lakes = \u0026#34;Lakes\u0026#34; case rivers = \u0026#34;Rivers\u0026#34; case mountains = \u0026#34;Mountains\u0026#34; var id: FilterCategory { self } } var title: String { let title = filter == .all ? \u0026#34;Landmarks\u0026#34; : filter.rawValue return showFavoritesOnly ? \u0026#34;Favorite \\(title)\u0026#34; : title } } Protocol SwiftではProtocolというものがあり、JavaやKotlinのinterfaceと大体同じ感覚で使えます。ここまでだとあまり差はないように思いますが、実際にはstructやclass、enumなどを定義するときには、必要に応じでprotocolを採用(adopt)する必要があるというところが体験できる違いかなと思います。\n例えば、Kotlinで一つのdata classを定義するとしたら、以下のようなメンバが自動て追加されます。\nequals() hashCode() toString() componentN() copy() しかし、Swiftのstruct, class, enumなどにはこのようなメンバは基本的に追加されません。なので、必要なメンバがあればそれに関するprotocolを採用し、実装する必要があります。例えばハッシュ値が使いたい場合はHashable、 Jsonなどに変換するためにはCodable、Listでループしたい場合はIdentifiable、enumの全ケースを網羅してループしたい場合はCaseIterable、同一化を比較したい場合はEquatableを採用するなどです。\nもちろんJavaやKotlinでも必要に応じてintefaceやannotationを使う必要はありますが、SwiftだとKotlinで気軽に使える機能がstructやclassなどを定義した時点では揃ってない可能性があるので、ここは気をつけるべきところですね。\nsome Swiftでは少し変わった感覚のキーワードがあるます。そのキーワードの説明するために、まずは以下のようなprotocolとstructの定義があるとしましょう。\nprotocol Something { func define() } struct GoodThing: Something { func define() { print(\u0026#34;It\u0026#39;s good thing!\u0026#34;) } } 上記のようなコードがある場合、変数の型宣言や関数の戻り値で少し独特なキーワードを使うことができます。someというものです。実際使う時は、以下のようなコードとなります。\nvar good: some Something = GoodThing() func returnSomething() -\u0026gt; some Something { return GoodThing() } これだけではsomeというキーワードが一体どんなものかわからないですね。ここでKotlinの概念を持ってくるとどうでしょうか。実は、Kotlinでもこれによく似た機能があります。\u0026lt;T extends Something\u0026gt;です。KotlinやJavaの経験がある型ならこれで十分に何を意味しているかがしっくり来るかなと思います。\nつまり、someはとあるprotocolを満足する何かしらのインスタンスを示すものです。Swiftではそれを満足するオブジェクトであってもprotocolを直接変数の型や関数の戻り値として定義して直接使うことはできない場合があります。その場合にsomeを使うことで問題を回避できます。JavaやKotlinでinterfaceを使って、その具体的な実装は問わなく使うのと一緒だと言えます。このキーワードのおかげで、SwiftUIではViewを満足して入れば画面を構成するどんなコンポーネントとして扱えるようになります。\nただ、interfaceを扱うのとは概念的に同じだとしても、コードを書く側の感覚としては全く違うのでここは注意しなければならないと思います。\nCompiler Control Statements SwiftにはCompiler Control Statementsという仕様があり、コンパイル時の処理を指定できます。例えば、SwiftUIのチュートリアルでは一つのアプリを実装して、OSによって違う機能を実現するためにこれを利用しているケースがあります。以下がその例です。\n// watchOSで起動する場合は、通知を使う #if os(watchOS) WKNotificationScene(controller: NotificationController.self, category: \u0026#34;LandmarkNear\u0026#34;) #endif // macOSで起動する場合は、設定を使う #if os(macOS) Settings { LandmarkSettings() } #endif Kotlinの場合もAndroidでアプリを実装する場合はこのような設定が必要になる場面もあるかもしれませんが、バックエンドの経験上ではコードによりコンパイラをコントロールするというケースはあまりなかったので、かなり新鮮な感覚でした。\n最後に いかがでしたか。SwiftUIの話をするつもりが、Swiftのことだけでかなりの量になってしまったので、SwiftUIについては次のポストで述べようかなと思っています。しかし、Swiftだけでもかなり興味深いところが多かったので、やはりチュートリアルを触ってみて色々な経験ができたので良い選択をしたかなと思います。\nまた、やはりKotlinとSwiftがなんとなく似ている部分があるのは感覚的には確かなので、やはりどちらかの経験があると残りの片方への入門もしやすくなるのかなという感覚はあります。これは外国語の教育（自分の専攻です）でいうスキーマ、いわゆるバックグラウンドの知識ある故のことだろうなと思うと、少しうれしくもなりますね。やはりKotlinやってよかったなと思います。\nでは、また！\n","date":"2022-07-31T00:00:00Z","image":"https://retheviper.github.io/images/swift.jpg","permalink":"https://retheviper.github.io/posts/swift-ui-first-impression-1/","title":"SwiftUIを触ってみた〜その1〜"},{"content":"この度はProfessional Cloud Developerの資格を取得しました。去年は特に資格を取得してないので、久々の資格取得となります。今までは主にAWSを使っていたので関連した資格を取得するのが良いかなと思っていましたが、GCPのサービスがもっと知りたいというのもあり、AWSの資格と比べどう違うかも気になったための受験となります。\n前回もそうであったように、無事資格の取得ができましたので、今回のポストではどのように試験の準備をしたかなどについて簡単に述べたいと思います。\n試験はどんな感じか 以前、AWSの資格に関する記事を書いたことがありますが、プロバイダが違ってもCloudというプラットフォームが提供するサービスの根本は変わらないので、試験もまたそのような感覚です。基本的にIaaSやPaaSのようなサービスの概念や、バックエンドサービスのクラウドでのデプロイと構成の経験があれば試験の準備はそう難しくないかなと感じました。\n受験は遠隔監視とオンサイト監視の二つを選べ、前者は自宅での受験、後者はテストセンターでの受験となります。自分の場合はオンサイト監視が平日しか選べなかったため遠隔監視を選んでいます。AWSの試験の時もそうでしたが、遠隔監視の場合は受験できる部屋の準備をきちんとする必要があります。準備に必要なものはKRYTERIONのサイトに明示されているので、事前にチェックした方が良いです。個人的にはこの要件に合わせて部屋を片付けるのもかなり大変なものだったので、オンサイト監視の方が楽だろうと思いました。\n遠隔監視での試験はKRYTERIONが提供する専用のブラウザを使って進行します。ブラウザが起動すると他のアプリは最小化され、外部モニタを繋いでいる場合はそのモニタの画面が黒くなります。私の場合はApple Silicon Macを使っているので互換性の方が気になりましたが、問題なく実行できました。システムチェックのサイトも提供しているので、自分のPCで受験できるかはこちらでも確認したほうが良いでしょう。また、生体認証情報の登録が必要となりますが、こちらは単純に写真を撮るだけです。当日は試験が始まる前に登録した生体認証を確認し、監視官の指示の元に部屋の様子を見せたり身分証明書を提示するなどの簡単な手続きがあります。\n他の資格の場合はその場や当日で結果がわかるケースも多かったかなと思いますが、GCPの資格は認定のメールが届くまで1週間ほどの時間がかかっています。満点や得点がわからないという特徴を踏まえて考えてみると、受験当時のさまざまな状況を考慮して総合的に判断するのではないかと思っています。受験専用のブラウザを使うことになっているので、おそらくキー入力やカメラの映像なども全部録画されていたのではないかと思います。\nどうやって準備したか まずはUdemyから模擬試験を検索してみました。中には英語になっているものと日本語版がありますが、どれもレビューに書いてある通りで、実際のテストと比べ簡単な問題が多いです。自分の場合は5回分を提供する英語の方を4周してどれも8割以上の正解を記録しましたが、本番ではより難しい（模擬試験では問われてないものやより複雑な）問題が多かったため、これだけでは不十分と思いました。\n模擬試験は公式の方もあり、こちらは比較的に実際のテストに近い問題が多いので（こく一部ですが、ほぼそのままの問題もあります）こちらの方を問題で問われる各サービスのドキュメントを調べながら勉強した方が良いかなと思います。\n模擬試験の他にはKubernetesの問題が多いという情報があり、実際に使ったことがなかったのでそちらを中心に色々調べたことが多かったです。そのほかはテストやデプロイの概念（Blue-Greenやカナリアなど）を一度チェックしているくらいです。\n何を問われるか HipLocalのケーススタディを参考にして準備をした方が良いです。HipLocalに関する問題自体は全体の割合として高くないものの、このケーススタディを用いた問題の場合は、模擬試験で問われる問題以外でもいろいろなパターンがありますので試験の前にケーススタディに合わせてどうやってマイグレーションを行ったらいいかをGCPのサービスと照らし合わせながら確認した方が良いかと思います。\n一つ、HipLocalのケーススタティを全て覚えておく必要はありません。個人的にはここがかなり気になっていたところなのですが、実際のテストではブラウザの右側にケーススタディが提示されていたのでそれを参考して問題を解くことができました。ブラウザに表示された内容ではHipLocal以外のケーススタディが提示される可能性もあるように書いてありましたが、おそらく他のテストでも似たようなパターンの問題があるためそのような記述があるのかなという感覚です。\n問われる問題の範囲や詳細に対しては他のブログでも見つけることができ、自分としては詳細までは覚えてないので詳しくは述べませんが、やはりKubernetesに関する問題が多い感覚で、次にはストレージやサーバレスに関する問題が多かったかなと思います。そのほか、印象に残ったものはAnthosやistioが言及された問題があったことです。どちらも名前しか聞いてないくらいだったので、その問題に対しては正解となっているかどうかすらわかりませんでした。\n受かったあとは Google Cloud Certification Perks Webstoreから特典をもらえるコードがメールで送られてきました。もらえる特典の種類は随時変わるらしく、他のブログではBluetooth Speakerなどをもらえると書いてありましたが、私の場合は以下から選ぶようになっていました。\n私はジッパーフードの方を選びたかったのですが、今回はサイズを2XLしか選べなかったので、今回はジッパーのない方を選んでいます。欲しいものが特になかったら、寄付できるオプションもありますのでそちらを選んでも良いかもですね。\n資格の認証はAccredibleというサイトから確認できます。こちらのサイトではAWSやOracleの資格をCredlyで確認できたように、認定の証明をサイトにリンクできたり、Linkedinなどに共有できる機能があります。認定書はPDFとしてダウンロードもできました。\n一つ、AWSの場合は資格の有効期限が3年となっていましたが、GCPの場合は2年となっていました。クラウドだとやはりサービスの仕様が変わることも多く、新しいサービスが導入される場合も多いので有効期限を設けていること自体は当たり前とは思いますが、勉強すべき量と受験料($200)を考えると少し短い感覚です。\n最後に 受験後に模擬試験とあまりにも違う問題が多かったため今回こそ落ちたのではないかという不安があり、のちに合格したというメールを受けてもやはり実感があまりありませんでした。ただ、それはあくまで自分が模擬試験で問われるもののみを覚えていただけなのが原因な気もします。\n実際の問題は問題と選択肢を注意深く読むことで正解が見えてくるものもあったり、GCPのドキュメントを確認していたらわかるようなことも多いので、まずは公式の認定資格ガイドに提示されてある内容を中心に各サービスのドキュメントを一読することをお勧めします。模擬試験はあくまで模擬試験なので、良い点数が取れたからと油断したら落ちる可能性も十分にあり得るのではないかと思います。\n問題の難易度に関しては、やはりどうしても自分が以前取得したAWSの資格と比較してしまいますが、そちらはアソシエイトレベルのものだったため、こちらの資格の方が難しい感覚ではありました。個人的には合格してからも本当にプロフェッショナルを名乗るにはより勉強が必要なのではないかという気がするくらいでした。ただ、普段からGCPのサービスを色々と利用してオンプレからのマイグレーション経験がある方ならそこまで難しくないのかもしれませんので、油断だけしないなら、十分受かる資格なのではないかなと思っています。\nでは、また！\n","date":"2022-07-11T00:00:00Z","image":"https://retheviper.github.io/images/gcp.jpg","permalink":"https://retheviper.github.io/posts/gcp-certification-professional-cloud-developer/","title":"Google Cloud認定Professional Cloud Developerについて"},{"content":"Androidアプリのように、GUIを使う場合にはマルチスレッドで処理するのはもはや常識のようなものです。シングルスレッドだと何か思い処理が行われる間に画面が固まるからです。他にもプログレスバーのようにリアルタイムで変化されるコンポーネントの状態を更新したり、チャット、通知の表示などさまざまな場面でスレッドを分けて処理する必要がある場合が多いですね。\nただ、バックエンドの処理においては少し事情が違うものです。そもそもGUIを考慮する必要がないということもありあすが、サーバでは一つのリクエストに対しての処理を「順次的に」行う場合が多いため、マルチスレッドを利用した処理の分散の利点を活かすのはなかなか難しいものです。Reactive Streamsのようなものもありますが、これは一つのリクエストを分散するというより少ないリソースで多くのリクエストに対する処理を行うためのものなので、一つの処理を分散して効率を上げたいという場合にはあまりふさわしくないものですね。\nもちろん、だからと言ってバックエンドにおいて分散処理が全く必要ないというわけではありません。確かに一つのリクエストに対しての処理を行う中でも、処理によってスレッドを分けて性能向上を期待できる場面があります。例えば後続の処理と関係のない処理を途中に挟んでい場合では、別スレッドで処理したくなりますね。\nなので、今回はCoroutineを使ったバックエンドでの処理の分散するという一例を紹介したいと思います。1\nAPIの呼び出しを並列化する まず並列化で効率を上げられるケースとして、バッチ処理を考えられます。バッチ処理では、条件に当てはまるデータを複数抽出し、それぞれのデータに対して同じ処理を繰り返すことが多いですね。このように個別のデータに対しての処理が独立的に実行されるものであり、並行して走っても特に問題はないという場合は十分その処理を分散できるものです。\n仕事ではGoで作成されたサーバから定期的に日付を基準にDBから処理対象のデータを抽出し、そのデータを配列にしてループしながらKotlinで作成されたサーバのAPIを呼び出すようになっています。また、KotlinサーバでもBackendのAPIを呼び出すケースがあり、これもまたループでデータの参照を行っている状態です。サービスが成長するにつれて、処理にかかる時間によりAPI呼び出しがタイムアウトになるなどパフォーマンスの問題が出てきたので、このループの中でのAPI呼び出しを並列化することで処理にかかる時間を減らすことにします。\n実装してみる まずはCoroutineにより、ループの中でのAPIの呼び出しを実現してみます。上述したとおり、実際の仕事で使えるかどうかを検証してみたく書いたコードなので、各サーバの処理は大して変わらないものとなっています。まずループの中で互いのAPIを呼び出すような処理を書き、呼び出される側では5秒を待ってレスポンスを送るようになっています。これをCoroutineを利用して並列化していきます。\nGo まず、以下のような処理があるとします。\ntype callResults struct { Response []*client.Response `json:\u0026#34;response\u0026#34;` } func CallKotlinServer(c *gin.Context) { log.Print(\u0026#34;[CallKotlinServer] start\u0026#34;) results := \u0026amp;callResults{} tries := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} for _, i := range tries { log.Print(\u0026#34;[CallKotlinServer] before request with id: \u0026#34;, i) var result string r, err := client.Post(i) // KotlinサーバにPOSTでリクエストを送る if err != nil || r == nil { result = \u0026#34;failed\u0026#34; } else { result = r.Result } log.Print(\u0026#34;[CallKotlinServer] after request with id: \u0026#34;, i) results.Response = append(results.Response, \u0026amp;client.Response{ ID: i, Result: result, }) } log.Print(\u0026#34;[CallKotlinServer] done\u0026#34;) c.JSON(http.StatusOK, results) } 上記のコードはGinを使ったサーバのサンプルで、handlerの部分です。この関数の中ではAPIが呼び出されると、10回のループの中でKotlinサーバにリクエストを送ります。そして帰ってきたAPI呼び出しの結果を持ってレスポンスのstructを作成して、最終的には10回の実行結果をまとめてJSONとして返す構造となっています。\nここでKotlin側が返すレスポンスは5秒かかるため、ループの回数が多くなれば多くなるほどレスポンスが帰ってくるのも遅くなります。ログを吐くようにしているので、サーバのログを確認するとリクエストからレスポンスに50秒がかかっているのを確認できます。\n2022/06/05 18:49:31 [CallKotlinServer] start 2022/06/05 18:49:31 [CallKotlinServer] before request with id: 1 2022/06/05 18:49:36 [CallKotlinServer] after request with id: 1 2022/06/05 18:49:36 [CallKotlinServer] before request with id: 2 2022/06/05 18:49:41 [CallKotlinServer] after request with id: 2 2022/06/05 18:49:41 [CallKotlinServer] before request with id: 3 2022/06/05 18:49:46 [CallKotlinServer] after request with id: 3 2022/06/05 18:49:46 [CallKotlinServer] before request with id: 4 2022/06/05 18:49:51 [CallKotlinServer] after request with id: 4 2022/06/05 18:49:51 [CallKotlinServer] before request with id: 5 2022/06/05 18:49:56 [CallKotlinServer] after request with id: 5 2022/06/05 18:49:56 [CallKotlinServer] before request with id: 6 2022/06/05 18:50:01 [CallKotlinServer] after request with id: 6 2022/06/05 18:50:01 [CallKotlinServer] before request with id: 7 2022/06/05 18:50:06 [CallKotlinServer] after request with id: 7 2022/06/05 18:50:06 [CallKotlinServer] before request with id: 8 2022/06/05 18:50:11 [CallKotlinServer] after request with id: 8 2022/06/05 18:50:11 [CallKotlinServer] before request with id: 9 2022/06/05 18:50:16 [CallKotlinServer] after request with id: 9 2022/06/05 18:50:16 [CallKotlinServer] before request with id: 10 2022/06/05 18:50:21 [CallKotlinServer] after request with id: 10 2022/06/05 18:50:21 [CallKotlinServer] done [GIN] 2022/06/05 - 18:50:21 | 200 | 50.250251292s | 127.0.0.1 | GET \u0026#34;/api/v1/call-kotlin-server\u0026#34; Goroutineで並列化する(1) では、以上の処理を並列化することにします。GoにはGoroutineが基本的に含まれています。使い方は単純で、実行したい関数の前にgoのキーワードをつけるだけですね。ただ、レスポンスでは10回の実行結果を待ってから返す必要があるのですが、goroutineでAPIの呼び出しをするとメインスレッドが先に終わってしまう可能性があります。\nというわけで、ループの中でのAPIの呼び出しにgoroutineを使い、さらにそのgoroutineが全て終了してから結果を返すようにします。goにはsyncというパッケージにWaitGroupがあり、goroutineの終了を待つことができるようになっています。また、goroutineをループの中で実行する場合、順番はランダムになるのでレスポンスを返す際は一度ソートをかけるようにします。以上を考慮して実装した結果は以下の通りです。\nfunc CallKotlinServerAsync(c *gin.Context) { log.Print(\u0026#34;[CallKotlinServerAsync] start\u0026#34;) results := \u0026amp;callResults{} tries := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} group := \u0026amp;sync.WaitGroup{} // WaitGroupを定義 for _, i := range tries { group.Add(1) // ループごとに実行するgoroutineの数を追加 go func(i int) { // goroutineでAPIの呼び出す log.Print(\u0026#34;[CallKotlinServerAsync] before request with id: \u0026#34;, i) var result string r, err := client.Post(i) if err != nil || r == nil { result = \u0026#34;failed\u0026#34; } else { result = r.Result } log.Print(\u0026#34;[CallKotlinServerAsync] after request with id: \u0026#34;, i) results.Response = append(results.Response, \u0026amp;client.Response{ ID: i, Result: result, }) group.Done() // waitGroupにgoroutineの終了を設定 }(i) } group.Wait() // 全てのgoroutineが終了するのを待つ sort.Slice(results.Response, func(i, j int) bool { return results.Response[i].ID \u0026lt; results.Response[j].ID }) log.Print(\u0026#34;[CallKotlinServerAsync] done\u0026#34;) c.JSON(http.StatusOK, results) } このように修正して実行した結果のログは以下のとおりです。\n2022/06/05 18:52:30 [CallKotlinServerAsync] start 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 10 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 5 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 7 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 8 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 6 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 1 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 9 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 3 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 4 2022/06/05 18:52:30 [CallKotlinServerAsync] before request with id: 2 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 6 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 1 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 8 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 10 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 4 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 7 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 3 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 9 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 5 2022/06/05 18:52:35 [CallKotlinServerAsync] after request with id: 2 2022/06/05 18:52:35 [CallKotlinServerAsync] done [GIN] 2022/06/05 - 18:52:35 | 200 | 5.012657333s | 127.0.0.1 | GET \u0026#34;/api/v1/call-kotlin-server-async\u0026#34; 10回のループがほぼ同時に実行されたため、レスポンスまで5秒ほどかかっているのがわかります。そしてやはりgoroutineの実行が順番に行われてないことがわかりますね。なので、実行の順番が重要でなくても、結果は順番を守って返す必要がある時はやはりソートが必要ということがわかります。\nGoroutineで並列化する(2) 場合によっては並列化できるからって、全ての処理を同時に走らせるのは危険な時もあります。上記のコードの場合、リクエストの数は10となっていますが、もしそれより多くのリクエストが必要か、さらに重い処理のAPIを呼び出す場合はどうでしょうか。Go側はリクエストを投げるだけなので処理の負荷はあまり変わらないものですが、APIを呼び出されている側としてはかなりの負荷になるはずです。\nだとすると、やはり並列の数を制限する必要があるはずです。例えば並列の数を2にすると、リクエストは2件づつ送られるのでリクエストの全体の数がいくら増えても負荷は一定に保てます。同時に全てのリクエストを送るよりは遅くなりますが、リソースの状況を見ながら並列数を増やすだけで柔軟に対応ができるので、外部設定ファイルなどで並列数を指定できるようにするとアプリのビルドなしでも柔軟に対応ができるというメリットもありますね。\nスレッドを使う場合だと、このような処理をするためにはかなり複雑な処理を書くことになるはずです。例えば、並列数に合わせてスレッドを定義して、さらにスレッドごとに割り当てる処理を分けなければならないですね。今は10件のリクエストを想定しているので、スレッドごとに5件づつというふうにリクエストを分けるだけで対応ができますが、リクエスト数をスレッド数で割った結果を考慮してループする要素数を適宜分割するような処理をまず足す必要があります。\nしかし、実はgoroutineを使うとそのような複雑な処理をまた足す必要はないです。goroutineではChannelを利用して、同時に実行されるgoroutineの数を指定できます。以下のようにです。\nfunc CallKotlinServerAsyncDual(c *gin.Context) { log.Print(\u0026#34;[CallKotlinServerAsyncDual] start\u0026#34;) results := \u0026amp;callResults{} tries := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} concurrency := 2 // goroutineの同時実行数を指定 group := \u0026amp;sync.WaitGroup{} guard := make(chan struct{}, concurrency) // 同時実行数でChannelを定義 for _, i := range tries { group.Add(1) guard \u0026lt;- struct{}{} // Channelに実行を一つたす go func(i int) { log.Print(\u0026#34;[CallKotlinServerAsyncDual] before request with id: \u0026#34;, i) var result string r, err := client.Post(i) if err != nil || r == nil { result = \u0026#34;failed\u0026#34; } else { result = r.Result } log.Print(\u0026#34;[CallKotlinServerAsyncDual] after request with id: \u0026#34;, i) results.Response = append(results.Response, \u0026amp;client.Response{ ID: i, Result: result, }) group.Done() \u0026lt;-guard // Channelを準備させる }(i) } group.Wait() sort.Slice(results.Response, func(i, j int) bool { return results.Response[i].ID \u0026lt; results.Response[j].ID }) log.Print(\u0026#34;[CallKotlinServerAsyncDual] done\u0026#34;) c.JSON(http.StatusOK, results) } Channelには指定した数分だけ送信すると、Channelから値を受信するまでは新しいgoroutineの実行はブロックされます。なので、実際に実行してみると、意図通り最大2件づつのリクエストが送信されているのを確認できます。\n2022/06/05 19:56:10 [CallKotlinServerAsyncDual] start 2022/06/05 19:56:10 [CallKotlinServerAsyncDual] before request with id: 2 2022/06/05 19:56:10 [CallKotlinServerAsyncDual] before request with id: 1 2022/06/05 19:56:15 [CallKotlinServerAsyncDual] after request with id: 2 2022/06/05 19:56:15 [CallKotlinServerAsyncDual] before request with id: 3 2022/06/05 19:56:15 [CallKotlinServerAsyncDual] after request with id: 1 2022/06/05 19:56:15 [CallKotlinServerAsyncDual] before request with id: 4 2022/06/05 19:56:21 [CallKotlinServerAsyncDual] after request with id: 4 2022/06/05 19:56:21 [CallKotlinServerAsyncDual] before request with id: 5 2022/06/05 19:56:21 [CallKotlinServerAsyncDual] after request with id: 3 2022/06/05 19:56:21 [CallKotlinServerAsyncDual] before request with id: 6 2022/06/05 19:56:26 [CallKotlinServerAsyncDual] after request with id: 5 2022/06/05 19:56:26 [CallKotlinServerAsyncDual] before request with id: 7 2022/06/05 19:56:26 [CallKotlinServerAsyncDual] after request with id: 6 2022/06/05 19:56:26 [CallKotlinServerAsyncDual] before request with id: 8 2022/06/05 19:56:31 [CallKotlinServerAsyncDual] after request with id: 7 2022/06/05 19:56:31 [CallKotlinServerAsyncDual] before request with id: 9 2022/06/05 19:56:31 [CallKotlinServerAsyncDual] after request with id: 8 2022/06/05 19:56:31 [CallKotlinServerAsyncDual] before request with id: 10 2022/06/05 19:56:36 [CallKotlinServerAsyncDual] after request with id: 9 2022/06/05 19:56:36 [CallKotlinServerAsyncDual] after request with id: 10 2022/06/05 19:56:36 [CallKotlinServerAsyncDual] done [GIN] 2022/06/05 - 19:56:36 | 200 | 25.194952625s | 127.0.0.1 | GET \u0026#34;/api/v1/call-kotlin-server-async-dual\u0026#34; Kotlin まずは順次処理する場合のコードから見ていきます。基本的にGoの場合と同じ処理をKotlin側にも用意していて、特に変わったものはありません。以下がそのコードです。\nfun callGoServer(): List\u0026lt;CallGoServerDto\u0026gt; { logger.info(\u0026#34;[CallGoServer] start\u0026#34;) return tries.map { logger.info(\u0026#34;[CallGoServer] before request with id: $it\u0026#34;) goServerClient.call(it) ?: CallGoServerDto(it, \u0026#34;failed\u0026#34;) // GoのAPIを呼び出す .also { result -\u0026gt; logger.info(\u0026#34;[CallGoServer] after request with id: ${result.id}\u0026#34;) } }.also { logger.info(\u0026#34;[CallGoServer] done\u0026#34;) } } Curlで実行してみた結果は以下の通りです。\n2022-06-05 20:06:33.429 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] start 2022-06-05 20:06:33.430 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 1 2022-06-05 20:06:38.483 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 1 2022-06-05 20:06:38.483 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 2 2022-06-05 20:06:43.490 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 2 2022-06-05 20:06:43.491 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 3 2022-06-05 20:06:48.498 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 3 2022-06-05 20:06:48.499 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 4 2022-06-05 20:06:53.509 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 4 2022-06-05 20:06:53.510 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 5 2022-06-05 20:06:58.518 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 5 2022-06-05 20:06:58.518 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 6 2022-06-05 20:07:03.530 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 6 2022-06-05 20:07:03.531 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 7 2022-06-05 20:07:08.538 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 7 2022-06-05 20:07:08.539 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 8 2022-06-05 20:07:13.552 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 8 2022-06-05 20:07:13.553 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 9 2022-06-05 20:07:18.561 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 9 2022-06-05 20:07:18.562 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] before request with id: 10 2022-06-05 20:07:23.570 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] after request with id: 10 2022-06-05 20:07:23.570 INFO 60551 --- [nio-8900-exec-2] c.e.c.d.service.CallGoServerService : [CallGoServer] done こちらもGoの時と同じく、リクエストからレスポンスまで50秒ほどかかっているのがわかります。これをCoroutineを持って並列化していきましょう。\nCoroutineで並列化する(1) Goと違って、KotlinのCoroutineは言語の基本仕様ではありません。なので、依存関係をまず追加する必要があります。ただ、公式の説明ではcoroutine-coreだけを追加すると対応できそうなイメージですが、SpringのようにReactive Streamが必要な場合はcoroutine-reactorを依存関係に追加する必要があります。\n依存関係を追加した上で、コードを直していきます。ここではSpring Bootを使っていて、Controllerの関数をsuspendにすることができるので、Contollerから呼び出している関数にもsuspendにしていきます。また、coroutineでの処理はスコープの指定が必要なのでループの周りをcoroutineScopeで包むようにします。その後はmap関数の中でasyncとしてAPIの呼び出しを行い、mapした結果はDeferredとして帰ってくるのでawaitAllで終了を待ちます。説明では複雑ですが、以下のコードをみるとわかりやすいかなと思います。\nsuspend fun callGoServerAsync(): List\u0026lt;CallGoServerDto\u0026gt; { logger.info(\u0026#34;[CallGoServerAsync] start\u0026#34;) return coroutineScope { // coroutineとして処理する tries.map { async { // 並列に実行 logger.info(\u0026#34;[CallGoServerAsync] before request with id: $it\u0026#34;) goServerClient.call(it) ?: CallGoServerDto(it, \u0026#34;failed\u0026#34;) } }.awaitAll() // APIの呼び出し結果を待つ .also { it.forEach { result -\u0026gt; logger.info(\u0026#34;[CallGoServerAsyncDual] after request with id: ${result.id}\u0026#34;) } logger.info(\u0026#34;[CallGoServerAsyncDual] done\u0026#34;) } } } また、APIを呼び出している関数(goServerClient.call())もsuspendにしておく必要があります。ここではSpringのRestTemplateを使い、以下のような関数を定義しておきました。\nprivate val client = RestTemplate() private val header = HttpHeaders().apply { set(HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE) } suspend fun call(id: Int): CallGoServerDto? { val request = HttpEntity(CallGoServerRequest(id), header) return withContext(Dispatchers.IO) { client.postForObject(\u0026#34;http://localhost:8800/api/v1/some-process\u0026#34;, request, CallGoServerDto::class.java) } } 上記のようにコードを修正して実行してみると、Goの時と同じく並列で10件のリクエストが送られているのがわかります。ただ、違う点としてはgoroutineと違って実行の順番が保証されているというところですね。この特徴があるため、Kotlinの場合はレスポンスのソートが必要ないです。\n2022-06-05 20:46:52.934 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] start 2022-06-05 20:46:52.939 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 1 2022-06-05 20:46:52.939 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 2 2022-06-05 20:46:52.939 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 3 2022-06-05 20:46:52.940 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 4 2022-06-05 20:46:52.940 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 5 2022-06-05 20:46:52.940 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 6 2022-06-05 20:46:52.941 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 7 2022-06-05 20:46:52.941 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 8 2022-06-05 20:46:52.941 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 9 2022-06-05 20:46:52.941 INFO 60551 --- [nio-8900-exec-1] c.e.c.d.service.CallGoServerService : [CallGoServerAsync] before request with id: 10 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 1 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 2 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 3 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 4 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 5 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 6 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 7 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 8 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 9 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 10 2022-06-05 20:46:57.951 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] done Coroutineで並列化する(2) 上記のコードもGoの時と同じく、リクエストを同時に全部送っているのは問題になる可能性があるので、同時に送信するリクエストの数を制限することにします。Goでもそうであったように、KotlinでもCoroutineの同時実行の数を制限する仕組みがあります。Semaphoreというものです。\nSempaphoreに数値を指定し、asyncの中でSemaphoreに指定した数で実行数を制限することで並行実行数を制限するような形です。以下がそのコードです。\nsuspend fun callGoServerAsyncDual(): List\u0026lt;CallGoServerDto\u0026gt; { logger.info(\u0026#34;[CallGoServerAsyncDual] start\u0026#34;) val semaphore = Semaphore(2) // 同時実行数を制限するためのSempahoreの定義 return coroutineScope { tries.map { async { semaphore.withPermit { // asyncの同時実行数をSemaphoreに指定した数値に制限 logger.info(\u0026#34;[CallGoServerAsyncDual] before request with id: $it\u0026#34;) goServerClient.call(it) ?: CallGoServerDto(it, \u0026#34;failed\u0026#34;) } } } }.awaitAll() .also { it.forEach { result -\u0026gt; logger.info(\u0026#34;[CallGoServerAsyncDual] after request with id: ${result.id}\u0026#34;) } logger.info(\u0026#34;[CallGoServerAsyncDual] done\u0026#34;) } } 書き方が少し違うだけで、Goとほぼ同じ感覚でasyncの処理を制限できるコードが出来ました。特にコンパイルエラーが出ることはないので勘違いしやすいところではないかと思います。async{ semaphore.withPermit{ } }の順番をちゃんと守る必要がありますので注意しましょう。実行結果は以下の通りです。\n2022-06-05 20:50:50.361 INFO 60551 --- [nio-8900-exec-6] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] start 2022-06-05 20:50:50.365 INFO 60551 --- [nio-8900-exec-6] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 1 2022-06-05 20:50:50.366 INFO 60551 --- [nio-8900-exec-6] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 2 2022-06-05 20:50:55.369 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 3 2022-06-05 20:50:55.369 INFO 60551 --- [atcher-worker-2] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 4 2022-06-05 20:51:00.377 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 6 2022-06-05 20:51:00.379 INFO 60551 --- [atcher-worker-2] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 5 2022-06-05 20:51:05.386 INFO 60551 --- [atcher-worker-2] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 7 2022-06-05 20:51:05.386 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 8 2022-06-05 20:51:10.393 INFO 60551 --- [atcher-worker-2] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 10 2022-06-05 20:51:10.393 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] before request with id: 9 2022-06-05 20:51:15.404 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 1 2022-06-05 20:51:15.404 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 2 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 3 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 4 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 5 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 6 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 7 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 8 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 9 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] after request with id: 10 2022-06-05 20:51:15.405 INFO 60551 --- [atcher-worker-8] c.e.c.d.service.CallGoServerService : [CallGoServerAsyncDual] done ログを吐く場所が微妙だったのですが、リクエストを送っている時間をみると、5秒置きで2つづつを送信しているのがわかります。\n最後に あまりCoroutineに詳しくないゆえ、もっと良い書き方はあったかなと思いますが(goroutineの実行順を決めておく、WaitGroup.Done()はdeferで定義する、Kotlinのログ出力箇所を調整するなど)、これで簡単にAPIの呼び出しを並列化することができるというのがわかったので個人的にはかなり満足しています。Jetpack Composeを少し触りながらcoroutineに触れたことはあったものの、こうやって仕事で必要となり調査と検証をしてみたのは初めてだったのでかなりの収穫を得たと言えますね。また、各言語においての感想は以下の通りです。\nGo 依存関係の追加なしで使えるのはメリット Kotlinのようにsuspendやscopeを意識しなくていいので便利 Kotlin asyncでも実行順が保証されているのはメリット goroutineよりは注意点が多い 二つの言語を比べると一長一短があるという感覚ですが、どれも応用が難しいものではないので、すぐにプロダクションコードにも適用できそうなものが書けるのは確かに良いものという印象を受けました。これからもcoroutineを使って性能向上ができる箇所はないか、色々と試してみたくなるものです。ちなみに、この記事に載せてあるコードを全体のソースはこちらのリポジトリから参照できます。\nでは、また！\n厳密にいうと、Coroutineによる処理はマルチスレッドによる並列化とは概念的には違うものですが、実装と結果の取得という面では感覚が大きく変わらないので、ConcurrencyとParallelismによる違いなどの理論的な話は割愛しています。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-06-05T00:00:00Z","image":"https://retheviper.github.io/images/magic.jpg","permalink":"https://retheviper.github.io/posts/server-side-coroutine/","title":"BackendでCoroutineを使う"},{"content":"1年ほどサーバサイドKotlinを扱いながら、ふと「今のKotlinはどこまできていて、これからはどうなるんだろう」と思うようになりました。色々な観点があると思いますが、とりあえず市場においてどれほどの需要があり、展望（これからも積極的に採用され続けそう、苦戦しそうなどの）はどうかなど、いわゆるkotlinという言語の「ステータス」について自分が感じていることについて考えてみたくなったというわけです。\n最近のトレンドを見ると、一つの言語において専門家になるというよりはさまざまな言語を使いこなせる、いわゆるPolyglotなプログラマが求められていて、常識みたいになっているとも言われているようです。確かに私自信もその経験があるかどうかは関係なく、案件によりさまざまな言語に触れるケースを多くみています。そして今は充実したドキュメントや記事をインターネットに溢れていて、UdemyやCourseraなど良質の講義を提供するサイトも色々とあるので経験がない言語だとしても入門が難しくてできないとは言えない時代になっている感覚でもあります。なので、自分が現在使っている言語がメインストリームに属してあるかどうかの問題は以前よりは重要でなくなった、といえるかもしれません。\nただ、立場や観点によっては一つの言語に集中したい場合もあるかと思います。例えば学生や、未経験からエンジニアに転職しようとする人にいきなり二つ以上の言語を扱えるように注文するのは難しいことでしょう。エンジニアの追求する技術においてもそうです。フロントエンドエンジニアがいきなり今すぐ使う予定でもないGoやJavaのようなバックエンドで使われる言語を勉強する必要はないはずです。そして会社としては、複数の言語を扱えるエンジニアを求めるということは採用において非常に厳しい条件となるはずです。なので、依然として市場において一つの言語のステータスというのは無視できないものなのではないかと私は思っています。\nというわけで、今回は多少主観的な観点からの話になりますが、他の言語や分野で、Kotlinという言語の展望について考えてみたことを述べたいと思います。それでは、どうぞ。\nvs Java Better javaという捉え方 Kotlin(JVM)をJavaと比べると、コンパイル結果がバイトコードを生成するため、「Javaと互換性が完璧であり、性能もまた変わらない」というのが世間一般でいうKotlinの評価ではないかと思います。その上拡張関数やCoroutine、スコープ関数、Null安全性などさまざまな機能が揃っているので、表面上はbetter javaと読んでも良いのではないかと思わせる面もあります。それに、JavaのバージョンアップでJVMの改良が行われると、結局それもKotlinの改善につながることとなりますね。Javaも1.8以降は半年に1回のリリース政策によりバージョンアップが早くなっていまずが、まだアプリケーションエンジニアの立場からするとKotlinと比べ惜しいところもなくはないかなと思います。\nここまでの話だと、Kotlinは完璧にJavaを代替できる言語であるかのように聞こえます。つまり、これからは全くJavaを使う理由はなくて、何もかもKotlinに移行するという選択肢しかないかのようにですね。しかし、業界の事情はどうなのでしょうか。\nまずJavaの歴史から考えてみましょう。Javaは長い間、「世界で最もよく使われる言語」であって、他の言語が人気を得た今でもTop 5に入るほどの人気な言語となっています1。そしてこれが示唆するのは、単純に今の人気、つまり、「これからも使われる可能性」だけの話ではなく、「今まで使われた回数」が圧倒的に高いということも意味するという点です。今まで作られた多くのシステムやアプリケーションがJava基盤になっているので、余程のことがない限りは維持保守や機能の拡張においてJavaのエンジニアを求めることになるでしょう。\nまた、こういう側面もあります。JVM言語としてJavaのメリットを活かしつつ、より発展したコードを書けるというコンセプトで登場した言語はKotlinだけではないということです。今までClojure・Scala・Groovyなどさまざまな言語が登場し、それぞれの言語がそれなりの需要や分野を確保・拡張できてはいるものの、そのうちどれも「Javaを超えた」という評価をもらってはいないのが現状かと思います。同じくKotlinの場合も、その立場が他のJVM言語と大きく変わっているとは言えないものではないでしょうか。なので、「JVM言語だ」「Javaよりモダンだ」という特徴は、少なくともKotlinが今後Javaを超えられるという根拠にはならないかと思っています。\nモバイルではAndroidの言語としてJavaよりKotlinを採用する例が多くなっているかと思いますが、これはOracleとGoogleの訴訟絡みでJavaを1.8しか使えなかったことも理由の一つかと思います。現在Javaがよく使われているWebの場合、OpenJDKのバージョンに特に法理的な問題もなく、Java 17からはOracleJDKも無償で利用できるようになったので、モバイルとはまた状況は違うのではないかと個人的には思っています。\nもちろん、上記の問題はJetbrainsでもその点は最初から認識していたため、最初からKotlinがJavaと相互運用できる言語として設計した部分はあります。なので、あくまで既存のJavaアプリケーションをKotlinでリプレイスする、というよりは、部分的な移行から新規開発で占有率を徐々に上げていくことを目標としているのではないかと思います。その戦略は十分に納得できるもので、あとは企業の方でJavaとKotlinという二つの言語を同時に運用することに抵抗がなければ、Javaを使っていた場合でも問題なくKotlinを受け入れられると思います。実際、自分の場合でもJavaからKotlinの移行は全く問題ありませんでした。\nKotlinも強くなる 最近のフレームワークやライブラリの方をみると、まだKotlinがモバイル以外の分野での認知度は劣るものの、少しづつJavaがメインストリームであった分野で採用されているケースが増えてきているような気もします。例えば、自分が仕事で使っているSpring boot、Jackson、AWS SDKなどウェブアプリケーションで有名なものがKotlinに対応していて、他にもjOOQ、jooby、JavalinのようにJavaとKotlinの両方に対応しているものも増えています。\nもしくは、Javaで存在していたライブラリをKotlin向けに調整したものもあります。例えばTornadoFX、gRPC、RxKotlinのようなものがそうです。そして、最初からKotlin専用として設計されたものも少なくないです。Kotlin Serialization、Klaxon、DGS、Ktorm、Kotest、MockK、Exposed、KMongo、Xodus、Koin、Kodein-DIなどがそうですね。なので、Javaの世界に寄生していた数年前とは違って、Kotlinだけでもウェブアプリケーションを十分構築できるレベルまできているのではないか、というのが自分の考えです。\n結論として、まだ二つの言語を比べると、Javaの方が圧倒的に規模はでかく、知名度でも上にあるのですが、Kotlinも競争できる力を身につけてきたので、これからは十分状況が変わる可能性がある、と思っています。\nvs Go 「早い」の美徳 仕事でGoを使っている立場からすると、Kotlinに比べGoの優越な側面はやはり「とにかく早い」ということではないかと思います。基本的にネイティブにコンパイルされる言語なのでランタイム性能も優秀なはずですが、コンパイルもビルドもとにかく早いのは確かに良いなと思いました。特に、コードの修正後にユニットテストで検証してみることが多いのですが、Kotlinのプロジェクトと比べるととにかく早いのでストレスがないですね。(Kotlinの場合は使っているウェブフレームワークがSpringで、テストケースがより多い、ビルド時にはシングルスレッドでやっているということもありますが)\nそのほかにもGitHubのパッケージをそのまま使えたり、別途ライブラリを使わなくてもstructをすぐにJSONとして扱える(omitemptyとかも便利な場面がある)なところは印象的で、かなりウェブ開発に特化されているなという印象までありました。ネイティブなのでビルドして生成されるバイナリのサイズが小さいのも良いですね。これらの特徴からして、最近トレンドとなっているサーバレスやマイクロサービスなどおいてはKotlinよりGoを採用した方が有利な面が多いかなと思います。\nまだサーバがクラウド上のVMに移行したばかりの頃は、JVMを使う言語の問題はだいたいマシンスペックの向上により無視できました。しかし、サーバレスとマイクロサービスアーキテクチャが流行りながらJVMの特徴が再び問題となってきていますね。まずサーバレスだと、JVMが完璧にロードされるまで時間がかかるる上に、さらにコールドスタートにも時間がかかります2。また、マイクロサービスにおいては、JVMが占めるヒープメモリとストレージが増えることでインスタンスごとのコストが増えるという問題が挙げられています。\nKotlin != 遅い このような問題に対して、サーバレスだとKotlessのようなフレームワークが開発されていたり、GraalVMを利用してネイティブビルドができるQuarkusやSpring Nativeが開発されるなど、JVM言語でも最近のトレンドに合わせて改善が行われていはいます。\nランタイム性能という面では、JITによる最適化でJVM言語でもGoには劣らないという面もありますね。ベンチマークを見るとKotlin/JVMとの比較やKotlin/Nativeでわかるように、Goに対してKotlinが性能で劣る部分もあれば、優位にある部分もあるのがわかります。\nまた、Go 1.18でジェエリックが導入されていますが、ジェネリックにより遅くなる可能性があるという話もあり、これからもしGoに新しい機能が追加されるとしたら、それがコンパイル速度やランタイム性能に影響を及ぼす可能性もあるかなと思います。\nなので、KotlinとGoという二つの言語で考えると、少なくともパフォーマンスという観点だけではGoにこだわる必要はないかなと思います。しかし、アプリケーションの開発において言語を選ぶ基準はパフォーマンスだけでなく、生産性やクラウドで対応している言語、エンジニアが確保できるかなど色々な側面があるので、Goの代わりにKotlinを選んだほうが効率的だとは言えないのも事実です。自分が転職を決めた時も、サーバサイドではGoのエンジニアを募集している企業の方がKotlinより多かったのですが、単純にパフォーマンスが基準だとしたらこのようなことにはならなかったでしょう。Googleが推している言語であるとか、パフォーマンスだけでなく生産性の面でも優れているなどさまざまな理由が複合的に作用した結果だと言えるものかと思います。\nそれでも有利なのは あとは、そもそもの知名度の問題ですね。Kotlinにおいて、ネイティブイメージのビルドができ、性能が劣らないとしても、多くの場合はKotlinをモバイル(Android限定)用の言語だと認識しているのが一般的かなと思います。なので、このような認識がエンジニアと企業で変わらない限り、これからもGoの方がサーバサイドでは市場において優位に立つという状況がしばらくは続くのではないかと思っています。\n他にも、Goはその書きやすさからや入門のしやすさの人気もあると思いますが、それを踏まえると比較的書き方が複雑なKotlinの方が劣るのではないかという推測もできそうですね。自分にとってはKotlinの書き方が簡潔で良い感覚ですが、Goの書き方を簡潔だと思っている方もいるようです。確かに、キーワードが少なく、それらを覚えるのに労力が比較的少ないなら、よりロジックに集中した書き方ができるはずですね。Goで作られたアプリやCLIツールなどが増えているのも、そのような特徴からのものなのではないでしょうか。自分の場合は簡単なツールを作るときはPythonで書くのを好みますが、同じく気軽にコードが書ける稼働かの観点でいうと、KotlinよりGoが優れているとも言える気がします。なので、個人の趣味やサイドプロジェクトなどでよく使われ、それがまた人気につながるだろうと思うと、Goを好むエンジニアが増えるのもおかしくはないですね。\nvs Rust 最強の性能？ GCがないので同じネイティブでありながらもGoより性能が優秀だというRustですが、これもまたKotlinと同じく、知名度の問題で苦戦しているところがあるかなと思います。そもそもC/C++を代替するのが開発の目的でもあったため仕方ないのかもしれませんが、どちらかというとエンベデッドで使われるイメージがどうしてもあるような気がしますね。意外とFigma、1Password、Discord、Dropbox、Mozilla、Line、npm、Cloudflareなどさまざまな組織で採用されていて、exa、bat、difftastic、bottomなどのCLIツールからyew、seed、Dioxus、Rocket、tide、poemのようなGUIやウェブフレームワークなどがたくさん開発されていますが、これもまた特に調査してみないとわからないくらいです。\nさまざまなベンチマークでその性能が検証されていて、使ってみたエンジニアからも評判の高いものとなっているRustですが、やはり知名度が低いので、企業からも採用するのはかなり難しい判断になるでしょう。実際Jetbrainsの去年の設問ではRustは趣味もしくは個人用途、サイドプロジェクトで使うと答えた割合のエンジニアが多かったのを見ると、やはり企業の需要はあまりようです。ただ、逆にいうと、このようにRustに好意的なエンジニアが増え、さまざまなプロジェクトで使われ始めるといつか市場の状況も変わっていく可能性もあるということです。先ほど述べたGoのケースのように、比較的に歴史の短い若い言語でも十分その価値を立証できるのであれば市場でもメインストリームに合流できます。なので、Rustの未来はむしろ明るく、これからが期待される言語だと個人的には思っています。ただ、人気を得た後も、ウェブアプリケーションを開発するよりは今まで通りエンベデッド・システムプログラミングに特化していきそうな気がしますね。\nKotlinをネイティブにしたら RustをKotlinとの比較をするとしたら、Kotlin/Nativeがあるので、言語自体でできることはそう変わらないものの、Rustがエンベデッドやシステムプログラミングという分野でC/C++を代替していく傾向があるのに対して、これといった成果があまり見当たらないというのが問題かなと思います。特にKotlin/NativeはLLVM基盤なので、GraalVMによるネイティブコンバイるができるウェブフレームワークが登場している今はますますそのポジションが曖昧なものになっている気もします。Object-CやC/C++とのinteropができると言われていますが、そのようなユースケースだとそもそもObject-CやC/C++といった言語を使った方が色々と有利なのではないでしょうか。もちろん、Rustには所有権のような概念があり、他の言語と比べプログラミングが難しいとされているので、Kotlin/Nativeを採用した方がコーディングは楽になるかもしれません。でも、Nativeを追求するならやはりパフォーマンスが重視される場面が多いので、そこではGCのあるKotlinが不利な気がしますね。このような面からすると、やはりKotlin/Nativeのポジショニングが難しそうな気がします。\n結論としては、Kotlin(JVM)とRustはそれぞれ特化した分野が違っていて、大きな変化がない限り互いの領域を蝕むことなく発展していきそうです。どちらかというとKotlin/Nativeが直接的なライバルになる可能性はありますが、そもそものポジショニングが曖昧なところがあるので、Nativeがどうしても必要な場面ではRustが使われる可能性が高いのではないか、という気がしています。\nvs Python 万能ツール ここ数年で最も人気を得ている言語の一つ、Pythonの場合は、Kotlinと比べて見るとどうでしょうか。まず自分の場合だと、日常での自動化や簡単なツールを作る場面ではPythonの方をよく使っていて、本格的なウェブアプリケーションを開発するとしたらKotlinを選ぶことが多いです。もちろん、なんでもできる言語なので大規模のアプリケーションを作るのにPythonがNGというわけではないです。実際Uber、Google、PayPal、Netflixなど有種の企業がPythonを使っていて、あの有名なInstagramのサーバサイドもPythonで書かれていると言われていますね。\nただやはり、PythonはデータサイエンスやAIといった分野でよく使われているイメージがあり、使いやすく、そこまで性能が求められていない場面でなら良いものの、個人的にはその限界が明確であることが問題かなという気もします。本格的な業務用のアプリを開発した経験がないのであくまで印象と推測の話となりますが、Pythonをサーバサイドに取り入れている企業は大概がスタートアップであって、サービスが古くなるとインタープリター言語特有のメンテが難しくなるという問題が出てくる可能性が高いではないかと思いますね。JavaScriptの例もありますが、Pythonのタイプヒントはあくまでヒントであって、TypeScriptのようにコンパイルタイムで検出できるエラーを確実にわかるわけでもないです。あとは性能ですが、GILのような問題もあります。このような問題を認識しているため、検証用のアプリ(プロトタイプ)をPythonで書いてから他の言語に移行するという例もあるのかなと思ったりしています。\nPythonだけの領域でもないが 逆にKotlinでいうと、Jupyterを使えるなどKotlinでもデータサイエンスに使えるのですが、すでにPythonが市場支配的な言語になっているところでどこまで伸びるかが問題な気がしますね。JetBrainsが主張するように、Pythonと比べ「静的型付け、Null安全性、パフォーマンス」というのは確かにKotlinが持つメリットではあるのですが、そもそものユーザ数が増える何かがないと占有率を上げるのはかなり難しくないのではないかと思います。Pythonは入門が簡単なので講座も多く、実際エンジニアではない人も使うケースが多いのですが、Kotlinはまだそのような面では弱い印象ですね。\n以上のことからして、Pythonは依然としてデータサイエンスなど元々強かった分野に対してはこれからも需要が大きく変わることはなさそうです。ウェブという分野では競合になる可能性はありますが、どちらかというとKotlinを採用した方がより安定した開発ができるので大規模なアプリの開発ではKotlin、小規模ではPythonという形になるのではないかと思います。もちろん、大規模のアプリを開発するにあたってはまたの選択肢があるのでKotlinではない他の言語が採用される可能性の方が高そうですが、あくまで二つの言語を比べた場合の話となります。\nvs JavaScript 多芸多才 一つの言語でなんでもできちゃう言語が何かというと、過去はJava、少し前はPython、そして今はなんといってもJavaScriptではないかと思います。フロントエンド、バックエンド、モバイル、データサイエンスなどさまざまな分野で活躍している言語ですね。ランタイムの性能が問題となっている部分に対してもDenoのような新しいランタイムが登場したり、V8エンジンの持続的な改善によりだんだん補完されていって、静的型付けに関してもTypeScriptの台頭によって解決されています。まさに無敵の言語のようにも見えます。\nフロントエンドにおいてはJavaScript以外は考えられない3というのもあり、WebAssemblyのような技術も発達していますが、これはまたウェブの画面描画だけでなく違う方向に向かっているような感じなので、これから何かあって(あるとは思いませんが)色々な分野で使われなくなるとしてもJavaScriptそのものが使われなくなることはないでしょう。そして同じ意味で、Kotlinがそのような分野に進出するのもかなりハードルが高いと思います。\nKotlinでフロントエンド？ Kotlinで言えば、Kotlin/JVMとKotlin/Native以外に3つの軸として存在しているのがKotlin/JSであり、JetBrainsの発信を見るとそこそこ力を入れている感覚ではあります。他にも、Compose Multiplatformを通じて、モバイルだけでなくウェブやデスクトップアプリにおいてもKotlinでGUIを作成できるようになったので、なるべく自分のサイドプロジェクトなどではKotlinで完結したいと思っている私の場合はこちらも応援したいと思っています。ただ、まだモバイル以外ではそこまでメジャーではなく、新しい技術の問題(ライブラリの不足、バージョンアップによる変化が激しいなど)が考えられるのでしばらくは様子見な感じですね。あと自分のような特殊な目的がない場合は、個人でも企業側としても無理して採用すべきメリットが薄いという問題もあるかなと思います。\nバックエンドだとKotlinが競合になる可能性はまだ十分ではないかと思います。特に、今までJavaが採用されていた分野だと主にJVMの安定性や数値計算の精度など検証された安全性というものがあるので、これから言語を変えるとしたらKotlinを採用する確率が高いのではないかと思っているところですが、そのような分野だと、最初からJavaScriptによるバックエンドの採用は考えない可能性が高そうです。スタートアップのようにエンジニアの求人が難しく、使われる技術の数を減らしてなるべく工数の削減しようとするか、Pythonのようにプロトタイプのアプリを作るかなどの特殊な状況ではない限り積極的にバックエンドの言語としてJavaScriptを採用する例はあまりなさそうな気がしていて、これからもおそらくそれは大きく変化していく気はしません。ただ、自分のようにKotlinで何もかも解決したい、という方がJavaScript側にもいらっしゃるとしたら、そこはまた話が変わってくるかもしれませんね。フロントエンド、バックエンド、モバイル、デスクトップまで対応したい場合にはJavaScript以上のものがないので、その会社や個人の目的次第でJavaScriptが採用される可能性は高く、そのような状況こそKotlinは採用されない可能性が高いかなと思います。\nvs Dart GUIの最強者？ Dartの場合は、言語そのものというよりはFlutterが最近熱いですね。最初はモバイルでクロスプラットフォーム開発ができるということで注目されたものですが、Dartに対してはFlutterの最大の競合はReact Nativeだと言えるかなと思いますが、それも最近のトレンドを見ると少しづつ逆転してきているような気がします。もちろんこれはあくまでも「クロスプラットフォーム用のフレームワーク」という基準での比較であり、実際は色々と複雑な事情があるでしょう。例えば、フロントエンドエンジニアがモバイルの開発も担当していて、フロントエンドのライブラリとしてはReactを使っているなどの状況を考えると、ここでいきなりFlutterを採用する可能性は低くなるはずですので。\n自分が思うにDartの最大の問題は、その最初の意図(JavaScriptを代替するという)はともかく、言語そのものの印象が薄いということです。少しだけ触ってみた感覚では、いわゆるC-Family言語としての馴染みはあっても、特段ここが魅力的だというところはあまり感じ取れなかったです。それが今はFlutterのおかがで使用率は上がってきていますが、それ以外の分野ではどうかなという疑問がまだあります。\n可能性は他にもあるかもだけど ただ、以前からGoogleの次世代OSであるFuchsiaではメインの開発環境となるという噂もあり、FuchsiaそのものがどんなOSになるかはまだ不明な状態ですが、もし噂通りAndroidの次世代のOSになるとしたら、ネイティブの開発そのものがDartによるものにもなり得る可能性はありますね。もしそうなると、ChromeOSを含めレガシーの環境を捨てることになるので、公式の開発の言語にKotlinを指定した時とは比べ物にならないインパクトがあることを想定すると、なかなか想像できない事態です。\nもちろんDartもプログラミング言語なので、これからのフレームワークやライブラリの開発次第でいくらでも状況は変わる可能性があります。こちらのリポジトリを参照すると、サーバサイドのフレームワークもすでにいくつか存在しているので、自分の考えているKotlinで全てを解決する、という目標においてはむしろDartの方がやりやすい可能性がありますね。Kotlinの方だとKotlin/Multiiplatform Mobileがありますが、これはどちらかというとビジネスロジックの共通化を目標としているものなので、結局iOSのコードを書く必要があります。もちろん、一部の企業でやっているように「UIはFlutterで、ビジネスロジックはネイティブで」ということもできるかとは思いますが、あまりメジャーなやり方にならないかなと思います。実際、Swiftの場合もVaporのようなフレームワークがあり、サーバサイドでも十分使えるということをアピールしていますが、採用しているエンジニアや企業が限りなく少ないというのを見ると、単純に「できる」だけでは十分ではなさそうですので。\nモバイルでも強くなっていく 特に今年開催されたGoogle I/Oで確認できるように、Flutter 3ではさらにパフォーマンスの向上やFlutter Desktopの正式リリースなど様々な面での発展を見せていて、これからもFlutterの未来は明るく見えます。Flutterを採用している企業も増えてきているので、このような発展の恩恵を受け入れるのは結局時間の問題に過ぎない気がします。もちろん、ネイティブアプリの開発においても需要はこれからもあり得ると思いますが、クロスプラットフォームアプリでも事足りる分野が増えてくるとしたら、どちらがメインストリームになるかは目に見えるようなものですね。\nこのような状況では、今の占有率においてKotlinのホームグラウンドとなっていると言っても過言ではないモバイルの分野で、Flutterの成長ぶりはある意味、Kotlinにおいては脅威のようなものではないかという気がします。なので、これからKotlinならではのメリットをより強化していく必要がありそうですね。先の述べたKotlin/Multiplatform Mobileのようなものが、その役割をしてくれるのではないかと期待しています。そのほかでも、Kotlinでできることは多いので、分野を問わない連携を強化していくと十分Kotlinを利用するメリットはこれからも出てくるでしょう。\n最後に 今回は、いつもと違って自分の考えが中心になる記事なので、色々と偏った判断があるかもしれませんが、とりあえずKotlinエンジニアとしての感想をまとめてみました。もちろん、自分の知見が足りてなく、モバイルやフロントエンド、データサイエンティスト、DevOpsエンジニアなど色々な分野で活躍されている方からしたら色々と間違っているか、的確ではない情報や判断も目立つかなとも思います。\nただ、一人のエンジニアとして、ただの時流を淡々と見つめているよりは、目指す目標に対して使っている技術や興味のいくものに注目し、自分なりの判断をしてみるのもまた必要なものではないかという気がして、このような記事を作成することになりました。また、このような記事を作成することで、この後に色々な変化があって自分の展望がどれだけあっているか、実際と比べてみるのもまた有意義な振り返りとなりそうな気もします。\n今回はあまり情報がなく、Twitterにでもつぶやいたら良いかも知れない雑談に近いものですが、少しでもここでKotlinのことを改めて認識できたという方がいらっしゃるなら幸いです。\nでは、また！\nIEEE Spectrum、TIOBE、Stack Overflow、Jetbrainsの調査結果を参照しました。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWarm(アイドルインスタンスを常に立ち上げておく)で対応できる部分ではありますが、スケールアウトするとコールドスタートが必要となる場合があり、インスタンスを立ち上げておくことでコストがかかる問題は避けられないですね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDartのような言語でJavaScriptを代替しようとした歴史がありますが、今は失敗していて、JavaScriptがより高度化した今はTypeScriptのようなスーパーセットやJavaScriptにトランスパイルできる言語でないとフロントエンドの言語を代替するのは難しいかと思われます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-05-22T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-prospect/","title":"Kotlinのこれからを語る"},{"content":"最近友人がYoutubeのチャンネルを始めました。主に海外のYoutubeのチャンネルにアップされてあるドラマのメイキングやインタビュー動画などに字幕をつけて配信するというチャンネルですが、そこで毎回字幕を作るのはかなりめんどくさい作業なので、ちょっとしたコマンドaliasとして作成して提供することにしました。元の動画には原語の字幕がついていて、編集用ツールでそれを読み込むことができるらしいので動画だけでなく字幕もダウンロードするコマンドを作成しています。これなら翻訳した字幕ファイルを編集ツールで読み込んで使えるので、かなり作業量が減りました。\nただ、チャンネルの登録者数が増えながら、中には色々な国からの人もいたようで、友人から「自動翻訳を使って多国語の字幕を追加できる手段が欲しい」という依頼が来ました。簡単なツールを作れば良さそうだったので、早速ツールを作って公開することになったので、今回はそのツールを作った話をしたいと思います。\n設計 まずは友人の要望と、技術スタック、アプリの基本設計を行いました。といっても、複雑なことはあまりしたくなかったので、（友人の）やりたいことをベースに、自分がやってみたいことを混ぜたくらいのレベル感です。\n字幕 youtube-dlやyt-dlpを使ってYouTubeの動画をダウンロードする際、元の動画に字幕がついてある場合はオプションを追加することで字幕も落とせます。Youtubeで字幕をダウンロードすると、多くの場合ファイル形式がWebVTTになるので、とりあえず.vtt形式のファイルに対応する必要があると思いました。他のフォーマットまで対応するには時間がかかりそうだったので、まずはこれのみにします。\n幸い、実際の字幕ファイルを落としてみたところ、フォーマットはシンプルで(SubRipとほぼ一緒)、翻訳は字幕が表示される時間の次の行の文字列と、その行のインデックスとともに取ればなんとか入れ替えができそうな気がしました。実際のファイルは以下のようなフォーマットで作成されています。\nWEBVTT 00:01.000 --\u0026gt; 00:04.000 液体窒素を絶対に飲まないでください。 00:05.000 --\u0026gt; 00:09.000 - それはあなたの胃に穴をあけます。 - あなたは死ぬ可能性があります。 言語翻訳API 翻訳についてはGoogle翻訳もありますが、友人の要請によってPapagoを使うことにしました。幸いAPIを無料で利用できて、PythonのサンプルコードやAPIの詳細も提供されていたのであまり利用は難しくはい感じでした。事前にPostmanでAPIを実行してみてからこちらを採用することにしました。\n技術 まずPythonで作ることにしました。この規模の小さいアプリは、Kotlinがメインの私でもPythonで書きたいと思っています。JVMだと起動が遅くなるのもあり、テキストを扱うだけでCPUの処理がクリティカルな作業でもないためパフォーマンスを考慮する必要もあまりありません。そして既にPythonでファイルを読み込んだり、ファイルを編集するなど色々とCLI用のツールを作っていたので「ファイルを読み込んで、編集して、保存する」という最重要の機能がすぐに実装できそうと思いました。\nただ、自分が使うものならCLI上でも十分使えますが、友人が使うものなのGUIを実装することにしました。Pythonは.pyファイルで渡すと、Pythonとスクリプトのdependencyのインストールが必要となるなどエンジニアではない人が触るには不便なところがあるので、成果物はPyInstalerなどで実行できるバイナリにします。\nGUIに関してはPySimpleGUIを使ったことがありますが、今回は違うフレームワークを使ってみたいと思いました。PythonのGUIフレームワークといえばtkinterやKivy、wxPython、Libavgなど様々なものがあるのですが、中でもQtがPythonだけでなく色々と使われているので、PythonのコードだけでなくC++のコードでも参考できる例が多いのではないかと思いPyQtの方を選びました。\nプロダクションレベルのものを作るならまた色々と基準を持って検討してみたかもしれませんが、このような趣味レベルのコードを書く場合はなるべく手軽に書ける、サンプルの多いものを選ぶのがちょうど良いかもですね。効率というのも大事なので。\nコード ファイル読み込み まずはファイルを読み込むところから始めます。.vttファイルはヘッダーに字幕の情報（言語コードなど）が書かれていて、その次からは字幕が表示される時間と字幕の文字列が繰り返されます。\n素直に全行を読み込んでも良いかもですが、翻訳のAPIを呼び出すときに一日で利用できる文字数の制限というのがあったので、極力リクエストにのせるデータは減らしたかったです。なので、後で入れ替えするためのもとのファイルのデータと、翻訳のためAPIのリクエストパラメータにのせる二つのデータにわけで読み込むようにしました。いかがそのコードです。\n# get original contents from vtt file def get_content(self, file_path: str): exported_contents: dict[int, str] = {} global source with open(file_path, \u0026#39;r\u0026#39;) as file: original_contents: list[str] = file.readlines() for index, line in enumerate(original_contents): content = line.strip() if source == \u0026#39;\u0026#39; and \u0026#39;Language:\u0026#39; in content: source = content.split(\u0026#39;:\u0026#39;)[1].strip() if \u0026#39;--\u0026gt;\u0026#39; not in content and content != \u0026#39;\u0026#39;: exported_contents[index] = content return original_contents, exported_contents APIのパラメータには元の言語(source)と、翻訳したい言語(target)が必要となるので、Global変数として字幕のヘッダにある言語情報をsourceに格納、翻訳したい行はインデックスと文字列をdictionaryとして格納します。そして最終的に元のデータと、翻訳対象のデータを返します。\n翻訳APIの呼び出し 抽出したデータのうち、翻訳対象のデータを渡してAPIを呼び出す部分です。APIには「1秒当たり10件のリクエストを許容」するという制約があり、翻訳した文字列を一行づつ送って翻訳してもらいながら、リクエストごとに120msを待つようにしました。基本的にはRequestsを使ってPOSTしているだけですが、もしエラーが返ってきたときはalertを表示してアプリを即終了するようにしています。\n# send translate request to papago and get translated message def send_request(self, contents: dict[int, str]): translated_contents: dict[int, str] = {} headers = { \u0026#39;X-Naver-Client-Id\u0026#39;: client_id, \u0026#39;X-Naver-Client-Secret\u0026#39;: client_secret, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/x-www-form-urlencoded; charset=UTF-8\u0026#39; } for index, content in contents.items(): payload = { \u0026#39;text\u0026#39;: content, \u0026#39;source\u0026#39;: source, \u0026#39;target\u0026#39;: target } response = requests.post(api_url, headers=headers, data=payload) if response.status_code == 200: body = response.json() translated_text: str = body[\u0026#39;message\u0026#39;][\u0026#39;result\u0026#39;][\u0026#39;translatedText\u0026#39;] translated_contents[index] = translated_text # wait for API\u0026#39;s limitation(only 10 request per second allowed) sleep(0.12) else: msgBox = QMessageBox().critical( self, \u0026#39;Error\u0026#39;, response.json()[\u0026#39;errorMessage\u0026#39;], buttons=QMessageBox.StandardButton.Abort ) if msgBox == QMessageBox.StandardButton.Abort: sys.exit(255) return translated_contents ファイルの保存 ロジックとしてはここが最後です。元のファイルのある場所に翻訳された字幕ファイルを保存するために、元のファイルのパス、翻訳したものに入れ替えるための元のデータ、そしてAPIで翻訳した結果のデータを渡します。\n基本的に翻訳したデータにはインデックスがkeyとして入っているので、そのキーを持って元のデータを入れ替えていきます。ここでヘッダには元の言語のコードが入っているので、翻訳した言語のコードに入れ替えます。そしてファイル名にも同じく言語のコードが入っているので、これもまた変えておきます。\n全てのデータが入れ替えられ、ファイル名を決めたら保存して終了です。\n# write translated contents to file def write_result(self, file_path: str, original_contents: list[str], translated_contents: dict[int, str]): contents = original_contents.copy() for index, content in translated_contents.items(): if \u0026#39;Language:\u0026#39; in content: contents[index] = content.replace(source, target) + line_separator else: contents[index] = content + line_separator root = os.path.dirname(file_path) file_name = os.path.basename(file_path).replace(source, target) target_file_path = os.path.join(root, file_name) with open(target_file_path, \u0026#39;w\u0026#39;) as file: file.writelines(contents) ファイルドロップダウン せっかくGUIを使っているので、QListWidgetにファイルをDrag \u0026amp; Dropすると勝手にファイル名が画面に表示され、翻訳対象にもなるようにしたかったです。ここはあまり自分がQtに詳しくないので、ネットで検索したものを流用しました。\n基本的にはファイルをDrag \u0026amp; Dropすると、拡張子が.vttの場合にリストに追加されます。ここでファイルのフルパスを得ることができるのですが、画面上はファイル名だけを見えるようにして、フルパスは別のグローバル変数に格納するようにしています。\n# drop down file list class FileListView(QListWidget): def __init__(self, parent=None): super(FileListView, self).__init__(parent) self.setAcceptDrops(True) def dragEnterEvent(self, event): if event.mimeData().hasUrls: event.accept() else: event.ignore() def dragMoveEvent(self, event): if event.mimeData().hasUrls: event.accept() else: event.ignore() def dropEvent(self, event): if event.mimeData().hasUrls: event.accept() for url in event.mimeData().urls(): file_path = url.toLocalFile() if file_path.endswith(\u0026#39;.vtt\u0026#39;) and str(file_path) not in original_files: # add file path to list original_files.append(str(file_path)) # add file name to file list view self.addItem(file_path.split(\u0026#39;/\u0026#39;)[-1]) else: event.ignore() ファイル削除 リストにファイルを追加したら削除したい場合もあるかと思ったので、リストからファイルを選択してボタンを押すとリストから消えるようにしたかったです。これは以下のようなコードで実現しています。ボタンをクリックすると、リストで選択したものが消え、翻訳対象（フルパス）からも消えるようにしています。\n# create remove button def create_remove_button(self): button = QPushButton() button.setText(\u0026#39;Remove file\u0026#39;) button.clicked.connect(self.remove_file) return button # remove file from file list def remove_file(self): global original_files for selected_item in self.list_view.selectedIndexes(): index = selected_item.row() self.list_view.takeItem(index) original_files.pop(index) 言語選択 元の字幕ファイルの言語はヘッダから抽出できるのですが、どの言語に翻訳したいかは自分で選べるようにしました。ここは翻訳APIの使用を参考にして、翻訳可能な言語のコードをdictionaryとしています。あえてdictionaryとしている理由は、QComboBoxのアイテムは画面に見えるものなので実際の言語名にしたかったからです。アイテムがdictionaryのキーとなっているので、選択したアイテムに対応するvalueが翻訳したい言語となるようにしています。\n# languages supported_language_code: dict[str, str] = { \u0026#39;Korean\u0026#39;: \u0026#39;ko\u0026#39;, \u0026#39;English\u0026#39;: \u0026#39;en\u0026#39;, \u0026#39;Japanese\u0026#39;: \u0026#39;ja\u0026#39;, \u0026#39;Chinese(China)\u0026#39;: \u0026#39;zh-CN\u0026#39;, \u0026#39;Chinese(Taiwan)\u0026#39;: \u0026#39;zh-TW\u0026#39;, \u0026#39;Vietnamese\u0026#39;: \u0026#39;vi\u0026#39;, \u0026#39;Indonesian\u0026#39;: \u0026#39;id\u0026#39;, \u0026#39;Thailand\u0026#39;: \u0026#39;th\u0026#39;, \u0026#39;German\u0026#39;: \u0026#39;de\u0026#39;, \u0026#39;Russian\u0026#39;: \u0026#39;ru\u0026#39;, \u0026#39;Spanish\u0026#39;: \u0026#39;es\u0026#39;, \u0026#39;French\u0026#39;: \u0026#39;fr\u0026#39; } # create drop down menu def create_target_language_selector(self): label = QLabel() label.setText(\u0026#39;Select target language\u0026#39;) selector = QComboBox() selector.addItems(supported_language_code.keys()) selector.textActivated.connect(self.set_target_language) return label, selector # set translate target language when dropdown menu selected def set_target_language(self, selectd: str): global supported_language_code global target target = supported_language_code[selectd] 翻訳ボタン 最後に翻訳ボタンを追加して、実際の翻訳はボタンを押下した時に行われるようにしました。ボタンを押すと、翻訳対象のファイルのパスをループしながらデータの読み込み、元の言語と翻訳したい言語が違うと翻訳APIの呼び出し、結果の保存という作業を行うようになっています。念の為処理が行われる間はこのボタンを非活性化するという処理も追加しています。\n# create translate button def create_translate_button(self): self.translate_button = QPushButton() self.translate_button.setText(\u0026#39;Translate\u0026#39;) self.translate_button.clicked.connect(self.translate_files) self.translate_button.setDisabled(False) # read vtt file and do translate def translate_files(self): self.translate_button.setDisabled(True) for file_path in original_files: original_contents, exported_contents = self.get_content(file_path) if source == target: continue translated_contents = self.send_request(exported_contents) self.write_result(file_path, original_contents, translated_contents) self.translate_button.setDisabled(False) 実際作ったツールで重要な部分は以上で、全体のソースコードはこちらから確認できます。\n改善したい こうやってとりわけ欲しい機能は実現できたのですが、まだ自分がPyQtに慣れてないのもあり、ロジック上でも少し改善したいところは残っていますので、いくつかを挙げてみました。\nテキストをまとめて翻訳する 翻訳APIの仕様上、リクエストは1秒で10回となっているため、sleepを入れて一つのリクエスト毎に120msを待つようになっています。I/Oが発生することを考慮するともっと間隔を短くしてよかったかもしれませんが、そもそもの問題は、読み込んだファイルのデータから一行づつ翻訳のリクエストを送っているところです。短い動画でも字幕は数百行となるケースがあるので、こうなった場合は全体の翻訳が終わるまで処理にかなり時間がかかってしまいますね。\nなので、最初はリクエストパラメータに翻訳したいデータを全て送る（dictionaryのvalueを一つのstrにjoinして）方法も考えてみましたが、APIの仕様として明示されてないだけで、リクエストパラメータのサイズには制限があるようでした。だとすると、翻訳したいデータをいくつかのchunkに分けて送るという方法があると思いますが、こうなった場合は最後にファイルを保存するときに入れ替えるデータの行をどうやって判断するか、それをうまく処理するための方法が悩ましくなります。\nこの処理のためファイルを翻訳するまでの時間がかなり長くなるので、いつかは解消したいものですが、翻訳した結果をうまくまとめて保存できる方法が思いつくまでは少し時間がかかりそうです。\nプログレスバーを追加する 今の作りだと、シングルスレッドであるため、一つのファイルに対して処理が行われている間は画面が固まってしまうという問題があります。そもそも処理が遅いので、かなり長い時間を固まっていますが、これはUXの観点ではあまり良くないですね。また、処理がどこまで終わっているかもわからないので、プログレスバーを追加して、一つのファイルおよび全体のリストでどれぐらいの処理が行われているかを視覚化したいと思っています。\nただ、プログレスバーを追加する場合、スレッドをわけ、さらに何を基準にプログレスバーを動いていくかというロジックを考えなければならないので、そこもまた時間がかかりそうですね。\nボタンの非活性化 翻訳対象のファイルが追加される前と処理の途中ではボタンを非活性化したいのですが、これもまだうまく実装されていません。処理中に画面が固まってしまうからか、処理の前後でボタンを非活性化するという処理も思い通りにならなかったのでこちらも直したいと思っています。\n最後に 今回はGUIを中心に色々と新しいチャレンジができたので、かなり面白い作業となっています。そしてやはり元がサーバサイドだからか、UI/UXの観点では考慮しきれなかった問題が出てきたり、画面を実装するためにも色々と試行錯誤があったのでこれは良い勉強になっています。PythonとPyQtが優秀だったのでできたものなのですが、これをまたJetpack Composeなどで書き換えてみるとどうなるかなという好奇心もありますね。やはり、普段やってみてないことに挑戦してみるのは自分がエンジニアとして成長するための良い糧となるような気がします。\nでは、また！\n","date":"2022-05-01T00:00:00Z","image":"https://retheviper.github.io/images/python.jpg","permalink":"https://retheviper.github.io/posts/python-qt-app/","title":"PyQtで字幕翻訳ツールを作ってみた"},{"content":"JavaとPython、そして少しのJavaScriptの経験してなかった私が、転職先でGoとKotlinを触って1年が経ちました。最近のプログラミング言語は大体収斂進化している傾向があるので、一つの言語ができれば大体他の言語もできる、もしくは読めるようになると言います。\nしかし、言語が違うということは、その設計思想が違うということなので、同じ結果を期待して書いたつもりのコードが全く思い通りにならないケースもあります。理由は大きく二つ、「あの言語ではこうだったから、この言語でもそうだろう」という慣性と「これはこの言語の特別な仕様だろう」という思い込みなのではないかと思います。（実際私がそうでしたが）\nというわけで、今回はJava/Kotlinをバックグラウンドとして持つエンジニアがGoを触るときの落とし穴的な部分を一部紹介したいと思います。あくまで個人的な経験によるものなのですが、これからGoでコードを書くことになる方には少しでも参考になればと思います。\ntime Goでは時間を扱うためのスタンダードライブラリとしてtimeが存在していて、以下のように使えます。\n// 現在時間を取得 now := time.Now() // 時間を指定して取得(2022-04-01 12:30:00 +0000 UTC) someDay := time.Date(2022, 4, 1, 12, 30, 0, 0, time.UTC) Java/Kotlinだとこれに対応するAPIとしてjava.timeがありますね。Goと比べて違う点は、「時間」のみでなく、もっと細かい単位でクラスを分けているというところと言えますでしょう。\n// 年度 Year year = Year.now(); // 年月 YearMonth yearMonth = YearMonth.now(); // 年月日 LocalDate date = LocalDate.now(); // 時間 LocalDateTime time = LocalDateTime.now(); ただ、違う点はここだけではありません。当然ながら、言語が違うとライブラリの実装も変わってくるものなので、処理の結果も違うケースがありますね。代表的には、「月」単位の時間を扱う時、実装によってはGoでは意図通りの範囲にならないケースがあります。例えば以下のようなコードがあるとします。\nfunc getOneMonthBefore(t *time.Time) time.Time { // 月に -1 を指定して返す return t.AddDate(0, -1, 0) } 一件なんの問題もなさそうなコードですが、一部のケースで以下のような問題が発生します。\ndate := time.Date(2022, 3, 31, 0, 0, 0, 0, time.UTC) oneMonthBefore := getOneMonthBefore(\u0026amp;date) // 2022-03-03 上記のコードの結果が「2月28日」でなく、「3月3日」になるのは、処理が以下のように行われるからです。\n2022-03-31から1ヶ月前の2022-02-31になる 2022-02-31という日付は存在しないので、2月の末日から日付の補正を行う 2月の末日である28日から、31日の差分ほど日付をプラスする というわけで「基準となる月より先月の日付が少ない場合」にこのような結果を得られるわけです。ただ、月末から1ヶ月前というものは、意図としては2022-02-28を期待するはずですね。人間の思う処理と、実際のコードが算出する結果が違うというのは十分にあり得る状況ですが、AddDateのドキュメントでは上記のような処理になるという話は特に言及されてないので誤解する可能性もあるのかなと思います。\nまた、Java/Kotlinで使っているLocalDateの場合は期待通り2022-02-28になるので、Java/Kotlinの経験のあるエンジニアが無意識的にこのような問題を起こすコードを書く可能性もあるかなと思います。ちなみに、LocalDateを使ったコードがGoと違う結果になるのは、minusMonth()では最後に以下のメソッドを呼び出すからです。\nprivate static LocalDate resolvePreviousValid(int year, int month, int day) { switch (month) { case 2: day = Math.min(day, IsoChronology.INSTANCE.isLeapYear(year) ? 29 : 28); break; case 4: case 6: case 9: case 11: day = Math.min(day, 30); break; } return new LocalDate(year, month, day); } なので、Goでも3月31日から1ヶ月前の日付が2月28日になるという処理を期待したい場合は、以下の二つの方法を考慮した方がいいかなと思います。\nLocalDateと同じく、閏年と月別の末尾を考慮した処理を足す AddDate()で得られた月が基準となるtimeと同じ月である場合、先月の末日を返す 前者の場合は月を計算した後、上記のresolvePreviousValid()と同じ処理を足すことで実現でき、後者の場合は、以下のように末日を取得することが可能なので参考にしてください。\ndate := date.AddDate(2022, 3, 0, 0, 0, 0, 0, time.UTC) // 3月0日を指定すると2月28日になる map Goで変数を宣言する方式は以下の二つがありますね。\n// 型だけを宣言 var intSlice []int // 初期化と共に宣言 stringSlice := make([]string, 10) 問題は、宣言の仕方によって要素を足す場合に問題が起こり得るということです。先にsliceの例を見ましょう。varで宣言した場合でもmake()で初期化した場合でもappendを使った要素の追加には問題がありません。\nvar intSlice []int // sliceに値を追加する intSlice = append(intSlice, 1) // [1] ただ、mapの場合はvarで宣言すると問題が起こる可能性があります。以下のコードは、nil pointerとなります。\nvar stringMap map[string]string stringMap[\u0026#34;A\u0026#34;] = \u0026#34;a\u0026#34; // panic: assignment to entry in nil map これはvarで宣言した変数は基本的にnilになるからですね。nilのmapに要素を追加しようとしたのでエラーが発生するのは当たり前ですが、Goland(Intellij)上では警告も表示されず、コンパイルも無事通るので実際にコードを実行するまでこのコードが動くかどうかはわかりません。むしろ、sliceを先に扱っていたなら、nilでもappendできるので「Goではこれでいいのかな」と思い込みやすいかなと思います。\nJavaやKotlinでもインスタンスを生成していないMapに対して要素を足すことはできないのですが、ここはJavaやKotlinの週間というよりは「Goの特殊性」と考えてしまうケースだと思いますので、要注意なところなのではないかと思います。\nswitch GoのswitchはJavaとよく似ています。ただ、形が似ているだけで、決定的な違いがあります。まずはJavaのswitchから見ていきましょう。以下のようなコードがあるとします。\nint i = 1; switch (i) { case 0: System.out.println(\u0026#34;zero\u0026#34;); case 1: System.out.println(\u0026#34;one\u0026#34;); case 2: System.out.println(\u0026#34;two\u0026#34;); default: System.out.println(\u0026#34;else\u0026#34;); } Javaのswitchは、breakを明示的に書かない限り、条件が一致するcaseに分岐されたとしても、その下のcaseでも流れてしますね。なので、上記のコードを実行した結果は以下のようになります。\none two else Kotlinではwhen式になり、breakなしでも条件と一致するコードブロックを実行することで処理は終了します。例えば以下のようなコードを書くとしましょう。\nval i = 1 when (i) { 0 -\u0026gt; println(\u0026#34;zero\u0026#34;) 1 -\u0026gt; println(\u0026#34;one\u0026#34;) 2 -\u0026gt; println(\u0026#34;two\u0026#34;) else -\u0026gt; println(\u0026#34;else\u0026#34;) } 実行した結果はJavaと違うのがわかります。省略されているだけで、一つの枝ごとに処理がbreakするからです。\none ここでGoのswitchの場合はどうなるかを見ていきましょう。形上はJavaと似ていますが、結果もそうでしょうか？\ni := 1 switch i { case 0: fmt.Println(\u0026#34;zero\u0026#34;) case 1: fmt.Println(\u0026#34;one\u0026#34;) case 2: fmt.Println(\u0026#34;two\u0026#34;) default: fmt.Println(\u0026#34;else\u0026#34;) } 上記のコードを実行した結果は、Kotlinと同じです。つまり、「one」と出力されるということです。これはGoのswitchもまた、Kotlinと同じく枝ごとにbreakするからです。なので、Javaの場合と同じ結果が欲しい場合は、fallthroughを追加し、次の枝に進むということを明示的に書く必要があります。以下のようにです。\ni := 1 switch i { case 0: fmt.Println(\u0026#34;zero\u0026#34;) fallthrough case 1: fmt.Println(\u0026#34;one\u0026#34;) fallthrough case 2: fmt.Println(\u0026#34;two\u0026#34;) fallthrough default: fmt.Println(\u0026#34;else\u0026#34;) } Javaの経験がある場合、スタイルが似ているのでつい挙動も同じだろうと思ってfallthroughを省略してしまうというケースもあり得るかなと思います。ここは言語が違うだけ使用も違うということなので、要注意ですね。\nif Goではif文の条件がおかしいと思われる場合、コンパイルが通りません。例えば以下の例を見てください。\ntype Role int const ( SystemAdmin = 1 Operator = 2 Developer = 3 ) type User struct { Name string Role Role } // SystemAdminかDeveloperではない場合はエラーを返す func checkRunnableUser(u User) error { if u.Role != SystemAdmin || u.Role != Developer { return errors.New(\u0026#34;user is not runnable\u0026#34;) } return nil } func Test_checkRunnableUser(t *testing.T) { u := User{Name: \u0026#34;John\u0026#34;, Role: Operator} err := checkRunnableUser(u) if err != nil { t.Errorf(\u0026#34;unexpected error: %s\u0026#34;, err) } } 上記のコードをコンパイルしようとする場合、Goland(Intellij)では条件に警告が表示され、コンパイルするとsuspect or: u.Role != SystemAdmin || u.Role != Developerというエラーメッセージが表示されるのを確認できます。エラーメッセージでもわかるように、これはif文の条件が間違っているからですね。「UserのRoleがSystemAdminかDeveloperの場合のみ許容する」という要件を満たすためには、orではなくandを使う必要があります。なので、if文の条件を以下のように修正すると意図通りに動くし、IDE上の警告やコンパイル時のエラーも発生しなくなります。\n// SystemAdminかDeveloperではない場合はエラーを返す func checkRunnableUser(u User) error { if u.Role != SystemAdmin \u0026amp;\u0026amp; u.Role != Developer { return errors.New(\u0026#34;user is not runnable\u0026#34;) } return nil } Javaの場合だと、Intellijでは条件が怪しいという警告は表示されるものの、Kotlinと同じくコンパイル時のエラーは発生しません。なので実行はできるようになりますが、警告の出ている箇所を確認していないと実際に実行してみるまでロジックが間違えていることには気づかなくなりますね。\nenum Role { SYSTEM_ADMIN, OPERATOR, DEVELOPER } record User(String name, Role role) {} static void checkRunnableUser(User user) { if (user.role() != Role.SYSTEM_ADMIN || user.role() != Role.DEVELOPER) { throw new IllegalArgumentException(\u0026#34;user is not runnable\u0026#34;); } } public static void main(String[] args) { checkRunnableUser(new User(\u0026#34;John\u0026#34;, Role.SYSTEM_ADMIN)); } しかし、問題は同じ処理をKotlinで書いてみると、Intellijで警告が出ることもなく、コンパイルも通るということです。Javaのケースと同じくランタイムでエラーが発生するコードになりますが、警告すら表示されないのでコードを注意深く確認しないと意図通りに動作している理由が何かを見逃しやすくなっているのではないかと思います。\nenum class Role(val value: Int) { SystemAdmin(1), Operator(2), Developer(3) } data class User(val name: String, val role: Role) fun checkRunnableUser(user: User) { if (user.role != Role.SystemAdmin || user.role != Role.Developer) { throw IllegalAccessException(\u0026#34;user is not runnable\u0026#34;) } } fun main() { checkRunnableUser(User(name = \u0026#34;John\u0026#34;, role = Role.Operator)) } コンパイルタイムでエラーを事前に検知できるという点は確かにGoのコンパイラの方が優秀かなと思います。ただ、KotlinやJavaに慣れている場合、条件がおかしいということに気づくより、「constを使っているせいか」「switchを使うべきか」など、問題の本質に気づかないようになる可能性もあるのではないかと思います。\nこれは、そもそも正しく条件を書くことが何よりも大事であることでありながら、他の言語で形成された習慣で違う言語のコードを書くと問題を起こし得るということを実例として適切ではないかと思いますね。\nRange loop Goのループにはfor文しかないですが、indexを利用する伝統的な形以外でもRangeでのループがあります。以下のような形で使えます。\nvar pow = []int{1, 2, 4, 8, 16, 32, 64, 128} for i, v := range pow { fmt.Printf(\u0026#34;2**%d = %d\\n\u0026#34;, i, v) } 同じ結果を出すコードをKotlinで書くとしたら、以下のようになるかと思います。一目でわかりますね。\n// kotlin val pow = listOf(1, 2, 4, 8, 16, 32, 64, 128) for ((i, v) in pow.withIndex()) { println(\u0026#34;2**$i = $v\u0026#34;) } ただ、Goにはポインタがあり、Range loopでポインタを使おうとすると問題が発生するケースがあります。例えば以下の例をみましょう.\nvar pow = []int{1, 2, 4, 8, 16, 32, 64, 128} // 新しいスライスにpowの参照を格納 var ppow []*int for _, v := range pow { ppow = append(ppow, \u0026amp;v) } // ppowの値を出力 for i, v := range ppow { fmt.Printf(\u0026#34;2**%d = %d\\n\u0026#34;, i, *v) } 想定通りだと、ppowにはそれぞれ1, 2, 4, 8, 16, 32, 64, 128の参照が格納されるべきでしょう。しかし、このコードを実行してみると、実際には128と全て同じ値となっています。Range loopないで参照している値が全部同じアドレスを参照することになるからです。\nなので、ポインタを使うスライスを用いてループの処理をする場合は、以下のようにvを再代入するか、indexによる参照で問題を回避する必要があります。\nvar pow = []int{1, 2, 4, 8, 16, 32, 64, 128} // 新しいスライスにpowの参照を格納 var ppow []*int for _, v := range pow { v := v // ここでvの値をコピー ppow = append(ppow, \u0026amp;v) } // ppowの値を出力 for i, v := range ppow { fmt.Printf(\u0026#34;2**%d = %d\\n\u0026#34;, i, *v) } うっかりRange loopを使ってしまうと、ポインタを使うスライスを用いてループの処理をする場合に問題が発生する可能性があるので、気をつけたいですね。\n最後に 幾つかの例を挙げましたが、まだ自分もGoでアプリを書いた歴も短く、言語に対しての理解も深くないのでこれからも色々と問題に遭遇する可能性はあるのかなと思います。その度はまたこうやってブログに載せていきたいと思います。ブログのネタができるという面では嬉しいですが、失敗からのポストは結局自分が辛くなることなので、うれしくはないですね…\nとにかくここであげた問題は全て自分が経験したものですが、大事なのは、違う言語に挑戦するときは自分の持つバックグラウンドの知識を活かしながらも、それを偏見にしたいこと、そして先走らないことかなと思いました。Goに限らず、新しいものに触れるときは常に注意しないと、という感じですね。\nでは、また！\n","date":"2022-04-17T00:00:00Z","image":"https://retheviper.github.io/images/go.jpg","permalink":"https://retheviper.github.io/posts/jvm-to-go/","title":"JVM言語経験者がGoを触る時のハマりどころ"},{"content":"Rustの勉強を始めたい、と思ったのはおよそ2年前のことです。当時はJavaとPythonを主に触っていたので、パフォーマンスがクリティカルな部分では対応しきれない部分があると思い、ネイティブにコンパイルされる言語に触れてみる必要があると思いました。そしてできれば、GCのなくポインタを扱う言語でアプリを書いてみたら、本業と言えるJavaの理解もより深くなるのではないかと思った次第です。\nそこで候補として考えたのがGoとRustです。ただ、Goは世間の評価はともかく、自分の立場からすると少し追求している目標とずれているところがあるなと思いました。特に転職してからGoとKotlinという言語を並行で触っていると、良くも悪くも自分がやりたいことがなんなのかわかってきた気分にもなったのです。\nそこで、そろそろ次の候補として考えていたRustに触れてみたいと思った次第です。これもまた、世間の評価は置いといて、実際自分に合うかどうかを確認してみたくなりました。最近は色々な言語が扱えるPolyglot Programmerの時代だという概念もあり、多くのプログラミング言語が互いの良いところを吸収しながらどれも似たようなものになったという評価もありますが、私の場合は、あくまで自分に合うのは何かを探るという感覚としてRustという新しい言語を接してみたいと思っています。\nなので、今回はまずこちらのThe Rust Programming Languageを読みながら、興味深かった部分について、自分が今まで経験してみた他のプログラミング言語と比べながら感想を述べたいと思います。ドキュメントが長く、自分の理解もまだ浅いのでまずは一部だけを紹介しましょう。\nLoop Rustでは伝統のforとwhile以外にも、ループの条件を指定しない(無限ループ)loopというのがありました。特定の条件でループを終了したい場合のみ、breakすることで終了できます。例えば以下のような形です。\nloop { // do something } 他の言語だと、普通はwhile(true)のような形が多いかと思います。例えばPythonは以下のようになりますね。\nwhile true: # do something KotlinやJavaでも事情は変わりません。以下のようになりますね。\nwhile(true) { // do something } Kotlinの場合だと、拡張関数があるのでloopというものを定義したらどうかと思いましたが、そうするとコンパイラ上でループだと認識されないので、breakを書くとコンパイルエラーとなります。なので以下のように拡張関数と作ることはできませんでした。\nfun test() { loop { break // コンパイルエラー } } fun loop(doSomething: () -\u0026gt; Unit) { while(true) { doSomething() } } Goの場合は、forに条件式を書かないことでシンプルな無限ループを書くことができます。\nfor { // do something } 個人的にwhile(true)や条件式を指定しないforは、慣習でしかなく、直感的な理解を招くものではないと思いますので、loopというキーワードを設けた方がコードの可読性という面ではよりわかり安いものなのではないかと思いました。細かい部分ではありますが、一回仕様としてとして決まるとなかなか変更できないものなので、どんなキーワードを使うかを決めるということも言語の設計においては大事だという気がします。\nArray Rustでは配列のindexを基準に一部を抽出するとき、以下のような書き方をします。参照(\u0026amp;)を利用して定義する必要があって、標準出力する形も少し独特ですね。また、indexを指定して切り出したものは「所有権のない別のデータ型」として定義されています。ここで切り抜いた配列の一部を、Rustではsliceと呼んでいるらしいです。\nlet arr = [0, 1, 2, 3, 4]; let slice = \u0026amp;arr[1..3]; println!(\u0026#34;{:?}\u0026#34;, slice); // [1, 2] Pythonでもかなり似たような感じでコードが書けます。以下は上記と同じ挙動をするコードの例です。ただ、ここで切り抜いたsliceのデータ型は同じくlistになるというのがRustとの違いですね。\nlist = [0, 1, 2, 3, 4] slice = list[1:3] print(slice) # [1, 2] Kotlinの場合は、Listの関数にRangeオブジェクトを渡すことで同じことができます。少し問題になるのは、Kotlin特有のRangeの書き方がどのような範囲を示すのか覚えてないとその範囲が分かりづらいということです。幸い、ここはIntellij Idea 2021.3のアップデートでヒントを表示してくれるようになったので、これ以前のバージョンを使っている場合はアップデートした方が良いですね。\nval list = listOf(0, 1, 2, 3, 4) val subList = list.slice((1..2)) println(subList) // [1, 2] val subListUntil = list.slice((1 until 3)) println(subListUntil) // [1, 2] Javaの場合は、インデックスの範囲をList.subList()に渡すことで同じことができます。\nList\u0026lt;Integer\u0026gt; list = List.of(0, 1, 2, 3, 4); List\u0026lt;Integer\u0026gt; subList = list.subList(1, 3); System.out.println(subList); // [1, 2] Goの場合は、Pythonと全く同じ方法で定義ができますね。また、配列からインデックスの範囲を指定して切り取ったviewをsliceと呼ぶのはRustと一緒です。ただ、Goのsliceはarrayと違って、可変長ですね。\narr := []int{0, 1, 2, 3, 4} slice := arr[1:3] fmt.Println(slice) // [1 2] Immutability Rustでの変数の宣言は基本的にlet一つで、不変になります。もちろん可変できる変数を定義するのは不可能ではなくて、以下のようにmutキーワードを使うことで値を再代入することはできます。例えば以下のようにです。\nfn main() { let mut x = 5; println!(\u0026#34;The value of x is: {}\u0026#34;, x); // The value of x is: 5 x = 6; println!(\u0026#34;The value of x is: {}\u0026#34;, x); // The value of x is: 6 } 他のプログラミング言語だと、変数の宣言時にその変数の可変性をあらかじめキーワードで表現するようになっているケースが多いかと思います。もしくは、基本的に変数は可変で、不変にしたい場合にだけ特別なキーワードを使うとかですね。しかし、Rustでは変数は基本的にimmutableであるというのが特徴的です。GCのない言語として、メモリの安全性を確保するための工夫がここで現れていると言っていいでしょうか。\nもちろん、Pythonのように変数の宣言と再代入の区別が付かないケースもありますね。\nx = 5 print(\u0026#34;The value of x is: {}\u0026#34;.format(x)) # The value of x is: 5 x = 6 print(\u0026#34;The value of x is: {}\u0026#34;.format(x)) # The value of x is: 6 Kotlinの場合は不変だとval、可変だとvarで宣言するようになっていますね。\nval x = 5 x = 5 // コンパイルエラー var y = 6 y = 7 // OK Javaの場合は、Rustと逆です。finalをつけない場合、基本的に再代入ができる構造ですね。\nint x = 5 x = 6 // OK final int y = 6 y = 7 // コンパイルエラー Goの場合は、変数をimmutableにできる方法はないようですね。なので、再代入は自由ですが、逆にJavaのfinalのようなキーワードが欲しい気分にもなります。\nx := 5 x = 6 fmt.Println(x) // 6 Shadowing これは全く予想できなかった部分ですが、Rustのドキュメントには変数にシャドーイングを使えると紹介されています。mutキーワードをつけると再代入は可能なので、それで良いのではという気もしますが、変数を不変にしながら、違うデータ型として定義し直す場合などに使えるという説明でした。\nRustではシャドーイングを使って以下のようなコードを作成できます。\nfn main() { let x = 5; let x = x + 1; // 6 let x = x * 2; // 12 println!(\u0026#34;The value of x is: {}\u0026#34;, x); // The value of x is: 12 } Pythonの場合も似たようなことができます。同じ挙動をするコードを以下のように書くと、問題なく動きます。変数の宣言と再代入が厳密に区別されない故のことかと思いますが、形的にはRustと全く一緒と言えますね。\nx = 5 x = x + 1 x = x * 2 print(\u0026#34;The value of x is: {}\u0026#34;.format(x)) # The value of x is: 12 Kotlinの場合、シャドーイングは一部の場合のみ可能です。関数の引数と、その関数で宣言している変数名が一致する場合ですね。\nfun shadow(value: Int) { val value = value + 1 // Name shadowed: value println(value) // valの方が出力される } Javaでは、シャドーイングができないです。ただ、以下のような形は可能です。\nclass Clazz { private int value = 0; public void setValue(int value) { this.value = value; } } Goの場合、少し複雑になります。以下の例をみると、xの宣言と代入を2回していますが、スコープが分かれてあるから可能なことです。Kotlinのケースと似ているとも言えますね。\nx := 0 fmt.Println(\u0026#34;Before the decision block, x:\u0026#34;, x) // Before the decision block, x: 0 if true { x := 1 x++ } fmt.Println(\u0026#34;After the decision block, x:\u0026#34;, x) // After the decision block, x: 0 上記のコードは、以下のようにif文でのxに対して再代入することで挙動が変わります。\nx := 0 fmt.Println(\u0026#34;Before the decision block, x:\u0026#34;, x) // Before the decision block, x: 0 if true { x = 1 x++ } fmt.Println(\u0026#34;After the decision block, x:\u0026#34;, x) // After the decision block, x: 2 このように他の言語だとなるべく使わないように誘導しているシャドーイングですが、Rustでは一つの機能として紹介しているのが面白いところでした。これもまた、後述する「所有権」というものと強く関係しているような気がします。\nOwnership 他の言語と比べたときに、Rustならではの特徴と言えるものは所有権ではないでしょうか。今まで私はGCのない言語を触ってみたことがないので、これはかなり興味深い概念でした。例えばKotlinの場合はNativeでコンパイルする場合、参照カウントを使うと言われています。JavaやPython, Goの場合はGCが働いて参照されていないオブジェクトが占めているメモリを解放することになりますね。\nしかし、Rustでは定数、不動小数点数、論理値、文字というスカラー型を除いた全ての参照型に関しては「一度使われたらメモリは解放される」「スコープを外れたら解放される」という原則を持っているようです。参照型とスカラー型という区分はJavaのプリミティブ型と参照型の関係を思い出させるところがありますね。より積極的かつ攻撃的なメモリ解放が行われるという違いはありますが。\n基本的には一回使った変数に対しては2回使えなかったり、値の変更ができないかと思った方が良い、ということかなと思いますが、他にも色々と興味深いものがありました。\nMove 所有権と関係する概念で、ムーブがあります。変数とデータが実際どうやって相互作用するかによるものらしいです。早速下のコードを見ていきましょう。なんの問題もなさそうなものです。\nlet s1 = \u0026#34;hello\u0026#34;; let s2 = s1; println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); // s1 = hello, s2 = hello ただ、上記のString literalをStringに変えたら問題が起こります。以下のコードを見ましょう。\nlet s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1; println!(\u0026#34;{}, world!\u0026#34;, s1); 上記のコードは、コンパイルしようとすると以下のようなエラーが発生します。\nerror[E0382]: use of moved value: `s1` --\u0026gt; src/main.rs:4:27 | 3 | let s2 = s1; | -- value moved here 4 | println!(\u0026#34;{}, world!\u0026#34;, s1); | ^^ value used here after move | = note: move occurs because `s1` has type `std::string::String`, which does not implement the `Copy` trait つまり、s1のデータがs2に移動したのでもう使えないということです。なので、二つの変数に同じデータを保証したい場合は、明示的に値をコピーする必要があります。例えば以下のようにです。\nlet s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1.clone(); println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); // s1 = hello, s2 = hello なぜこうなっているかというと、Rustでは変数がスコープの外に移動するときにメモリの解放が起こりますが、ここで複数の変数が同じポインタを使っている場合は二重解放が起こる危険があるからと説明されています。また、String literalと違ってStringはimmutableではないので、s1の再代入でs2のデータまで変わってしまうという問題を防ぐための意図もあるような気がします。\n実際このような代入が問題となる言語のケースもありますね。例えばPythonの方を見ましょう。二つの変数が同じポインタを使っているので、再代入で両方とも値が変更されたのを確認できます。\ns1 = \u0026#34;hello\u0026#34; s2 = s1 print(\u0026#34;s1 = {}, s2 = {}\u0026#34;.format(s1, s1)) # s1 = hello, s2 = hello s1 = \u0026#34;world\u0026#34; print(\u0026#34;s1 = {}, s2 = {}\u0026#34;.format(s1, s1)) # s1 = world, s2 = world JavaではStringをimmutableとして扱っているため、s1の値を再代入してもs2に影響はありません。KotlinもJVMの場合は、基本的にJVMのバイトコードを生成するためか同じ挙動をします。以下をご覧ください。\nvar s1 = \u0026#34;hello\u0026#34; val s2 = s1 println(\u0026#34;s1 = $s1, s2 = $s2\u0026#34;) // s1 = hello, s2 = hello s1 = \u0026#34;world\u0026#34; println(\u0026#34;s1 = $s1, s2 = $s2\u0026#34;) // s1 = world, s2 = hello Javaの場合も前述した通りです。\nvar s1 = \u0026#34;hello\u0026#34;; var s2 = s1; System.out.println(String.format(\u0026#34;s1 = %s, s2 = %s\u0026#34;, s1, s2)); // s1 = hello, s2 = hello s1 = \u0026#34;world\u0026#34;; System.out.println(String.format(\u0026#34;s1 = %s, s2 = %s\u0026#34;, s1, s2)); // s1 = world, s2 = hello Goでも変数はimmutableとして定義できませんが、この再代入により値が変わる可能性があるものに対しては安全性を担保されています。\ns1 := \u0026#34;hello\u0026#34; s2 := s1 fmt.Println(fmt.Sprintf(\u0026#34;s1 = %s, s2 = %s\u0026#34;, s1, s2)) // s1 = hello, s2 = hello s1 = \u0026#34;world\u0026#34; fmt.Println(fmt.Sprintf(\u0026#34;s1 = %s, s2 = %s\u0026#34;, s1, s2)) // s1 = world, s2 = hello Rustで明示的にコピーをしない場合はデータそのものが移動してしまうというのは確かにコーディング時には気を使わないといけないものですが、幸いコンパイルタイムで確認できる問題であり、他の言語を扱うときには思わぬ挙動をする可能性がある習慣を矯正してくれる可能性もあるかなと思うと、良い仕様ではなイカという気もしますね。\nClosure Rustではclosureを関数内の関数として定義することももちろん可能ですが、|val| val + xの形式で書きます。他の言語でlambdaと呼ばわれているものですね。多少は独特な書き方な気もしますが、型の省略が可能なのが他の言語と比べ便利なものな気がします。もちろん型の明示的な表記もできるので、以下のような使い方ができます。\nfn main() { // i32の引数を必要とする場合 let closure_annotated = |i: i32| -\u0026gt; i32 { i + 1 }; let closure_inferred = |i | i + 1 ; let i = 1; println!(\u0026#34;closure_annotated: {}\u0026#34;, closure_annotated(i)); // closure_annotated: 2 println!(\u0026#34;closure_inferred: {}\u0026#34;, closure_inferred(i)); // closure_inferred: 2 // 引数がない場合 let one = || 1; println!(\u0026#34;closure returning one: {}\u0026#34;, one()); // closure returning one: 1 } Pythonの場合は以下のように書くことができますね。もちろん、関数の中に関数を定義することもできますが、lambdaを使った方がが便利なのかなと思います。ただ、3.5から型ヒントを使えるようになっていて、コンパイルタイムで確実にエラーをチェックしたい場合は明示的に型を書いたほうが良さげな気はします。\nclosure = lambda x : x + 1 print(closure(1)) // 2 Kotlinでも簡単に定義はできるものですが、少なくとも引数の型は書く必要があります。もしくは、変数に型を指定することが必要ですね。\nval closure = { x: Int -\u0026gt; x + 1 } println(closure(1)) // 2 Javaではメソッド内にメソッドを定義することができなく、1.8から追加されたFunctional Interfaceを使う必要があります。また10からはvarで型推論を使えるようになりましたが、Functional Interfaceをvarとして宣言するのはできないという制約があります。他の言語と比べると最も制約が多いですね。\nFunction\u0026lt;Integer, Integer\u0026gt; closure = i -\u0026gt; i + 1; System.out.println(closure.apply(1)); // 2 Goの場合は関数内に関数を定義するのは不可能ではないものの、他の言語のlambdaのような書き方はできず、匿名関数として定義ができます。また型を明示する必要があるので、名前を除いて完全な関数を定義して変数に代入しているようなものになりますね。\nclosure := func(x int) int { return x + 1 } fmt.Println(closure(1)) // 2 また、closureにおいてRustの特徴はもう一つあります。closureを引数とする関数を定義するときの書き方です。closureに対してgenericを使って、whereというキーワードで関数の中にclosureを書いていくような形です。他の言語だとclosureが引数でも書き方は大きく変わらないのですが、Rustでは全く違う形になっているのが興味深いですね。例えば以下のようなコードになります。\n// Fというclosureを引数とする関数 fn apply_to_3\u0026lt;F\u0026gt;(f: F) -\u0026gt; i32 where // Fはi32を受け取ってi32を返すclosure F: Fn(i32) -\u0026gt; i32 { f(3) } fn main() { let double = |x| 2 * x; println!(\u0026#34;3 doubled: {}\u0026#34;, apply_to_3(double)); // 3 doubled: 6 } 最後に まだドキュメントの半分の読んでなく、実際に何かしらのアプリを作ってみたわけでもないので今回のポストだけでは十分ではないというのは十分承知のつもりですが、久々に違う言語を学びながら、色々と興味深いところが多かったのでひとまず感想を書いてみました。\n噂ではRustのコンパイラは優秀で、そのコンパイラの指示通りにアプリを組むだけでのかなり勉強になる瞬間が多いというのと、言語自体の設計が良いという話だったので、これからも勉強しながら気づいたことや感じたこと、学んだことについてブログにまとめていきたいと思います。今回のポストだけでの企画として終わらせたくないので、今年はこれで頑張っていきたいですね。\nでは、また！\n","date":"2022-03-27T00:00:00Z","image":"https://retheviper.github.io/images/rust.jpg","permalink":"https://retheviper.github.io/posts/rust-first-impression/","title":"Kotlinプログラマが見たRust"},{"content":"今回は久々に本を読んだのでそれに関する感想を少し書こうと思います。転職前は主にJavaを扱っていたため、Effective Javaを読み自分の書いたコードを振り返って見たことがありました。転職後はKotlinという違う言語を触るようになったものの、やはりJVMで動く言語であり、現在使っているフレームワークもSpringから変わってないので基本的には同じ観点でコードを作成すると良いのかなと思っていました。しかし、Kotlinに触れてから1年が経った今、やはり言語が違うとコードを作成するときの週間も一度は見直す必要があるのではないかと思っています。\nそこで、ちょうどEffective Kotlinという本を発見したので早速読んでみました。そして今回のポストではその内容について色々と書こうと思います。\nちなみに、この本自体は出版されて少し経つので、ネット上でもたまにその内容やPDFの資料などを発見することあができました。例えば、この本で「可読性」のチャプタに関しての話は[こちらのブログ]の方によく整理されてあるので、参考にしてください。\n全体的な印象 個人的にEffective Javaは上級者向けの本で、ある程度Javaでアプリを書いたこと経験のある人でないと理解が難しいところが多かったかなと思います。例えば、「try-finallyを try-with-resourceに代替した方がいい」とか、「Streamで副作用のない関数を書く方法」などが紹介されていますが、これらはやはりある程度Javaという言語の設計と仕様に対する基盤知識を必要とするものですね。\nそれに比べ、Effective Kotlinには初心者向けの内容も結構あります。例えば、そもそものオブジェクト志向が何かのような内容がありました。ただそれだけではどう思ってもEffective Javaを意識したような題名が意味をなくすと判断したからか（前書きでもEffective Javaを言及しています）他には「ベストプラクティス」として書かれているものも多いです。\nそして当たり前ながら、Kotlinにおいても有効なことはEffective Javaと重なっているような部分もあります。例えば、オブジェクトのインスタンスを作るときはfactory methodを書いた方が良いとかがそうですね。\nただ、Kotlinのバージョンアップの速さに追いついてないと感じるところがあったり（これは出版物の限界でもありますが）、上級者向けの内容は多少十分ではない感覚でしたので、どちらかというとジュニア向けの感覚ではあります。\n興味深い ジュニア向けといいつつ、自分もまだジュニア（の気持ち）であるため、興味深いと思ったところもありました。ここでその一部を紹介したいと思います。\nSingle responsibility principle いわゆるSOLIDに触れるパートです。Kotlinでは拡張関数を用いることで、単一責任の原則を守れるという主張をしていました。まず以下のようなケースがあるとしましょう。\nclass Student { // ... fun isPassing(): Boolean = calculatePointsFromPassedCourses() \u0026gt; 15 fun qualifiesForScholarship(): Boolean = calculatePointsFromPassedCourses() \u0026gt; 30 private fun calculatePointsFromPassedCourses(): Int { //... } } ここでisPassing()はaccreditationsというモジュールで、qualifiesForScholarship()はscholarshipというモジュールで使われるとします。そしたら、Studentというクラスがこれらの関数を持つのは単一責任としてどうか、ということですね。\nなので、モジュール別にこれらの関数を拡張関数として定義することが良いとのことでした。\n// scholarship module fun Student.qualifiesForScholarship(): Boolean { /*...*/ } // accreditations module fun Student.calculatePointsFromPassedCourses(): Boolean { /*...*/ } もしくはcalculatePointsFromPassedCourses()を外に出す方法を考えられるでしょう。しかし、この場合はこれらの二つのメソッド専用のprivateメソッドとしてつかえません。なので、\nどのモジュールでも使える共通関数を作っておく department別にhelper関数を作っておく とかの方法も考えられます。\n確かに、よく考えると拡張関数の良いところは「interfaceの実装ややスーパークラスの継承なし」でも簡単に処理を追加できるということなので、このような使い方をするのがユースケース別に処理を分けられて良さげな気がします。特に拡張関数を使うと、関数を配置するパッケージと可視性の制御が効くというところが個人的には新しい発見でした。\nConsider defining a DSL for complex object creation オブジェクトの作成時の複雑な処理はDSLを使いましょう、というパートです。Kotlinですでに提供している例としたら、HTMLがありますね。以下のような形で定義することになります。\nbody { div { a(\u0026#34;https://kotlinlang.org\u0026#34;) { target = ATarget.blank +\u0026#34;google\u0026#34; } } +\u0026#34;Some content\u0026#34; } 確かにKtorのようなフレームワークでもよく使われている物なので、ある程度需要はあるのかなという気がしました。Kotlinだと高階関数を作るのが難しくはないので、十分挑戦できるところでもありますね。\nただ、DSL特有の書き方を確立し、その書き方をエンジニアに共有することや最初の設計と維持管理が難しそうな気がするので、アプリの縮小が求められる今のご時世に果たして合うかとうかは少し疑問ののころところでした。\n個人的に何かのライブラリやフレームワークを作るとしたら、挑戦してみたいなと思いました。\nまあそうだよねって思ったところ なんとなくそうではないかと思っていたところを（もしくはどこかで聞いて理論的な部分は忘れたけど、無意識的のうちに習慣化されていた部分を）文として親切に整理してくれているようなパートもありました。なのでもう一度自分の考えを再確認できたといえるところでしょうか。\nDo not repeat common algorithms 「スタンダードライブラリで解決できる一般的なアルゴリズムを自前のコードで書くな」というパートです。理由は以下の通りです。\n呼び出しの方がコードを書くより時間が短くかかる わかりやすい名前になっている コードがわかりやすくなる 最適化が効く 私自身もなるべくスタンダートライブラリを活用した方が良いと思っていたので、ここはすぐに納得できました。自分で書いた処理が果たして最適化されたものかどうかもわからないし、業務使用以外のロジックを触るのは避けたいという理由でした。\nこの本では、以下のようなコードを上げています。自前のロジックを書いた場合です。\nval percent = when { number \u0026gt; 100 -\u0026gt; 100 number \u0026lt; 0 -\u0026gt; 0 else -\u0026gt; number } 上記のコードは、coerceIn()を使うことでシンプルにできます。以下がその例です。\nval percent = number.coerceIn(0, 100) Kotlinには特にスタンダードラリブラリに良い関数が多いので、自前のロジックを書くよりは一度どんなAPIがあるのかを確認した方が良いケースが個人的には多かった気がします。そしてそれが納得できる理由が書いてあってよかったと思います。\nImplementing your own utils スタンダードライブラリで解消できる問題以外で、プロジェクトに必要な共通の処理はユーティリティ関数として作っておきましょうってパートです。ユーティリティはクラスでなく、拡張関数として作ったら以下のようなメリットがあるらしいです。\n関数は状態を持たないので副作用がない トップレベル関数と比べると型が決まっているので使い勝手がいい 引数よりはクラスについた形が直観的 オブジェクトに関数をまとめるより必要な機能を探しやすい 特定のクラスに従属されるので親クラスのものか、子クラスのものか悩まない 確かにJavaを使っていた時は、私もいわゆるSingleton Patternでユーティリティクラスを作ったり、DIして使えるクラスを定義しておいて、staticメソッドを書いていました。Kotlinだと、ユーティリティクラスなしでも特定のクラスに関数を追加することができるのでより使い勝手がよくなります。\n例えば、同じことをするとしても、拡張関数で書く場合とユーティリティクラスを作る場合のコードは以下のような違いがあります。\n// 拡張関数を使う場合 val isEmptyByExtension = \u0026#34;Text\u0026#34;.isEmpty() // ユーティリティクラスを使う場合 val isEmptyByUtilClass = TextUtils.isEmpty(\u0026#34;Text\u0026#34;) ユーティリティクラスを使う場合はまず、「どのユーティリティクラスの関数を使うか」を考えなければならないです。それに比べ、拡張関数はIDEでも自動補完ですぐ欲しい関数を見つけることができるので、より直観的なものになっていますね。\n他にも具体的なクラスにのみ追加ができたりするので、より安全な使い方ができるというのも良いところですね。色々と拡張関数は使い道が多いなという、再発見ができたと言えるところでしょうか。\nBuilder pattern Kotlinではnamed argumentsが使えて、Builderパターンがいらないというパートです。KotlinでもBuilderパターンを使うことが技術的に不可能ではないが、named parameterを使った方が良い理由としては以下が挙げられています。\nより短い より綺麗 使い方がシンプル スレッドセーフ 私自身も、Javaで使っていたのでKotlinでもBuilderパターンが必要かなと思ったことがありますが、いらないという結論を出しています。上記で挙げた理由ももちろん妥当ですが、Builderパターンだとインスタンスを生成するときに必須パラメータが揃っているかどうかを判断するのが難しくなるという理由からでした。\n例えば本に出てくるBuilderパターンの例があるとしましょう。\nclass Pizza private constructor( val size: String, val cheese: Int, val olives: Int, val bacon: Int ) { class Builder(private val size: String) { private var cheese: Int = 0 private var olives: Int = 0 private var bacon: Int = 0 fun setCheese(value: Int): Builder = apply { cheese = value } fun setOlives(value: Int): Builder = apply { olives = value } fun setBacon(value: Int): Builder = apply { bacon = value } fun build() = Pizza(size, cheese, olives, bacon) } } このBuilderは以下のような使い方ができると思います。\nval villagePizza = Pizza.Builder(\u0026#34;L\u0026#34;) .setCheese(1) .setOlives(2) .setBacon(3) .build() しかし以下の場合でもビルドはできますね。\nval villagePizza = Pizza.Builder(\u0026#34;L\u0026#34;).build() もしcheese、olives、baconが0を許容しない作りになっていると、これを修正するのは大変なことになるでしょう。もしくは、パラメータが複雑な作りのオブジェクトだったらデフォルト値を設定するか、強制null check(!!)などを入れるか…より複雑になるだけですね。\nしかし、named parameterを使うと簡単に解決できる問題です。デフォルト値を指定しないvalだったら、それが必須項目であるということもわかりやすいですね。\nval myFavorite = Pizza( size = \u0026#34;L\u0026#34;, cheese = 5, olives = 5, bacon = 5 ) Consider factory functions instead of constructors Javaでも最近は色々とfactory functionを導入していて、簡単にimmutableなオブジェクトを作りやすくなりました。Kotlinでもコンストラクタの作成や、named parameterによるインスタンスの生成が色々と便利ではあるものの、それでもfactory functionが良いケースがあるというパートです。理由は以下の通りです。\n関数には名前があるので、どうやってオブジェクトが生成されるかわかる ArrayList(3)よりはArrayList.withSize(3)がわかりやすい 戻り値としてサブタイプのオブジェクトを指定できる 具体的な実装を時と場合によって違う形にすることができる 呼び出されるたび新しいオブジェクトを作るわけではない Connections.createOrNull()のようにnullを返すこともできる まだ存在しないオブジェクトを提供できる プロキシなしで動くようなオブジェクトを作るなどで応用できる オブジェクトの外に作ることで可視性を制御できる inlineにできるので、reifiedにもできる インスタンスを作るのが複雑なオブジェクトの手間を省く スーパークラスやプライマリコンストラクタを呼び出さずにインスタンスを生成できる こちらも読みながらなるほどと納得しました。特に私の場合でも、Service層のDTOとController層のResponseなどのオブジェクト間のマッピングではfactory functionを導入してコードを再使用性を高められたと思っていたので、良い判断だったなと今は思っています。\n他に、factory functionを作る方法としても以下のようなものが提示されてありました。一般的にはcompanion object内に定義しておくことが多いかと思いますが、他の方法も必要であれば考慮したいものですね。\ncompanion object Javaのstaticメソッドのようなパターン。最もわかりやすいですね。以下のような形です。\nclass MyLinkedList\u0026lt;T\u0026gt;( val head: T, val tail: MyLinkedList\u0026lt;T\u0026gt;? ) { companion object { fun \u0026lt;T\u0026gt; of(vararg elements: T): MyLinkedList\u0026lt;T\u0026gt;? { /*...*/ } } } // Usage val list = MyLinkedList.of(1, 2) factory functionは大体以下の規則を持って命名されるという説明もありました。\nfrom 一つのパラメータを渡し、タイプを変える時（自分自身のインスタンスを返す）\nval date: Date = Date.from(instant) of 複数のパタメータを渡し、それを束ねたタイプに変換するとき\nval faceCards: Set\u0026lt;Rank\u0026gt; = EnumSet.of(JACK, QUEEN, KING) valueOf ofの冗長な形\nval prime: BigInteger = BigInteger.valueOf(Integer.MAX_VALUE) instance / getInstance Singletonのインスタンス取得（パラメータが同じだと常に同じインスタンスが帰ってくる）\nval luke: StackWalker = StackWalker.getInstance(options) createInstance / newInstance instance / getInstanceは似ているが、常に新しいインスタンスを返す\nval newArray = Array.newInstance(classObject, arrayLen) getType instance / getInstanceと似ているが、違うタイプのインスタンスを返すとき\nval fs: FileStore = Files.getFileStore(path) newType createInstance / newInstanceに似ているが、違うタイプのインスタンスを返す時\nval br: BufferedReader = Files.newBufferedReader(path) extension クラスにからのcompanion objectを定義しておいて、外部から拡張関数でfactory functionを付ける形です。元のクラスをいじらなくても良くなるし、パッケージと可視性の制御など拡張関数の持つ特徴を活用できますね。\ninterface Tool { companion object { /*...*/ } } fun Tool.Companion.createBigTool( /*...*/ ): BigTool { //... } top-level スタンダードライブラリに含まれている、listOf()、setOf()、mapOf()のようなものです。\nよく使うタイプに関しては使い勝手がいいので便利なものの、IDEの自動補完などに現れたら混乱するケースもあるので命名は慎重にする必要がある、とのことでした。\nfake constructor Pascal Caseを使って、関数をコンストラクタに見せかけるものです。Kotlinのスタンダードライブラリとしては、以下のようなものがあります。\nList(4) { \u0026#34;User$it\u0026#34; } // [User0, User1, User2, User3] これは実際は以下のような関数ですね。\npublic inline fun \u0026lt;T\u0026gt; List( size: Int, init: (index: Int) -\u0026gt; T ): List\u0026lt;T\u0026gt; = MutableList(size, init) public inline fun \u0026lt;T\u0026gt; MutableList( size: Int, init: (index: Int) -\u0026gt; T ): MutableList\u0026lt;T\u0026gt; { val list = ArrayList\u0026lt;T\u0026gt;(size) repeat(size) { index -\u0026gt; list.add(init(index)) } return list } これはinterfaceに対してコンストラクタを作る必要があったり、reifiedタイプの引数が必要な時に考慮できるものらしいです。\n他にもfake constructorを作る方法があります。\nclass Tree\u0026lt;T\u0026gt; { companion object { operator fun \u0026lt;T\u0026gt; invoke(size: Int, generator: (Int) -\u0026gt; T): Tree\u0026lt;T\u0026gt; { // ... } } } // Usage Tree(10) { \u0026#34;$it\u0026#34; } ただ、この場合constructor referenceではコードが複雑になる問題があるらしいですね。\n// Constructor val f: () -\u0026gt; Tree = ::Tree // Fake Constructor val d: () -\u0026gt; Tree = ::Tree // Invoke in companion object val g: () -\u0026gt; Tree = Tree.Companion::invoke なのでfake constructorを使うとしたら、関数として定義したほうがよさそうです。\nfactory class 別途Factoryというクラスを置いてインスタンスを返すようにする方法ですね。Javaではinterfaceでそのようなことをするケースがありますが（List.of()みたいな）、Kotlinでも良いのか？という疑問が湧きました。結論から言いますと、「factoryクラスは状態を持つことが可能」なため、場合によっては考慮しても良いとのことです。これは思ったより活用できそうな可能性がありますね。\ndata class Student( val id: Int, val name: String, val surname: String ) class StudentsFactory { var nextId = 0 fun next(name: String, surname: String) = Student(nextId++, name, surname) } val factory = StudentsFactory() val s1 = factory.next(\u0026#34;Marcin\u0026#34;, \u0026#34;Moskala\u0026#34;) println(s1) // Student(id=0, name=Marcin, Surname=Moskala) val s2 = factory.next(\u0026#34;Igor\u0026#34;, \u0026#34;Wojda\u0026#34;) println(s2) // Student(id=1, name=Igor, Surname=Wojda) 最後に ざっくりなまとめとなりますが、以上が私のこの本で得られた知識への感想となります。新しい発見もあり、自分の習慣が間違ってなかったということを人の説明で補ってもらったような気にもなり、かなり興味深かったです。\nただやはり、Kotlinがまだ新しい言語であり、いろいろなパラダイムを吸収しているためか、Effective Javaのようなレベルの高い作法に対する議論は少し足りてないような気がしていて、そこは多少残念に思います。まあ、こう思うようになったということ自体が、少しは自分が成長した証拠でもあるかなという生意気な想像もしてみるのですが。\nでは、また！\n","date":"2022-02-20T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/effective-kotlin/","title":"Effective Kotlinを読む"},{"content":"すでに気づいている方もいらっしゃるかと思いますが、2年ぶりにブログのテーマを変えました。正確にはテーマだけでなく、静的ページ生成ツールをJekyllからHugoに変えました。こういうのが気軽にできるので、最初からブログをGithub Pagesで公開したことは本当によかったなと今は思っています。\nサイトの生成ツールを変えたことで各ポストのURLにも変化があり、もしお気に入りなどに設定しておいた方には申し訳ないことをしましたが、それを踏まえても改善できたところが多いと思いますので、ご了承ください。\nさて、今回はそんなブログの刷新に関する話となりますが、大きくわけて「ブログがどう変わったか」と「これからブログでやっていきたいこと」について述べさせていただきたいと思います。\nUXの改善 UXの観点で改善されたことは、ブログ刷新での第一順位の目標でもありました。他にも色々とあるかと思いますが、まずは以下のようなものがあります。\n画面遷移の改善 以前のブログはメインから個別のポストに遷移するときにアニメーションが含まれていました。（ポスト一覧を表示するところでも同じものが使われていました）画面遷移時にアニメーションを入れるというのはもうトレンドとしては古いものでもあり、何より画面遷移が遅いような感覚があるので改善したかったです。なので、今回は反応が早くしてみました。\n検索機能 タグとカテゴリ、そしてアーカイブがあれば十分かなと思いましたが、キーワードでポストを探したい時もありますね。以前は検索機能を付けようとしてもうまくいかなかったので、ちゃんと検索のできるテーマを採用しました。\nデザイン 個人的にはダークモードが好きで、全体的に黒いテーマを選ぼうかなと思いましたが、幸いボタンを押すだけでダークモードへの転換ができるテーマがあったので、こちらを選びました。もっと良いのは、このダークモードはシステムの設定と連動されるということです。なので、ダークモードが好みではない場合は白い画面を見ることができます。一つ残念なのは、コードブロックのスタイルはまたブログとは別ということですが…これはのちに何か方法があったら対応したいと思います。\n他には、モバイル画面でもちゃんと画像が見えたり、メニューやレイアウトなども色々とモダンなデザインになったのが嬉しいところです。\nポスト表示 以前はポストの一覧から、ポストの画像をクリックするとそのポストの詳細画面に遷移するのではなく、画像が拡大されるという問題があ理ましたがそれを改善できました。そしてポストごとに読みにかかる大まかな時間を表示できるようになったので、記事を読まれる方にもどれを読むか参考するに良い情報を提供できているかと思います。\n内部的な変化 さて、今までは主にUXの観点からの話でしたが、実は上記で挙げた項目は単純にJekyllのテーマを変えるだけでも対応ができるものでした。それをあえてHUGOに変えてまでブログの刷新を追求した理由を今から話したいと思います。\n管理がしやすい Jekyllの場合、一つのテーマがRubyで書かれた一つのアプリを扱うようなものだったので、Gemfileの依存関係のアップデートが必要だったり、ローカルでサーバを起動するまで時間がかかったり、テーマを変えるたびに色々と設定や構成をいじる必要がありました。HUGOの場合、そのような問題はほとんどありませんでした。テーマはGithubのリポジトリをサブモジュールとしてインポートすることができて、あとはコンテンツと基本的な設定をいじるだけです。なので、実際に時間がかかったのは添付画像などリソースのパスが変わったための対応などを除くと、1時間ほどだったような気がします。\nまた、以前は画像が一つのフォルダに集約されていたため、ポストの添付する画像を管理するのががかなり面倒臭い作業でしたが、今はポストごとに別フォルダを使用していて、そのフォルダに画像を入れるだけなので管理がより簡単になりました。なので、今後は画像の添付も以前よりは積極的にやっていこうと思います。\nHUGOはGoで書かれてあるため、ローカルでサーバを起動するのが早いのも良いところです。\nカスタマイズが楽 以前は自動でRSSフィードを作ってくれる機能があるものの、全体のポストが対象となっていませんでした。テーマやJekyllの設定をいじれば解消できる問題だったとは思いますが、前述した通り、テーマが一つのアプリのようなものなので、カスタマイズが簡単ではありません。それに比べ、HUGOはより単純なものとなっていてテーマや設定のカスタマイズも簡単にできます。今もメニューは元々テーマの設定を変えるだけでカスタマイズできるものもありますが、一部は自分の方で修正したものもあります。どれも時間があまりかからなく、すぐにできるのが魅力的でした。\nshortcodeでHTMLとJavaScriptの添付をテンプレート化できるのも個人的にはHUGOの魅力ではないかと思います。使い勝手がいいので、色々と活用できそうなところが多いですね。\nこれからは ブログの生成ツールを変えて物理的に良くなったところの話ができたので、次はそのブログで何をしていきたいかを述べたいと思います。\n最初は意欲も溢れていて、さまざまな技術に触れてそこで自分が学んだことや感じたこと、そして試行錯誤など多彩な記事を書いていたと思います。今振り返ってみると「当時の自分はこんなにもわかってないものが多かったのか」と、恥ずかしくなる瞬間もありますが、少なくとも今よりはもっと力を入れていた気がします。\n個人的に、このブログの更新は少なくとも月に2回を目指しているのですが、気持ちだけが先走ってあまり読む価値のないポストもいくつか書いてきたのではないかと反省しています。\nなので、今年はいくつかの技術を実際使ってみるという目標をたて、それらに関する記事を書いていきたいと思います。今のところは以下のようなものを考えています。\nJetpack Compose 新年の目標としても挙げていたものですが、実は去年から少しづつ触っている状態で、今年は実際にAndroidとデスクトップアプリケーションを作ってみたいと思っています。ちょうど去年はCompose Multiplatformも正式リリースしているので、時期的にちょうどいいかと思います。\nSwiftUIの方も、XCode Cloudの発表やリンクでのアプリ公開が可能になったり、Swift Playgroundsでアプリのビルドが可能になったことにより興味を持っている状態ではありますが、まずは自分が仕事でKotlinを使っていて、Composeの方をある程度使いこなせるようになってから挑戦してみた方がいいかなと思っているところです。\nせっかくMacを使っているので、一度はSwiftでアプリを作ってみないとですね。\nSvelte 会社の方ではNuxt.jsを使っているので、こちらでも良いかと思いましたが、結局自分が画面を触ることになるのはプライベートでしかチャンスがない気がしたので、興味本意で選んだのがSvelteです。\nまだ技術としては成熟したものではない気はしますが、そろそろSveltekitのようなものも登場していて、何よりも学習にかかる時間や効率を考えたときに、自分が画面を作るとしたら最も生産性が高くなりそうな技術だなと思いました。まぁ、これは実際触ってみないとわからないものですが…\n他に理由としては、Stackoverflow Surveyで最も愛されたウェブフレームワークに選定されたから、というのもあります。多くのエンジニアが好きという技術は、まずその理由が知りたくなりますので。（同じ理由で、Rustも触ってみたいと思っています）\nQuarkus 会社で使っているSpring bootのビルドとテスト、デプロイにかかる時間を短くしたいのですが、その対策としてQuarkusへの移行を考慮しています。稼働中のサービスのフレームワークを変えるということはかなりのリスキーなことではありますが、成功した場合は生産性も上がり起動速度やメモリのようなメリットがかなりでかい気がしているので、いつになるかはわかりませんが、ぜひ挑戦してみたいタスクとして設定しています。移行に成功しても失敗しても、色々と勉強になりそうではありますね。\n最後に ブログの外見は変わり、十分成功的な移行になったと思いますので、次はその中身を充実にしていくのが課題ですね。今まで通り、一人前のエンジニアになりたいという気持ちを忘れず、少しづつでも前に進むような（そして読まれる方にも役立つような）ポストを書けたらなと思います。\nでは、また！\n","date":"2022-02-06T00:00:00Z","image":"https://retheviper.github.io/images/magic.jpg","permalink":"https://retheviper.github.io/posts/renew-blog/","title":"今年のブログの話"},{"content":"DBを設計する時と、最終的にアプリで活用するデータの形は大きく変わるケースがあります。特に後から機能を付け加えるとそうなりますね。もちろん正規化などを通じてより効率的にデータを保存する方法を考える必要のあるDBと、データをいかに加工して使うかを工夫するアプリの違いによるものもありますが、アプリの改修が続くと同じデータでも活用する箇所や表現の仕方が変わってくるからでもあるかなと思います。\nそういうわけで、今回はそのようなケースで一つ活用できる方法をご紹介したいと思います。アルゴリズムというわけでもありませんし、より効率的な方法はあるかなと思いますが、応用すれば結構色々な場所で使えそうな方法ではないかと思います。\nシナリオ 例えば以下のようなシナリオがあるとします。\n社員はA、Bという二つの部署に配属される 社員が部署に配属される日付はそれぞれ この場合、データの作りには色々な観点があるかと思いますが、まず部署の配属日を基準にデータを作るとしたら、部署の種類、配属日とその日付に配属となった社員のリストを持つような形になるかと思います。Kotlinのコードとして表現するとしたら以下のような形ですね。\nenum class DepartmentType { A, B } data class Department( val departmentType: DepartmentType, val date: LocalDate, val employers: List\u0026lt;Employer\u0026gt; ) { data class Employer( val id: Int ) } ここで社員の3人がいて、それぞれ部署Aと部署Bに配属された日付が違うケースがあるとしましょう。データとしては、以下のようなものです。\n社員番号 部署A配属 部署B配属 1 1月1日 1月1日 2 1月1日 2月1日 3 2月1日 2月1日 上記のデータを持って、先ほどの部署のデータを実際のリストとして作るとしたら以下のようになるかなと思います。\nval departments = listOf( Department( departmentType = DepartmentType.A, date = LocalDate.of(2022, 1, 1), employers = listOf( Department.Employer(id = 1), Department.Employer(id = 2) ), ), Department( departmentType = DepartmentType.A, date = LocalDate.of(2022, 2, 1), employers = listOf( Department.Employer(id = 3) ), ), Department( departmentType = DepartmentType.B, date = LocalDate.of(2022, 1, 1), employers = listOf( Department.Employer(id = 1) ), ), Department( departmentType = DepartmentType.B, date = LocalDate.of(2022, 2, 1), employers = listOf( Department.Employer(id = 2), Department.Employer(id = 3), ) ) ) ただ、これを社員を基準に、それぞれの部署に配属された日付をデータとして加工するにはどうしたら良いでしょうか。社員番号と部署に配属となった日付の二つを持つような形です。例えば、コードで表現すると以下のようなものです。\ndata class JoinedDates( val employerId: Int, val departmentA: LocalDate, val departmentB: LocalDate ) つまり、やりたいことは先ほどのdepartmentsを、最終的に以下のようなデータにしたいということですね。\n[ JoinedDates(employerId=EmployerId(value=1), departmentA=2022-01-01, departmentB=2022-01-01), JoinedDates(employerId=EmployerId(value=2), departmentA=2022-01-01, departmentB=2022-02-01), JoinedDates(employerId=EmployerId(value=3), departmentA=2022-02-01, departmentB=2022-02-01) ] データの整列の基準がひっくり返されるので、どうしたら良いかと悩ましくなる場面です。今回は、これを解決した自分の方法を紹介したいと思います。\nロジック Departmentを基準に考えるとEmployerのデータが複数になりますが、これを逆転させて、Employerを基準に複数のDepartmentを持つという形に加工したいというのが今回の要件です。だとすると、考えられるロジックは以下がポイントかなと思います。\nEmployerのID単位でまとめる EmployerごとにDepartmentをType別に分けた配列を持たせる まずはネスとしているEmployerのリストに入り、そのIDを抽出する必要がありますね。このIDは重複させたくないので、MapのKeyにしておくと良さげです。\nあとは、そのMapにEmployerのIDがKeyとして存在するかどうかで以下の処理をすると良いでしょう。\nKeyが存在しない場合は、新しくDepartmentのタイプとその日付をinsert Keyが存在する場合は、そのvalueを取り出してDepartmentのタイプと日付を追加 なので、一回DepartmentのListをMapに変換して、さらにJoinedDatesのListに変換することとなります。ちょうど上記の分岐については、compute()を活用するとできるので、中間データとしてのMapがどんな形になるかを考えると良いかなと思います。\n私の場合はMapの方がデータを取りやすいので、最終的には以下のような処理となりました。\nfun List\u0026lt;Department\u0026gt;.toJoinedDates(): List\u0026lt;JoinedDates\u0026gt; { // 中間データ val tempMap = mutableMapOf\u0026lt;Int, Map\u0026lt;DepartmentType, LocalDate\u0026gt;\u0026gt;() this.forEach { department -\u0026gt; // Departmentのタイプとその日付のPair val departmentJoined = department.departmentType to department.date department.employers.forEach { employer -\u0026gt; // EmployerのIDがKeyとして存在したら足して、なかったらMapを追加 tempMap.compute(employer.id) { _, value -\u0026gt; value?.let { value + departmentJoined } ?: mapOf(departmentJoined) } } } // 中間データをJoinedDatesのListに変えて返却 return tempMap.map { (id, department) -\u0026gt; JoinedDates( employerId = id, departmentA = department.getValue(DepartmentType.A), departmentB = department.getValue(DepartmentType.B) ) } } 共通ロジック化 ジェネリックを使ったclassとして上記のロジックを一部分離すれば、似たようなケースで色々使い回せるのではないかと思いましたので、以下のようなコードも書いてみました。\nclass Aggregator\u0026lt;T, K, V, R\u0026gt; { private val tempMap = mutableMapOf\u0026lt;T, Map\u0026lt;K, V\u0026gt;\u0026gt;() // データの追加 fun add(key: T, value: Pair\u0026lt;K, V\u0026gt;) { tempMap.compute(key) { _, existingValue -\u0026gt; existingValue?.let { existingValue + value } ?: mapOf(value) } } // 指定したListとして取得 fun getList(transfer: (T, Map\u0026lt;K, V\u0026gt;) -\u0026gt; R): List\u0026lt;R\u0026gt; { return tempMap.map { transfer(it.key, it.value) } } } これの場合は以下のような使い方ができます。\nval aggregator = Aggregator\u0026lt;Int, DepartmentType, LocalDate, JoinedDates\u0026gt;() // データの追加 departments.forEach { a -\u0026gt; a.employers.forEach { b -\u0026gt; aggregator.add( key = b.id, value = a.departmentType to a.date ) } } // Listの結果を取得 val joinedDates = aggregator.getList { id, joinedDate -\u0026gt; JoinedDates( employerId = id, departmentA = joinedDate.getValue(DepartmentType.A), departmentB = joinedDate.getValue(DepartmentType.B) ) } 汎用性はあるものの、呼び出し元のコードが増えたり、指定している型の意味や意図がよくわからないので適切なKDocやコメントがないと少しわかりにくいところがデメリットかもしれませんね。ただ大事なのは中間データの型とcompute()による分岐処理なので、そこだけをうまく取り出して他でも応用できるかなと思います。\n最後に サーバサイドKoltinだと、多くの場合にデータをListとして扱うのが普通かなと思いますが、場合によってはMapを使うのもロジックを書いていく中では良い選択になるかと思います。特に、今回紹介したcompute()以外でも、getOrPut()、getOrDefault()などの機能が便利なので色々と活用できる場面が多いかなと思います。この処理は前回のポストでも似たようなものを紹介したことがありますので、興味のある方はそちらも参考にしてください。\nプログラミング言語が提供するスタンダードライブラリは色々と見逃しやすいところがあるかなと思いますが、よくドキュメントや自動補完で一覧に登場する関数に注目すると、こういう風に必要なものがいきなり現れることもあるかと思います。まだ私もKotlinを触って1年ほどしか経ってないひよこなものなので、これからもどんどん新しい発見があると嬉しいなと思いますね。\nでは、また！\n","date":"2022-01-29T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-reverse-groupping/","title":"Kotlinでデータの逆転グルーピング"},{"content":"このブログを書き始めてから、今年で4年目となります。振り返ってみるとブログを書き始めた頃はSEとして主にSIerの基盤チームの仕事をしていたので、インフラやライブラリを含めさまざまな技術に触れることが多かったのですが、去年に転職をして本格的に業務系エンジニアとしてバックエンドを担当することになり触れる技術や興味などにも変化があったかなと思います。なので、今回は振り返りを兼ねて今年のロードマップに関して少し述べたいと思います。\n過去分のポストを振り返ってみると最初は主にJava、Spring、Linux、Jenkinsなどに関するものが多かったのですが、最近はやはりKotlinで使えるさまざまなフレームワークなどに興味が移っている感覚です。基本的にJavaでもKotlinでもJVM言語であることは同じなのでできることや分野は大差なく、転職してからも使っているフレームワークはSpringなのですが、Kotlinの開発元であるJetBrains社がいろいろなフレームワークを開発しているのもあり自然にそれらに興味を持つようになっていますね。\nSophomore jinxを克服する Sophomore jinxは日本語で「2年目のジンクス」といわれている言葉です。大学2年目になると新入生だった頃と比べ、成績が下がり熱意が消えるというということを指す言葉らしいです。これの意味を拡張して、1年目に新人王などに選ばれた選手がその次の年からは成績が悪化したり、人気の映画の続編が面白くなかったりする場合など最初より何か劣化した場合を指すことになっているとか。\n自分の場合はエンジニアになってから4年目になるのですが、確かに2年目からは1年目の時と比べ熱意はなくなっている気がします。だからと言って新しい技術に興味がなくなったり、プログラミング自体が飽きたという訳ではありませんが、前はやりたいことがあったらずっとモニタと睨めっこしながら徹夜でコードを書いたりしていたのに、今はとてもそういう気にならないというのが違うところですね。年を取ったためかとも思いますが、\nこれについては自己分析とやりたいこと、そしてできることを冷静に整理して少しづつでも何か成果物を出せるようにしないと思っています。去年もそうでしたが、このようなポストを書くのはそのためでもあります。計画しているもの全てを成せるとは思いませんが、多くの目標の一部でも達成した方が最初から少ない数の目標を立てるよりは良いのかなという気がしています。なので、まずは「やるかどうかわからないけどアンテナは張っておく」という感覚でいろいろな技術に目を通しておこうかなと思っています。\nFrontend 私はJavaScriptとTypeScriptの基礎を研修やUdemyの講座で学んだくらいのレベルで、フロントエンドの仕事をあまりやることがなかったです。しかし、昨今のウェブアプリの開発においてのトレンドをみるとやはりフロントエンドの技術を一つは学んでおいた方が良さげな気もしますし、バックエンドの役割を吸収しているような気配すらするなという気がしますね。何より、エンドユーザにとって画面のないアプリは想像できないので、今まで自分が使うためのAPIやライブラリ、コマンドラインアプリだけでなく本格的にGUIを活用した何かを作ってみるべき時が来たかなと思っています。\n何よりフロントエンドの場合、数年前はいろいろなライブラリとフレームワークが存在してどれを使った方がいいか全くわからない状況でしたが、最近は3強だといわれていた中でもAngularを抜いたReactとVueのみが生き残り、さらにそれらを基盤にしたフレームワークが登場するなど、そろそろ技術の成熟期と言ってもいい時代になったのではないかと思いますので、React基盤のNext.jsか、Vue基盤のNuxt.jsのどちらかを選べばよい時期なのではと思います。私自身も今年はそのうちのどちらかに触れてみたいなと思っています。\nせっかくなのでNestJSのようなJavaScript用のサーバサイドフレームワークにも触れてみるのは良いかもしれませんが、サーバサイドというバックグラウンドがある自分にとってはまずはフロントエンドのみでちょうどよいチャレンジかなと思ったりもします。後述しますが、バックエンドではまた別に触ってみたいものもありますのでなおさらですね。\nQuarkus 個人的にはSpringを長く使っていたので、新しいフレームワークを使ってみたいという願望があります。一つのフレームワークに慣れると、それを使い続けるのも選択肢としては悪くないと思いますが、新しい技術にはメリットもデメリットもあるものなので、少なくとも触れてみる必要はあるかなと思います。なので去年はQuarkusとKtorの二つを触ってみました。\n個人的にはJetBrainsのプロダクトに信用を持っていて、Kotlin向けという点でもKtorは悪くなかったと思ったのですが、機能が不十分であるところや、アーキテクチャで悩ましいところがあるという面で躊躇しています。一方でQuarkusはネイティブにコンパイルでき、Springのライブラリを一部そのまま使えたり、そもそもSpringとあまり変わらない感覚でコードをサクサク書けそうなイメージなので本格的に使用してみたいと思っています。最大の問題はやはり、ネイティブの場合ビルドにかなり時間がかかるということですが、これはCIとの連携をうまくやっていくしかないかもしれませんね。\nFastAPI いきなりPythonになりますが、FastAPIにも触れてみたいと思っています。以前から違う分野に転職をするとしてもバックエンドはやり続く可能性が高いと思い、いろいろな言語とフレームワークを触ってみたいと思っていました。その候補としてはExpress、Rocket、Vaporなどがあって、これらを全部触ってみた後、最も自分の好みに合ったものをプライベートで使い続けようと思っていたのです。\nそんな中、Pythonは普段もたまに簡単な自動化のスクリプトを作るなどの目的で使っているので、Djangoを触ってみようかと思っていたところ、最近はFastAPIで爆速の開発ができるという話を聞いて興味を持つようになりました。今もKotlinとSpringで開発はできますが、シンプルなプロジェクトならこういった軽いオプションが一つあっても悪くなさそうな気がします。インタープリタ言語なので起動も早く、Techempowerのベンチマークでも意外と悪くないパフォーマンスを見せてくれているのも魅力的ですが、SwaggerとReDocによるドキュメンテーションが自動で行われるというところがかなり良さげです。なんでもサクッと作れそうな感じがしますね。\nまた、直接使わないとしてもコードが綺麗で勉強になる噂を聞いているので、少なくとも一度はコードを読んでみたくなります。\nSwiftUI and Jetpack Compose 個人的に本当にやりたかった分野は、GUIを持つアプリを作り上げることです。エンジニアという職業を持つ前から作ったのもJavaFXによるデスクトップアプリでしたので。最近はJavaScriptだけでもElectronやReact Nativeなどを使ってなんでもできるという時代にはなっていますが、せっかくJavaとKotlinができるようになったので、ネイティブアプリを作ってみた方が良さそうな気がします。\n以前Flutterが発表されてまもない時期に、一度React Nativeと一緒にチュートリアルだけ触れてみたことがあるのですが、当時にもFlutterのいわゆる「宣言型GUI」というものに魅力を感じていたのでこれからモバイルをやるとしたらFlutterかなと悩んでいました。最近はモバイルだけでなく、デスクトップアプリやウェブアプリまで作れるようになったのでなおさらでしたね。\nしかし、SwiftUIとJetpack Composeというものが登場してからは完全にこちらに傾きました。どうしても同時にマルチプラットフォームアプリが開発できるという面ではFlutterが有利だとは思いますが、それと似たような感覚でネイティブのUIが作れられるようになったのでもう悩む必要がないかなという気がします。\n特に、SwiftUIだとMac用のデスクトップアプリを作ることもできますが、Jetpack Composeならデスクトップアプリだけでなくウェブアプリも作ることができて、さらにKotlin Multiplatform Mobileを使うとビジネスロジックの共有ができるようになるのでこちらの方が自分の場合にはより合うのではないかという気がしています。自分に合うというのは、私がめんどくさがり屋なので一つの言語で全てを解決したいという願望を持っているというだけの話ですが…とにかく一度使ってみて、よかったらフロントエンドでもJetpack Composeを使ってみるのはありかなと思っています。Kotlin/JSという選択肢もありますが、こちらはまた次の機会で。\nただこれらのデメリットは、やはりどれもまだ完成されてない技術ということですね。分野を問わず新しい技術のジレンマでもありますが、新技術がどれだけ良くてもそれだけでは完全ではない（もしくはかなり不便）という場面が出てくる可能性があるので、当面は少し様子を見ながらシンプルなアプリを作ってみることから初めてみようと思います。Flutterという良い先例があるので、良さげな機能はすぐに吸収してくれるという期待もあります。\nOracle Cloud 他のクラウドと比べかなり後発したためか、無料プランでもメモリ1GBのVMインスタンスを二つも提供するという破格の政策で知られたOracle Clouですが、2021年からはさらに選べるVMインスタンスにARM（Ampere A1というオプションを追加しています。\nAmpereが既存のAMDやIntel製CPUを使うインスタンスと比べて目立つのはやはり性能です。ARMだと互換性の問題がありx86と比べ利用率が下がると思ったためか、無料プランでも4つのOCPU、24GBのメモリという良い性能のインスタンスを提供しています。無料のインスタンスを2つまで作ることができるので、2つのOCPUと12GBのメモリという構成を2つのインスタンスに分けて指定することもできます。\n個人的には既存の無料で使えるインスタンスが1つのOCPU、1GBのメモリというオプションだったので、Javaアプリのビルドなどヘビーな作業には向いてないのが惜しいところでした。なのでMattermostのサーバとして使うなど軽い感じでしか使い道がなかったのですが、Ampereの導入でCI用サーバなどに使える道もできたかなと思います。Oracle Cloudのホームページで紹介していること以外でも、他のベンチマークを参照するとCPU性能は期待しても良さそうな気がします。無料でOracle DBも提供されているのでそちらを使うか、それともVMインスタンスを一つDB用に使うか、GCPやAWSの無料サービスと組み合わせて使うとかでも色々できそうな気がします。\nただ、やはり互換性が気になっていたのですが、個人的にはApple Silicon搭載Macを使ってみながら「意外と悪くない」という結論に至っています。プログラミング言語などはすでにARM対応済みのものが多く、サーバで使うとしたらFFmpeg、ImageMagick、GraphicsMagickなどを使うケースもあるかなと思いますが、どれもARMバージョンをインストールもしくはビルドできるので特に問題はなさそうです。\n他に問題なら、今の所VMインスタンスを作ろうとしてもハードウェアが十分ではないのか、2つのOCPU以上のスペックでは作れないというのが問題ですね。時間が解決してくれる問題かもしれませんが、いつになったらインスタンスを自由に作れるかわからないというのは確かに問題と言えるでしょう。\n最後に 意欲がないといいつつ、これだけやってみたいものが多いというのはまだ自分がエンジニアとして気持ちが死んでいるわけではないからよかったなと思わせます。本当は意欲がないというより「面倒臭いだけ」と訂正するべきかもですね…\nというわけで、色々とやりたいことだけを並べてみましたが、今年はそろそろ何か実際使えるアプリを作り出すのを第一の目標にしたいと思っています。何か作ってみるだけでも間違いなく良い経験、良い経歴になるはずなので。\nでは、また！\n","date":"2022-01-11T00:00:00Z","image":"https://retheviper.github.io/images/map.jpg","permalink":"https://retheviper.github.io/posts/my-roadmap-in-2022/","title":"個人的な2022年のロードマップ"},{"content":"M1 Macが発売されてもう1年以上が経ちます。最初は流石にRosettaがあるとはいえ、ネイティブアプリが少なく問題が起こったり性能が低下するケースも少なくなかったので、すぐにApple Silicon Macを購入しようとは思わなかったのです。それが今はネイティブアプリも増え、OSのアップデートもあったので十分移行しても良いタイミングになっている気がしました。そして今年、M1 ProとM1 Maxを搭載した新しいMacが発表され、既存のintelチップ搭載モデルと比べメリットが多いと思ったので購入を決めることになりました。\n新しいMacについてはチップの性能という面でも、改善されたハードウェア（ディスプレイやスピーカ、キーボードなど）という面でもかなり良いものになっている印象はありますが、ここではあえてそれらについては述べません。多くのベンマークやレビューなどで明らかになっていることが多いと思いますので、ここではintel機からの移行経験に関して述べさせてください。\n移行方法 まず、Apple Silicon Macに移行を決めてから、以下のような基準を立てました。\n既存のMacからマイグレーションする なるべくネイティブアプリを使う マイグレーションをすると決めた理由は、現在利用中のアプリの設定を最初から見直したくないものが多かったからです。アーキテクチャが違うのでマイグレーションを行わない方と比べ何か問題になる可能性もあるかと思いましたが、実際使用中の今までは特に問題はなかったので、最初から全てをやり直すということがめんどくさい場合はマイグレーションをしても良さそうです。\nまた、移行には「移行アシスタント」を使いました。この場合、選べる移行の方法は以下の通りです。\nTimeMachine Macから移行 Wi-Fiを使う Thunderboltケーブルで接続 TimeMachineを使うと最新の状態が反映されないので、使っているMacから移行することにしました。また、データのコピーにかかる時間が最も短いというのでThunderboltで二つのMacを直結して移行を行なっています。移行には時間は思ったよりそう掛からなく、終わった直後の状態は既存のものと変わらなかったです。\n移行後はApple Siliconネイティブアプリを使いたいので、アプリの情報を一つ一つ確認してUniversalではないものからバイナリを切り替えていくことにしました。\nバイナリの確認 まずマイグレーションが終わった時点で、インストールされているアプリがApple Siliconネイティブであるかどうかを見分ける方法は以下があります。\nFinderから「情報を見る」 アクティビティモニタの「種類」をみる 「このMacについて」→「システムレポート」→「ソフトウェア」→「アプリケーション」から探す Is Apple Silicon ready?から探してみる 他にもintell版のアプリの場合は最初実行時にRosettaをインストールするか確認するダイアログが出るのでそれで確認するという方法もあります。主にターミナルで使うプログラミング言語などがそうですね。ただ、一度でもRosettaをインストールしたらintell版のバイナリでもダイアログなしで実行されてしまうので、全体の移行が終わるまではRosettaをインストールしない方が良いかと思います。\nアプリケーションの切り替え 以下はUniversal Binary(IntelとApple Siliconの両方に対応)を提供しているので、Intel機から移行した場合でも特に何もしなくて良いです。\nChrome Edge Firefox EdgeView2 Movist Pro Amphetamine Bandizip Obsidian Magnet Macs Fan Control iStat Menus ユニコーン Microsoft Remote Desktop Microsoft Word Microsoft PowerPoint Microsoft Excel Microsoft OneNote Microsoft Outlook 一つ、Macs Fan Controlの場合、私はメニューバーにCPUの温度を表示するために使っているのですが、温度を表示する項目を選択する際にintell機だと「CPU PECI」を選べる方が一般的かなと思いますが、Apple Siliconだとそのような項目がありません。しばらく全体的な温度をみたところ、「CPU Performance Core」が最も温度が高いように見えたので、温度を確認したい場合はそれを選んでおいた方がいいかなと思います。\nバイナリを切り替える必要があるケース 以下はApple Silicon用のバイナリを別途提供しているので、ホームページからダウンロードして既存のアプリを上書きするだけで対応できました。\nNotion Desktop Docker Desktop Postman Zoom Webex draw.io ただ、DockerはデスクトップアプリそのものはApple Siliconネイティブを使うとしても、イメージがamd64のみ対応するというケースも多いので、ここは色々と検証が必要かなと思います。私の環境ではまだMySQL（5.7）がApple Siliconに対応してなかったのですが、特に問題なく動いています。\nHomebrew Rosettaを使って既存のintell版バイナリを使うこともできるらしいのですが、Apple Siliconに対応したバージョンはインストールされるパスが違うし（既存は/usr/local、Apple Silicon版は/opt/homebrew）、Apple Silicon版だとインストールされるパッケージは基本的にApple Siliconネイティブになるか、intell版でも新しくビルドしてくれるらしいので、使っていたintell版を消して新しくインストールし直すことにしました。\nアンインストールとインストールは、別に何も意識する必要はありませんでした。公式ホームページに出ている通り、以下のコマンドを実行するだけで良いです。自動でintell版を消してくれて、新しくインストールする場合はApple Silicon版になります。\n# アンインストール /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/uninstall.sh)\u0026#34; # インストール /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; ただ、homebrewでインストールしたパッケージ関しては自分の場合はとりあえず全部削除しておいて、後で必要になったらそれだけインストールしようという方針でしたが、以下のコマンドで使っていたパッケージのリストがバックアップできるらしいです。\n/usr/local/homebrew/bin/brew bundle dump 開発環境 開発環境の構築に関しては色々なケースがあるので、こちらで別途説明します。\nJetBrains IDE JetBrains社の製品（+ Android Studio）ならToolboxで簡単に管理ができるのですが、このアプリ自体もApple Silicon対応のバイナリに変える必要があります。\nToolboxをApple Siliconネイティブに切り替えた後は、メニューからIDEをアンインストール後に再インストールするだけです。もしToolboxを使ってない場合は、使っているIDEのバイナリをダウンロードし直す必要があります。\nちなみに、ToolboxからインストールされるIDEは~/Applications/JetBrains Toolboxの配下にLauncherが置かれ、それをシステムレポートなどで確認するとintel版のバイナリになっています。ただ、実行時はちゃんとApple Siliconネイティブになっているので（アクティビティモニタから確認可能）安心してください。\nVisual Studio Code Visual Studio Codeの場合は、Universal/Intel/Apple Silicon用のバイナリを全部提供していました。多分あえてIntelバージョンをインストールしてなかったら、自然にUniversalにアップデートされていたのではないかと思います。\nあえてUniversalを使う必要はなく、サイズが小さいのでApple Siliconバージョンをダウンロードしたほうがいいかなと思います。\nまた、最近はモブプロなどでVisual Studio Live Shareを使うケースが多いかなと思いますが、こちらはまだApple Siliconに対応していません。こちらはGitHubのissueで今後対応する予定だというので、当面は待つしかないですね。\nJava Javaの場合、intellだとどのベンダのものを選んでも大差ないですが、Apple Siliconだと少し話が変わってきます。なぜかというと、ベンダ別にApple Siliconに対応しているJDKのバージョンが違うからです。Rosettaを使っていない場合、「bad CPU type in executable」というエラーが発生するので、インストールされているバージョンが\n各ベンダ別のApple Silicon対応済みのLTSバージョンのJDKの一覧は以下の通りです。\nJDK 対応バージョン Amazon Corretto 17 Azul Zulu 1.8, 11, 17 Bellsoft Liberica 1.8, 11, 17 Eclipse Temurin 17 Microsoft 17 Oracle Java SE 17 SapMachine 11, 17 上記のJDKのうち、一部はintellijでもダウンロードできるものとなっていて、intellij上ではApple Silicon対応バージョンをaarch64と表示していますのでインストール時は必ず確認しましょう。\n他にもRed HatやIBM Semeruなどがありますが、こちらの場合はApple Silicon用のJDKを提供していません。\n最近はどのJDKを選んでも特に問題はないかと思いますので、17を選ぶならOracleのでも良いし、他も好みで選んでも良さそうな気がします。私の場合はずっとAdpotOpenJDKを使っていたので、今回もTemurinを選びました。ただ、Temurinだと11がApple Siliconに対応してないので、そこはZuluを選んでいます。インストールにはhomebrewを使いました。\nApple Siliconとは直接的な関係はないですが、以下のコマンドで使うJDKのバージョンを簡単に切り替えできるのでさまざまなベンダのJDKを使ってみるのもありかもですね。\nexport JAVA_HOME=`/usr/libexec/java_home -v 11` $ java -version openjdk version \u0026#34;11.0.13\u0026#34; 2021-10-19 LTS OpenJDK Runtime Environment Zulu11.52+13-CA (build 11.0.13+8-LTS) OpenJDK 64-Bit Server VM Zulu11.52+13-CA (build 11.0.13+8-LTS, mixed mode) $ export JAVA_HOME=`/usr/libexec/java_home -v 17` $ java -version openjdk version \u0026#34;17.0.1\u0026#34; 2021-10-19 OpenJDK Runtime Environment Temurin-17.0.1+12 (build 17.0.1+12) OpenJDK 64-Bit Server VM Temurin-17.0.1+12 (build 17.0.1+12, mixed mode) GraalVMの場合は、まだApple Siliconに対応していません。ただ2020年からGithubのissueがオープンの状態であって、Linux+aarch64に対応したバイナリは提供している状態なので、いずれはリリースされるかと思います。\nKotlin Kotlinは1.5.30からApple Siliconサポートが発表されていますが、これはKotlin/Nativeに関するものなのでKotlin/JVMの場合だとJavaの方だけ気をつけたらいいと思います。こちらもhomebrewでインストールし、特に問題はありませんでした。\nGradle Gradleの場合はv6.8.3を使うプロジェクトがあったので、Javaの設定が終わった後にビルドしてみると以下のようなエラーが出ました。（依存関係やプロジェクトの設定によってエラーの種類は変わる可能性があるかと思います）\nJava 17で実行した場合 Java 17(Temurin)で実行した場合、Gradleそのものが実行時にエラーを吐きます。おそらくrefelction関係でdeprecatedになっていたAPIを使っているのが問題になったのではないかと思います。\n\u0026gt; java.lang.IllegalAccessError: class org.gradle.internal.compiler.java.ClassNameCollector (in unnamed module @0x8f1317) cannot access class com.sun.tools.javac.code.Symbol$TypeSymbol (in module jdk.compiler) because module jdk.compiler does not export com.sun.tools.javac.code to unnamed module @0x8f1317 Java 11で実行した場合 Java 11で実行するとコンパイルまでは行われるようですが、テスト（junit）の実行で以下のような問題が起こりました。\n*** java.lang.instrument ASSERTION FAILED ***: \u0026#34;result\u0026#34; with message agent load/premain call failed at src/java.instrument/share/native/libinstrument/JPLISAgent.c line: 422 FATAL ERROR in native method: processing of -javaagent failed, processJavaStart failed Process \u0026#39;Gradle Test Executor 1679\u0026#39; finished with non-zero exit value 134 調べてみると、Gradleはv6.9からApple Siliconに対応したようだったので、ラッパーを最新にバージョンアップ。gradle/wrapper/gradle-wrapper.propertiesのバージョン指定を変えるだけでも対応できますが、以下のコマンドを使っています。\n./gradlew wrapper --gradle-version=7.3.1 --distribution-type=bin いきなりv6.8.3からv7.3.1にアップデートしたのですが、テストまで正常終了しています。このプロジェクトはKotlin + Spring bootの構成なので、もし同じような構成のプロジェクトがあるとしたらJava/Kotlin/Gradleのバージョンアップをおこなってからアプリのビルドを試してみましょう。\nRuby / Python / Go RubyとPythonに関してはmacOS上ですでにインストール済みの状態ですが、バージョンやプロジェクトの設定などによっては問題が起こる可能性もあるのでhomebrewでインストールしました。このブログで使っているjekyllはrubyをインストールし直したので同じくインストールし直す必要がありました。こちらもhomebrewでインストールができ、既存のプロジェクトにおいてはなんの問題もなく実行することができました。\nPythonの場合、既存のプロジェクトのパッケージを再インストールする必要がありました。でもrequirements.txtがあれば特に問題にはならないくらいです。\nGoの場合も1.16からApple Siliconに対応しているので、特にバージョンの指定が必要なケースでなければ、homebrewで最新をインストールしても良いかなと思います。ただ、既存のプロジェクトのGOROOTやGOPATHの問題があるので、ホームページから別途ダウンロードして設定する必要のあるケースもあるかと思います。\nRosettaを使うしかないケース 多くのアプリがApple Siliconに対応してきましたが、バージョンアップそのものが終わったり、Rosettaを通じて問題なく動く（からApple Silicon対応は後回しにするという政策の）アプリに関してはネイティブのバイナリが存在しない場合もあるので、仕方なくRosettaを使うしかないかなと思います。\nソースコードをダウンロードして、ローカルでビルドするという方法もあるかと思いますが、使われているSwiftのバージョンが低い場合はXCodeですぐにビルドできない場合もあったりしました。いつかはRosettaのサボートも終わりそうなので、長期的な観点ではこのようなアプリは他のものに代替した方が良いかもしれません。\nMattermost MattermostはSlackと似たようなコミュニケーションツールで、サーバにインストールすることで無料利用ができるしマークダウンのサポートが優秀だったりするのでプライペートでよく使っています。ただ残念なことに、こちらはまだApple Silicon用の正式リリース版がないようです。\n正式リリースの予定はあるようなのでバージョンアップまでintell版を使うという選択肢もありますが、GitHubのリポジトリを見るとUniversalとApple Silicon用のバイナリのベータ版も存在しているので、どうしてもRosettaを使いたくない場合はこちらを選んでみても良いかもしれません。\nKeyboardCleanTool KeyboardCleanToolは、アプリを実行している間に全てのキー入力を無視するという単純なツールです。キーボードが汚れて拭きたいときによく使っていますね。残念ながらこちらもまだApple Siliconに対応していません。同じ会社で開発しているBetterTouchToolはUniversalバイナリで提供されていますが、その対応ができたのも11月のことなので他の製品が全部Apple Siliconに対応するにはかなり時間がかかるかもしれません。\nこのようなアプリは特にネイティブにならなくても困らないものなので、Apple Siliconネイティブ対応はかなり優先順位が低い感がありますね。\nOneDrive Microsoft社の製品にしてはかなり珍しいケースですが、対応が遅れていますね。ただ、今月PreviewとしてUniversalバージョンが利用できるようになったらしいので、もうすぐApple Siliconネイティブ版が出るかもしれません。その際にはApp Storeで自動的にアップデートされるはずなので、待つだけですね。\nFlutter Dartはv2.14からNative対応しているのですが、Flutterはまだ未対応らしく、公式を見てもRossettaを推奨しています。なのでRosettaを入れて実行した方が早いですね。\n他に、flutter doctorを実行していくつか問題が出るケースがあるかと思います。そういう場合は以下の手順で対応できました。\ncmdline-tools component is missingと出る場合 Android StudioからAppearance \u0026amp; Behavior -\u0026gt; System Settings -\u0026gt; Android SDK -\u0026gt; SDK Tools -\u0026gt; Android SDK Command-Line Toolsにチェックを入れる CocoaPods installed but not workingと出る場合 brew install cocoapodsでインストール 最後に 移行が終わって本格的にApple Silicon Macを使ったのはまだ一週間経たないくらいの短い期間ですが、思ったより移行がスムーズで、ネイティブ対応済みのアプリも多かったので、M1が発表された直後の私のように互換性に疑問を持った方がいるとしたら（十分な事前調査を前提として）新しいMacに移行するのもありかも知れないと思います。\n最初はAppleが公式的に全てのMacをApple Siliconに移行するまで2年という計画を立てたという話をしていましたが、ユーザとしてはより時間がかかるのではないかと思っていました。しかし、実際に触ってみると、今でも十分移行ができる状態になっていると思います。\nでは、また！\n","date":"2021-12-19T00:00:00Z","image":"https://retheviper.github.io/images/magic.jpg","permalink":"https://retheviper.github.io/posts/move-to-apple-silicon/","title":"Apple Silicon Macに移行する"},{"content":"帳票などで、たまに和暦を処理する必要な時がありますね。例えば元号を表記するとか、和暦の年度を表記するなどの場合があるかと思います。Kotlin(JVM)の場合、西暦だとJavaのAPIのDateやLocalDateなどのAPIを使うと簡単ですが、和暦が必要となるのはごく一部のケースなので方法がなかなか分かりづらいかと思います。なので、今回はKotlinで和暦を扱う方法について少しまとめてみました。\nJapanseEra / JapaneseDate Javaでは、1.8から和暦で日付を扱えるJapaneseDate及び元号を扱えるJapaneseEraというAPIを提供しています。なのでJapaneseDateのインスタンスを作り、そこからJapaneseEraを取得することで簡単に元号の情報を取得できるようになります。実際の使い方は以下の通りです。\n// 現在日付のJapaneseDateを取得 val japaneseDate = JapaneseDate.now() // JapaneseEraの取得 val japaneseEra = japaneseDate.era JapaneseDateの場合、LocalDateと同じくChronoLocalDateを継承しているのでインスタンスを作成する方法はそう変わりません。なので、以下のようなこともできます。\n// LocalDateをJapaneseDateに変換 val japaneseDateFromLocalDate = JapaneseDate.from(LocalDate.now()) // 特定の日付を指定してJapaneseDate val japaneseDateFromSpecificDate = JapaneseDate.of(2000, 12, 31) 元号を日本語で表記する 和暦を扱う場合にやりたいことは大きく二つかと思います。一つは、元号を文字列として扱うこと、そしてもう一つは、和暦での年度を数字として扱うことです。まずは、元号を文字列として取得できる方法について説明します。\nまず上記で紹介した通り、JapaneseDateのインスタンスを取得した上で、さらにそのオブジェクトが保持しているJapaneseEraを取得する必要があります。その後、JapaneseEra.getDisplayName()という関数にTextStyleとLocaleを指定して文字列を取得することができます。前者は文字の出力型を指定する列挙型定数で、後者は言語の指定と思ってください。\nTextStyleの場合、以下のような値があります。他の言語だと指定したものによって出力がかなり変わってくるかもしれませんが、日本語の場合はFULLとNARROWだけで十分ではないかと思います。\n定数 出力例 FULL 昭和 FULL_STANDALONE 昭和 NARROW S NARROW_STANDALONE S SHORT 昭和 SHORT_STANDALONE 昭和 Localeの場合、Locale.JAPANやLocale.JAPANESEのどちらを指定しても結果は同じです。ただ、実装としては以下のようになるのでなるべくLocale.JAPANを使った方が良さそうです。\nLocale 作られるBaseLocaleの設定 JAPAN language = ja, region = JP JAPANESE language = ja 以下はこれらの定数を渡して元号を文字列として取得する例です。\nval today = JapaneseDate.now() val era = today.era // 元号を漢字で取得 val eraName = era.getDisplayName(TextStyle.FULL, Locale.JAPAN) // 令和 元号だけでなく、年度までも合わせて表記したい場合もあるかと思います。その場合に使えるものはDateTimeFormatterです。これもJapaneseDateが実質LocalDateと同じくChronoLocalDateを継承しているから可能なことですね。\n// 日付を日本語で表記する val formatter = DateTimeFormatter.ofPattern(\u0026#34;Gy年\u0026#34;, Locale.JAPAN) val todayString = formatter.format(JapaneseDate.now()) // 令和3年 もしJava 1.8以前のバージョンを使うなどでLocalDateやJapaneseDateが使えなく、java.util.Dateの方を使うしかない場合は、以下のような方法で年号と年度の取得が可能です。\nval format = SimpleDateFormat(\u0026#34;Gy年\u0026#34;, Locale(\u0026#34;Ja\u0026#34;, \u0026#34;JP\u0026#34;, \u0026#34;JP\u0026#34;)) val year = format.format(Date()) // 令和3年 java.util.Dateを使う場合は、Localeに第3引数のvariantまで指定する必要があるので、既存の列挙型として定義されたものは使えません。\nまた、Locale.ENGLISHなどに設定すると、JapaenseDateを使っている場合でも取得した結果はAD2021年12月5日になります。\n合字で表記する 年号については、Unicodeで合字を取得して使いたい場合もあるかと思います。その場合は、以下のようにUnicodeのMapなどを定義しておいて取得するのが良いかと思います。拡張関数などを定義するのも良いでしょう。\nval eraUnicodeMap = mapOf( JapaneseEra.MEIJI to \u0026#34;\\u337e\u0026#34;, // ㍾ JapaneseEra.TAISHO to \u0026#34;\\u337d\u0026#34;, // ㍽ JapaneseEra.SHOWA to \u0026#34;\\u337c\u0026#34;, // ㍼ JapaneseEra.HEISEI to \u0026#34;\\u337b\u0026#34;, // ㍻ JapaneseEra.REIWA to \u0026#34;\\u32ff\u0026#34; // ㋿ ) val era = JapaneseDate.now().era // 元号を合字で取得する val eraUnicode = eraUnicodeMap[era] // ㋿ 上記のサンプルではJapaneseEraが列挙型なのでそのままキーとしていますが、JapaneseEraは数値としての情報も持っているのでそちらを使う方法もあるでしょう。それぞれの値に対する数値は以下の通りです。\nJapaneseEra 数値 MEIJI -1 TAISHO 0 SHOWA 1 HEISEI 2 REIWA 3 2021年から2022年の3月の場合は令和3年なので、JapaneseEra.REIWA.valueの値が年度だと勘違いされやすいかなと思います。実際の年度の情報はJapaneseDateの方にあるので注意しましょう。\n年度を数字で表示する JapaneseEraは元号を得るために使う列挙型定数のクラスなので、これ自体はJapaneseDateの日付情報を持っていません。なので参照できる情報は、あくまでも元となるJapaneseDateが属した元号の情報のみです。\nなので数値としての年度は、列挙型のChronoFieldをJapaneseDate.get()に渡して取得する必要があります。\nval today = JapaneseDate.of(2010, 12, 31) // 平成22年 // 年度をIntとして取得する val year = today.get(ChronoField.YEAR) // 2010 val yearOfHeisei = today.get(ChronoField.YEAR_OF_ERA) // 22 これはJapaneseDateがLocalDateと違って、直接yearをgetterで取得できないからです。実際オブジェクトの中を覗いてみると、LocalDateは年月日をintとshortのフィールドとして保持していることに対して、JapaneseDateはLocalDateとint型のyearOfEraを持っていて、get(ChronoField.YEAR_OF_ERA)を通じてはじめてyearOfEraを取得できることになります。getterを用意していないのはおそらくLocalDateとyearOfEraという二つの概念があるからなのではないかと思います。もちろん、Kotlinなのでこれは簡単に拡張関数を書くことでgetterを作ることはできますね。\nまた、日付のオブジェクトとしてLocalDateを使っている場合は場合はChronoField.YEAR_OF_ERAを渡しても西暦の年度が返ってくるので、和暦を使うためにJapaneseDateを使っているかどうかをまず確認しましょう。\n年度を2桁の文字で表示する 厳密に言って和暦とは関係のないことですが、年度を取得して使う場合、一貫して先端に「0」のついた2桁の文字列として扱いたい場合もあるかと思います。JapaneseDateを通じて年度を取得した場合はInt型になるので、1〜9の間は1桁の数字となるわけですが、これを01〜09に表示したい場合は以下の方法が使えます。\nDecimalFormatを利用する 一つは、JavaのAPIであるDecimalFormatを使うことです。小数点の範囲などをわかりやすく指定できるので個人的には好むやり方です。\nval today = JapaneseDate.now() // 令和3年 // 数字を表示するためのフォーマットを指定 val decimalFormat = DecimalFormat(\u0026#34;00\u0026#34;) val year = decimalFormat.format(today) // 03 String.formatを利用する もう一つの方法は、Kotlinのスタンダードライブラリの機能であるString.format()を使うことです。性能注視なら、こちらの方法が良いかなと思います。\nval today = JapaneseDate.now() // 令和3年 // 数字を表示するためのフォーマットを指定 val year = \u0026#34;%02d\u0026#34;.format(today) // 03 番外：kotlinx-datetime Kotlinには元々日付や時間を扱うAPIがなかったのですが、2020年からkotlinx-datetimeを提供しています。なのでKotlin/JSやKotlin/Nativeなど、JVM上で動かない場合でも日付を扱える公式のAPIができたわけですが、いくつかの懸念があるのでこれを導入するには検討が必要かと思います。\nPre-releaseの段階 kotlinx-datetimeはまだpre-releaseの段階で、2021年10月にv0.3.1がリリースされています。なので色々とバグがあったり、思い通りにならない可能性があります。また、開発途中のものなので仕方ありませんが、現時点で提供している機能もjava.timeのAPIに比べて少なく、簡単に年号の計算などができるわけではありません。今は必要最低限の機能だけを提供していると思って良いでしょう。\nマルチプラットフォーム向け Kotlinのスタンダードライブラリ、及びkotlinxとして提供されるライブラリはマルチプラットホームを考慮した実装となっているため、プラットホームが違っても同じ使い方ができるというメリットがありますが、かえってデメリットになる場合もあります。実際、kotlinx-datetimeのJVMの実装は内部的にjata.timeのAPIに依存しているため、JVMだけを使う場合はあえて導入する必要がないともいえます。\nまた、プラットフォームごとに実装が違うということはどこかで予期せぬ例外が発生したり、期待した結果にならないケースも発生しえる、ということにもなるかと思います。\njava.timeの懸念 JapaneseEraでは明治以前（慶応など）の元号は使えませんが、おそらくその理由は和暦でグレゴリウス暦が使われたのは明治からだったという歴史的な背景があるのではないかと思います。また、JapaneseDateでも明治6年(西暦1873年1月1日)以前の日付を指定すると以下のように例外が発生します。\nException in thread \u0026#34;main\u0026#34; java.time.DateTimeException: JapaneseDate before Meiji 6 is not supported at java.base/java.time.chrono.JapaneseDate.\u0026lt;init\u0026gt;(JapaneseDate.java:333) at java.base/java.time.chrono.JapaneseDate.of(JapaneseDate.java:257) なので、単純に帳票を作るなどのケースでなく、歴史的な研究のための日付計算ではここで紹介した方法は使えないケースもあるかと思います。\nまた、JDKのバージョンなどの問題があるためか、JapaneseEra.REIWAの取得ができなく、エラーとなるケースがあるので注意する必要があります。この場合でもvalueの値の取得は問題ないので、少し可読性は低下しながら分岐などの判定に定数をそのまま使うのは避けたほうが良さそうです。（正確な理由はわかりませんが…）\n最後に いかがでしたか。少し興味本位で調べ始めたもののまとめではありますが、本業の方で実際に必要な処理でもあり、これをどうやって拡張関数として落とせるかということも考えられる良い機会となったかなと思っています。\nまた、JavaのAPIに関してはJavaバージョン別の改元(新元号)対応まとめという良い記事があったので、興味のある方はご一読ください。\nでは、また！\n","date":"2021-12-05T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-japanese-era/","title":"Kotlinで和暦を使う"},{"content":"Kotlinの隠されたコスト、その最後の記事となります。今までの記事もかなり興味深かったですが、今回はさらにKotlinならではの機能に触れているので、Kotlinそのものに対する理解も含めてみる必要があり、さらに深い内容となっているかと思います。\n今回のアジェンダは、「委譲プロパティ」と「rangeを使ったループ」になります。この記事はExploring Kotlin’s hidden costs - Part 3の内容を要約したものです。\n委譲プロパティ 委譲プロパティとは、getterとsetterが委譲(delegate)というオブジェクトによって実装されたプロパティを指します。これによって再利用可能なカスタムプロパティを作ることができます。\nclass Example { var p: String by Delegate() } 委譲オブジェクトはプロパティの設定と読み込みのためgetValue()とsetValue()を実装する必要があります。そしてこれらの関数はプロパティのメタデータ（プロパティ名）とオブジェクトのインスタンスを引数として必要とします。\nクラスが委譲プロパティとして定義されると、コンパイラは下記のようなコードを生成します。\npublic final class Example { @NotNull private final Delegate p$delegate = new Delegate(); // $FF: 生成されたフィールド static final KProperty[] $$delegatedProperties = new KProperty[]{(KProperty)Reflection.mutableProperty1(new MutablePropertyReference1Impl(Reflection.getOrCreateKotlinClass(Example.class), \u0026#34;p\u0026#34;, \u0026#34;getP()Ljava/lang/String;\u0026#34;))}; @NotNull public final String getP() { return this.p$delegate.getValue(this, $$delegatedProperties[0]); } public final void setP(@NotNull String var1) { Intrinsics.checkParameterIsNotNull(var1, \u0026#34;\u0026lt;set-?\u0026gt;\u0026#34;); this.p$delegate.setValue(this, $$delegatedProperties[0], var1); } } 一部staticプロパティのメタデータがクラスに追加されます。そして毎回値の設定と読み込みが発生するたびにコンストラクタによる初期化が起こります。\n委譲インスタンス 上記サンプルでは新しい委譲のインスタンスがプロパティの実装のため生成されています。委譲がstatefulの場合にこのようになります。たとえはローカルで計算されたプロパティを使うなどの場合です。\nclass StringDelegate { private var cache: String? = null operator fun getValue(thisRef: Any?, property: KProperty\u0026lt;*\u0026gt;): String { var result = cache if (result == null) { result = someOperation() cache = result } return result } } またコンストラクタに追加のパラメータが渡されると、新しい委譲のインスタンスが必要となります。\nclass Example { private val nameView by BindViewDelegate\u0026lt;TextView\u0026gt;(R.id.name) } statelessであり、すでに渡されたオブジェクトのインスタンスとプロパティ名を保ちたいだけなら委譲クラスにobjectをつけてsingletonにする方法があります。たとえば下記のようなものです。\nobject FragmentDelegate { operator fun getValue(thisRef: Activity, property: KProperty\u0026lt;*\u0026gt;): Fragment? { return thisRef.fragmentManager.findFragmentByTag(property.name) } } また既存のオブジェクトを拡張して委譲することもできます。つまり、getValue()やsetValue()を拡張関数として定義することもできるということです。KotlinではすでにMapとMutableMapに拡張関数として委譲するパターンを使っています。（プロパティ名をキーで使っています）\nもし一つのクラス内でローカルの委譲インスタンスに複数のプロパティを保持して再利用したいなら、そのクラスのコンストラクタでインスタンスを初期化しましょう。\nKotlin 1.1以降、関数内のローカル変数を委譲プロパティにすることもできます。この場合、委譲は後で初期化できます。\nクラスに定義された委譲プロパティごとにオーバーヘッドとメタデータの追加が発生するのでなるべくプロパティを再利用できるようにした方が良いでしょう。また、定義したい項目が多い場合に、果たして委譲プロパティが良い選択肢であるかを考慮すべきです。\nジェネリック委譲 委譲関数はジェネリックでも定義できます。なので委譲クラスをさまざまな型のプロパティとして定義することもできます。\nprivate var maxDelay: Long by SharedPreferencesDelegate\u0026lt;Long\u0026gt;() ただ、上記のようにprimitiveをジェネリック委譲を使う場合、boxingとunboxingが値の指定と読み込みで発生することに注意する必要があります。これはプロパティがnon-nullの場合でも起こることです。\nなのでnon-nullなprimitive型の委譲プロパティを定義する場合はジェネリックで定義を避けたほうが良いです。\nスタンダード委譲（lazy()） KotlinではDelegates.notNull()、Delegates.observable()やlazy()のような委譲のための標準機能が存在しています。\nlazy()は読み込み専用の委譲プロパティのための関数です。初めて読み込みが発生する際、プロパティを初期化するた目にlambdaを指定できます。\nprivate val dateFormat: DateFormat by lazy { SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;, Locale.getDefault()) } これはその値が実際に読み込まれるまで高いコストの初期化を遅延させるという、パフォーマンスと可読性の側面で優れた方法です。\nただ、lazy()はinline関数ではなく、引数として渡されたlambdaは別のFunctionクラスとしてコンパイルされ、戻り値の委譲オブジェクトもまたinline化されないことには注意する必要があります。\nそしてlazy()関数で見逃しやすいのはmodeという引数で戻り値の委譲タイプを決められるということです。\npublic fun \u0026lt;T\u0026gt; lazy(initializer: () -\u0026gt; T): Lazy\u0026lt;T\u0026gt; = SynchronizedLazyImpl(initializer) public fun \u0026lt;T\u0026gt; lazy(mode: LazyThreadSafetyMode, initializer: () -\u0026gt; T): Lazy\u0026lt;T\u0026gt; = when (mode) { LazyThreadSafetyMode.SYNCHRONIZED -\u0026gt; SynchronizedLazyImpl(initializer) LazyThreadSafetyMode.PUBLICATION -\u0026gt; SafePublicationLazyImpl(initializer) LazyThreadSafetyMode.NONE -\u0026gt; UnsafeLazyImpl(initializer) } modeを指定していない場合、デフォルトとしてはLazyThreadSafetyMode.SYNCHRONIZEDが使われますが、これは複数のスレッドで初期化ブロックが安全に実行されることを保証するためにコストの高いdouble-checked lockを行います。\nシングルスレッドしかプロパティに対するアクセスがないというのがわかっているなら、無駄なロックは下げた方がいいでしょう。こういう場合はLazyThreadSafetyMode.NONEを使えます。\nval dateFormat: DateFormat by lazy(LazyThreadSafetyMode.NONE) { SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;, Locale.getDefault()) } Ranges Rangesで限定された範囲の値のセットを定義できます。この値はComparableなものならなんでも指定できますね。そして、この表現式を使うとClosedRangeというインタフェースの実装ができることになります。\n包含テスト rangeを使って範囲内に特定の値が含まれているかどうかをinや!inを使って検知することができます。\nif (i in 1..10) { println(i) } rangeはnon-nullなprimitive型（Int, Long, Byte, Short, Float, Double, Char）に対する最適化が行われるので、コンパイルされた結果は以下のようになります。\nif(1 \u0026lt;= i \u0026amp;\u0026amp; i \u0026lt;= 10) { System.out.println(i); } なので、オーバーヘッドや追加オブジェクトの割り当てなどは起こらないです。しかし、primitiveではない場合はどうでしょう。\nif (name in \u0026#34;Alfred\u0026#34;..\u0026#34;Alicia\u0026#34;) { println(name) } Kotlin 1.1.50以前はコンパイル時にClosedRangeオブジェクトが常に生成されました。しかし、1.1.50からは以下のようになります。\nif(name.compareTo(\u0026#34;Alfred\u0026#34;) \u0026gt;= 0) { if(name.compareTo(\u0026#34;Alicia\u0026#34;) \u0026lt;= 0) { System.out.println(name); } } rangeはまた、whenの条件式でも使えます。if-elseより可読性が良くなりますね。\nval message = when (statusCode) { in 200..299 -\u0026gt; \u0026#34;OK\u0026#34; in 300..399 -\u0026gt; \u0026#34;Find it somewhere else\u0026#34; else -\u0026gt; \u0026#34;Oops\u0026#34; } ただ、rangeを使う場合、特定の値が含まれているかどうかをチェックするとき、指定された範囲とそれを使うコードの間に間があるとコストがかかることになります。たとえば以下のようなコードがあるとします。\nprivate val myRange get() = 1..10 fun rangeTest(i: Int) { if (i in myRange) { println(i) } } この場合はコンパイルするとIntRangeオブジェクトが追加されます。\nprivate final IntRange getMyRange() { return new IntRange(1, 10); } public final void rangeTest(int i) { if(this.getMyRange().contains(i)) { System.out.println(i); } } これはプロパティのgetterをinlineとして定義しても同じです。なのでなるべくrangeが使われるテストの方に直接書くことでオブジェクトが追加されない要因した方が良いです。また、primitiveではないオブジェクトを使う場合は定数として定義し、ClosedRangeのインスタンスを再利用する方法があります。\nforループ FloatとDoubleを除いたprimitive型の範囲をループで使うのも良い選択です。\nfor (i in 1..10) { println(i) } コンパイルされた結果にはオーバーヘッドが発生しません。\nint i = 1; for(byte var2 = 11; i \u0026lt; var2; ++i) { System.out.println(i); } 逆順にループしたい場合はdownTo()を使えます。\nfor (i in 10 downTo 1) { println(i) } これにもまた、オーバーヘッドは発生しません。\nint i = 10; byte var1 = 1; while(true) { System.out.println(i); if(i == var1) { return; } --i; } untilを使って特定の値未満にループするのも良いですね。\nfor (i in 0 until size) { println(i) } 以前は少しコストがかかることになりましたが、Kotlin 1.1.4以降は以下のようなコードが生成されます。\nint i = 0; for(int var2 = size; i \u0026lt; var2; ++i) { System.out.println(i); } ただ、そのほかは最適化があまり効いてないケースもあります。reversed()を使う例があるとしましょう。\nfor (i in (1..10).reversed()) { println(i) } コンパイルされたコードがあまり綺麗とは言えません。\nIntProgression var10000 = RangesKt.reversed((IntProgression)(new IntRange(1, 10))); int i = var10000.getFirst(); int var3 = var10000.getLast(); int var4 = var10000.getStep(); if(var4 \u0026gt; 0) { if(i \u0026gt; var3) { return; } } else if(i \u0026lt; var3) { return; } while(true) { System.out.println(i); if(i == var3) { return; } i += var4; } IntRangeオブジェクトが範囲を再定義するため生成され、さらにIntProgressionオブジェクトが逆順に要素を整列するために生成されます。\nprogressionを作るのに二つ以上の関数が使われていると、二つ以上のオブジェクトを作るようなオーバーヘッドが発生することになります。\n上記のルールはstep()を使う場合も同じで、step 1を指定しても状況は変わりません。\nfor (i in 1..10 step 2) { println(i) } さらに、生成されたコードで最後の値を読み込む時、IntProgressionオブジェクトの最後の要素とstep()で指定した範囲を考慮して追加の処理が行われます。上記のサンプルだと最後の要素は9です。\nなので、forを利用したループをするときはなるべく..、downTo()、until()を利用してオーバーヘッドを避けた方が良いでしょう。\nforEachループ forループの代わりに、rangeに対してinline拡張関数のforEach()を使う場合も結果はあまり変わりません。\n(1..10).forEach { println(it) } しかし、forEach()はIterableに対してのみ最適化されてないです。これはつまり、iteratorを生成する必要があるということを意味します。なので、コンパイルされると以下のようになります。\nIterable $receiver$iv = (Iterable)(new IntRange(1, 10)); Iterator var1 = $receiver$iv.iterator(); while(var1.hasNext()) { int element$iv = ((IntIterator)var1).nextInt(); System.out.println(element$iv); } これは今までのサンプルよりもコストのかかるものです。IntRangeオブジェクトを生成するだけでなく、IntIteratorオブジェクトも生成しているからです。primitiveではない場合はさらにコストがかかるでしょう。\nなので、rangeを使ったループが必要な場合はforEach()よりforループを使ってオーバーヘッドを減らした方が良いです。\ncollectionインデックスループ Kotlinのスタンダードライブラリはindicesという拡張プロパティで配列とCollectionのインテックスを提供します。\nval list = listOf(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;) for (i in list.indices) { println(list[i]) } indicesのコンパイルされた結果は良い最適化を見せてくれます。\nList list = CollectionsKt.listOf(new String[]{\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;}); int i = 0; for(int var2 = ((Collection)list).size(); i \u0026lt; var2; ++i) { Object var3 = list.get(i); System.out.println(var3); } IntRangeオブジェクトが作られてないです。では、自前で実装してみるとどうなるのでしょう。\ninline val SparseArray\u0026lt;*\u0026gt;.indices: IntRange get() = 0 until size() fun printValues(map: SparseArray\u0026lt;String\u0026gt;) { for (i in map.indices) { println(map.valueAt(i)) } } 拡張プロパティとして定義してコンパイルすると、あまり効率的ではないコードになっていることがわかります。IntRangeオブジェクトが作られてます。\npublic static final void printValues(@NotNull SparseArray map) { Intrinsics.checkParameterIsNotNull(map, \u0026#34;map\u0026#34;); IntRange var10000 = RangesKt.until(0, map.size()); int i = var10000.getFirst(); int var2 = var10000.getLast(); if(i \u0026lt;= var2) { while(true) { Object $receiver$iv = map.valueAt(i); System.out.println($receiver$iv); if(i == var2) { break; } ++i; } } } この場合は代わりにuntil()とforループを使った方が良いでしょう。\nfun printValues(map: SparseArray\u0026lt;String\u0026gt;) { for (i in 0 until map.size()) { println(map.valueAt(i)) } } 最後に いかがでしたか。個人的にはあまり委譲プロパティを使ったことがなく、そもそもの理解を兼ねてかなり勉強になりました。また、rangeに関しても、Javaでの習慣でテストクラスのフィールドとして定義していろいろな関数で使い回していましたが、まさかそれがよりコストのかかることだとは思ってなかったので少しショックでした。\nまた、改めてKotlinで提供している機能とAPIに対して正しく理解する必要があると思いました。そしてオッカムの剃刀でも話しているように、なるべくシンプルなロジックとコードを追求する必要があるとも思いましたね。intellijのメニューのうち、Tools \u0026gt; Kotlin \u0026gt; Show Kotlin Bytecode でいつでもJavaのコードにdecomplieされたコードを確認できるので、最新だとどのように変換されるのかを確認してみながらコードを最適化を行なった方が良いかもしれません。\n今月はいつもの、自分の経験や仮説を紹介するようなポストでなく、ほぼ翻訳のみになってしまいましたが、私自身としてはかなり貴重な知識を得られたと思っています。またの機会で何か良いものがあったら、是非とも紹介させていただきたいですね。\nでは、また！\n","date":"2021-11-28T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-hidden-cost-3/","title":"Kotlinの隠されたコストーその３"},{"content":"今回はまたKotlinの隠されたコストに対するポストです。今となってはあまり気にすることはないかもしれませんし（検証は必要そうですが、バージョンアップごとにコンパイラが生成するコードを追うのは大変そうですね…）、極限のチューニングをするよりもマシンスペックを上げた方がよい時代になったとはいうものの、この記事で紹介していることをコーディングの習慣として身につけておくと良いかなと思います。\n前回は高階関数とLambda、そしてcompanion objectに関する記事を紹介しました。今回はローカル関数、Null安定性、Varargsに隠されたKotlinのコストについて述べます。この記事はExploring Kotlin’s hidden costs - Part 2の内容を要約したものです。\nローカル関数 関数内に定義した関数を「ローカル関数」と言います。これらローカル関数は、アウター関数（ローカル関数が定義された関数）の範囲にアクセスできます。例えば以下だと、sumSquareでsomeMathのパラメータにアクセスしているのがわかります。\nfun someMath(a: Int): Int { fun sumSquare(b: Int) = (a + b) * (a + b) return sumSquare(1) + sumSquare(2) } ローカル関数は基本的にLambdaと似ていますが、他に制限があります。ローカル関数そのものと、ローカル関数を含む関数もまたinlineとして定義できません。なので関数の呼び出しにかかるコストを避ける方法がありません。\nコンパイルされたローカル関数はFunctionオブジェクトに変わります。なので前回の記事で述べた「インライン化してないLambda」と同じ問題を持っています。上記のコードをJavaのコードで表すと以下のような形になります。\npublic static final int someMath(final int a) { Function1 sumSquare$ = new Function1(1) { // $FF: 生成されたメソッド // $FF: ブリッジメソッド public Object invoke(Object var1) { return Integer.valueOf(this.invoke(((Number)var1).intValue())); } public final int invoke(int b) { return (a + b) * (a + b); } }; return sumSquare$.invoke(1) + sumSquare$.invoke(2); } ただ、Lambdaと比べ一つ性能が劣化されない点があります。関数のインスタンスが呼び出し元からわかるので、ジェネリックなインタフェースを使わず、匿名クラスになりメソッドが直接呼び出されます。これは外の関数からローカル関数を呼び出す際に、castingやboxingが発生しないということを意味します。実際のBytecodeを見ると以下の通りです。\nALOAD 1 ICONST_1 INVOKEVIRTUAL be/myapplication/MyClassKt$someMath$1.invoke (I)I ALOAD 1 ICONST_2 INVOKEVIRTUAL be/myapplication/MyClassKt$someMath$1.invoke (I)I IADD IRETURN ここでメソッドが2回呼び出されていますが、メソッドの引数も戻り値もint型になっていて、boxingとunboxingがないのを確認できます。\nただ、依然としてメソッドが呼び出されるたびにFunctionオブジェクトのインスタンスを生成していますが、ローカル関数をvalue caputeなしのものに代替することでこの問題は回避できます。\nfun someMath(a: Int): Int { fun sumSquare(a: Int, b: Int) = (a + b) * (a + b) return sumSquare(a, 1) + sumSquare(a, 2) } 上記のようにすることで、Functionオブジェクトのインスタンスは再利用できるようなものになります。こうすることで既存のprivate関数に比べ、ローカル関数のデメリットは追加のクラス（メソッドを含む）を生成するということだけになります。\nローカル関数はprivate関数の代替として、アウター関数の変数にアクセスできるというメリットがあります。ただこれによってFunctionオブジェクトを生成するというコストがかかりますので、non-capturingにする工夫が必要です。\nNull安全性 Kotlinの最も良い機能の一つは明視的にnullになり得る型とそうでない型を区別できるということです。これによってコンパイラがランタイムで予期せぬNullPointerExceptionを投げるのを防止できます。\nNon-nullパラメータのランタイムでのチェック 例えば以下のような関数があるとします。\nfun sayHello(who: String) { println(\u0026#34;Hello $who\u0026#34;) } これはJavaのコードで以下のようになります。\npublic static final void sayHello(@NotNull String who) { Intrinsics.checkParameterIsNotNull(who, \u0026#34;who\u0026#34;); String var1 = \u0026#34;Hello \u0026#34; + who; System.out.println(var1); } @NotNullアノテーションが追加され、Java側にnullが渡されてはいけないということを知らせています。\nしかし、アノテーションは呼び出し側にnull safetyを強制するものではありません。なのでstaticメソッドを呼び出してパラメータをもう一度確認しています。この関数はIllegalArgumentExceptionを投げて呼び出し元の修正を簡単にします。\npublicな関数には常にnon-nullなパラメータに対してIntrinsics.checkParameterIsNotNull()でのチェックがが追加されますが、privateな関数に対しては追加されません。なぜなら、Kotlinクラスはnull safeであることをコンパイラが保証するからです。\nこのNullチェックによるパフォーマンスへの影響は無視しても良いほどでテストにも有用ですが、ビルド時にもっと時間がかかる原因になります。これに対してはコンパイラのオプションに-Xno-param-assertionsを追加するか、ProGuardのルールに以下の設定を追加することでランタイムNullチェックをなくすことができます。\n-assumenosideeffects class kotlin.jvm.internal.Intrinsics { static void checkParameterIsNotNull(java.lang.Object, java.lang.String); } ただ上記のルールを追加する場合、AndroidのProGuardのOptimization設定が有効になっているかのチェックがまず必要です。この設定はデフォルトでは無効になっています。\nNullable primitive型 まず先に覚えておくべきことは、nullableで宣言したprimitive型は常にJavaのintやfloatなどの代わりにInteger、Floatといったboxed reference型が使われるので追加のコストが発生するということです。\nautoboxingとnull-safetyを無視するのでJavaではIntegerでもintでもコードはあまり変わらないJavaに対して、Kotlinだとnullableに対して安全なコードを書くように強制しているので、non-nullの方を使った方が良いというのが明確にわかります。\nfun add(a: Int, b: Int): Int { return a + b } fun add(a: Int?, b: Int?): Int { return (a ?: 0) + (b ?: 0) } なので、なるべくコードの可読性と性能を考慮してnon-nullの方を選んだ方が良いです。\n配列 Kotlinには、以下の3通りの配列があります。\nIntArray、FloatArrayのようなもの：primitive型の配列。int[]、float[]のような型にコンパイルされる。 Array\u0026lt;T\u0026gt;：non-nullオブジェクトの型が指定された配列。primitiveに対してboxingが起こりえる。 Array\u0026lt;T?\u0026gt;：nullableオブジェクトの型が指定された配列。明確にboxingが起こる。 もしnon-nullなprimitive型の配列が必要な場合は、なるべくArray\u0026lt;Int\u0026gt;の代わりにIntArrayを使いましょう。\nVarargs KotlinではJavaとは書き方が少し違いますが、可変長引数を定義することができます。\nfun printDouble(vararg values: Int) { values.forEach { println(it * 2) } } Javaと同じく、varargはコンパイルされると指定した型の配列になります。そして上記の関数は以下のように、３つの方法で呼び出すことができます。\n複数のパラメータを渡す printDouble(1, 2, 3) Kotlinのコンパイラはこれを新しい配列の生成と初期化に変えます。これはJavaと一緒です。\nprintDouble(new int[]{1, 2, 3}); これはつまり新しい配列を作るためのオーバヘッドがあるということです。ただJavaと変わらないやり方です。\n配列を渡す Javaでは配列をそのまま渡すことができますが、Kotlinだとそれができず、spread operatorを使う必要があります。\nval values = intArrayOf(1, 2, 3) printDouble(*values) Javaでは配列の参照がas-isとして関数に渡され、新しい配列の割り当ては起こりません。しかし、Kotlinのspread operatorは以下のようなことをします。\nint[] values = new int[]{1, 2, 3}; printDouble(Arrays.copyOf(values, values.length)); 配列のコピーが関数に渡されるので、より安全なコードといえます。呼び出し側には影響なしで、配列を修正できますので。しかしメモリを追加的に消費してしまいます。\n配列と他の引数を混ぜて渡す spread operatorの良い点は、配列と他の引数を混ぜて渡すこともできるということです。\nval values = intArrayOf(1, 2, 3) printDouble(0, *values, 42) この場合はどうコンパイルされるか気になりませんか？結果はかなり面白いです。\nint[] values = new int[]{1, 2, 3}; IntSpreadBuilder var10000 = new IntSpreadBuilder(3); var10000.add(0); var10000.addSpread(values); var10000.add(42); printDouble(var10000.toArray()); 配列を新しく生成するだけでなく、一時的なビルダオブジェクトを使って配列の最終的なサイズを計算しています。なので配列を渡す時よりもコストは追加されます。\nなので、呼び出される回数の多くパフォーマンスが重要なコードに対してはなるべく可変長引数より実際の配列をパラメータとして使った方が良いです。\n最後に いかがでしたか。個人的にprivate関数をよく使うので、よりスコープを制限できるという面でローカル関数を積極的に使いたいと思っていましたが、ここでも隠されたコストがあるというというのは興味深かったです。primitive型についてはJavaがそうだったので、なんとなくboxingが起こるんじゃないかなと思っていたものの、nullableに対してのみそうだというのも面白かったですね。逆に、primitiveのままになるnon-null型に対してはどうやってチェックが走るのだろうという新しい疑問もありました。（例えばintだとデフォルト値の0が常に割り当てられるので）\nあと、配列の場合はJavaでもIntStream、DoubleStreamなどがあったのでなんとなくすぐ理解ができましたが、まさかvarargsで渡したパラメータに対して色々とコストが追加されるとは思わなかったです。そもそもあまり配列を使わないので、可変長引数を使う場面もなかったのですが…よく使わないものほど重要なことを忘れやすそうなので、これは覚えておかないとですね。色々と勉強になりました。\nでは、また！\n","date":"2021-11-21T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-hidden-cost-2/","title":"Kotlinの隠されたコストーその２"},{"content":"Kotlinは便利ですが、何が便利かというと代表的に挙げられるものがたくさんのシンタクスシュガーではないかと思います。同じJVM言語のJavaと比べ、多くの場合でコード量が劇的に減るのが嬉しいという評価も多いものですね。しかし、この便利さの裏には隠されたコスト（性能面での）があるという話があります。今回はそれについて説明している良い記事を見つけたので、共有したいと思います。ただ、翻訳よりは要約に近いものなので、そこはご了承ください。\nちなみにここで紹介している記事（Exploring Kotlin’s hidden costs - Part 1）は、2017年に作成された（Kotlinがまだ1.1だったころ）ので、1.5にまでバージョンアップを成している今からすると、コンパイラの改善などで少し状況が違うケースもあるかと思いますが、述べている内容のレベルが高いので一度は目を通してみても良いかなと思います。また、記事で紹介しているKotlinのBytecodeに対しても、直接最近のKotlinが生成しているコードと比較してみるのも面白いかもですね。\nまた、今回紹介している記事はPart 1ですが、そのほかにもPart 2やPart 3の記事がありますので、今後も順次紹介させていただきたいと思います。では、まずLambda表現式とcompanion object編を、どうぞ。\n高階関数とLambda表現式 例えば以下のような関数を定義しておいたとしましょう。渡されたパラメータをDBのトランザクションの中で実行し、実行結果の行数を返すものです。\nfun transaction(db: Database, body: (Database) -\u0026gt; Int): Int { db.beginTransaction() try { val result = body(db) db.setTransactionSuccessful() return result } finally { db.endTransaction() } } 上記の関数は、Lambdaを渡して以下のように使えますね。\nval deletedRows = transaction(db) { it.delete(“Customers”, null, null) } KotlinはJava 1.6のJVMから使えますが、Java 1.6のJVMではLambdaに対応していないのです。なので、Kotlinはその互換性を維持するためにLambda（匿名関数も）をFunctionというオブジェクトを生成することで対応しています。\nFunctionオブジェクト では、実際コンパイルされたLambda（body）がJavaのコードとしてはどうなっているかをみていきましょう。（ここでは、Intellij/Android StudioのShow Kotlin BytecodeのDecompile機能を使っています）\nclass MyClass$myMethod$1 implements Function1 { // $FF: 生成されたメソッド // $FF: ブリッジメソッド public Object invoke(Object var1) { return Integer.valueOf(this.invoke((Database)var1)); } public final int invoke(@NotNull Database it) { Intrinsics.checkParameterIsNotNull(it, \u0026#34;it\u0026#34;); return it.delete(\u0026#34;Customers\u0026#34;, null, null); } } これを見るとわかりますが、Lambda（匿名関数）を使う場合、コンパイルされた結果としては基本的に3、4個のメソッドが追加で生成されるということになります。ここで追加されたFunctionオブジェクトのインスタンスは、必要な時にだけ生成されます。正確には、以下のような動作をします。\nvalue captureがある場合、毎回パラメータが渡されるたびFunctionのインスタンスが生成され、GCの対象になる value captureがない場合、FunctionはSingletonとしてインスタンスが生成され再利用できる 先ほどのコードでは、value captureがないため、Lambdaの呼び出し元は以下のようなコードとしてコンパイルされます。\nthis.transaction(db, (Function1)MyClass$myMethod$1.INSTANCE); しかし、value captureのある高階関数を繰り返し呼び出す場合はGCによる性能の低下を考えれます。\nBoxingオーバーヘッド Lambdaに対応しているJava 1.8以降のバージョンでは、Functionインタフェースを複数提供していることでなるべくboxing/unboxingを避けようとしています。しかし、Kotlinでコンパイルされた場合はgenericを利用しています。\n/** 引数を一つ受け取る関数 */ public interface Function1\u0026lt;in P1, out R\u0026gt; : Function\u0026lt;R\u0026gt; { /** 引数を受け取り関数を実行する */ public operator fun invoke(p1: P1): R } これらをみてわかるのは、高階関数でパラメータとして渡された関数を呼び出す時に、その関数にprimitiveタイプの値が存在する場合（パラメータ、もしくは戻り値）boxing/unboxingが起こるということです。先ほどのコンパイルされたLambdaにおいて、戻り値がIntegerとしてboxingされたのを確認できましたね。\nprimitiveタイプを使用するLambdaをパラメータとしてとる高階関数は、参照回数が少なければあまり意識しなくてもよいコストになりますが、そうでない場合は性能に影響があると推定できます。\nInline関数 幸い、Kotlinではinlineと言うキーワドを提供しています。これを使うと高階関数をインライン化できますね。インライン化されると呼び出し元のコードにFunctionの中身を直接含ませてコンパイルします。なので、インライン化された場合は以下のような面で性能の向上を考えられます。\nFunctionオブジェクトのインスタンスが生成されない primitiveタイプを使う関数に対してboxing/unboxingが起こらない メソッドカウントが増えない（Androidの場合、アプリが参照できるメソッドの数字に制限がある） 関数の呼び出しが増えない（CPU依存が高く、呼び出される頻度の高いコードのパフォーマンスの改善を期待できる） インライン化された場合のコードを確認してみましょう。transaction関数が消え、db.deleteを直接呼び出しているのがわかります。また、戻り値のresultもWrapperクラスからprimitiveタイプになっているのがわかります。\ndb.beginTransaction(); try { int result$iv = db.delete(\u0026#34;Customers\u0026#34;, null, null); db.setTransactionSuccessful(); } finally { db.endTransaction(); } ただ、inlineキーワードを使うときは以下のことを考慮しなければならないです。\nインライン関数は自分自身を直接呼び出したり、他のインライン関数から呼び出せない クラスに定義されたpublicなインライン関数はそのクラスのpublic関数とフィールドのみアクセスできる コンパイルされたコードが大きくなる（繰り返し参照される場合はより大きくなる） なるべく高階関数をインライン化し、必要であれば長いコードブロックをインラインではない関数に写した方がいいです。また、性能が大事なところでは呼び出された関数をインライン化することも考えられます。\nCompanion object Kotlinではクラスがstaticなフィールドやメソッドを定義できません。その代わりにcompanion objectを使うことになっていますね。\nクラスのprivateフィールドをcompanion objectからアクセスする 以下のような例があるとしましょう。\nclass MyClass private constructor() { private var hello = 0 companion object { fun newInstance() = MyClass() } } 上記のコードがコンパイスされると、companion objectはSingletonクラスになります。なので、クラスのprivateフィールドに外部クラスからアクセスできるようにする必要があり、コンパイラがgetter、setterを追加で生成することになるということです。生成されたメソッドはcompanion objectから参照されることになります。以下を見てください。\nALOAD 1 INVOKESTATIC be/myapplication/MyClass.access$getHello$p (Lbe/myapplication/MyClass;)I ISTORE 2 Javaだとこれを避けるためにアクセス制限をpackage単位にすることができましたが、Kotlinではそのようなキーワードがないですね。publicやinternalを使う場合もgetterとsetterは基本的に生成されます。また、これらのメソッドはinstanceメソッドであり、staticメソッドよりもコストが高いですね。なので、最適化のためフィールドのアクセス制限を変えるということは避けた方が良いです。\nもしcompanion objectからクラスのフィールドに頻繁なアクセスが発生するとしたら、この隠れているメソッドの呼び出しを避けるためにフィールドの値をキャッシュするという方法も考慮できます。\nCompanion objectの定数にアクセスする Kotlinでは、クラス内のstaticな定数はcompanion objectの中に定義するのが一般的です。\nclass MyClass { companion object { private val TAG = \u0026#34;TAG\u0026#34; } fun helloWorld() { println(TAG) } } 一見シンプルで良さげなコードですが、Kotlin 1.2.40以前の場合だとかなり裏のコードは汚くなっています。\nKotlin 1.2.40以前の場合 companion objectに定義されたprivateな定数にアクセスする場合、上記のようなこと（getterを利用する）が起こります。\nGETSTATIC be/myapplication/MyClass.Companion : Lbe/myapplication/MyClass$Companion; INVOKESTATIC be/myapplication/MyClass$Companion.access$getTAG$p (Lbe/myapplication/MyClass$Companion;)Ljava/lang/String; ASTORE 1 問題はこれだけではありません。生成されたメソッドは実際の値を返すわけでなく、instanceメソッドとして生成されたgetterを呼び出すことになります。\nALOAD 0 INVOKESPECIAL be/myapplication/MyClass$Companion.getTAG ()Ljava/lang/String; ARETURN 定数がpublicになっている場合はダイレクトにアクセスできるようになりますが、依然としてgetterメソッドを通して値にアクセスことになります。\nそして定数の値を格納するために、Kotlinコンパイラはcompanion objectではなく、それを持つクラスの方にprivate static finalフィールドを生成します。さらにcompanion objectからこのフィールドにアクセスするため、またのメソッドを生成することとなります。\nINVOKESTATIC be/myapplication/MyClass.access$getTAG$cp()Ljava/lang/String; ARETURN こういう長い道のりで、やっと値を読み込むことになります。\nGETSTATIC be/myapplication/MyClass.TAG : Ljava/lang/String; ARETURN まとめると、Kotlin 1.2.40以前のバージョンを使っている場合は以下のようになります。\ncompanion objectから静的メソッドを呼び出す companion objectからinstanceメソッドを呼び出す クラスのstaticメソッドを呼び出す staticフィールドから値を読み込む これをJavaのコードで表現すると以下の通りです。\npublic final class MyClass { private static final String TAG = \u0026#34;TAG\u0026#34;; public static final Companion companion = new Companion(); // 生成されるメソッド public static final String access$getTAG$cp() { return TAG; } public static final class Companion { private final String getTAG() { return MyClass.access$getTAG$cp(); } // 生成されるメソッド public static final String access$getTAG$p(Companion c) { return c.getTAG(); } } public final void helloWorld() { System.out.println(Companion.access$getTAG$p(companion)); } } よりコストの低いBytecodeを生成することも可能ですが、それは簡単ではないです。\nまずconstキーワードを使ってコンパイルタイム定数を定義することでメソッドの呼び出しをなくすことができます。しかし、KotlinではprimitiveかStringに対してのみ可能な方法です。\nclass MyClass { companion object { private const val TAG = \u0026#34;TAG\u0026#34; } fun helloWorld() { println(TAG) } } または@JvmFieldを使ってJavaのアプローチを取る方法を考えられます。こうすることでgetterやsetterが生成されず、フィールドに直接アクセスができるようになります。ただ、@Jvm系のアノテーションはJavaとの互換性のためのものであるのでこれが果たして良い方法かどうかを考えた方が良いでしょう。そしてpublicなフィールドのみ可能な方法です。\nAndroidの開発の場合だと、Parcelableオブジェクトを自前で実装する場合のみ有効な方法に思われます。例えば以下のようにですね。\nclass MyClass() : Parcelable { companion object { @JvmField val CREATOR = creator { MyClass(it) } } private constructor(parcel: Parcel) : this() override fun writeToParcel(dest: Parcel, flags: Int) {} override fun describeContents() = 0 } 最後の方法として、ProGuardやR8のようなツールを使ってBytecodeの最適化を狙うという方法があるでしょう。\nKotlin 1.2.40以降の場合 Kotlinn 1.2.40からは、companion objectに定義された値はメインクラスの方に格納されるということには変わりがありませんが、メソッドの生成と呼び出しなしで直接アクセスができるようになりました。これをJavaのコードとして表現すると以下の通りです。\npublic final class MyClass { private static final String TAG = \u0026#34;TAG\u0026#34;; public static final Companion companion = new Companion(); public static final class Companion { } public final void helloWorld() { System.out.println(TAG); } } また、上記のようにcompanion objectにメソッドが一つもない場合は、ProGuardやR8によるツールと使うとクラス自体が消えることで最適化されます。\nただ、companion objectに定義さえたメソッドの場合はコストが少しかかります。フィールドがメインクラスの方に格納されてあるため、companion objectに定義されたprivateフィールドにアクセスするためには依然として生成されたメソッドを経由することになります。\n最後に 今回は人の書いた記事を読んだだけですが、かなり勉強になる内容でした。特に私個人としては、intellijを使っていると何を基準にinlineキーワードを使った方がいいという警告が出るのか悩ましい場面がありましたが、それが少し理解できました。companion objectに関する話も、今は問題が解決されたものの、何も考えず「定数だからcompanion objectだな」と思っていた自分を反省することになりましたね。そしてこの後の記事でも面白い内容が色々と出てくるので、またの機会でぜひ紹介したいと思います。\nでは、また！\n","date":"2021-11-14T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-hidden-cost-1/","title":"Kotlinの隠されたコストーその１"},{"content":"今はどんなプログラミング言語を選んでもできることはあまり違わなく、まさに好みで選んでもいいと思えるくらいの時代となっていると思います。特に、Kotlin/JSのようなトランスパイラーやFlutterのようなフレームワークも続々と登場している時代なので、こういう傾向はこれからもどんどん加速していくのではないかと思います。\nしかしそのような変化がある一方で、今現在はプログラマに一人が扱えるプログラミング言語の数に対する要求も増えいている状況ではないかと思います。実際の業務ではさまざまな理由で使われる言語が決まっていて、自分が今まで触ったことのないものでも使えるようになる必要があり、一人のエンジニアが固定されたポジションでなく、さまざまな分野にかけて実装を行うケースもありますしね。いわゆるPolyglotの時代とも言えます。\nなので、少なくともいろいろな言語の特徴を把握しておくということが大事になっているのではないかと思います。そして、そのような必要によるものでなくても、自分が普段接してない言語のコンセプトに触れてみることで、メインとなる言語への理解が深まることもあるのではないのかなと思ったりもします。これはどんな言語でもできることはあまり変わらないということともある意味通じているのですが、他の言語のコンセプトを受け入れた新しいAPIや機能を導入したり、そのようなライブラリが登場する場合もあるので。\nさて、前置きが長くなりましたが、ということで、これからはたまにとある操作をするときにいろいろな言語ではどうやってできるのか、そしてそうした場合の特徴などを簡単に比べてみたいと思います。今回は、配列のソートになります。\nJavaScript JavaScriptではArray.prototype.sort()で配列のソートができます。なので、以下のようなコードを使えます。シンプルですね。\n\u0026gt; const a = [22, 1, 44, 300, 5000] \u0026gt; a.sort() [ 1, 22, 300, 44, 5000 ] また、元の配列の値は変更せず、新しくソートされた配列を作りたい場合は以下の方法を使えます。\n\u0026gt; const a = [22, 1, 44, 300, 5000] \u0026gt; const b = [...a].sort() // aをコピーしてソート \u0026gt; console.log(b) [ 1, 22, 300, 44, 5000 ] ただ、ここで気づいた方もいらっしゃると思いますが、ソートされた値が期待通りにはなっていません。本当なら、1, 22, 44, 300, 5000になるのが普通でしょう。ここで昇順に値をソートしたい場合は、ソートの方法を自前で作成する必要があります。例えば以下のような方法がありますね。\n\u0026gt; const a = [22, 1, 44, 300, 5000] \u0026gt; a.sort((a, b) =\u0026gt; a - b) [ 1, 22, 44, 300, 5000 ] このsort()では、引数として渡すcompareFunction（引数が二つ、戻り値はnumber）の戻り値の結果によって、以下のことが起こります。\n0より小さいと、aのインデックスをbの先に置く 0だと、aとbは変更しない 0より大きいと、bのインデックスをaの先に置く これはJavaをやっていた方だと、Comparatorと同じだなとすぐわかる内容ですね。アロー関数の形もJavaのLambdaに似ているので、あまり違和感なく適応できるかと思います。かなりシンプルなのですが、number型の配列に対しては自前のcompareFunctionが必要となるということは大事なので、気を付ける必要はあるでしょう。\n配列のインデックスを反転したい場合は、Array.prototype.reverse()を使うだけで良いです。この場合はnumberの配列でも自前のcompareFunctionが必要ないので、便利ですね。\n\u0026gt; const a = [22, 1, 44, 300, 5000] \u0026gt; a.reverse() [ 5000, 300, 44, 1, 22 ] Java では、次にJavaの方も見ていきましょう。先に述べた通り、Comparatorを使うと簡単にソートの方法を実装できるので、基本的には同じです。ただ、Javaの場合だとそもそもList.sort()、Collections.sort()、Arrays.sort()、[Stream.sorted()]など方法が色々あり、ソートしたいCollectionやArrayなどがImmutableであるかどうか、ComparatorやComparableを自前で実装するか、それともスタンダードライブラリに用意されてあるものを使うかなどのさまざまな選択肢も考慮する必要があるということですね。\n色々な選択肢がある中で、もっとも簡単なのは、Collections.sort()やArrays.sort()を使う方法かなと思います。これを使う場合、primitive型やStringのListは短いコードでソートができるという（そして標準機能という）メリットがありますね。\njshell\u0026gt; var a = new ArrayList\u0026lt;\u0026gt;() {{ add(22); add (1); add(44); add(300); add(5000); }}; a ==\u0026gt; [22, 1, 44, 300, 5000] jshell\u0026gt; Collections.sort(a); jshell\u0026gt; System.out.println(a); [1, 22, 44, 300, 5000] 次に、List.sort()が簡単です。Comparatorを引数として渡す必要がありますが、昇順・降順でソートしたい場合は既に用意されてあるメソッドを呼び出すだけですね。\njshell\u0026gt; var a = new ArrayList\u0026lt;\u0026gt;() {{ add(22); add (1); add(44); add(300); add(5000); }}; a ==\u0026gt; [22, 1, 44, 300, 5000] jshell\u0026gt; a.sort(Comparator.naturalOrder()); jshell\u0026gt; System.out.println(a); [1, 22, 44, 300, 5000] ちなみにComparatorで使える既定のソート方法は以下があります。\n昇順: naturalOrder() 降順: reverseOrder() 逆順: reversed() また、Comparatorは、Collections.sort()の引数としても使えます。なので、降順にソートしたい場合は以下のようなコードを使えます。\njshell\u0026gt; var a = new ArrayList\u0026lt;\u0026gt;() {{ add(22); add (1); add(44); add(300); add(5000); }}; a ==\u0026gt; [22, 1, 44, 300, 5000] jshell\u0026gt; Collections.sort(a, Comparator.reverseOrder()); jshell\u0026gt; System.out.println(a); [5000, 300, 44, 22, 1] 他に、元のListの値を変更せず、新しくソートされた結果を取得したい場合は、元のListをコピーする方法もありますが、もう一つの方法としてStreamを使う方法を考えられます。\njshell\u0026gt; var a = List.of(22, 1, 44, 300, 5000); a ==\u0026gt; [22, 1, 44, 300, 5000] jshell\u0026gt; var b = a.stream().sorted().collect(Collectors.toList()); b ==\u0026gt; [1, 22, 44, 300, 5000] Streamでソートする場合でも、Comparatorを使えます。\njshell\u0026gt; var a = List.of(22, 1, 44, 300, 5000); a ==\u0026gt; [22, 1, 44, 300, 5000] jshell\u0026gt; var b = a.stream().sorted(Comparator.reverseOrder()).collect(Collectors.toList()); b ==\u0026gt; [5000, 300, 44, 22, 1] また、DTOのListをソートしたい場合は、DTOがComparableを継承するという方法も考えられますが、多くの場合はソート時の条件が明確にわかるComparatorを実装したいいかなと思います。汎用性や柔軟性を考えても、Comparableの場合、条件が変わるとクラスを修正する必要があるので、Comparatorを使った方が無難かなと思います。\nArrayの場合、Arrays.sort()を利用してソートできる（もちろんComparatorも使えます）上に、ListやStreamに変換することもできるので上記の方法をそのまま使えます。なので選択肢はもっと多いわけですが、便利な（好みに合う）方法を選ぶといいかなと思います。個人的にはArrays.sort()にComparatorを渡した方が可読性という面で良さそうな気がします。\nKotlin Sytax Sugarをたくさん提供しているKotlinらしく、選べるソートのオプションがたくさんあります。なので、少しまとめてみました。\nOrderの種類 ソート結果 fun 備考 Natural 呼び出し元 Array/MutableList.sort() 昇順 Array/MutableList.sortDescending() 降順 Array/MutableList.reverse() 逆順 Array Array.sortedArray() 昇順 Array.sortedArrayDescending() 降順 Array.reveredArray() 逆順 List Array/List.sorted() 昇順 Array/List.sortedDescending() 降順 List/MutableList.asRevered() 逆順 Custom 呼び出し元 Array/MutableList.sortBy() 昇順、selector((T) -\u0026gt; R)必要 Array/MutableList.sortByDescending() 降順、selector((T) -\u0026gt; R)必要 List Array/Iterable.sortedBy() 昇順、selector((T) -\u0026gt; R)必要 Array/Iterable.sortedByDescending() 降順、selector((T) -\u0026gt; R)必要 Array Array.sortedArrayWith() Comparator必要 List Array/Iterable.sortedWith() Comparator必要 かなり多い選択肢があるように見えますが、こうやって表としてまとめてみるとまぁまぁわかりそうな気はします。自前の比較処理を書く必要があるか、ソートした結果が元の配列かどうか、そしてArrayになるかListになるかなどいくつかの基準で分けられるということが分かれば大体どれを使った方がいいか悩む必要はないかなと思います。\nなので、まずやりたいことを明確にした上で、どのAPIを使うかを選んで書くだけです。以下はListから、ソートされた新しいListを作成する例です。それぞれ昇順と降順の場合となっています。\n\u0026gt;\u0026gt;\u0026gt; val a = listOf(22, 1, 44, 300, 5000) \u0026gt;\u0026gt;\u0026gt; val b = a.sorted() \u0026gt;\u0026gt;\u0026gt; println(b) [1, 22, 44, 300, 5000] \u0026gt;\u0026gt;\u0026gt; val c = a.sortedDescending() \u0026gt;\u0026gt;\u0026gt; println(c) [5000, 300, 44, 22, 1] また、data classの配列をソートしたい場合はsortByやsortedByを使えます。ここで引数に必要なのは(T) -\u0026gt; R型のselectorですが、単純にどれを基準にソートするかを指定すれば良いだけですので実装は簡単です。以下の例を見てください。\n\u0026gt;\u0026gt;\u0026gt; data class Data(val number: Int) \u0026gt;\u0026gt;\u0026gt; val a = listOf(Data(22), Data(1), Data(44), Data(300), Data(5000)) \u0026gt;\u0026gt;\u0026gt; val b = a.sortedBy { it.number } \u0026gt;\u0026gt;\u0026gt; println(b) [Data(number=1), Data(number=22), Data(number=44), Data(number=300), Data(number=5000)] \u0026gt;\u0026gt;\u0026gt; val c = a.sortedByDescending { it.number } \u0026gt;\u0026gt;\u0026gt; println(c) [Data(number=5000), Data(number=300), Data(number=44), Data(number=22), Data(number=1)] 他にも、より複雑な比較の条件を指定したい場合はJavaの場合と同じく、Comparatorを実装すると良いでしょう。やはりJavaと似ているようで、より単純化した（そしてそのせいで選択肢は増えた）感覚ですね。\nSwift Swiftでは、シンプルに元のCollectionをソートするかソートされた新しいCollectionを作るかの選択肢しかないようです。あまり変わったことはないですが、元のCollectionをソートする場合は以下のようになります。\n1\u0026gt; var a = [22, 1, 44, 300, 5000] a: [Int] = 5 values { [0] = 22 [1] = 1 [2] = 44 [3] = 300 [4] = 5000 } 2\u0026gt; a.sort() 3\u0026gt; print(a) [1, 22, 44, 300, 5000] そして新しいCollectionを作成したい場合は以下のようになります。\n1\u0026gt; let a = [22, 1, 44, 300, 5000] a: [Int] = 5 values { [0] = 22 [1] = 1 [2] = 44 [3] = 300 [4] = 5000 } 2\u0026gt; let b = a.sorted() b: [Int] = 5 values { [0] = 1 [1] = 22 [2] = 44 [3] = 300 [4] = 5000 } 3\u0026gt; print(b) [1, 22, 44, 300, 5000] ただ、Swiftのソートが独特なのはどうやってソートするか、その方法を指定する時です。sort()でもsorted()でも引数としてareInIncreasingOrderという関数を渡すことができるようになっていますが、JavaScriptやJava、Kotlinで使われていたcompareFunctionやComparatorの戻り値が数字であったことに対して、areInIncreasingOrderはpredicate型として戻り値がBoolになっています。なので以下のような形でソートの方法を指定可能です。\nlet students: Set = [\u0026#34;Kofi\u0026#34;, \u0026#34;Abena\u0026#34;, \u0026#34;Peter\u0026#34;, \u0026#34;Kweku\u0026#34;, \u0026#34;Akosua\u0026#34;] let descendingStudents = students.sorted(by: \u0026gt;) print(descendingStudents) // \u0026#34;[\u0026#34;Peter\u0026#34;, \u0026#34;Kweku\u0026#34;, \u0026#34;Kofi\u0026#34;, \u0026#34;Akosua\u0026#34;, \u0026#34;Abena\u0026#34;]\u0026#34; 他に、classのフィールドを基準にソートしたい場合は以下の方法を使えます。\nstruct Data { var number = 0 } let datas = [Data(number: 1), Data(number: 3), Data(number: 4), Data(number: 2)] let descending = datas.sorted { $0.number \u0026gt; $1.number } dump(descending) /** descending: [Data] = 4 values { [0] = { number = 4 } [1] = { number = 3 } [2] = { number = 2 } [3] = { number = 1 } } */ Go Goにはジェネリックがないからか、sortというパッケージに、sliceの種類によってソート用のfuncが色々と用意されています。例えば以下のようなものがあります。\nfunc Float64s(x []float64) func Ints(x []int) func Strings(x []string) なので、structのsliceではい場合はこれらの中でどれかを選んでソートすることになりますね。例えば以下のようになります。\na := []int{22, 1, 44, 300, 5000} sort.Ints(a) fmt.Println(a) // [1 22 44 300 5000] structの場合は、以下のような方法が使えます。ソートの基準がまたboolになっています。\npeople := []struct { Name string Age int }{ {\u0026#34;Gopher\u0026#34;, 7}, {\u0026#34;Alice\u0026#34;, 55}, {\u0026#34;Vera\u0026#34;, 24}, {\u0026#34;Bob\u0026#34;, 75}, } sort.Slice(people, func(i, j int) bool { return people[i].Name \u0026lt; people[j].Name }) fmt.Println(people) // [{Alice 55} {Bob 75} {Gopher 7} {Vera 24}] 面白いのは、Goのソートにはsort.SliceStable()というものが別に存在しているということです。これは安定ソートを行うもので、その定義に関してはWikiでは以下のように述べています。\n同等なデータのソート前の順序が、ソート後も保存されるものをいう。つまり、ソート途中の各状態において、常に順位の位置関係を保っていることをいう。\nつまり、安定ソートの場合、ソートの基準となる値が同等の要素間の元の位置関係（インデックス）が保証されるということですね。その結果が実際どうなるのかを見てみましょう。\npeople := []struct { Name string Age int }{ {\u0026#34;Alice\u0026#34;, 25}, {\u0026#34;Elizabeth\u0026#34;, 75}, {\u0026#34;Alice\u0026#34;, 75}, {\u0026#34;Bob\u0026#34;, 75}, {\u0026#34;Alice\u0026#34;, 75}, {\u0026#34;Bob\u0026#34;, 25}, {\u0026#34;Colin\u0026#34;, 25}, {\u0026#34;Elizabeth\u0026#34;, 25}, } sort.SliceStable(people, func(i, j int) bool { return people[i].Age \u0026lt; people[j].Age }) fmt.Println(people) // [{Alice 25} {Bob 25} {Colin 25} {Elizabeth 25} {Alice 75} {Alice 75} {Bob 75} {Elizabeth 75}] コードの実行結果でわかるように、Alice 25、Bob 25、Colin 25、Elizabeth 25とAlice 75, Bob 75, Elizabeth 75の元の順が維持されたままソートされたのがわかります。ここでもしsort.Slice()を使うと以下のようになります。\nsort.Slice(people, func(i, j int) bool { return people[i].Name \u0026lt; people[j].Name }) fmt.Println(people) // [{Alice 25} {Alice 75} {Alice 75} {Bob 75} {Bob 25} {Colin 25} {Elizabeth 75} {Elizabeth 25}] 安定ソートはそうでないソートに比べ性能が劣る可能性が高いので（元のインデックスをまで考慮しているので）、一つの値を基準にソートしても問題ない場合はsort.Slice()でも十分な気がしますが、そうでない場合は安定ソートを考慮する必要がありそうですね。\nPython Pythonではlist.sort()か、sorted()を使えます。他の言語でも大体同じだったので命名だけでも推測が可能かと思いますが、前者は元のlistをソートするもので、後者は新しいlistを作り出すものです。\nまずlist.sort()は、以下のように使えます。他の言語とあまり変わらないですね。\n\u0026gt;\u0026gt;\u0026gt; a = [22, 1, 44, 300, 5000] \u0026gt;\u0026gt;\u0026gt; a.sort() \u0026gt;\u0026gt;\u0026gt; print(a) [1, 22, 44, 300, 5000] それに対して、sorted()は以下のように使えます。\n\u0026gt;\u0026gt;\u0026gt; a = [22, 1, 44, 300, 5000] \u0026gt;\u0026gt;\u0026gt; b = sorted(a) \u0026gt;\u0026gt;\u0026gt; print(b) [1, 22, 44, 300, 5000] また、これらの関数ではkeyやreverseのようなパラメータを指定することで、どれを基準にソートするか、逆順にソートするかなどを指定できます。Pythonらしいシンプルさですね。\nclass Data: def __init__(self, number): self.number = number def __repr__(self): return repr((self.number)) datas = [Data(1), Data(3), Data(2), Data(4)] datas.sort(key=lambda data: data.number) # [1, 2, 3, 4] sorted(datas, key=lambda data: data.number, reverse=True) # [4, 3, 2, 1] 番外：Stable sort Goのソート方法の中で少し安定ソートの話が出ましたが、ここで比較した他の言語だとGoのように安定ソートとそうでないソートのどれを使うかという選択肢がなかったので、それぞれの言語での安定ソートはどうやって扱われているのかを表にしてみました。以下をご覧ください。\n言語 stable non-stable 備考 Go ⭕️ ⭕️ funcによって選べられる Java ⭕️ ⭕️ Streamはnon-stable JavaScript ⭕️ ⭕️ ブラウザのバージョンによる Python ⭕️ ❌ Kotlin ⭕️ ❌ SequenceでもStable Swift ❌ ⭕️ stableを保証できないと表現 多くの言語が安定ソートに対応していますが、少しづつ仕様が違う場合がありました。例えばJavaの場合、Streamによるソートは安定ソートではないため、安定ソートの結果を保証したい場合は既にソートされたCollectionを使うことをおすすめしています。Kotlinの場合はStreamに似たSequenceを使う場合でも、statefulなためか、安定ソートに対応していました。\nまた、JavaScriptの場合はブラウザのバージョンによって違いますが、最新のブラウザを使っている場合は大抵安定ソートに対応していました。ただ、JavaScirptを使った案件の場合はIEも対象ブラウザとして考慮される場合があるのですが、IEだと安定ソートに対応していないので確認が必要かなと思います。\nSwiftの場合はまだソート時のデフォルト値をstableにするかどうかを検討している中で、APIとしてもGoのようにstableとそうでないものを分離するかどうかを検討しているらしいです。またどのアルゴリズムを使うかについて議論しているらしく、しばらくは安定ソートを期待できないかと思います。\nKotlinとPythonはどの場合でも安定ソートとなるので、悩み事が一つ減るのが嬉しいですね。\n最後に 今回は色々な言語のソートについて調べてみましたが、いかがでしたか。一度ソートしたデータはその後の要素に対するアクセスが早くなるので、チューニングの観点からは必要なものかと思います。そしてこうやって色々な言語のソートのAPIを調べてみると、その言語の設計思想や発展の過程のようなものが少し見えるようで面白く、勉強にもなりますね。個人的にはあまり意識してなかった安定ソートがかなり勉強になりました。\nこれからもこうやって色々な言語の使用やAPI、同じことをする場合の各言語による違いなどを比べてみたいと思います。時間と体力が十分であればの話ではありますが…！\nでは、また！\n","date":"2021-11-10T00:00:00Z","image":"https://retheviper.github.io/images/magic.jpg","permalink":"https://retheviper.github.io/posts/languages-comparsion-sorting/","title":"色々な言語でやってみた（ソート編）"},{"content":"Spring MVCは良いフレームワークではありますが、最近流行りのマイクロサービスには向いてないという批判もあります。理由としては、アプリの起動時間が遅い、サイズが大きい、メモリの使用量が多いなどの問題が挙げられていますね。アプリの起動速度が遅い場合は、変更があった場合の素早い反映が期待できません。アプリのサイズが大きいのとメモリ使用量が多いとインスタンスが増えれば増えるほどコストが高くなるということになりますね。また、これはマイクロサービスだけの話でもないです。サーバレスアプリで、あえてJavaScirptやPythonのようなインタプリタ言語を採用しているのも同じ理由があってこそですね。\nでは、これらの問題はどうやって回避できるのでしょうか。そもそもの問題から考えると、全てが全てSpringに局限する問題でもないはずです。他のフレームワークに比べてSpringの起動時間が決して早いとは言えなかったり、メモリの使用量が多いという問題があるのは確かですが、JVMをベースにしている言語を使う限り、ある程度は仕方ない問題にも見えます。JVM言語においてはそのアプリの起動時間、サイズ、メモリ使用量のどれにもJVMが占める割合を含めて考える必要があるからですね。\nただ、これらの問題を解決できる方法が全くないわけでもありません。今回紹介するのが、その答えとして開発されているQuarkusです。\nQuarkusとは Quarkusは、RHELで有名なRed Hatが作ったJava用のウェブフレームワークです。公式ホームページの説明が何よりも正確だと思いますので、以下の文を確認してください。\nA Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards.\nJavaのアプリをKubernetes Nativeとして作成できる、というのがこのフレームワークの正体性です。Javaと説明していますが、もちろんKotlinのような他のJVM言語も使えるので、そのような言語を使っている場合でも導入を考えられます。\nここでKubernetes Nativeという言葉が気になりますが、これは単純にコンテナを作ることに特化されている、という表現ではないと思います。Spring Boot 2.3から導入されたDocker Image作成機能があり、Googleが提供しているJibのようなライブラリでいくらでもJavaアプリケーションをコンテナ化することができますし、そのほかにもコンテナを作る方法はいくらでもあります。なので、ここでKubernetes Nativeという表現をあえて使っているのは、Kubernetesに特化したものとして設計されているということを意味すると思った方が自然でしょう。\nでは、一体何を持ってKubernetes Nativeと言えるのでしょう。インフラストラクチャの観点でいうKubernetes Nativeは、Kubernetesだけで完結するアーキテクチャを指しているようです。Kubernetesで完結するということは、それに合わせて最適化しているということと同じ意味合いでしょう。アプリケーションの観点からしてもそれは大きく違わないはずです。まず、Quarkusでは以下のような特徴があると紹介されています。\nNativeコンパイルができる 起動速度が速い メモリ使用量が少ない Nativeコンパイルができるということは、JVMを使用する必要がなくなるということなので、先に挙げた三つの問題を全部解消できます。だとすると、マイクロサービスやサーバレスのみでなく、コンテナ単位でのデプロイでもかなり有利になりますね。そして、JVMを使った場合でも他のフレームワークに比べて起動速度とメモリ使用量で優位にあると言われているので、これが本当だとNativeコンパイルしない場合でも十分メリットがあると思われます。\n実際触ってみると 特徴として挙げられているもの全てが魅力的ではありますが、実際そのフレームワークを使ってみないとわからないこともあります。なので、ちょっとしたサンプルを作り触ってみました感想について少し述べたいと思います。\n起動速度 Spring Bootの場合 Springの起動速度は、DIしているクラスによって大きく異なるので、ここではSpring initializrから以下の項目のみ設定したアプリケーションを使って起動してみました。\nProject: Gradle Language: Kotlin Spring Boot: 2.5.5 Packaging: War Java: 11 Dependencies: なし そしてローカルでは、Oracle JDK 17を使って起動しています。気のせいかもしれませんが、Java 11を使っていた時より起動が早いような気がしますね。とりわけ、上記通りの設定を済ましたアプリを起動してみると、以下のような結果となりました。（ローカルマシンの情報は消しています）\n. ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.5.5) 2021-10-17 19:10:21.472 INFO 48844 --- [ main] com.example.demo.DemoApplicationKt : Starting DemoApplicationKt using Java 17 on Local. 2021-10-17 19:10:21.475 INFO 48844 --- [ main] com.example.demo.DemoApplicationKt : No active profile set, falling back to default profiles: default 2021-10-17 19:10:22.058 INFO 48844 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2021-10-17 19:10:22.067 INFO 48844 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2021-10-17 19:10:22.068 INFO 48844 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.53] 2021-10-17 19:10:22.117 INFO 48844 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2021-10-17 19:10:22.118 INFO 48844 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 607 ms 2021-10-17 19:10:22.335 INFO 48844 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path \u0026#39;\u0026#39; 2021-10-17 19:10:22.342 INFO 48844 --- [ main] com.example.demo.DemoApplicationKt : Started DemoApplicationKt in 1.129 seconds (JVM running for 1.396) JVMの起動に1.396秒、アプリの起動に1.129秒がかかっていますね。何も依存関係がないので、おそらくこれが自分のマシンでは最速の起動時間と言えるのではないかと思います。これが実際の業務用のアプリとなると、アプリの起動だけで10秒以上かかることもあリますね。一回の起動では10秒でもあまり問題になることはありませんが、ローカルでのテストではテストごとにアプリが起動するような\nQuarkus Nativeの場合 では、Quarkusの場合を見ていきたいと思います。まずNativeコンパイルができるというので、GraalVMを利用してビルドしてみました。実際のビルドはGradleのタスクとして実行できて（固有のパラメータは必要ですが）、簡単です。そしてそれを実行してみた結果が以下です。\n__ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,\u0026lt; / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2021-10-23 19:24:18,395 INFO [io.quarkus] (main) quarkus-sample 1.0.0-SNAPSHOT native (powered by Quarkus 2.3.0.Final) started in 0.018s. Listening on: http://0.0.0.0:8080 2021-10-23 19:24:18,397 INFO [io.quarkus] (main) Profile prod activated. 2021-10-23 19:24:18,397 INFO [io.quarkus] (main) Installed features: [cdi, config-yaml, kotlin, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, vertx] 0.018秒がかかっています。ビルドしたプロジェクトの構造が単純であるのもありますが、それでもこの起動速度は確かに速いですね。これなら確かにマイクロサービスだけでなく、リクエストの多いサーバレスアプリケーションでも十分使えると思います。\nQuarkus JVMの場合 Springの場合と同じく、Oracle JDK 17を利用して起動してみました。Quarkusには開発モードというものがあり、サーバを起動したまま修正ができるのですが、ここではあえてJarを作って起動しています。余談ですが、Springでは依存関係を全部含む場合はwarになりますが、Quarkusではuber-jarと言っているのが面白いです。\n__ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,\u0026lt; / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2021-10-23 19:20:59,897 INFO [io.quarkus] (main) quarkus-sample 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.3.0.Final) started in 0.761s. Listening on: http://0.0.0.0:8080 2021-10-23 19:20:59,905 INFO [io.quarkus] (main) Profile prod activated. 2021-10-23 19:20:59,906 INFO [io.quarkus] (main) Installed features: [cdi, config-yaml, kotlin, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, vertx] 今回は0.761秒がかかりました。Nativeと比べると確かに数十倍も遅くなっていますが、それでもSpringと比べ速い方ですね。\nこうやってアプリの起動が速くなると、ローカルで開発するときもユニットテストが早くなるので即座で確認ができるというメリットもあるかと思います。特にSpringで[RestTemplate]やWebTestClientなどを使ったテストケースを書くとテストごとにアプリを起動することになるので、テストケースが増えれば増えるほどかかる時間が多いのが辛いものですね。なので、起動が速いと同じようなテストをQuarkusで書いてもかなり時間が節約できそうです。\nSpringからの移行が簡単 最初あまり意識してなかった部分ですが、Quarkusの良い点の一つは、Springからの移行が簡単ということです。アプリを新規に開発するときや既存のアプリのフレームワークを変更する場合には、技術選定において色々と考慮すべきものや観点があると思いますが、その中でいくつかを取り上げると「いかに工数を減らせるか」、「エンジニアを募集しやすいか」などがあるのはないかと思います。こういう観点からすると、現在のエンジニアにとって全く新しい技術だったり、業界であまり使われてない技術だったりすると会社としてもエンジニアとしても大変でしょう。こういう問題があるので、企業にとって新しい技術の導入は難しくなっていると思います。\nなので、新しい技術でありながらも業界でよく使われているものと似ているという点は、エンジニアの学習コストを減らせるのでかなりのメリットと言えるでしょう。では、実際のコードを観ながら、SpringのコードをQuarkusに移行するとした場合はどうなるかを見ていきたいと思います。\nSpringの場合 まず、クエリパラメータにIDを渡し、Personというレコードのデータを取得するAPIがあるとしましょう。Springなら、以下のようなコードになるかと思います。\n@RestController @RequestMapping(\u0026#34;/api/v1/person\u0026#34;) class PersonController(private val service: PersonService) { @GetMapping fun getPerson(id: Int): PersonResponse { return service.getPerson(id).let { PresonResponse.from(it) } } } @Service class PersonService(private val repository: PersonRepository) { fun getPerson(id: Int): PersonDto { return repository.findById(id).let { PersonDto.from(it) } } } Ktorの場合 Quarkusのコードを見る前に、まず同じコードをこのブログでも紹介したことのあるKtorで書くとどうなるかをまず見ていきたいと思います。これでSpringと全く違うフレームワークを選ぶという場合の比較ができるでしょう。\nKtorもよいフレームワークではありますが、フレームワークそのものの設計思想はSpringと異なるので、既存のアプリを移行するとしたら色々と考慮すべきものが多いです。例えば、基本的にDIに対応していないのでライブラリを別途導入する必要がありますね。\n以下は、上記のSpringと同じAPIを、DIライブラリとしてKoinを導入して実装したKtorの例です。かなり違う構造になっているのがわかります。\nfun main() { // DIの設定 val personModule = module { single { PersonService(get()) } single { PersonRepository() } } // Koinをアプリにインストール install(Koin) { modules(personModule) } // ルーティングをモジュール化して設定 configureRouting() } // ルータにControllerを登録 fun Application.configureRouting() { routing { personController() } } fun Route.personController() { // Serviceのインジェクション val service: PersonService by inject() get(\u0026#34;/api/v1/person\u0026#34;) { return service.getPerson(id).let { PresonResponse.from(it) } } } // Service class PersonService(private val repository: PersonRepository) { fun getPerson(id: Int): PersonDto { return repository.findById(id).let { PersonDto.from(it) } } } Quarkusの場合 では、続いてQuarkusでAPIを作成した場合のコードを見ていきましょう。QuarkusでAPIを作成する方法はRESTEasyとReactive Routesの二つのパターンがありますが、どちらを使った場合でもアプリの作成そのものに大きい違いはないので、ここではRESTEasyを使った実装を紹介したいと思います。まずは以下のコードをご覧ください。\n@Path(\u0026#34;/api/v1/person\u0026#34;) class PersonController(private val service: PersonService) { @GET fun getPerson(id: Int): PersonResponse { return service.getPerson(id).let { PresonResponse.from(it) } } } @ApplicationScoped class PersonService(private val repository: PersonRepository) { fun getPerson(id: Int): PersonDto { return repository.findById(id).let { PersonDto.from(it) } } } Springのコードど比較して、使っているアノテーションの種類が違うだけで、ほぼ同じ感覚で実装ができるのがわかります。なので、Ktorの場合のようにアーキテクチャを考え直す必要もなく、移行も簡単になるわけですね。また、RESTEasyを使う場合、ReactiveのAPIを作りやすいというメリットもあります。ReactiveだとMutinyを使うことになりますが、Uni/Multiの概念がMono/Fluxと1:1対応していると思って良いので、Spring WebFluxや他のReactive Streamを触ったことのある方ならすぐに適応できそうです。\n@Path(\u0026#34;/api/v1/person\u0026#34;) class PersonController(private val service: PersonService) { @GET fun getPerson(id: Int): Uni\u0026lt;PersonResponse\u0026gt; { return Uni.createFrom().item(service.getPerson(id).let { PresonResponse.from(it) }) } } KtorやSpring WebFluxのRouter Functionのような書き方もそれなりの良い点はあるかと思いますが、やはりSpring MVCのような書き方に慣れている人も多いだろうし、そのような書き方で特に問題になることもないので、新しいフレームワークだとしてもこのように既存のものと同じような感覚でコードを書けるというのもそのフレームワークを選択しやすくする一つのセールズポイントになるのではないかと思います。例えばNestJSのように、JavaScript用のフレームワークでもSpring MVCに似たようなコードを書けるのですが、おそらくこれもまたSpringを触った経験のあるエンジニアにアピールするためでしょう。\nこういう面からすると、すでにSpringの経験があるエンジニアならすぐにQuarkusに移行できて、既存のSpringプロジェクトも簡単に移行できそうなので良さそうです。\n懸念 Quarkusを実際触ってみて、最も良いと思われたのは上記の通りですが、Nativeでアプリをビルドしながら、いくつかの懸念もあると感じました。例えば以下のようなものがあります。\nNativeのビルドは遅い Nativeで起動速度が早くなるのは確かに良いところですが、問題はビルド自体は遅いということです。当然ながら、Nativeとしてビルドするということは、最初から全てのコードをマシンコードとしてコンパイルするということを意味します。JVM用のバイトコードはどの環境でも同じですが、マシンコードはそうではないので、そのプラットフォームに合わせたコードを生成するのに時間がかかるのは当然のことですね。例えば、ローカルでテストに使ったプロジェクトをNativeイメージとしてビルドした場合は以下のような時間がかかりました。\n./gradlew build -Dquarkus.package.type=native \u0026gt; Task :quarkusBuild building quarkus jar [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] classlist: 2,311.58 ms, 1.19 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (cap): 3,597.91 ms, 1.19 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] setup: 5,450.32 ms, 1.19 GB 19:22:21,827 INFO [org.jbo.threads] JBoss Threads version 3.4.2.Final [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (clinit): 779.71 ms, 4.33 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (typeflow): 14,308.32 ms, 4.33 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (objects): 16,140.38 ms, 4.33 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (features): 1,145.40 ms, 4.33 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] analysis: 33,857.15 ms, 4.33 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] universe: 1,718.32 ms, 5.14 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (parse): 2,635.36 ms, 5.14 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (inline): 7,363.76 ms, 5.99 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] (compile): 26,314.40 ms, 6.15 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] compile: 40,954.87 ms, 6.15 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] image: 10,493.47 ms, 6.15 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] write: 2,111.59 ms, 6.15 GB [quarkus-sample-1.0.0-SNAPSHOT-runner:49336] [total]: 97,207.01 ms, 6.15 GB BUILD SUCCESSFUL in 1m 43s CIでビルドを行っていたり、頻繁にアプリの修正とデプロイが必要な場合にこれでかなりボトルネックになる可能性もあるかと思います。マシンパワーが十分か、デプロイまでの時間があまり気にならない場合は問題にならないと思いますが、起動速度が大事であるなら、その分ビルドに時間がかかると結局は同等のトレードオフになるだけですね。こういう場合はJarとしてビルドする時間や、他のフレームワークを使ってビルド〜起動までにかかる時間を測定してから判断した方が良いかなと思います。\nピークパフォーマンス 一般的にCやC++のような言語と比べ、Java(JVM言語)は性能で劣るという話は常識のように受け入れられています。しかし、全ての状況においてそういうわけでもありません。適切なアルゴリズム、アプリケーションのデザインなど言語そのものとは無関係と言えることが理由な場合もありますが、言語の特性を考えてもそういうケースがあるということです。なぜなら、CやC++のようなネイティブコードを生成する言語と、JVM言語のコンパイラの特性が違うからです。\n仮想マシンを挟み、バイトコードをマシンコードにもう一度変換する必要があるJVM言語と比べ、最初からマシンコードを生成する言語の方が性能が優秀であることは当然です。実際それは数値としても表れていて、Javaが登場した当時には性能問題で色々と批判を受けていたらしいですね。今はJavaが比較的性能が大事であるサーバサイドアプリケーションを作成する場合によく採用されていますが、これも「ハードウェアの発展がある」からと言われるケースも多いです。\nただ、全ての場合においてJVMを挟むアプリケーションがNativeより遅いわけでもありません。なぜなら、コンパイルには「最初から全てコンパイルしておく」AOTだけでなく、「必要に応じてコンパイルする」JITの方式もあるからです。\nJVMではJITによりバイトコードの分析と最適化を行い、マシンコードを生成することでより良い性能のコーどを作り出すと言われています。ここで最適化とは、利用頻度の高いメソッドや定数などを含めてオーバヘッドを減らすことを含みます。このような最適化が行われたコードをマシンコードに変換するとしたら、当然性能がより良くなることを期待できますね。ただ、ITは全ての場合に動作してくれるわけでもありません。コンパイルには時間とマシンパワーが必要なので、一度しか利用されないコードをいちいちマシンコードに変換するのは無駄なことですね。なので、JITでコンパイルされるコードは、そのコードの利用頻度により決定されます。よくJavaのマイクロベンチマークで使われているJMHでテストを行うとき、事前にウォーミングアップをおこなっているのも、JITによる最適化でベンチマークの精度を上げるための工夫です。\n検証してみると 実際NativeかJVMかによってどれくらいランタイム性能が違うのか気になったので、ループで10万件のデータを作って返すだけのServiceを作成して処理時間を計測してみました。ここでControllerの戻り値にMultiを使ったせいか、APIが呼び出されるたびにレスポンスまでの時間が大きく変化していたので、測定しているのはリクエストからレスポンスまでの時間より「forループによるデータの生成にかかった時間」を計測していると理解してください。\nNativeビルドとJarの実行に使ったのはどれもGraalVM CE 21.3.0(OpenJDK 11)で、処理時間の測定はKotlinのmeasureTimeMillisで取得した値をログに吐くという方法を使っています。\nまずNativeで起動した場合の結果です。\n2021-10-23 17:12:14,061 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-6) measured time was 89 ms 2021-10-23 17:12:15,630 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-1) measured time was 52 ms 2021-10-23 17:12:17,079 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-15) measured time was 106 ms 2021-10-23 17:12:18,174 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-5) measured time was 49 ms 2021-10-23 17:12:19,523 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-11) measured time was 50 ms 2021-10-23 17:12:20,468 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-4) measured time was 50 ms 2021-10-23 17:12:21,739 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-7) measured time was 124 ms 2021-10-23 17:12:23,113 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-12) measured time was 53 ms 2021-10-23 17:12:24,073 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-13) measured time was 49 ms 2021-10-23 17:12:25,308 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-2) measured time was 53 ms また、以下がJVMで起動した場合の結果です。\n2021-10-23 17:10:32,240 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-8) measured time was 163 ms 2021-10-23 17:10:35,057 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-6) measured time was 33 ms 2021-10-23 17:10:39,418 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-11) measured time was 40 ms 2021-10-23 17:10:42,211 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-3) measured time was 25 ms 2021-10-23 17:10:44,149 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-10) measured time was 38 ms 2021-10-23 17:10:46,283 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-2) measured time was 24 ms 2021-10-23 17:10:48,262 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-20) measured time was 22 ms 2021-10-23 17:10:49,854 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-12) measured time was 26 ms 2021-10-23 17:10:51,552 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-23) measured time was 23 ms 2021-10-23 17:10:52,967 INFO [com.ret.dom.ser.MemberService] (vert.x-eventloop-thread-7) measured time was 51 ms やはりJITが関与しているせいか、JVMでは最初の実行で時間がかかっていて、その次から大幅に処理速度が早くなっているのがわかります。GraalVMのコンパイラのバージョンアップでさらにパフォーマンスが向上する可能性はあると思われますが、それはJVMの場合でも同じなので、どうしてもランタイムのピークパフォーマンスが大事な場合はJVMの利用を考慮しても良いかなと思います。\n最後に 本当はメモリ使用量などをより正確に測る必要があると思いますが、それに関してはすでに記事があったので、ここでは割愛します。結論から言いますと、確かにメモリ使用量はヒープを含めQuarkusの方が少ないですが、CPU使用量の最大値とLatencyにおいてはSpring Bootの方が優れているのを確認しました。ただ、ここはQuarkusの方が歴史が短いためであるということもありそうですね。\nとりあえず触ってみた感覚では、確かにKubernetes nativeと言えるだけのものではあると思われます。Nativeビルドしてみると、Jarと比べアプリのサイズ自体は大きくなるものの（倍ほど）、JDKがいらなくなるというのも良いですね。JDKのサイズはAdoptOpenJDKを基準におよそ300MBくらいです。インスタンスが一つの場合だとしたらあまり問題になりそうではないですが、もしインスタンスが増えるとしたらJDKだけでも必要なストレージのサイズが乗数で増えることになるので、Nativeにしたくもなるかなと思います。\nそのほかにも、さまざまなライブラリやフレームワークの組み合わせができるし、Spring Securityなどをそのまま用いることができるのも魅力的です。Springの経験のあるエンジニアなら誰でもすぐに慣れそうなので、会社の立場からも他のフレームワークを使う場合に比べ比較的エンジニアの募集に負担がなくなるのでは、と思ったりもします。\nSpring WebFluxやKtorもよかったのですが、また新しい強者が現れてどれを使うか悩ましい時代になりましたね。本当はRocketも触ってみたいんですが、果たして今年内にできるかどうか…\nでは、また！\n","date":"2021-10-24T00:00:00Z","image":"https://retheviper.github.io/images/quarkus.jpg","permalink":"https://retheviper.github.io/posts/quarkus-first-impression/","title":"Quarkusを触ってみた"},{"content":"今月は新しいLTSバージョンであるJava 17のリリースがありました。まだJava 1.8を使っている案件も多いかなと思いますが、Java 1.8は2022年まで、Java 11は2023年までのサポートとなるので、いずれにせよJava 17に移行する必要はあるかなと思います。特にJava 9からモジュールが導入されたため、8からの移行はかなり大変だったらしいですが、11から移行する場合はそれほどでもないと言われているので、今からでも17では何が変わっているか、目を通しておくのもそう悪くはないでしょう。\n現時点ではEclipse Temurin(旧AdoptOpenJDK)、Zuluなどの有名JDKはほとんどが17のリリースを完了しているか、対応の最中にありますね。また、Oracle JDK 17は無料になったので、こちらを選ぶもの悪くない選択肢の一つかもしれません。\nまた、こういう無料化やJDKの多様化のみでなく、GoogleとOracleの訴訟の件もGoogleの勝利で終わったので、AndroidでもJava 17を使える可能性ができた以上、これからJava 17を使える場面は増えてくるかもしれません。実際、まだ遠い話ではあります、Springを使う場合、2022年のSpring 6はJava 17がベースラインとなるらしいですね。なので、Java 11は採択されてなかった現場でも、サポート期間などを考慮して17に転換する可能性はあると思います。\nというわけで、今回はそんなJava 17では何が変わったかを述べていきますが、大きく分けて新しい予約語の追加、新しい書き方など言語スペックとして変わったものと、新しく追加されたAPIという二つの観点でその変化を辿っていきたいと思います。案件によってはJava 1.8から17に移行するケースもあるかと思いますが、9〜11までの間にあった変更事項や新しいAPIなどはこのブログでも扱っていて、他でも参考にできるサイトが多いと思いますので、今回は8~11までの変化については割愛し、11〜17の間の変化だけを扱うことにさせてください。\n言語スペック New macOS Rendering Pipeline (17) macOSでは長い間、SwingなどJavaの2DレンダリングにOpenGLを使っていましたが、新しいMetal frameworkを導入しながら、10.14からOpenGLはdeprecatedとなりました。\n従ってJava側でも、Metalを利用する新しいグラフィック・レンダリング・パイプラインを実装するというProject Lanaiが進められていましたが、17からNew macOS Rendering Pipelineという名で導入されました。JavaであまりGUIを使うことないのでは？と思いがちかと思いますが、intellijのようなJavaベースのIDEでも画面描画で性能向上があるという噂です。ただ、intellijでは基本的にJetbrains Runtimeを使っていて、現時点ではそれがJava 17に対応していないので少し待つ必要はあります。\nmacOS/AArch64 Port (17) 17からはM1など、Apple Siliconを搭載した新しいMacに対応しました。Zuluなどの他のJDKでは独自に対応してるケースもありましたが、OpenJDK(OracleJDK)で対応したことで、これをベースとするEclipse TemurinやMicrosoft Build of OpenJDKのような他のJDKでも自然にARMベースMacでネイティブとして使えるということになると思います。\nRecord (17) 14からPreviewとして導入されたRecordが、17ではstableになり正式に導入されました。指定したフィールドをprivate finalにして、コンストラクタ、getter、toString、hashcode、equalsなどを自動生成してくれるものです。最初はLombokの@Dataのようなものかと思いきや、実際は@Valueに近いものになっていますね。値はコンストラクタでしか渡せなくて、後から変更はできなくなります。こういうところは、フィールドをvalとして指定したKotlinのdata classに近い感覚でもあります。なので、実際の使用例を見ると、以下のようになります。\n// Recordの定義 record MyRecord(String name, int number) {} // インスタンスの作成 MyRecord myRecord = new MyRecord(\u0026#34;my record\u0026#34;, 1); // フィールドの取得 String myRecordsName = myRecord.name(); int myRecordsNumber = myRecord.number(); KotlinではNamed Argumentsに対応しているのですが、Javaではまだそのような機能がないので、Recordだとフィールドが多くなるとどれがどれだかわからなくなりそうな気はします。これに対してKotlin側でRecordを使う場合、何らかのラッパークラスを作って対応するなどの方法は考えられますね。もしくは普通にsetterをもつDTOを定義するか、builderパターンを利用する方が良いでしょう。\nまた、Recordではgetter名もフィールド名そのままになるという特徴もありますが、自動生成されるコンストラクタをカスタマイズするときも少し書き方が違うという特徴があります。\nrecord MyRecord(String name, int number) { // コンストラクタにバリデーションをつける例 public MyRecord { if (name.isBlank()) { throw new IllegalArgumentException(); } } } 他に、Recordとして定義しても実際はClassが作られることになるので、以下のようなこともできます。\nコンストラクタを追加する getterをオーバライドする インナークラスとしてRecordを定義する インターフェイスを実装する また、ReflectionでもクラスがRecordであるかどうかを判定するisRecordも追加されています。\nText Blocks (15) Javaでは長い間、HTMLやJSON、SQLなどをリテラルとして使うためにはエスケープや文字列の結合などを使う必要がありました。これはあまり可読性という面でよくなく、コードの修正も難しくなる問題がありましたね。例えば、HTMLを表現するとしたら以下のようなことをしていたかと思います。\nString html = \u0026#34;\u0026lt;html\u0026gt;\\n\u0026#34; + \u0026#34; \u0026lt;body\u0026gt;\\n\u0026#34; + \u0026#34; \u0026lt;h1\u0026gt;This is Java\u0026#39;s new Text block!\u0026lt;/h1\u0026gt;\\n\u0026#34; + \u0026#34; \u0026lt;/body\u0026gt;\\n\u0026#34; + \u0026#34;\u0026lt;/html\u0026gt;\\n\u0026#34;; String query = \u0026#34;SELECT \\\u0026#34;EMP_ID\\\u0026#34;, \\\u0026#34;LAST_NAME\\\u0026#34; FROM \\\u0026#34;EMPLOYEE_TB\\\u0026#34;\\n\u0026#34; + \u0026#34;WHERE \\\u0026#34;CITY\\\u0026#34; = \u0026#39;INDIANAPOLIS\u0026#39;\\n\u0026#34; + \u0026#34;ORDER BY \\\u0026#34;EMP_ID\\\u0026#34;, \\\u0026#34;LAST_NAME\\\u0026#34;;\\n\u0026#34;; 幸い、15からText Blocksが導入され、他の言語のように簡単かつ可読性の高い文字列を定義することができるようになりました。これを使うとエスケープを意識しなくて良いので、複数行でなくても色々な分野で有効活用できそうですね。Text Blocksを使って上記のコードを変えると、以下のようになります。\nString html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;This is Java\u0026#39;s new Text block!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34;; String query = \u0026#34;\u0026#34;\u0026#34; SELECT \u0026#34;EMP_ID\u0026#34;, \u0026#34;LAST_NAME\u0026#34; FROM \u0026#34;EMPLOYEE_TB\u0026#34; WHERE \u0026#34;CITY\u0026#34; = \u0026#39;INDIANAPOLIS\u0026#39; ORDER BY \u0026#34;EMP_ID\u0026#34;, \u0026#34;LAST_NAME\u0026#34;; \u0026#34;\u0026#34;\u0026#34;; Kotlinでは全く同じ書き方で同じことができるので、ここでは割愛します。\nSealed Class (17) JDK 15からPreviewで導入されたsealed classesが、Stableとなりました。classやinterfaceをsealedにすれば、それを拡張・実装できるクラスやインターフェイスを限定できるようになります。こうすることで、ライブラリなどで勝手に拡張して欲しくないクラスやインターフェイスを守ることができますね。また、将来的にはsealedとして定義されてあるクラスの子クラスをswitchのcaseに指定するときは全部のケースが指定されているかどうかをコンパイラがチェックするようにするとかの話もあるようです。以下は、sealedクラスがpermitsキーワードを使って継承できるクラスを指定する例です。\npublic abstract sealed class Shape permits Circle, Rectangle, Square, WeirdShape { } KotlinでもSealed Classesは存在していますが、interfaceをsealedにするためには1.5以降を使う必要があって、拡張・実装できるクラスやインターフェイスを指定するわけではなく、コンパイルされたモジュール以外でsealedとして定義されているクラスやインターフェイスを拡張・実装できない仕様となっています。なので書き方的には、以下のようになります。より簡単ですね。\nsealed interface Error sealed class IOError(): Error class FileReadError(val f: File): IOError() class DatabaseError(val source: DataSource): IOError() object RuntimeError : Error また、Javaの場合はRecordと同じく、このクラスがsealedであるかどうかを判定するisSealedが追加されています。\nSwitch Expressions (14) Java 12からPreviewでSwitch Expressionsが導入され、14からはStableになっています。従来のswitchを改善したもので、以下のようなことができるようになりました。\ncaseをまとめて指定できる caseの処理をラムダのような書き方で記述できる caseの処理を戻り値にして、switchを式として使える 例えば、dayというenumの値を見て、int値を返すメソッドを実装するとしましょう。従来の方法では以下のようになるはずです。\nint numLetters; switch (day) { case MONDAY: case FRIDAY: case SUNDAY: numLetters = 6; break; case TUESDAY: numLetters = 7; break; case THURSDAY: case SATURDAY: numLetters = 8; break; case WEDNESDAY: numLetters = 9; break; default: throw new IllegalStateException(\u0026#34;Wat: \u0026#34; + day); } 上記の処理は新しいswitchでは以下のように書くことができます。\nint numLetters = switch (day) { case MONDAY, FRIDAY, SUNDAY -\u0026gt; 6; case TUESDAY -\u0026gt; 7; case THURSDAY, SATURDAY -\u0026gt; 8; case WEDNESDAY -\u0026gt; 9; }; Kotlinだと以下のようになるはずですね。\nval numLetters = when (day) { Day.MONDAY, Day.FRIDAY, Day.SUNDAY -\u0026gt; 6 Day.TUESDAY -\u0026gt; 7 Day.THURSDAY, Day.SATURDAY -\u0026gt; 8 Day.WEDNESDAY -\u0026gt; 9 } また、whenだとargumentなしでも使えて分岐を条件文によるものにすることもできるなどの特徴もあるので、使い勝手はJavaのswitchよりいいかなと思います。ただ、Javaでもバージョンアップと共に後述する機能も追加されてあるので、今後Kotlinのように色々と改良が行われる可能性はあるかと思いますね。\nPattern Matching for instanceof (16) / switch (17) Java 14からは、Pattern Matching for instanceofが導入され、16ではStableになりましt。今まではinstanceofを使ってオブジェクトのインスタンスの種類を判定した後、そのインスタンスの種類にあった処理を行うには以下のようにキャストが必要でしたね。\nstatic String formatter(Object o) { String formatted = \u0026#34;unknown\u0026#34;; if (o instanceof Integer) { formatted = String.format(\u0026#34;int %d\u0026#34;, (Integer) i); } // ... } 一度どれのインスタンスかわかった上でさらにキャストをする必要はあるのはだるいし、ミスをしたら例外の原因にもなり得る問題がありますね。なので、Pattern Matchingを利用して、キャストをなくすことができるようになりました。instanceofを使った条件文の中に、キャストする変数名を指定しておくと、if分の中でそのまま自動にキャストされた変数を使えるようになります。なので、以下のようなことができるようになります。\nstatic String formatter(Object o) { String formatted = \u0026#34;unknown\u0026#34;; if (o instanceof Integer i) { formatted = String.format(\u0026#34;int %d\u0026#34;, i); } else if (o instanceof Long l) { formatted = String.format(\u0026#34;long %d\u0026#34;, l); } else if (o instanceof Double d) { formatted = String.format(\u0026#34;double %f\u0026#34;, d); } else if (o instanceof String s) { formatted = String.format(\u0026#34;String %s\u0026#34;, s); } return formatted; } さらに、17からはPreviewとしてPattern Matching for switchが導入されています。これを使うと、instanceofなしで、switch文を使ったよりシンプルな処理を書けるようになります。これを先に紹介したSwitch Expressionsと組み合わせることで、上記の処理は以下に変えることが可能になります。かなりシンプルになったのがわかりますね。\nstatic String formatterPatternSwitch(Object o) { return switch (o) { case Integer i -\u0026gt; String.format(\u0026#34;int %d\u0026#34;, i); case Long l -\u0026gt; String.format(\u0026#34;long %d\u0026#34;, l); case Double d -\u0026gt; String.format(\u0026#34;double %f\u0026#34;, d); case String s -\u0026gt; String.format(\u0026#34;String %s\u0026#34;, s); default -\u0026gt; o.toString(); }; } Packaging Tool (16) 実行できるバイナリを生成するPackaging Toolが導入されています。これを使うと、Java runtimeとライブラリ、それぞれのOSにあった実行ファイルが一つのパッケージになる機能です。Java runtimeが含まれるということはOSのJavaのバージョンに関係なく実行できるものになるという意味なので、Javaのバージョンを固定したり、複数のアプリでそれぞれ違うバージョンのJavaを使って起動したい場合は役立つ機能かもしれません。\nAPI Java 17からは、APIドキュメントから、新しく追加されたAPIの一覧だけを見られるタブができたということです。今回は11以降に追加されたもののみですが、今後新しいLTSバージョンがリリースすると、17以降のものをこちらから確認できそうですね。新しいAPIの一覧はこちらから確認できます。\nここで全てのAPIの詳細まで探るのは難しいと思いますので、個人的に興味深いと思ったのを一部紹介したいと思います。\n@Serial (14) java.ioパッケージに、Serialというアノテーションが追加されました。これはSerializableを実装したクラスで、そのシリアライズのメカニズムを@Overrideするような機能のようです。例えば以下のようなことができます。\nclass SerializableClass implements Serializable { @Serial private static final ObjectStreamField[] serialPersistentFields; @Serial private static final long serialVersionUID; @Serial private void writeObject(ObjectOutputStream stream) throws IOException {} @Serial private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {} @Serial private void readObjectNoData() throws ObjectStreamException {} @Serial Object writeReplace() throws ObjectStreamException {} @Serial Object readResolve() throws ObjectStreamException {} } このアノテーションをつけることで、コンパイルタイムでエラーをキャッチできるのも特徴的です。例えば、このアノテーションを以下のようなクラスのメンバに使う場合はコンパイルエラーとなります。\nSerializableを実装してないクラス Enumのように、Serializeの効果がないクラス Externalizableを継承しているクラス このようなアノテーションが追加されたことによって、JacksonやGsonなどのライブラリの実装にも何か影響があるかもしれません。\nString 同じ文字列だとしても、Javaではjava.lang.String、Kotlinではkotlin.text.Stringを使うことになるので、Kotlinを使う場合はあまりJavaのAPIを使うことはないかと思います（また、JavaでのString関連のAPIは、Kotlinだとdeprecatedになるケースが多いです）。なので、ここでは新しいAPIと、Kotlinで同じような処理をするために使える方法を中心に紹介します。\nformatted (15) JavaではString.format()をで文字列をフォーマットとして使うことができました。多くの場合、文字列は+を使うよりフォーマットを使った方が性能が良いと言われていて、よく使っていたものです。\nString name = \u0026#34;formatted string\u0026#34;; // 15以前 String formattedString = String.format(\u0026#34;this is %s\u0026#34;, name); // 15以降 String newFormattedString = \u0026#34;this is %s\u0026#34;.formatted(name); KoltinだとString.formatとString Templatesが使えます。\nval name = \u0026#34;formatted string\u0026#34; // Format val formattedString = \u0026#34;this is %s\u0026#34;.format(name) // String Template val templateString = \u0026#34;this is $name\u0026#34; indent (12) indentでは、対象の文字列に引数で指定した分のwhite spaceを入れます。引数がint型なので、負数を渡すことでwhite spaceを減らすこともできます。\nString nonIndent = \u0026#34;A\u0026#34;; // インデントを追加 String indented10 = nonIndent.indent(10); // \u0026#34; A\u0026#34; // インデントを削除 String indented5 = indented10.indent(-5); // \u0026#34; A\u0026#34; Kotlinの場合は、インデントを追加するためのprependIndentや代替するするためのreplaceIndentなどがあり、渡すパラメータも文字列となるのでJavaのものとは少し使い方が違います。\nval nonIndent = \u0026#34;A\u0026#34; // インデントを追加 val prepended = nonIndent.prependIndent(\u0026#34; \u0026#34;) // \u0026#34; A\u0026#34; // インデントを代替（なかった場合は追加） val replaced = prepended.replaceIndent(\u0026#34;|||||\u0026#34;) // \u0026#34;|||||A\u0026#34; stripIndent (15) Text Blockで複数行の文字列を扱う場合、ソースコード上の可読性の都合で任意のインデントを入れたら実際のデータとしては扱いづらい場合もあるはずです。ここでインデントを削除するためののものがstringIndentです。\nKotlinではtrimIndentが同じ役割をしています。\ntransform (12) 文字列に対してFunctionを実行するという単純なAPIです。replaceでは不可能な、条件による処理などが必要なときに使えそうです。実装を見ると極めて単純です。\npublic \u0026lt;R\u0026gt; R transform(Function\u0026lt;? super String, ? extends R\u0026gt; f) { return f.apply(this); } Kotlinでは文字列でもmap・filter・reduceのような高階関数が使えるのでこれらを使うこともできますね。もしくは以下のような拡張関数を定義することで同じことができるかと思います。\nfun \u0026lt;R\u0026gt; String.transform(f: (String) -\u0026gt; R): R = f(this) translateEscapes (15) エスケープになっている一部の文字をリテラルに変えてくれる機能です。こちらはコードを見た方が理解が早いかなと思います。\nString string = \u0026#34;this\\\\nis\\\\nmutli\\\\nline\u0026#34;; String escapeTranslated = string.translateEscapes() // \u0026#34;this\\nis\\nmutli\\nline\u0026#34; 以前はMatcherと正規式を組み合わせるなど独自の処理を書くか、ライブラリに依存していたと思いますので、こういうのができると嬉しいですね。変換されるエスケープ文字は以下の通りです。\nEscape Name Translation \\b backspace U+0008 \\t horizontal tab U+0009 \\n line feed U+000A \\f form feed U+000C \\r carriage return U+000D \\s space U+0020 \\\u0026quot; double quote U+0022 \\' single quote U+0027 \\\\ backslash U+005C \\0 - \\377 octal escape code point equivalents \\\u0026lt;line-terminator\u0026gt; continuation discard Kotlinでは似たようなAPIがないので、必要なら独自の処理を書いた方が良さそうです。（ライブラリは知らず…）\nMap.Entry.copyOf (17) Map.Entryのコピーを作成します。コピーしたエントリは元のMapとは何の関係もないデータとなります。以下のようなサンプルコードを公式ドキュメントから提示していますね。\nvar entries = map.entrySet().stream().map(Map.Entry::copyOf).toList(); ちなみにMapそのもののコピーは、10から追加されたcopyOfでできます。\nvar copiedMap = Map.copyOf(map); Kotlinだと、Entryのコピーは以下のようにできます。型はList\u0026lt;MutableMap.MutableEntry\u0026lt;K, V\u0026gt;\u0026gt;となります。\n// Map.Entryを使う場合 val entriesJava = map.entries.map { Map.Entry.copyOf(it) } // KotlinのMap.Entryを使う場合 val entriesKotlin = map.entries.toSet() また、KotlinでのMapのコピー方法は以下のようにできます。\nval copiedMap = map.toMap() Stream mapMulti (16) 16からStreamにmapMultiというメソッドが追加されました。基本的には「Streamの要素に1:Nの変換を適用して結果をStreamを返す」という処理なので、flatMapに似ていますが、以下のケースではflatMapを使う場合より良いと言われています。\n要素を減らす場合 要素をStreamに変換するのが難しい場合 まずはオブジェクトがネストされているCollectionに対してflatMapを使う場合を考えてみましょう。要素を減らすケースでは、flatMapでまず全ての要素を展開し、filterを使って条件に合う要素だけを取る必要があります。ここで要素を展開するには、全ての要素をStreamに変換しなければならないので、全ての要素のグループに対してStreamのインスタンスを作ることになります。また、オブジェクトがネストしている場合は、その個別の要素に対してどうやってStreamに変換するか、処理の中で定義する必要があります。\n問題はStreamのインスタンスを毎回作るためオーバヘッドが発生することにもなるし、要素がさまざまな型のオブジェクトである場合はStreamに変換する処理を書くのも大変ということです。例えば以下のようなListがあるとしましょう。\nList\u0026lt;Object\u0026gt; numbers = List.of(List.of(1, 2L), 3, List.of(4, 5L, 6), List.of(7L), 8L); このListから、Integerのみを抽出して別のListにしたい場合はどうしたら良いでしょうか。まずflatMapを使うとしたら、以下のような処理を書くことになるかと思います。\nList\u0026lt;Integer\u0026gt; integers = list.stream() .flatMap( // 要素をStreamに変換する it -\u0026gt; { if (it instanceof Iterable\u0026lt;?\u0026gt; l) { return StreamSupport.stream(l.spliterator(), false); } else { return Stream.of(it); } }) .filter(it -\u0026gt; it instanceof Integer) // Integerのみを取る .map(it -\u0026gt; (Integer) it) // ObjectからIntegerへキャスト .toList(); これをmapMultiを使って処理する場合は以下のようになります。よりシンプルになりましたね。\nclass MultiMapper { static void expandIterable(Object e, Consumer\u0026lt;Integer\u0026gt; c) { if (e instanceof Iterable\u0026lt;?\u0026gt; i) { i.forEach(ie -\u0026gt; expandIterable(ie, c)); } else if (e instanceof Integer i) { c.accept(i); } } } List\u0026lt;Integer\u0026gt; integers = list.stream().mapMulti(MultiMapper::expandIterable).toList(); 他にもmapMultiToInt、mapMultiToLong、mapMultiToDoubleなどのメソッドも追加されていますので、数字を扱う場合はこちらを使った方が便利でしょう。例えば、上記のmapMultiをmapMultiToIntで書く場合は以下のようになります。\nclass MultiMapper { static void expandIterable(Object e, IntConsumer c) { if (e instanceof Iterable\u0026lt;?\u0026gt; i) { i.forEach(ie -\u0026gt; expandIterable(ie, c)); } else if (e instanceof Integer i) { c.accept(i); } } } List\u0026lt;Integer\u0026gt; integers = list.stream().mapMultiToInt(MultiMapper::expandIterable).boxed().toList(); mapMultiToIntの戻り値はIntStreamなので、Stream\u0026lt;Integer\u0026gt;に変換するためにboxedを呼び出し、ConsumerがIntConsumerに変わり、mapMultiの型指定が変わるなど少しの違いがあります。\nKotlinではそもそもflatMapをStreamとして扱わないので、そもそもの処理を違う観点から考える必要があります。幸い、KotlinのCollectionには色々なAPIがあるので、そこまで難しくはないです。例えば、オブジェクトのインスタンスを基準に要素を集約したい場合は以下のようなコードを書くことができます。\nval list = listOf(listOf(\u0026#34;A\u0026#34;, \u0026#39;B\u0026#39;), \u0026#34;C\u0026#34;, setOf(\u0026#34;D\u0026#34;, \u0026#39;E\u0026#39;, \u0026#34;F\u0026#34;), listOf(\u0026#39;G\u0026#39;), \u0026#39;H\u0026#39;) val result: List\u0026lt;String\u0026gt; = list.flatMap { if (it is Iterable\u0026lt;*\u0026gt;) { it.filterIsInstance\u0026lt;String\u0026gt;() } else { listOf(it).filterIsInstance\u0026lt;String\u0026gt;() } } // [A, C, D, F] ただ、JavaではList.of(1, 2L)でListを作成した場合、1はint、2LはLongとして扱われますが、KotlinではlistOf(1, 2L)がList\u0026lt;Long\u0026gt;となってしまうので、そもそもの型に注意する必要があります。\nval list = listOf(listOf(1, 2L), 3, setOf(4, 5L, 6), listOf(7L), 8L) val result = list.flatMap { if (it is Iterable\u0026lt;*\u0026gt;) { it.filterIsInstance\u0026lt;Int\u0026gt;() } else { listOf(it).filterIsInstance\u0026lt;Int\u0026gt;() } } // [3] toList(16) Streamの終端処理として使用頻度の高い「Listに集計する」をシンタックス・シュガーとして作ったような感覚のメソッドです。ここはKotlinの機能をJavaが受け入れたような気もしますね。処理の結果として生成されるListはUnmodifiableです。\nList\u0026lt;String\u0026gt; list = List.of(\u0026#34;a\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;D\u0026#34;); // 旧 List\u0026lt;String\u0026gt; upper = list.stream().map(String::toUpperCase).collect(Collectors.toUnmodifiableList()); // 新 List\u0026lt;String\u0026gt; lower = list.stream().map(String::toLowerCase).toList(); Kotlinでは基本的にCollectionで高階関数を呼び出した結果がUnmodifiableなListになるのですが、streamに変換して使うこともできるので、場合によっては便利なのかもしれませんね。\nCollectors.teeing (12) Collectorsに、二つのCollectorを結合するteeingというメソッドが追加されました。ちなみにTeeは二つの水道管を接続して一つにしてくれる「T字継手」の意味を持つらしいです。引数に二つのCollectorと、それを結合する処理のBiFunctionを指定する形となっています。\n例えば以下のようなStreamがあるとしましょう。\nrecord Member(String name, boolean enabled) {} /** * [ * Member[name=Member1, enabled=false], * Member[name=Member2, enabled=true], * Member[name=Member3, enabled=false], * Member[name=Member4, enabled=true], * ] */ Stream\u0026lt;Member\u0026gt; members = IntStream.rangeClosed(1, 4).mapToObj(it -\u0026gt; new Member(\u0026#34;Member\u0026#34; + it, it % 2 == 0)); これをteeingを使って、Memberのenabledを基準に二つのListに分けるとしたら以下のようになります。\n/** * [ * [ * Member[name=Member2, enabled=true], * Member[name=Member4, enabled=true] * ], * * [ * Member[name=Member1, enabled=false], * Member[name=Member3, enabled=false] * ] * ] */ List\u0026lt;List\u0026lt;Member\u0026gt;\u0026gt; result = members.collect( Collectors.teeing( Collectors.filtering( Member::enabled, Collectors.toList() ), Collectors.filtering( Predicate.not(Member::enabled), Collectors.toList() ), (list1, list2) -\u0026gt; List.of(list1, list2) ) ); Kotlinではそもそもcollectする必要がないので、Collectionの高階関数を使った処理をした方が良いでしょう。（Javaでもそうした方がわかりやすいような…）\n最後に いかがだったでしょうか。さすがに全ての変更事項を整理するのは難しかったので、目立っている変化だけをいくつか取り上げてみましたが、それでもかなりの量ですね。ただ確かなのは、Java 17が11よりもさらにモダンな言語になったバージョンであるので、Javaを使っている案件なら十分導入する価値がありそうです。また、Java 15からは11に比べてG1GCの改良による性能向上もあったようですので、性能という面でも良いですね。\nKotlinを使っている場合でも、APIだけを見るとあまりメリットはないかもしれませんが、JVMを使っている限り性能向上などの恩恵を受けることはできると思われるので、導入を考慮しても良いかなと思います。また次のLTSでは色々と面白いAPIが続々と登場するかもしれませんしね。\nでは、また！\n","date":"2021-09-25T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-enter-to-17/","title":"Java 17は何が変わったか"},{"content":"JavaからKotlinに移行したものの立場から言うと、Kotlinはスタンダードライブラリだけでも色々な関数を提供しているので、Javaに比べてかなり生産性が上がるといえますが、逆にいまいち関数の有効な使い方がわからなかったり、どうやって処理を書いたら「Kotlinらしい」かわからない場合もあるかと思います。なのでもう3回目のポストになりますが、今回もKotlinで色々とコードを書いてみて、そのうち良さそうなものをいくつか共有します。\nListの要素をスワップ Listの要素の順番を変える方法はソートなどを含め色々とありますが、二つの要素をスワップ（インデックスを交換）したい場合もあるかと思います。こういう時に活用できる拡張関数を考えてみました。\nインデックスがわかる場合 スワップしたい要素のインデックスがわかる場合は、そのインデックスを交換すればいいだけですね。ここでインデックスの交換は、二つの変数の値をスワップすることと変わらないです。変数の値を交換するのは伝統的には以下のような方法がありますね。\nvar a = 10 var b = 20 var c = a a = b b = c もう少しKotlinらしい方法では、alsoを用いたものがあります。その方法だと、必要な処理は以下のようにもっとシンプルになります。\nvar a = 10 var b = 20 a = b.also { b = a } これと同じく、Listの要素をスワップする処理を拡張関数で書くとしたらと以下のようになります。\nfun \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt;.swapByIndex(indexFrom: Int, indexTo: Int): List\u0026lt;T\u0026gt; = toMutableList().apply { this[indexFrom] = this[indexTo].also { this[indexTo] = this[indexFrom] } }.toList() インデックスがわからない場合 スワップしたい要素のインデックスがわからない場合もありますが、これも結局インデックスを持って値をスワップすることになるので、まずインデックスを抽出する処理だけを足せば良いかなと思います。\nインデックスを取得する方法は、要素を渡して取得するindexOfとPredicateを渡して取得するindexOfFirstがあるので、これらを活用することにします。あとはこれらの方法で取得したインデックスを、先に実装しておいた拡張関数に渡すだけで良いです。例えば以下のような実装ができます。\n// indexOf(element)を使うケース fun \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt;.swapByElement(from: T, to: T): List\u0026lt;T\u0026gt; = swapByIndex(indexOf(from), indexOf(to)) // indexOfFirst(predicate)を使うケース fun \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt;.swapByCondition(from: (T) -\u0026gt; Boolean, to: (T) -\u0026gt; Boolean): List\u0026lt;T\u0026gt; = swapByIndex(indexOfFirst { from(it) }, indexOfFirst { to(it) }) 時間を数字に java.timeパッケージのLocalDateやLocalDateTimeのようなオブジェクトは、コード上で時間を扱うには便利ですが、ファイルに書き込むなどでフォーマットを変更する必要がある時もあります。つまり、yyyy-MM-ddではなくyyyyMMddhhmmssのような形にしたい場合があるということです。こういうときは、簡単にInt型に変更できる拡張関数を書いておくと便利でしょう。例えば以下のようなものを考えられます。\nfun LocalDate.toInt(): Int = \u0026#34;$year$monthValue$dayOfMonth\u0026#34;.toInt() val date = LocalDate.of(2021, 12, 31) // 2021-12-31 println(date.toInt()) // 20211231 ただ、こうする場合、以下のように月や日付が一桁のものになってしまうケースもあります。\nval date = LocalDate.of(2021, 9, 1) // 2021-09-01 println(date.toInt()) // 202191 この問題を解決するには、まず月や日付を二桁の文字列に変える必要がありますね。例えば以下のようなことができます。\nfun LocalDate.toInt(): Int = \u0026#34;$year${monthValue.toString().padStart(2, \u0026#39;0\u0026#39;)}${dayOfMonth.toString().padStart(2, \u0026#39;0\u0026#39;)}\u0026#34;.toInt() val date = LocalDate.of(2021, 9, 1) // 2021-09-01 println(date.toInt()) // 20210901 しかし、これでも完璧とはいえません。LocalDateのみでなくLocalDateやLocalDateTime, YearMonthなど、java.timeパッケージに属する他のオブジェクトも使いたい場合には、全てのオブジェクトに対して同じような拡張関数を書く必要があるからです。\n幸い、LocalDate、LocalDateTime、YearMonthは共通的にTemporalというインタフェースを継承しているので、Temporalに拡張関数を追加することで問題は解決できます。\nそしてこれらの実装クラスで扱っている時間の範囲はオブジェクトごとに違うので、実装も変える必要がありますね。これらのオブジェクトはどれも時間を数字として表現しているので、まずtoStringで文字列に変換した後、数字だけを抽出することです。StringはCharSequenceを継承しているので、filterで数字だけを抽出すると良いでしょう。そうすると、以下のような方法が使えます。\nfun Temporal.toDigit(): Long = toString().filter { it.isDigit() }.toLong() val yearMonth = YearMonth.of(2021, 8) // 2021-08 println(yearMonth.toDigit()) // 202108 val dateTime = LocalDateTime.of(2021, 10, 2, 10, 10, 10) // 2021-10-02T10:10:10 println(dateTime.toDigit()) // 20211002101010 Stringのフォーマットで数字に変換する場合はtoIntやtoLongでループが一回発生するだけですが、CharSequenceとして扱う場合はループが2回発生するという違いがあるので性能的には前者が良いはずですが、時間を扱うくらいではそこまでループは長くないので気にするほどではないかと思います。\n要素の一部を合算 Listの値を一つに集約したい（合算値を出したい）場合があります。sumを使っても良いですが、これはそもそも要素が数字ではないと難しいですね。例えば要素が以下のようなクラスとなっているケースはどうしたら良いでしょうか。\ndata class Data( val name: String, val amount: Int, val price: Int ) 合算したい値が一つの場合 合算したい値が一つだけの場合は、sumOfで合算したい値だけを指定すれば良いです。以下は、Dataクラスのamountだけを合算したい場合に使える方法です。\nval list = listOf(Data(\u0026#34;data1\u0026#34;, 10, 100), Data(\u0026#34;data2\u0026#34;, 20, 200)) val totalAmount = list.sumOf { it.amount } 合算したい値が複数の場合 ここでamountのみでなく、priceも合算したい場合はどうすれば良いでしょう。同じくsumOfをpriceにも使うことで実装はできますが、同じListに対してループが2回も発生するのあまり効率的ではありません。こういうときは、素直にそれぞれの合算値を変数として宣言しておいてforEachループの中で値を足していく方が効率が良いでしょう。例えば以下のようにです。\nvar totalAmount = 0 var totalPrice = 0 list.forEach { totalAmount += it.amount totalPrice += it.price } もう一つの方法は、foldを使う方法です。foldはreduceと似たようなもので、初期値(initial)を指定できるという違いがありますが、reduceToのようにこの初期値の型はListの要素とは違うものに指定できます。そして関数を実行した結果はinitialと同じ型になるので、これを応用するとDataのリストを二つの値(Pair)にreduceすることもできます。例えば上記の処理はfoldを使うと以下のようにワンライナで実装ができます。\nval (totalAmount, totalPrice) = list.fold(0 to 0) { acc, value -\u0026gt; (acc.first + value.amount) to (acc.second + value.price) } foldを使う場合、合算したい値が三つある場合はTripleを使うこともできますし、さらに値が多い場合は専用のクラスを作ることで対応もできるかと思います。ただ、こうする場合、合算した値をvalとして宣言できるというメリットはありますが、ループごとにインスタンスが作成されるので合算したい項目が増えれば増えるほど性能的にはあまり良くない可能性が高いので場合によって適切なものを選ぶ必要がありそうですね。\n最後に いかがだったでしょうか。私はずっとJavaでコードを書いていたので、完全にKotlinに転向した今でもついJavaらしいコードを書いてしまうのではないか、と思う時があります。元を辿ると、「Javaらしいコード」や「Kotlinらしいコード」がそもそも何であるかを考えなければならないとは思いますが、それでも、確かに言語が違うとその言語に合わせて自分のコーディングスタイルも変化する必要はあるのではないかと思います。そうすることで、より良いコードが書けるようになりそうな気がしていますので。\nというわけで、これからもKotlinならではの、Kotlinに特化したコードを書くための工夫はこれからも続きます。特に今月はJava 17もリリースされたので、新しいAPIの一覧を眺めてKotlinではどう活用できるか考えてみたいですね。\nでは、また！\n","date":"2021-09-18T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-code-in-my-style-3/","title":"Kotlinで書いてみた〜その三〜"},{"content":"前回、WebFluxではFunctional Endpointを使うべきかというポストを書いたことがありますが、今回はController/ServiceとRouter/Handlerのパターン間の比較ではなく、Functional Endpointを使う場合に、どんな形で実装をしていくべきかについて少し考えたことを述べようと思います。\n実際の業務でWebFluxを使っているわけではないので、さまざまなパターンがあるかなとは思いますが、このFunctional Endpointを使う場合に考慮すべきものが、Router Function(以下Router)とHandler Function(以下Handler)をどう分けるかについての問題かと思います。RouterとHandlerは概念的には別のものではありますが、実装としては一つのクラスにまとめでもアプリは問題なく動くので、フレームワークの仕様や思想というよりかは、アプリのアーキテクチャに関する内容に近いますね。\nなので、今回はRouterとHandlerを分けた場合と分けない場合について、いくつかの観点から考えてみたいと思います。\nRouterとHandlerは分離するべきか Spring MVCの場合、ControllerとServiceを明確に分けるのが常識のようになっています。アーキテクチャとしてもそうですが、フレームワークの思想（デザインの観点）としてもそうですね。\nこういう前例があるので、同じくSpring Frameworkに属するWebFluxの場合でも、Functional Endpointという新しい概念を導入するとしても、RouterとHandlerを分ける必要があると思いがちかなと思います。一見、Controller ≒ Router, Service ≒ Handlerという対応関係が成立するようにも見えて、ネットで検索できるサンプルコードも多くがそのような構造で書かれています。\nしかし、実際のアプリをFunctional Endpointを持って書くとしたら、いくつか考えなければならないことがあると思います。例えば、そもそもRouterとHandlerはそれぞれControllerとServiceに一対一の対応関係であるという前提は確かであるか？もしそうでなければ、あえてMVCのパターンに合わせる必要があるのか？実装においてはどう影響するのか？などがあるかと思います。なので、今回はこれらの観点からFunctional Endpointについて述べていきます。\n対応関係について Springの公式ドキュメントでは、WebFluxのFunctional Endpointの紹介において以下のようなサンプルコードを提示しています。\nval repository: PersonRepository = ... val handler = PersonHandler(repository) val route = coRouter { accept(APPLICATION_JSON).nest { GET(\u0026#34;/person/{id}\u0026#34;, handler::getPerson) GET(\u0026#34;/person\u0026#34;, handler::listPeople) } POST(\u0026#34;/person\u0026#34;, handler::createPerson) } class PersonHandler(private val repository: PersonRepository) { // ... suspend fun listPeople(request: ServerRequest): ServerResponse { // ... } suspend fun createPerson(request: ServerRequest): ServerResponse { // ... } suspend fun getPerson(request: ServerRequest): ServerResponse { // ... } } 公式のサンプルとしてHandlerが別のクラスになっているのを見ると、やはりController ≒ Router, Service ≒ Handlerという対応関係が成立するようにも見えます。@RestControllerや@Serviceと違って、@Routerや@Handlerというアノテーションは存在しないことに注目する必要があります。これはつまり、Springというフレームワークの思想としてはRouterとHandlerを必ず分ける必要はない、ということを意味しているのではないでしょうか。\nなので、少なくともアプリケーションのアーキテクチャという観点からしてController ≒ Router, Service ≒ Handlerという対応関係が成立する、という結論を出すのは難しいのではないかと思います。\nでは、実際RouterとHandlerをあえてアノテーションを使ってDIをするとしたら、どうなるのでしょうか。サンプルとしては、以下のような形が一般的かなと思います。\n@Configuration class PersonRouter(private val handler: PersonHandler) { @Bean fun route(): RouterFunction\u0026lt;ServerResponse\u0026gt; = coRouter { accept(APPLICATION_JSON).nest { GET(\u0026#34;/person/{id}\u0026#34;, handler::getPerson) GET(\u0026#34;/person\u0026#34;, handler::listPeople) } POST(\u0026#34;/person\u0026#34;, handler::createPerson) } } @Component class PersonHandler(private val repository: PersonRepository) { // ... suspend fun listPeople(request: ServerRequest): ServerResponse { // ... } suspend fun createPerson(request: ServerRequest): ServerResponse { // ... } suspend fun getPerson(request: ServerRequest): ServerResponse { // ... } } クラスそのものを@Componentとして登録する必要があるContollerに対して、RouterFunctionはFunctional Interfaceなのでそれを実装したメソッドを@Beanとして登録する必要があります。そしてSpringで@Beanをアプリケーションに登録するのは一般的に@Congifurationが担当するので自然にRouterのアノテーションもそうなります。Handlerは普通に@Componentとして登録することになりますね。\nこうなった場合、クラスやその実装を見てRouterとHandlerを分離しているのはわかりますが、アノテーションだけだと違和感を感じられますね。実装は簡単なのでそれぞれに対応するアノテーションを作るのが難しいわけでもないようですが、なぜこのような構造になっているのでしょうか。公式のドキュメントでは、以下のような説明があります。\nThe big difference with annotated controllers is that the application is in charge of request handling from start to finish versus declaring intent through annotations and being called back.\nつまり、「アノテーションをつけたContoller」と「Functional Endpoint」の違いは、前者が「アノテーションでコールバックと意図を表す」に対して、後者は「リクエストのハンドリングを開始から終了まで担当する」ということです。プログラミングモデルとしてこのような観点の差があるので、アノテーションがないのは当たり前なのかもしれません。そして結果的に、Controller ≒ Router, Service ≒ Handlerという対応関係は、少なくともプログラミングモデルという観点では当てはならないと考えられます。\n責任の分散という側面で アノテーションの実装を見ると、@Controllerと@Serviceを分けているのがフレームワークのアーキテクチャや思想によるものであることがより明確になります。それぞれのアノテーションの実装は、以下の通りです。\n@Target(value=TYPE) @Retention(value=RUNTIME) @Documented @Component public @interface Controller @Target(value=TYPE) @Retention(value=RUNTIME) @Documented @Component public @interface Service 両方とも実装が同じであるので、極端的にいうとControllerに@Serviceをつけても機能的には同一ということになります。そして@Serviceでは、以下のようなコメントでこのアノテーションが存在する理由をあくまで「デザインパターンに基盤を置いている」ことを明示しています。\nIndicates that an annotated class is a \u0026ldquo;Service\u0026rdquo;, originally defined by Domain-Driven Design (Evans, 2003) as \u0026ldquo;an operation offered as an interface that stands alone in the model, with no encapsulated state.\u0026rdquo; May also indicate that a class is a \u0026ldquo;Business Service Facade\u0026rdquo; (in the Core J2EE patterns sense), or something similar. This annotation is a general-purpose stereotype and individual teams may narrow their semantics and use as appropriate.\nなので、アプリケーションデザインの観点からするとControllerはリクエストを受信、レスポンスを返す、エンドポイントをServiceにつなぐという義務だけを持ち、Serviceはビジネスロジックを処理する義務を持つと考えられます。同じ観点から考えると、アノテーションはないものの、RouterとHandlerもまた同じ義務を持つように書くこともできるでしょう。\nただ、問題は「リクエストのハンドリングを開始から終了まで担当する」という定義です。先程のサンプルコードをよく見ると、HandlerのメソッドはどれもServerRequestを引数として、戻り値はServerResponseになっています。これはつまり、RouterとHandlerをあえて別のクラスとして分割するとしても、リクエストとレスポンスまでをHandlerで処理することを意味します。\nここで「Controller/Serviceの場合と同じく、Handlerの引数と戻り値だけを変えて良いのでは？」と考えられます。しかし、それこそフレームワークの思想に反することです。ServerRequestとServerResponseのJavaDocでは、以下の通り「ServerRequestとServerResponseはHandlerFunctionでハンドリングする」ことを明示しています。\n/** * Represents a server-side HTTP request, as handled by a {@code HandlerFunction}. * * \u0026lt;p\u0026gt;Access to headers and body is offered by {@link Headers} and * {@link #body(BodyExtractor)}, respectively. * * @author Arjen Poutsma * @author Sebastien Deleuze * @since 5.0 */ public interface ServerRequest { /** * Represents a typed server-side HTTP response, as returned * by a {@linkplain HandlerFunction handler function} or * {@linkplain HandlerFilterFunction filter function}. * * @author Arjen Poutsma * @author Juergen Hoeller * @author Sebastien Deleuze * @since 5.0 */ public interface ServerResponse { 以上のことでわかるように、WebFluxではServerRequestとServerResponseはHandlerFunctionで扱うようにデザインされています。なので、既存のServiceのように、Handlerがビジネスロジック「のみ」を扱うというのはそれが実装として可能かどうか以前の問題になるのです。\nただ、「責任の分散」という観点からして、責任によってクラスを分けるという発想は間違っているわけではないですね。なのでビジネスロジックを担当するクラスをHandlerと分離して運用するケースは考えられますが、必ずしもクラスを分ける基準がRouterとHandlerである必要はないのではないかと思われます。\nテストの観点で JavaでJUnitなどを用いてユニットテストを作る場合、テスト自体はユースケース単位で作成しますが、それらのテストはクラス単位でまとめるというケースが多いかなと思います。なので同じ観点でユニットテストを書く場合、RouterとHandlerが分けられているとしたら当然ユニットテストもその単位で分けられるでしょう。\nただ、こうする場合の問題は、テスト自体があまり意味を持たなくなる可能性があるということです。まずRouterは単純にエンドポイントとHandlerをつなぐ役割しか持たなくなるので、そのテストも「想定通りのHadlerFunctionを呼び出しているのか」に限るものになります。そしてHandlerの場合、ServerRequestを受信してServerResponseを発するので、テストが非常に難しくなるという問題があります。\nなぜServerRequestを受信してServerResponseを発するのが問題になるかというと、ServerRequestのインスタンスを生成するのが難しく、ServerResponseの場合でもレスポンスボディーを抽出するのが難しいからです。なので、WebTestClientで行うことになるかと思いますが、WebTestClientを使う場合はエンドポイントとHTTPメソッドなどを利用して実際のAPIを呼び出すことになるので、結果的にHandlerのテストをするつもりがRouterのテストまでふくむしかないということになります。こうするとクラス単位でテストケースをまとめることが難しいだけでなく、Routerのみのテストも実質的には意味をなくすということになります。\nではどうすればいいか 今まで論じた3つの観点からすると、RouterとHandlerは別のクラスにする理由もあまりなく、むしろ別クラスに色々と問題が生じるように見えます。しかし、これが必ずしもエンドポイントに対するルーティングとビジネスロジックを分離する必要はない、ということではないかと思います。先に述べた通り、クラスを分ける基準をRouterとHandlerにしないだけで良いかなと思います。例えば、以下のようなコードがあるとします。\n@Configuration class PersonRouter(private val repository: PersonRepository) { @Bean fun route(): RouterFunction\u0026lt;ServerResponse\u0026gt; = coRouter { GET(\u0026#34;/person/{id}\u0026#34;) { ServerResponse.ok() .contentType(MediaType.APPLICATION_JSON) .body( repository.findById(it.pathVariable(\u0026#34;id\u0026#34;).toLong()) .map { record -\u0026gt; PersonDto(record.id, record.name) } ).awaitSingle() } } } Handlerで、Bodyを作る箇所以外はビジネスロジックと言えるものがあまりありません。なので、ここではBodyだけを分離して別のクラス（Service）に一任しても良さそうです。例えば以下のようにです。\n@Configuration class PersonRouter(private val service: PersonService) { @Bean fun route(): RouterFunction\u0026lt;ServerResponse\u0026gt; = coRouter { GET(\u0026#34;/person/{id}\u0026#34;) { ServerResponse.ok() .contentType(MediaType.APPLICATION_JSON) .body(service.getPersonById(it.pathVariable(\u0026#34;id\u0026#34;).toLong())) .awaitSingle() } } } @Service class PersonService(private val repository: PersonRepository) { suspend fun getPersonById(id: Long): Mono\u0026lt;PersonDto\u0026gt; = repository.findById(id) .map { PersonDto(it.id, it.name) } } こうすると、Routerから直接Repositoryにアクセスこともなくなり、今まで挙げていたさまざまな問題も解消できるようになりますね。\n最後に ここで提示した方法でビジネスロジックを分けるのは可能だとして、その方法をとった場合に残る疑問はあります。これは果たしてFunctionalなのか？Functional EndpointはLambda-basedと説明されてあるが、Lambdaが使われないので設計意図とは違う形になってないか？そもそもSpring MVCとは違うコンセプトのフレームワークなので既存とは違うアプローチが必要なのでは？などなど。\nこれらの問題を判断するのはなかなか難しいですが、個人的には新しい技術だからといって常に新しい方法論を適用するということは難しく、既存の良い体系があるのならそれに従うのもそう間違っていることとは思いません。Springの公式ドキュメントでは「すでに問題なく動いているSpring MVCアプリケーションにあえてWebFluxを導入する必要はない(If you have a Spring MVC application that works fine, there is no need to change)」と述べていますが、これと同じく、既存の検証されてあるアーキテクチャがあるのならばそれをそのまま適用するもの悪くないのではと思います。まぁ、そもそもWebFluxを導入するところでMVCパターンを使うとしたらこういうことを気にする理由すら無くなるのですが…むしろこのようなプログラミングモデルが増えていくと今後は新しいアーキテクチャが生まれそうな気もしますね。今回のポストを書きながらはそういういうものを感じました。\nでは、また！\n","date":"2021-08-30T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-webflux-dividing-router-and-handler/","title":"WebFluxのFunctional Enpointに対する小考察"},{"content":"前回、Ktorを紹介しながら、KtorにはまだRole-based Authorizationに対応してないので、自前でそのような機能を実装する必要がある、と述べました。Ktorはまだ歴史が短く、SpringやDjango、Railsのように幅広く使われているフレームワークでもないので、おそらく他に比べ実のアプリケーションを作るにあたっては必要な機能が十分でない可能性がありますね。なので、こうやって必要な機能がない場合は直接その機能を実装するしかないです。\n幸い、Ktorでは機能をPluginといい、モジュール単位で追加できるため、必要な機能を実装するのもそのPluginを作ることでできるようになります。ただ、モジュールを利用するということは、機能単位の管理がやりやすくなるものの、そのモジュールはどうやって機能するか、また、どういうお作法が必要となるかを知る必要がありますね。\n今回はネット上に公開されてある記事を参考にしながら、KtorのRole-based AuthorizationをPluginとして実装してみました。そこで、今回のポストではこういう自作のPluginがどうやってKtorの機能として動作するか、どうやって実装するのかについて述べたいと思います。\nRole-based Authorizationとは まずは、そもそもRole-based Authorizationとは何か、からですね。これは、ウェブアプリケーションでよく言われている「認可」の方式のうち、ユーザのRole（役割）に基づいて、APIの実行を制御するものです。例えばECサイトの場合、商品に対して問い合わせをするのは認証されたユーザなら誰でもできるべきですが、「お知らせを書く」や「商品の在庫数を変更する」などの機能はその権限を持つユーザ（Admin）に限定すべきですね。なので、ここで「一般ユーザ」と「管理者」などのRoleを設け、APIに対してのリクエストが発生した際にそのRoleをまず確認し、その権限のあるユーザのみがAPIを実行できるようにする、というのがRole-based Authorizationの基本的な概念です。\nこれを実現するために既存のアプリに導入する必要のあるものは、大きく分けてRoleの概念と、それを元にリクエストをフィルタリングする機構の二つです。前者の場合はどんなロールがあり、どういう形でユーザに紐付けるかを考えればいいだけなので、テーブルやカラムを追加して既存のユーザの情報と紐づけるだけですみます。しかし、後者はまずフレームワークでどうやってリクエストをフィルタするか、まずその構造から理解する必要がありますね。なんので、まずはKtorでリクエストを扱う方法に対して紹介したいと思います。\nPipelineとFeature Ktorの特徴のうち、最も重要と言えるものは、Pipelineの概念です。このPipelineに対して、公式では以下のように説明しています。\nThe pipeline is a structure containing a sequence of functions (blocks/lambdas) that are called one after another, distributed in phases topologically ordered, with the ability to mutate the sequence and to call the remaining functions in the pipeline and then return to current block.\nこの説明だけでは理解が難しいものですが、要するに、Ktorにおいての処理の単位のことを指していると言ってもよいものです。PipelineではAPIのコールからレスポンスまで一連の流れとしての処理を定義することができます。なのでPipelineとして実現されている代表的な機能はRouter、リクエストに対してのハンドリングを定義する機能（SpringのControllerに対応するもの）となります。\nまた、Pipelineは拡張できるものなので、その形式に合わせて新しいPipelineを実装することでモジュール(公式の表現ではPlugin)を実現するのもできます。これらのモジュールを実装し、アプリケーションにインストールすることで、そのモジュールの機能を利用できるようになるのがKtorの特徴です。例えば、kotlin公式のJSON Mapperであるkotlinx.serializationをアプリケーションに追加するためには以下のようなコードを書きます。\nfun Application.main() { install(ContentNegotiation) { json() } } ここで呼び出しているinstall関数の実装を見ると、以下のようになっています。feature(モジュール)と、そのモジュールの設定となるconfigureが引数になっているのがわかります。\npublic fun \u0026lt;P : Pipeline\u0026lt;*, ApplicationCall\u0026gt;, B : Any, F : Any\u0026gt; P.install( feature: ApplicationFeature\u0026lt;P, B, F\u0026gt;, configure: B.() -\u0026gt; Unit = {} ): F 先ほどのkotlinx.serializationをインストールするために使っていたコードでは、featureとしてContentNegotiationを渡し、その設定としてjsonを使うという設定をしているのだなという推測ができますね。実際、ContentNegotiationの実装は、以下のような形となっています。一部のコードは省略していますが、クラスの中にはConfigurationというクラスと、ApplicationFeatureを継承したcompanion objectを中に持っているのがわかります。\npublic class ContentNegotiation internal constructor( public val registrations: List\u0026lt;ConverterRegistration\u0026gt;, private val acceptContributors: List\u0026lt;AcceptHeaderContributor\u0026gt;, private val checkAcceptHeaderCompliance: Boolean = false ) { // ... /** * Configuration type for [ContentNegotiation] feature */ public class Configuration { // ... } /** * Implementation of an [ApplicationFeature] for the [ContentNegotiation] */ public companion object Feature : ApplicationFeature\u0026lt;ApplicationCallPipeline, Configuration, ContentNegotiation\u0026gt; { // ... } 上記の実装でわかるように、Pipelineとして機能するためにはモジュールの設定のためのConfigurationというクラスと、モジュールとして機能するためのApplicationFeatureを継承したcompanion objectが必要であることがわかります。なので、この構造を持ったクラスを定義できれば、自作のモジュールをアプリケーションに実装できるということがわかりますね。\nPluginの実装 では、実際にPipelineとして、リクエストに対する認可を判定する機能を作るとしましょう。まずはロールを定義します。enumが良さそうですね。ここではシンプルに管理者と一般ユーザの2種を作ってみます。\nenum class Role(val value: String) { ADMIN(\u0026#34;ADMIN\u0026#34;), USER(\u0026#34;USER\u0026#34;) } これらのロールは、テーブルなどで管理する必要もありますが、ログイン中のユーザ情報から取得する必要もありますね。認可のためには、ログイン中のユーザにとあるロールが与えられているかどうかの確認が必要となるからです。なので、io.ktor.auth.Principalを継承したユーザの情報もクラスとして作り、ログインに成功した時はこのクラスにユーザのロールを格納することにします（方法は認可とは関係ないのでここでは割愛させてください）。以下はユーザの情報を格納するための簡単な例です。\ndata class UserPrincipal( val username: String, val roles: Set\u0026lt;Role\u0026gt; = emptySet() ) : Principal 次に、ロールでアクセスを制限する関数を作ります。Routerのエンドポイントに、どのロールの場合にアクセスできるかを指定するようなイメージです。例えば以下のような形で使えたらいいかと思います。\nfun Application.main() { routing { // 管理者のみアクセスできる withRole(Role.ADMIN) { get(\u0026#34;/admin\u0026#34;) { call.respond(\u0026#34;This is admin page\u0026#34;) } } // 一般ユーザがアクセスできる withRole(Role.USER) { get(\u0026#34;/user\u0026#34;) { call.respond(\u0026#34;This is user page\u0026#34;) } } } } Routerの使い方でわかるように、Pipelineでのコードブロック（関数）はネストが可能なのでこのように一つのレイヤーを挟むのも可能です。ここで追加したwithRoleという関数でロールを確認し、APIにアクセスできるかどうかを判定するようにしたら良いでしょう。\nAuthorizedRotueSelectorの実装 まずはRouteSelectorを実装します。これは、routingの中にこれから作る認可の関数がネストできるようにするためのものです。もっともシンプルな実装は以下のようになります。\nclass AuthorizedRouteSelector() : RouteSelector() { override fun evaluate(context: RoutingResolveContext, segmentIndex: Int) = RouteSelectorEvaluation.Constant } child routeの実装 先に実装したAuthorizedRouteSelectorを利用して、実際にchild routeとして機能する関数を作ります。このchild routeはRouterの下にネストすることになるので、Routeの拡張関数を作ることにします。引数としては判定のためのロールと、その下にネストするエンドポイントの関数を設定できるようにすれば良いでしょう。実装は以下のようにします。\nfun Route.withRole(val role: Role, build: Route.() -\u0026gt; Unit): Route { val authorizedRoute = createChild(AuthorizedRouteSelector()) application.feature(RoleBaseAuthorizer).interceptPipeline(this, role) build() return authorizedRoute } ここで実装しているものは、まずAuthorizedRouteSelectorでchild routeを作り、その後Pipelineをインターセプトして、ユーザが指定したロールに該当するかどうかを判定します。問題なければbuildを実行させますが、これがネストしているchild routeになります。最後に、エンドポイントをネストできるように先ほど作成したchild routeのインスタンスを返します。\nPipelineをインターセプトする時に呼び出しているRoleBaseAuthorizerは、別途クラスとして作ることにします。これをFeatureとして作ることになります。\n認可処理のモジュールの実装 では、本格的に認可の処理を担当するモジュール（Feature）を実装することにします。先に述べた通り、ConfigurationとFeatureを内部に持ったクラスを作ります。ここでConfigurationは、ログイン中のユーザからどうやってロールの情報を取得するかの設定ができるクラスにします。こうすることで、以下のようなことが可能になるでしょう。\nfun Application.main() { // RoleBaseAuthorizerをFeatureとしてインストール install(RoleBaseAuthorizer) { // ログイン中のユーザの情報からロールを取得する方法をConfigurationとして指定 getRoles { (it as UserPrincipal).roles } } } Configurationでは、ログイン中のユーザ情報となるUserPrincipalからrolesを取得する、という関数を渡します。これを持って、RoleBaseAuthorizerではwithRole関数で指定したロールとユーザのロールを比較するようにします。\n認可のモジュールの設定方法のイメージができたので、次にRoleBaseAuthorizerを実装します。例えば以下のようになります。\nclass RoleBaseAuthorizer(config: Configuration) { class Configuration { var userRoles: (Principal) -\u0026gt; Set\u0026lt;Role\u0026gt; = { emptySet() } // ログイン中のユーザの情報からロールの取得方法をセット fun getRoles(roles: (Principal) -\u0026gt; Set\u0026lt;Role\u0026gt;) { userRoles = roles } } private val getRoles = config.userRoles fun interceptPipeline( pipeline: ApplicationCallPipeline, role: Role ) { // Pipelineの位置付け pipeline.insertPhaseAfter(ApplicationCallPipeline.Features, Authentication.ChallengePhase) pipeline.insertPhaseAfter(Authentication.ChallengePhase, AuthorizationPhase) // インターセプト時の処理 pipeline.intercept(AuthorizationPhase) { // ログイン中のユーザの情報を取得 val principal = call.authentication.principal\u0026lt;UserPrincipal\u0026gt;() ?: throw AuthorizationException(\u0026#34;Missing principal\u0026#34;) // ユーザ情報からロールを取得 val roles = getRoles(principal) if (roles.none { role }) { // ログイン中のユーザのロールに、アクセス可能なロールが含まれてない場合の処理 } } } companion object Feature : ApplicationFeature\u0026lt;ApplicationCallPipeline, Configuration, RoleBaseAuthorizer\u0026gt; { override val key = AttributeKey\u0026lt;RoleBaseAuthorizer\u0026gt;(\u0026#34;RoleBaseAuthorizer\u0026#34;) val AuthorizationPhase = PipelinePhase(\u0026#34;Authorization\u0026#34;) override fun install( pipeline: ApplicationCallPipeline, configure: Configuration.() -\u0026gt; Unit ): RoleBasedAuthorization { val configuration = Configuration().apply(configure) return RoleBaseAuthorizer(configuration) } } } 先に説明した通り、Configurationではユーザのロール情報を取得する関数を設定し、保存します。そしてinterceptPipelineでは、その関数を持ってPipelineをインターセプトし、ロールの検証を行うようにします。\nまた、interceptPipelineでは、引数として渡されたPipelineの位置付けを設定する必要があります。上記のコードでは、「認証の後」に位置付けしています。その後のロジックは、色々な方法があると思いますので、ここでは割愛させていただきます。\n他に、Featureの場合は、RoleBaseAuthorizerが独立したモジュールとして使える設定を行います。単純に名前をつけてインスタンスを返すような、お作法的なものですね。\nここまでの実装が終わったら、一通り認可に関するモジュールの作成は終わります。ただ、interceptPipelineの処理としてユーザのロールが、APIにアクセスできない場合の処理として考えられることは二つほどあります。\nレスポンスを返して終了 まず考えられる方法は、適当なレスポンスを返し、そこで処理を終了させることです。この場合、以下のように実装ができます。\nif (roles.none { role }) { // レスポンスを返す call.respond( status = HttpStatusCode.Forbidden, message = \u0026#34;permission not granted: $role\u0026#34; ) // Pipelineの終了 finish() } ここで注意すべきことは、レスポンスを返すだけでPipelineは終わらないということです。レスポンスを返し処理を止めたい場合は必ずfinish()を呼び出して、Pipelineを終了させましょう。\nExceptionを投げる もう一つの方法は、例外を投げる方法ですね。例えば以下のようにします。\n// 認可されてない場合の例外 class AuthorizationException(override val message: String) : Exception(message) if (roles.none { role }) { throw AuthorizationException(\u0026#34;permission not granted: $role\u0026#34;) } 例外を投げる場合は、当然Pipelineの処理が止まることになりますが、アプリケーションのログでも例外になるのであまり良くないですね。幸い、KtorにもSpringのExceptionHandlerのような機能があるので、それを活用したら適切な例外のハンドリングが可能になります。例えば以下のようなことができますね。\n// 認可されてない場合のレスポンス data class AuthFailedResponse(val reason: String) // 例外のハンドリング install(StatusPages) { exception\u0026lt;Throwable\u0026gt; { cause -\u0026gt; when (cause) { // 認可の場合の処理 is AuthorizationException -\u0026gt; { call.respond( status = HttpStatusCode.Forbidden, message = AuthFailedResponse( reason = cause.message ) ) } } } } これでアプリケーションのログも綺麗になりますし、他の例外処理に対してもwhenの分岐を増やすだけで対応ができるようになります。\n最後に 最初に思っていたことよりも膨大な内容を扱うことになったので、いつもより説明が大雑把な気もしますが、これでPipelineとそれを応用した自作モジュールの実装についての説明は一通りできたかなと思います。なので、これを応用すれば、他のモジュールを追加するのもそう難しくなさそうな気がしますね。深堀すると色々また出そうな気がしますが、それについては機会があればまた今度のポストのネタにしましょう。（正直あまり詳しくありませんので…）\n個人的には、このようにRole-based Authorizationの機能を作りながら知った。一連の処理をPipelineという単位で扱うという概念ががかなり新鮮で、良いと思いました。処理に対してのインターセプトはSpringでもできるのですが、処理の流れ自体を一つの単位として扱えるならより色々なことができそうな気もしますね。まだKtorに触れたばかりなので、詳しいことはもっと時間をかけてゆっくり調べる必要がありそうですが。\n確かなのは、Ktorはかなり魅力的なフレームワークであるということです。最初はSpringなど、既存の有名なフレームワークと比べ色々と機能が足りない認証だったのですが、こうやって簡単にモジュールを作れるとしたら意外と問題ないかもしれない、という気がします。もちろんそれでも、プロダクションレベルのものを作るにはまだ色々と検証が必要そうな認証はありますが。\nでは、また！\n","date":"2021-08-09T00:00:00Z","image":"https://retheviper.github.io/images/ktor.jpg","permalink":"https://retheviper.github.io/posts/ktor-role-based-authorization/","title":"KtorでRole-based Authorizationを実装する"},{"content":"DBのテーブルとして、1:Nのリレーションは珍しいものではありません。例えば、ECサイトで会員登録をし、複数の配送先を設定できるようにするとしたら、この会員情報のテーブルに配送先のカラムを追加するよりは、配送先のテーブルを分離して別に管理した方がデータの持ち方としては合理的で安全なはずです。そして分離した配送先のテーブルは、会員情報のテーブルとN:1の関係になるのが一般的でしょう。\nただ、データの持ち方が優先的なDBと、そのデータを処理して形にするアプリケーションでは事情が違いますね。例えば上記の通り、一つの会員情報のレコードに対して複数の配送先のレコードが存在し得る場合、SQLでデータを表現すると、以下のような形になるはずです。\n|-----------|-------------|-----------------| | member.id | member.name | mailing.address | |-----------|-------------|-----------------| | 1 | John | Tokyo | | 1 | John | New York | | 1 | John | Beijing | | 2 | Simpson | Osaka | | 2 | Simpson | Nagoya | |-----------|-------------|-----------------| しかしアプリケーションではこのような形でデータを扱うことはあまりないですね。一つのレコードに対して複数のレコードが含まれるということは、Kotlinだと以下のように表現するのが一般的かと思います。\ndata class Member( val id: Int, val name: String, val mailingAdress: List\u0026lt;String\u0026gt; ) そして普通は、このようなオブジェクトをJSONの形にしてREST APIのレスポンスとして使う場合が多いですね。なので、先程のレコードをJSONにした場合は以下のようになるはずです。\n{ \u0026#34;members\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;mailingAddress\u0026#34;: [ \u0026#34;Tokyo\u0026#34;, \u0026#34;New York\u0026#34;, \u0026#34;Beijing\u0026#34; ] }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Simpson\u0026#34;, \u0026#34;mailingAddress\u0026#34;: [ \u0026#34;Osaka\u0026#34;, \u0026#34;Nagoya\u0026#34; ] } ] } ここで問題は、オブジェクトをJSONに変えることはそう難しくないのですが（Jackson, Gson, Kotlin Serializationのようなライブラリがすでにありますし）\nでは、このような場合、DBから取得したレコードをどうやってオブジェクトにマッピングしたら良いのでしょう。JPAのようなORMを使う場合、クラスにテーブル間の関係を表すフィールドとアノテーションを適切に使うことでレコードへのマッピングは自動に行われますが、jOOQやQuerydsl, Exposed, KtormのようなORMで、DSLを使ってSQLを書く場合はデータのマッピングを手動で行う必要があります。そして取得したデータは行の配列という形になるので、どうマッピングしたら（効率が）いいかは少し悩ましいところです。\nなので、今回はExposedのDSLを使って取得したOne to Manyのレコードを、コード上でどうやってオブジェクトにマッピングするかについて考えたことを述べたいとお思います。\nテーブルごとにSelectする もっとも簡単な方法は、そもそもレコードの取得時に個別のテーブルに対してSelectしてからマッピングすることですね。個別のテーブルに対してクエリを発行するので、書き方としては明瞭になるというメリットもあります。例えば、以下のようなことができます。\ntransaction { // まずはMemberテーブルをSelectし、オブジェクトにマッピングする val member = Member.select { Member.id eq id } .first() .let { MemberDto( id = row[Member.id].value, name = row[Member.name], role = listOf(row[Mailing.role]) ) } // MailingテーブルをSelectし、リストにする val mailingAddress = Mailing .select { Mailing.memberId eq member.id } .map { it[Mailing.address] } // オブジェクトのコピーを作り配送先のデータをマッピング member.copy(mailingAddress = mailingAddress) } もっとも簡単で、コードとしてもわかりやすい方法ではありますが、トランザクションの観点からするとあまりよくない方法ですね。Exposedでは、transactionブロックに包むことでトランザクションの単位を制御できるものの、一回で済ませるクエリの発行が複数になるという問題があります。ここではMemberテーブルを照会するたびにMailingテーブルも照会することになるので1回のクエリが追加されているだけですが、もしMemberテーブルに1:Nの関係となっているテーブルが増えれば増えるほどクエリの発行数も増えることになるでしょう。そして今回は一つのレコードに対してのコードとなっていますが、照会対象のMemberテーブルのレコードが増えれば増えるほど発行されるクエリの数も多くなります。\nまた、オブジェクトのインスタンスを作っておいて、わざわあコピーするというのもあまり効率が良いとは言えません。これもまたクエリの数が増える問題と同じく、照会対象のレコードが増えれば増えるほど生成されるオブジェクトのインスタンスも増えることになるという訳ですね。なので、全く性能や効率を考えてないコードと言えます。\njoinしてマッピングする 関係のあるデータを複数のテーブルを跨いで取得するには、やはりjoinが効率的でしょう。これならまず発行されるクエリの数は個別のテーブルに対してSelectする時に比べ、劇的に減ります。アルゴリズムでよく使われる表現のBig O記法で表現すると、前者はO(N^2)であり、これはO(1)と表現できるはずです。\nならばデータを取得する際に、クエリとしてはjoinを使うのが理想的なのはわかりますが、問題はそうやって取得したデータをどう加工するかです。先に述べましたが、取得したデータのうち重複するものがあるからですね。そしてこれが重複しているかどうかはクエリを実行した結果を取得して、確認するまではわかりません。\nここで考えられる方法は三つほどありますので、一つづつ紹介していきます。\nreduce まずはクエリの結果として取得した行を、それぞれオブジェクトにマッピングした後、reduceでまとめる方法です。例えば以下のようになります。\ntransaction { Member.leftJoin(Mailing) .select { (Member.id eq id) and (Mailing.memberId eq Member.id) } .map { // とりあえずオブジェクトにマッピングする MemberDto( id = it[Member.id].value, name = it[Member.name], mailingAddress = listOf(it[Mailing.address]) ) }.reduce { acc, memberDto -\u0026gt; // オブジェクトを一つに集約させる（mailingAddressは累計） acc.copy( mailingAddress = acc.mailingAddress + memberDto.mailingAddress ) } } このやり方で考えられる問題は、まず行数分のオブジェクトのインスタンスが作られるということです。このクエリとして取得しようとしているMemberのレコードは一つのみですが、そのレコードに紐づくMailingのレコードが多ければ多いほど件数は増え、当然生成されるオブジェクトの数も多くなります。また、マッピングだけでなく、reduceでもオブジェクトをコピーしているので、やはり行数分のオブジェクトが生成されていると考えられます。オブジェクトのインスタンス数は行数x2になる訳ですね。\nそしてもう一つの問題は、Memberのレコードを複数取得する場合、全部一つのオブジェクトにまとまってしまうという問題がありますね。なので、このやり方だと一つのレコードを取得する場合のみしか適応できなくなります。\ngroupBy 取得したレコードを、一度Mapに変換するとどうでしょうか。KotlinのCollectionにはgroupByというメソッドがあり、keyとvalueのマッピング方法を指定すると、一つのkeyにList形式のvalueになります。Mapなので、keyでMemberのオブジェクトをマッピングしておいて、valueとしてはMailingのレコードをまとめておくと良いでしょう。keyは同じものだと上書きされるので、取得したいMemberのレコードが複数の場合でも問題ないはずです。コードでは、以下のようになります。\ntransaction { Member.leftJoin(Mailing) .select { (Member.id eq id) and (Mailing.memberId eq Member.id) } // keyはMemberオブジェクトのマッピング、valueではMailingのレコードを集約 .groupBy({ MemberDto( id = it[Member.id].value, name = it[Member.name], ) }, { it[Mailing.address] }) // keyのオブジェクトにMailingのレコードを設定 .map { (key, value) -\u0026gt; key.copy(mailingAddress = value) } } この方法だと、今まで照会した他の方法で考えられる問題をだいぶ解消できそうな気がしますね。ただ、気になるのは、groupByの引数がLambdaであることです。引数として関数を渡すということは、ループしながらその関数を実行することになるという意味なので、reduceの時と同じ量のインスタンスが作られる可能性がありそうですね。なので、groupByの実装を見ていきたいと思います。中のコードは、以下の通りです。\npublic inline fun \u0026lt;T, K, V\u0026gt; Iterable\u0026lt;T\u0026gt;.groupBy(keySelector: (T) -\u0026gt; K, valueTransform: (T) -\u0026gt; V): Map\u0026lt;K, List\u0026lt;V\u0026gt;\u0026gt; { return groupByTo(LinkedHashMap\u0026lt;K, MutableList\u0026lt;V\u0026gt;\u0026gt;(), keySelector, valueTransform) } groupByの実装では、groupByToという関数に自分の引数と、作られるMapのインスタンスを渡しているだけですね。では、さらにgroupByToの中身を見ていきましょう。\npublic inline fun \u0026lt;T, K, V, M : MutableMap\u0026lt;in K, MutableList\u0026lt;V\u0026gt;\u0026gt;\u0026gt; Iterable\u0026lt;T\u0026gt;.groupByTo(destination: M, keySelector: (T) -\u0026gt; K, valueTransform: (T) -\u0026gt; V): M { for (element in this) { val key = keySelector(element) val list = destination.getOrPut(key) { ArrayList\u0026lt;V\u0026gt;() } list.add(valueTransform(element)) } return destination } ここで確かになっていることは、やはり最初のCollectionの要素数分、keySelectorとvalueTransformを実行しているということです。Mapに変えることになるので、reduceの時とは違ってMemberレコードがいくつあっても一つに集約されるような事態は起こらないと考えられますが、依然としてオブジェクトのインスタンスが複数できてしまうという問題はあります。なので、また他の方法を探してみましょう。\nMap 最後に考えられるのは、Selectした行をMapにまとめるのではなく、外部にMapを宣言し、それを利用することです。Mapには、computeという関数があり、引数として渡したkeyに対してどんな処理をするか（どんなvalueを作って入れるか）を指定できます。例えば、指定したkeyに対してvalueが存在しない場合は要素として追加し、存在する場合はそのvalueを変えるなどの処理ができるようになります。なので、これをうまく使うとインスタンスの生成問題を解決できる気がしますね。\nトランザクションとは関係のないMapをまず宣言し、Selectしたデータに対してcomputeを実行することにします。computeでは指定したkey（Memberのidなど）がMapの中にない場合にMemberのインスタンスを作成するようにして、すでにある場合はそのオブジェクトにMailingのデータを追加するようにすれば良いでしょう。そしてループが終わったらMapのvalueのみを取得すると良いですね。\n以上のことを、コードで表すと以下のようになります。\n// オブジェクトをまとめるためのMap（keyはMember.id） val helperMap = mutableMapOf\u0026lt;Int, MemberDto\u0026gt;() transaction { Member.leftJoin(Mailing) .select { (Member.id eq id) and (Mailing.memberId eq Mailing.id) } .forEach { helperMap.compute(it[Member.id].value) { key, value -\u0026gt; // valueがnullではない場合、コピーしてmailingAddressを累計 value?.copy( mailingAddress = value.mailingAddress + it[Mailing.address] // valueがnullの場合はインスタンスを作る ) ?: MemberDto( id = key, name = it[Member.name], mailingAddress = listOf(it[Mailing.address]) ) } }.let { // valueをListに変換 helperMap.map { it.value } } } いかがでしょうか。これで重複するデータなく、インスタンスの作成も最低限に抑えることができたかと思います。もちろん、mailingAddressを追加するたびにコピーが発生するという問題はありますが、ここは専用のsetterなどを作っておくことで回避できると思います。\n一つ注意しなくてはならないのは、ここで使っているMapをフィールドとして宣言したりするとデータの整合性やアプリケーションのメモリ使用量に響くということです。なので必ずメソッドの中でのみMapのインスタンスが作成されるようにする必要があります。\n最後に DSLを使ってクエリを直接作成する場合、JPAのようなORMの問題とされているN+1(必ず連関しているテーブルもjoinしてくる)のような問題を回避できますが、直接オブジェクトへのマッピングも書かなくてはならないという短所がありますね。個人的にクエリを書くのは楽しくないですが、クエリをコードとして管理でき、必要なクエリだけを書けるというところでDSLの方が良い点もあると思います。テーブルの構造や処理によってはORMが勝手にクエリもマッピングもしてくれるのが楽ではありますが。\nただ、ORMでどうやってデータを取得するかの問題だけでなく、ここで扱った「重複するデータをどう違う形のデータに加工するか」の問題は、必ずしもDBからレコードを取得する場合のみのことに限らないので（例えば他のAPIを読んだ結果としてもそんなデータはあり得ますね）、色々方法を考えておく必要はありそうです。今の時点ではMapを利用した方法がもっとも良さそうな気がしますが、他にもっと効率的な方法があるかもしれませんしね。\nでは、また！\n","date":"2021-07-26T00:00:00Z","image":"https://retheviper.github.io/images/exposed.jpg","permalink":"https://retheviper.github.io/posts/exposed-mapping-record-to-object/","title":"ExposedでOneToManyをどうマッピングするか"},{"content":"サーバサイド言語としてのKotlinは普及しつつありますが、Kotlinを使う場合でもウェブフレームワークとして使われるのはやはりSpringが多いかと思います。理由としては会社ごとの事情というものもあるはずですが、一般的な理由としてはJavaエンジニアにとってKotlinは馴染みやすい物であっても、フレームワークの場合はそうでなく、Springほど検証されたフレームワークはないからということからでしょう。いまだにStrutsを使っていて、Springに移行しようとするところもありますしね。\nKotlinはJavaと完璧（に近い）互換性があるので、Javaで書かれてあるアプリをそのままKotlinに移行しても大した問題はありません。Javaより生産性が高い上にSpringだけでなくJackson、Apache POI、JUnit、Jacocoなどの数多くのライブラリをそのまま使えるのは確かにメリットであって、企業側としてKotlinの導入を検討する理由は確かにそこにあると思います。Javaエンジニアはその数が多いので、エンジニアを募集し安くなるというところもメリットの一つと言えるでしょう。\nただ、Kotlinを使う場合に長期的にはKotlinで書かれたライブラリやフレームワークを導入することを検討した方が良いかもしれません。コンパイルした結果として生成されるByte codeがJavaと全く一緒だとしても、そもそものソースコードが違う言語なので、使う側のコード（クライアントコード）としては不便なところがあったり、Kotlinに最適化されてない場合もある可能性があるからです。また、KotlinはJVMだけでなく、ネイティブにコンパイルすることもできるので、ネイティブなアプリを作りたい場合はJavaに依存しないAPIを選ぶ必要があるでしょう。\nということで、今回はJetBrains制のウェブフレームワーク、Ktorと、Ktorと一緒に使えるORMのExposedを少し触ってみて、Springと比べながら紹介したいと思います。\nKtor Ktorは、JetBrainsで開発しているマイクロサービス向けの軽量ウェブフレームワークです。公式ホームページの紹介にも色々書いてありますが、特にSpringと比べて以下の特徴があるかと思います。\n軽量 Springも軽量とは言われているものの、起動が遅いので、実装する側としてはあまり軽量だという感覚はないです。Springで書かれたアプリケーションの起動が遅いのは、起動時にさまざまなアノテーションを読み込み、DIや設定などを完璧に終わらせているというフレームワークそのもののアーキテクチャに起因しているのではないかと思います。なのでDIされるオブジェクトをLate initにするなどで起動速度を短縮させるテクニックなどが知られていますね。\nしかし、Ktorは起動がかなり早いです。同一規模のアプリをSpringとKtorの両方で作成してベンチマークした訳ではないので正確な数値に関しては割愛しますが、体験だと数倍は早いですね。例えば、In memoryタイプのH2と基本的なCRUDを実装したSpring WebFluxアプリケーションの場合、自分のPCで起動に2.7秒ほどかかりました。\n2021-07-18 15:08:25.150 INFO 29047 --- [main] c.r.s.SpringWebfluxSampleApplicationKt : Started SpringWebfluxSampleApplicationKt in 2.754 seconds (JVM running for 3.088) 同じ構成でKtorのアプリを実装した場合、起動には1秒もかからなかったです。\n2021-07-18 15:09:29.683 [main] INFO ktor.application - Application started in 0.747 seconds. これはおそらく基本的にDIをしなく、アノテーションをあまり使わない（Reflectionを使わない）構造やKtorそのものはREST APIを作成するための必要最低限の機能だけを揃っているのが理由かと思われます。\nアプリの起動が早いというのは、テストにかかる時間を短縮させられるという面でもメリットといえますが、サーバレスなアプリにも適しているということにもなるでしょう。私もAWSのLambdaやAzureのFunctionsなどを触った経験がありますが、この場合にJavaやKotlinの使用を考慮したことはありません。サーバレスの場合、アプリが常に稼働中ではないので、リクエストが発生したたびにアプリを起動しなければならないです。なので起動の遅いSpringはそもそもの考慮対象にならなかったですね。Ktorを使う場合は起動速度が大幅に短縮できるので、JVMの起動速度が許されるというならば、サーバレスアーキテクチャで導入を検討できるレベルになっていると思います。\n拡張可能 Ktorが軽量であることとも繋がる話ですが、必要な機能があればプラグイン（モジュール）を追加したり、自分で実装する必要はあります。コードとしては、以下のようになります。\nfun main() { embeddedServer(Netty, port = 8080, host = \u0026#34;127.0.0.1\u0026#34;) { install(CORS) install(Compression) install(Sessions) { cookie\u0026lt;MyCookie\u0026gt;(\u0026#34;MY_COOKIE\u0026#34;) } install(ContentNegotiation) { // kotlinx.serialization json() } }.start(wait = true) } なので、Ktorの導入直後はモジュールの管理や開発のスピード感という側面ではマイナスになる部分もあるかなと思います。特にまだSpring Securityでは基本的に提供しているRole-Based Authorizationなどの機能が公式プラグインとして提供されてないので自前の処理を書くしかないという部分もあります。個人的には、モジュール化そのものは慣れたらメリットになる可能性の方が高いと思いますが、導入初期としてはSpringに比べ不利なところなのではないかと思います。\n特にKtorはDIに対応していなく、JetBrains公式のモジュールもないので、DIをするためにはInjekt, Kodein, Koinなどをディペンデンシーとして追加する必要があります。ただ、アーキテクチャによってはDIが必要なく、objectで代替することもできると思いますので、どんなアーキテクチャにするかはよく考えて決める必要があるかなと思います。\nCoroutine対応 Spring WebFluxもそうでしたが、最近は多くのウェブフレームワークに非同期・ノンブロッキング対応が行われていますね。PaaSが普及され簡単にインフラの構築ができ、ハードウェアが安くなった今でもソフトウェアで性能を改善できる箇所があるならそれは十分価値があると思っています。だとすると、非同期・ノンブロッキング対応のフレームワークを導入するということも良い選択ではないかと思います。\nKtorではルーティングの実装として、Routeのroute、もしくはget, post, put, deleteなどのfunctionを呼び出すことになります。これはSpring WebFluxのRouter/Hanlder Functionとよく似ていますね。コードで表すと、以下のようなものです。\nrouting { get(\u0026#34;/hello\u0026#34;) { call.respondText(\u0026#34;Hello\u0026#34;) } } そしてこのHttpメソッドごとの関数のbodyを実装することになりますが、これが基本的にsuspendとなっています。これはつまり、実装する側で特に意識しなくてもコードは非同期になるということですね。Spring WebFluxの場合も、Coroutineを使うと簡単に実装ができましたが、suspendすら意識しなくて良いというところはKtorならではのメリットなのではという気がします。\nテスト ktor-server-test-hostやkotlin-test、JUnitなどを使ってテストが可能です。Springでもユニットテストは色々な書き方があるかと思いますが、よりKotlinらしき書き方になっているだけで、基本的にテストの仕方が大きく変わったりはしません。例えば、Getをのレスポンスをテストするためには以下のようなコードを書くことができます。\n@Test fun getMember() { withTestApplication(Application::module) { handleRequest(HttpMethod.Get, \u0026#34;api/v1/web/members/$id\u0026#34;).apply { assertEquals( actual = response.status(), expected = HttpStatusCode.OK ) assertEquals( actual = response.content, expected = Json.encodeToString( MemberResponse( id = id, userId = userId, name = name ) ), ) } } } Exposed Ktorで使える、Kotlinで書かれたORMは代表的にExposedがあります。JavaのjOOQがそうであったように、SQL DSLを使うことでクエリをコードで書くような感覚で（実施はDSLを解釈してSQLは自動生成されますが）使えるというところが良いです。例えば、Userというテーブルからレコードを取得する場合のコードは、以下のようになります。\nval userInUsa: List\u0026lt;User\u0026gt; = transaction { UserTable.select { UserTable.deleted eq false }.map { User( id = it[UserTable.id], name = it[UserTable.name], country = it[UserTable.country] ) }.filter { it.country = Country.USA } } また、ExposedでははDAOパターンも使えるので、DAOパターンでクエリを書くとしたら以下のようなことができます。JPAやR2DBCと似たような感覚で使えそうですね。(デメリットもおそらく同じかと思いますが)\nval userInGermany: List\u0026lt;User\u0026gt; = transaction { User.find { (UserTable.country eq Country.GERMANY) and (UserTable.deleted eq false)} } また、Exposedの特徴は、テーブルをコードとして定義することでDBに反映させることができるということです。今までLiquibaseやFlywayでDBの形状管理をやっていたことが多かったのですが、個人的に実際のDBとアプリケーションのテーブル定義に乖離があるケースを考えるとこうやってコードの中に定義した方が、データのオーナーという観点からもかなり良いのではないかと思います。特に、頻繁なテーブル定義の修正があったり、マイクロサービスが多いケースではかなり開発が便利になるのではないかと思います。\nExposedのテーブル定義は、以下のようにできます。\nobject Member : IntIdTable() { val userId: Column\u0026lt;String\u0026gt; = varchar(name = \u0026#34;user_id\u0026#34;, length = 16) val name: Column\u0026lt;String\u0026gt; = varchar(name = \u0026#34;name\u0026#34;, length = 16) val password: Column\u0026lt;String\u0026gt; = varchar(name = \u0026#34;password\u0026#34;, length = 255) val deleted: Column\u0026lt;Boolean\u0026gt; = bool(\u0026#34;deleted\u0026#34;) val createdBy: Column\u0026lt;String\u0026gt; = varchar(\u0026#34;created_by\u0026#34;, 16) val createdDate: Column\u0026lt;LocalDateTime\u0026gt; = datetime(\u0026#34;created_date\u0026#34;) val lastModifiedBy: Column\u0026lt;String\u0026gt; = varchar(\u0026#34;last_modified_by\u0026#34;, 16) val lastModifiedDate: Column\u0026lt;LocalDateTime\u0026gt; = datetime(\u0026#34;last_modified_date\u0026#34;) } そして実際発行されるSQLは以下のようになります。\nCREATE TABLE IF NOT EXISTS \u0026#34;MEMBER\u0026#34; (ID INT AUTO_INCREMENT PRIMARY KEY, DELETED BOOLEAN NOT NULL, CREATED_BY VARCHAR(16) NOT NULL, CREATED_DATE DATETIME(9) NOT NULL, LAST_MODIFIED_BY VARCHAR(16) NOT NULL, LAST_MODIFIED_DATE DATETIME(9) NOT NULL, USER_ID VARCHAR(16) NOT NULL, \u0026#34;NAME\u0026#34; VARCHAR(16) NOT NULL, PASSWORD VARCHAR(255) NOT NULL) ここで、JPAやR2DBCの場合、Auditableクラスを定義して、エンティティがそれを継承することでカラムを共有したり、Spring Securityに連携することができましたが、Exposedでも似たようなことができました。\nabstract class Audit : IntIdTable() { val deleted: Column\u0026lt;Boolean\u0026gt; = bool(\u0026#34;deleted\u0026#34;) val createdBy: Column\u0026lt;String\u0026gt; = varchar(\u0026#34;created_by\u0026#34;, 16) val createdDate: Column\u0026lt;LocalDateTime\u0026gt; = datetime(\u0026#34;created_date\u0026#34;) val lastModifiedBy: Column\u0026lt;String\u0026gt; = varchar(\u0026#34;last_modified_by\u0026#34;, 16) val lastModifiedDate: Column\u0026lt;LocalDateTime\u0026gt; = datetime(\u0026#34;last_modified_date\u0026#34;) } object Member : Audit() { // Auditのカラムも含めてテーブルが作成される val userId: Column\u0026lt;String\u0026gt; = varchar(name = \u0026#34;user_id\u0026#34;, length = 16) val name: Column\u0026lt;String\u0026gt; = varchar(name = \u0026#34;name\u0026#34;, length = 16) val password: Column\u0026lt;String\u0026gt; = varchar(name = \u0026#34;password\u0026#34;, length = 255) } MyBatisなどに慣れている場合は少し適応に時間が必要かもしれませんが、基本的にはテーブルの定義を除くとほぼSQLの発行をKotlinのコードで書くことになるという感覚なので、便利になるかと思います。\n最後に 以上で、簡単なCRUDアプリをKtor + Exposedで実装してみた後の感想と紹介を少し書いてみました。まとめると、かなりサクサクコードを書けて性能も良いので、マイクロサービスに特化している構成ではないかと思いました。また、冒頭に述べた通り、ピュアなKotlin制のフレームワークであることも良いですね。Ktorの紹介でもKotlin Multiplatformに基づいていてどのプラットフォームにもアプリをデプロイできると強調していますので、色々なところで活用ができるかと思います。\nまだSpringと他のJavaライブラリに比べ足りないモジュールや機能もありますが、Exposed以外でもKtormのようなORMがあるなどKotlin制のライブラリの開発も進めていて、IntellijでもKtorのサポートは強力なので今後も発展を期待できそうであります。個人的にまだ仕事で使うことには無理があっても、自作アプリなどを作りたい時は導入をぜひ検討したいと思いました。Kotlinでできることがだんだん増えてきていて、嬉しいですね。\nでは、また！\n","date":"2021-07-18T00:00:00Z","image":"https://retheviper.github.io/images/ktor.jpg","permalink":"https://retheviper.github.io/posts/ktor-first-impression/","title":"Ktorを触ってみた"},{"content":"前回に続いて、今回も簡単にKotlinで色々書いてみましたのでその紹介となります。Kotlinではスタンダードライブラリや言語仕様として提供している機能がかなり多いので、これらを使いこなすだけでも生産性やコードのクォリティが大幅に上がるのではないかと思います。なので、今回もJava的な書き方を、Kotlinではどんな方法で効率よく実現できるかを中心に紹介したいと思います。\nもちろんKotlinでは基本的にJavaの書き方でも全く問題なく動くコードを書けますが、Kotlinならではのコードに変えた方がより簡単で短いコードを書ける場合が多く、色々と手間を省けることができるので（そして大抵の場合、スタンダードライブラリの実装の方が自分の書いたコードよりクォリティ高いような…）こういう工夫はする価値が十分にあるのではないかと思います。\nなので、今回は自分が調べたKotlinの小技を少し紹介したいと思います。\nSequentialなデータを作成する よくユニットテストなどでテスト用データを作成して使う場合がありますね。こういう時に必要となるデータの種類は色々とあるかと思いますが、複数のレコードを番号をつけて順番に揃えた感じのものを作りたい場合もあると思います。例えばData01、Data02、Data03…といったデータを作りたい場合ですね。\nこの場合は、ループでデータを作り、Listにまとめるというのが一般的ではないかと思います。例えば以下のような例があるとしましょう。\n// テスト用データを作成する fun createTestDatas(): List\u0026lt;String\u0026gt; { // テスト用データのリスト val testDatas = mutableListOf\u0026lt;String\u0026gt;() // 10件のデータを追加 for (i in 0 until 10) { testDatas.add(\u0026#34;テスト$i\u0026#34;) } // read-onlyに変換して返却 return testDatas.toList() } ただ、どちらかというとこれはJavaのやり方に近いので、まずはこれをベースに、Kotlinらしきコードではどうやって同じことができるかを考えてみたいと思います。\nrepeat まず考えられる方法は、ループの単純化ですね。サイズが10のリストを作りたいということは、ループが10回であることなので、それに相応しい関数を使います。例えばrepeatがありますね。repeatを使うと、スコープ内のパラメータとしてインデックスが渡されるので、簡単に\nfun createTestDatas(): List\u0026lt;String\u0026gt; { val testDatas = mutableListOf\u0026lt;String\u0026gt;() // 10回繰り返す repeat(10) { testDatas.add(\u0026#34;テスト$i\u0026#34;) } return testDatas.toList() } 次に考えたいのは、MutableListをImmutableに変えることです。テストで使うデータとしては問題ない場合はありますが、変更する必要のないデータをそのままMutableにしておくのはあまり良い選択ではありませんね。なので、データの作成を最初からListにできる方法を取りたいものです。\nここでは二つの道があって、最初からサイズを指定したListを宣言するか、ループの範囲、つまりRangeを指定する方法があります。\nList まずはサイズを指定したListを作る方法からみていきましょう。インスタンスの作成時に、サイズと要素に対してのイニシャライザを引数として渡すことで簡単に指定したサイズ分の要素を作ることができます。例えば、上で紹介したコードはListを使うことで以下のように変えることができます。\nfun createTestDatasByList(): List\u0026lt;String\u0026gt; = List(10) { \u0026#34;テスト$it\u0026#34; } この方法は、実は先に紹介した方法と根本的に違うものではありません。実装としては、以下のようになっているので、Syntax sugarとして使えるということがわかります。\n@SinceKotlin(\u0026#34;1.1\u0026#34;) @kotlin.internal.InlineOnly public inline fun \u0026lt;T\u0026gt; List(size: Int, init: (index: Int) -\u0026gt; T): List\u0026lt;T\u0026gt; = MutableList(size, init) @SinceKotlin(\u0026#34;1.1\u0026#34;) @kotlin.internal.InlineOnly public inline fun \u0026lt;T\u0026gt; MutableList(size: Int, init: (index: Int) -\u0026gt; T): MutableList\u0026lt;T\u0026gt; { val list = ArrayList\u0026lt;T\u0026gt;(size) repeat(size) { index -\u0026gt; list.add(init(index)) } return list } 他にもListを使う場合は、itnitとしてどんな関数を渡すかによって、stepの設定などができるのも便利ですね。例えば以下のようなことができます。\nList(5) { \u0026#34;Test${ it * 2 }\u0026#34; } // [Test0, Test2, Test4, Test6, Test8] List(5) { (it * 2).let { index -\u0026gt; \u0026#34;$index は偶数\u0026#34; } } // [0 は偶数, 2 は偶数, 4 は偶数, 6 は偶数, 8 は偶数] ただ、結果的に作られるListのインスタンスはMutableListなので、生成したデータをread-onlyにしたい場合はまたこれをtoList()などで変換する必要があるという問題があります。\nRange では、もう一つの方法をまた試してみましょう。Kotlinでは数字の範囲を指定することだけで簡単にRangeオブジェクトを作成することができます。Rangeを使う場合、上記のコードは以下のように変えられます。\n// Rangeを使ってテストデータを作る fun createTestDatasByRange(): List\u0026lt;String\u0026gt; = (0..10).map { \u0026#34;テスト%it\u0026#34; } Listの時とは違って、RangeにはIntRangeやLongRange、CharRangeなどがあり、引数の数字や文字を調整することで簡単にアレンジができるということも良いです。\nまた、一般的に性能はListよりRangeの方が良いようです。以下のようなコードでベンチマークした際、大抵Rangeの方がListの倍ぐらい早いのを確認できました。\nimport kotlin.system.measureTimeMillis data class Person(val name: String, val Num: Int) fun main() { benchmark { list() } benchmark { range() } } fun benchmark(function: () -\u0026gt; Unit) = println(measureTimeMillis { function() }) fun list() = List(200000) { Person(\u0026#34;person$it\u0026#34;, it) } fun range(): List\u0026lt;Person\u0026gt; = (0..200000).map { Person(\u0026#34;person$it\u0026#34;, it) } 一つ気にしなくてはならないのは、Rangeの場合は基本的に値が1づつ増加することになっているので、forやListのようなstepの条件が使えません。なので場合によってどちらを使うかは考える必要があります。\nCheck Validationなどで、パラメータの値を確認しなければならない場合があります。KotlinではNullableオブジェクトとそうでないオブジェクトが分けられているので、Javaと違って引数にnullが渡される場合はコンパイルエラーとなりますが、ビジネスロジックによってはそれ以外のことをチェックする必要もあり、自前のチェックをコードで書くしかないです。\nまず、お馴染みのJavaのやり方を踏襲してみると、以下のようなコードを書くことができるでしょう。関数の引数と、その戻り値のチェックが含まれている例です。\nfun doSomething(parameter: String): String { if (parameter.isBlank()) { throw IllegalArgumentException(\u0026#34;文字列が空です\u0026#34;) } val result = someRepository.find(parameter) if (result == null) { throw IllegalStateException(\u0026#34;結果がnullです\u0026#34;) } return result } ここで少し違う言語の例をみていきたいと思います。Kotlinとよく似ていると言われているSwiftの場合、ここでGuard Statementを使うのが一般的のようです。チェックのための表現が存在することで、ビジネスロジックとチェックが分離されるのが良いですね。Swiftをあまり触ったことがないので良い例にはなっていないかもしれませんが、イメージ的には以下のようなコードになります。\nfunc doSomething(parameter: String) throws -\u0026gt; String { guard !parameter.isEmpty else { throw ValidationError.invalidArgument } guard let result = someRepository.find(parameter) else { throw ValidationError.notFound } return result } 同じく、Kotlinでもチェックのための表現とビジネスロジックが分離できれば、コードの意味がより明確になるはずです。Kotlinではどうやってそれを実現できるのでしょうか。例えば以下のようなことを考えられます。\nfun doSomething(parameter: String?): String { val checkedParameter = requireNotNull(parameter) { \u0026#34;文字列がnullです\u0026#34; } val result = someRepository.find(checkedParameter) return checkNotNull(result) { \u0026#34;結果がnullです\u0026#34; } } requireNotNullは、渡された引数がnullである場合はIllegalArgumentExceptionを投げ、そうでない場合は引数をnon-nullタイプとして返します、明確にnullチェックをしていることが解るだけでなく、以降チェックがいらないので便利です。また、lazy messageとしてIllegalArgumentExceptionが発生した時のメッセージを指定できるのも良いですね。\ncheckNotNullの場合も機能的にはrequireNotNullと変わらないですが、nullの場合に投げる例外がIllegalStateExceptionとなります。なので、用途に合わせてこの二つを分けて使えますね。\n他に使えるものとしてはrequireがあります。こちらは条件式を渡すことで、nullチェック以外のこともできます。なので、以下のコードのように、Int型のデータに対して範囲をチェックするということもできるようになります。\nfun doSomething(parameter: Int) { require(parameter \u0026gt; 100) { \u0026#34;$parameterは大きすぎます\u0026#34; } // ... } 他にも、Elvis operatorを使う方法もありますね。この場合は、nullの場合にただ例外を投げるだけでなく、代替となる処理を書くことができますので色々と活用できる余地があります。例えば以下のようなことができますね。\nfun doSomething(parameter: String?): String { val checkedParameter = parameter ?: \u0026#34;default\u0026#34; val result = someRepository.find(checkedParameter) return result ?: throw CustomException(\u0026#34;結果がnullです\u0026#34;) } Listの分割 とある条件と一致するデータをListから抽出したい場合は、filterのようなoperationを使うことでできます。しかし、条件が二つだとどうすればいいでしょうか。正確には、一つのリストに対して、指定した条件に一致する要素とそうでない要素の二つのリストに分離したい場合です。\nこういう場合はとりあえず下記のように2回ループさせる方法があると思いますが、これはあまり効率がよくないです。\nval origin = listOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) // 奇数を抽出 val odd = origin.filter { it % 2 != 0 } // 偶数を抽出 val even = origin.filter { it % 2 == 0 } ループを減らすためには、あらかじめ宣言したリストに対してループの中で分岐処理を行うという方法があるでしょう。例えば以下のようにです。\nval origin = listOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) // 奇数と偶数のリストを宣言しておく val odd = mutableListOf\u0026lt;Int\u0026gt;() val even = mutableListOf\u0026lt;Int\u0026gt;() // ループ処理 origin.forEach { if (it % 2 != 0) { odd.add(it) // 奇数のリストに追加 } else { even.add(it) // 偶数のリストに追加 } } 幸い、この状況にぴったりな方法をKotlinのスタンダードライブラリが提供しています。partitionというoperationです。このopreationを使うと、元のリストの要素を条件に一致するものとそうでないもので分割してくれます。\nまた、partition戻り値はPair\u0026lt;List\u0026lt;T\u0026gt;, List\u0026lt;T\u0026gt;\u0026gt;なので、destructuring-declarationと組み合わせることでかなり短いコードになります。実際のコードは以下のようになるりますが、かなりスマートですね。\nval origin = listOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) val (odd, even) = origin.partition { it % 2 != 0 } // 条件に一致するものと一致しないものでリストを分離 最後に Kotlinは便利ではありますが、言語自体が提供する便利さ（機能）が多いゆえに、APIの使い方を正しく活用できるかどうかでコードのクォリティが左右される部分が他の言語と比べ多いような気がしています。さらにバージョンアップも早く、次々と機能が追加されるのでキャッチアップも大事ですね。\nでも確かに一つづつKotlinでできることを工夫するうちに、色々とできることが増えていく気もしていますね。研究すればするほど力になる言語を使うということは嬉しいことです。ということで、これからもKotlinで書いてみたシリーズは続きます。\nでは、また！\n","date":"2021-06-28T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-code-in-my-style-2/","title":"Kotlinで書いてみた〜その二〜"},{"content":"とある処理を書く方法が色々ある場合は、どれを選んだ方がもっとも良いかと悩ましくなります。こういう場合は、コードの読みやすさやコードの長さ、予想される問題のようなさまざまな観点からそれぞれの方式を比較してどれを選ぶか判断することになりますね。ただ、このような観点から判断するのは多くの場合「書き方が全く違う」場合に有効であって、そもそも似たようなコードを書くことになる場合は他の観点からも考える必要があります。ほんの少しだけ違うから、見た目だけでは違いがわからない場合。こういう時はそのAPIの内部、メカニズムからちゃんと考えて選ぶ必要がありますね。\nそういう意味で、今回はKotlinのCollectionの処理に使える方法の二つ、「Collectionのoperation直接使う」場合と「Sequenceに変換してから処理する」場合の違いに関して述べたいと思います。\n処理方式の違い Javaでは、Collectionの要素を持って処理をする方法は色々とありますが、大きく分けて1.8以前の方法(forやwhileなどを利用したループ)と1.8以降の方法(Streamを使った方法)があると言ってもいいのではないかと思います。この二つの方法はそもそもベースとなっているパラダイムそのものが違うので、コードを書くスタイルから大きく違います。例えば同じ処理をしたい場合でも、以下のコードで確認できるように、見た目が完全に違います。\n// forループの場合 List\u0026lt;String\u0026gt; filterEven() { List\u0026lt;Integer\u0026gt; list = List.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); List\u0026lt;String\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (Integer i : list) { if (i % 2 == 0) { result.add(i.toString()); if (result.size() == 3) { break; } } } return result; } // Streamを使う場合 List\u0026lt;String\u0026gt; filterEvenStream() { return List.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) .stream() .filter(i -\u0026gt; i % 2 == 0) .map(i -\u0026gt; i.toString()) .limit(3) .collect(Collectors.toList()); } Streamを使った処理の場合はoperationを積み重ねて行く形をしていますが、これは現代の関数型プログラミングに対応している言語ならどれも持っているAPIといえます。例えばKotlin, 公式的には呼び方が色々あるようですが、一部ではFunctional functionという名で呼ばれているようで、今回はこの操作方式、Functional functionについて述べたいと思います。\nKotlinではCollectionでもこのようなopreationがあり、Kotlin版のStreamとも言えるSequenceでも同様の操作できるようになっています。また、JavaのStreamをそのまま使うこともできるので、Functional functionを使った処理は三つがあるといえますね。それぞれの使い方もあまり変わりません。なので以下のようなコードで同じ処理ができますが、それが帰って悩ましくなるところでもあります。「どれを使ったらいい？」とですね。例えば同じ処理をしたい場合でも、Kotlinでは以下のように色々な方法を使えます。\n// Collectionの場合 fun filterEven(): List\u0026lt;String\u0026gt; = listOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10).filter { it %2 == 0 }.map { it.toString() }.take(3) // Sequenceを使う場合 fun filterEvenSequence: List\u0026lt;String\u0026gt; = listOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10).asSequence().filter { it %2 == 0 }.map { it.toString() }.take(3).toList() // JavaのStream APIを使う場合 fun filterEvenStream(): List\u0026lt;String\u0026gt; = listOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10).stream().filter { it %2 == 0 }.map { it.toString() }.limit(3).collect(Collectors.toList()) 上記のコードは見た目ではあまり違いがわからないですね。処理やロジックが大きく変わる訳でもありません。使い方があまり変わらなく、結果としても同じようなものを期待できるとしたら、やはり次に気にすべきは「性能」ではないかと思います。特にCollectionよりもSequenceの方がより性能がいいという話もありますので、それならなるべくSequenceを使った方が絶対良いはずですね。\nしかし、それを事実と受け止めるとしたら、いくつかの疑問が残ります。常にSequenceの方が性能で有利だとしたら、なぜCollectionからFunctional functionを呼び出す時は内部でSequenceに変換するようにするのでなく、わざわざasSequence()を呼び出して明示的な変換をさせるのでしょうか？もしくはなぜCollectionでもFunctional functionを呼び出せるようにしているのでしょうか？これはつまり、SequenceがCollectionよりも性能がよくなるのは「とある条件下に限る」ということではないでしょうか。なので、今回は主に性能の観点から、CollectionとSequenceの違いについて述べましょう。\nLazy evaluation KotlinのSequenceは、元々JavaのStreamと同じ名前になる予定だったそうです。これはただの偶然ではなく、実際の処理もStreamに似ているからです。何が似ているかというと、Lazy evaluationという概念です。これは簡単に言いますと、「なるべく処理を遅延させる = 必要とされるまでは処理をしない」ということですね。そして多くの場合、Sequenceを使うとこのLazy evaluationのおかげで性能がよくなるという話があります。これはつまり、Sequenceは処理を遅延することでCollectionより良い性能を期待できる、ということになるでしょう。\nしかし、単純に処理を遅延させることががなぜ性能を向上させる事になるのか、すぐに納得は行きません。まず、ループ処理の中で「必要によって処理をするかどうかを決定する」という概念がピンと来ないですね。我々が認識しているループ処理とは、対象となるデータモデルの全要素を巡回しながら処理をするという意味ですので。\nだからSequenceを使った方が性能がよくなると言っても、パフォーマンスはさまざまな要素によって劣化も向上もするものなので、その話だけを信じて全ての処理をSequenceに変えるということは危ないです。そもそもSequenceがそんなに良いものであれば、全てのIterableなオブジェクトをなぜSequenceとして処理しないか、という疑問も湧いてきますね。なので、まずはCollectionとSequenceでFunctional functionがどう違うか、コードどそれを実行した結果で説明したいと思います。\nEager evaluationのCollection CollectionでのFunctional functionは、Eager evalutionと言われています。これはLazy evaluationの逆で、必要とされてなくてもとりあえず処理を行っておくということです。こうする場合期待できることは、メモリ上にすでに処理の結果が残っていて、複数回呼ばれた場合はそのキャッシュを使うことができるということですね。\nEager evaluationだと、Functional functionが呼ばれるたび、その全要素に対しての処理をまず行うことになります。例えば、以下のような処理を書いたとしましょう。onEach()は処理の流れを視覚化するためのものです。\nlistOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) .filter { it %2 == 0 } .onEach { println(\u0026#34;Found even: $it\u0026#34;) } .map { it.toString() } .onEach { println(\u0026#34;Now $it is String\u0026#34;) } .take(3) .onEach { println(\u0026#34;$it has taken\u0026#34;) } このコードを実行した結果は以下の通りです。\nFound even: 2 Found even: 4 Found even: 6 Found even: 8 Found even: 10 Now 2 is String Now 4 is String Now 6 is String Now 8 is String Now 10 is String 2 has taken 4 has taken 6 has taken つまり、CollectionでのFunctional functionでは以下のような順で処理します。\nListからfilterのpredicateに当てはまる要素を探し、その結果でListを作る filterされたListの要素をmapし、その結果でListを作る mapされたListの要素からtakeする これを絵で表現すると以下の通りです。\n出典：Kotlin公式ドキュメント - Sequences\nCollectionのopreation Collectionでの処理は上記の通りですが、実装としてはどうでしょうか。ここではCollectionでのmap()のコードを見ていきたいと思います。コードとしては以下のようになっています。\npublic inline fun \u0026lt;T, R\u0026gt; Iterable\u0026lt;T\u0026gt;.map(transform: (T) -\u0026gt; R): List\u0026lt;R\u0026gt; { return mapTo(ArrayList\u0026lt;R\u0026gt;(collectionSizeOrDefault(10)), transform) } mapTo()という関数に、元のCollectionのサイズで新しくインスタンスを作成したArrayListとLambdaを渡しています。ちなみにcollectionSizeOrDefault()という関数は、以下のような実装となっています。Collectionである場合はそのサイズを、そうでない場合（Sequenceなど）はデフォルトとして10のサイズを持つListになるということがわかりますね。\ninternal fun \u0026lt;T\u0026gt; Iterable\u0026lt;T\u0026gt;.collectionSizeOrDefault(default: Int): Int = if (this is Collection\u0026lt;*\u0026gt;) this.size else default また、mapTo()という関数の中では、元のCollectionをループしながら新しいListにLambdaの実行結果を追加するという実装となっています。実際のコードは、以下の通りです。\npublic inline fun \u0026lt;T, R, C : MutableCollection\u0026lt;in R\u0026gt;\u0026gt; Iterable\u0026lt;T\u0026gt;.mapTo(destination: C, transform: (T) -\u0026gt; R): C { for (item in this) destination.add(transform(item)) return destination } ここでわかるのは、一つのFunctional functionが呼ばれるたびにListに対するループが発生し、さらに新しいListを作ることになるということです。なので上記のサンプルコードの場合だとループは6回、Listの作成は4回であるといえます。onEach()を除外するとしてもループは3回なので、かなり多い印象ですね。\nここで考えられるものは、「Sequenceの方が性能がいい」という話は、Sequenceを使った場合にこのようなループ回数やListの作成を減らせられるということになるのではないか、ということですね。Sequenceではどんな処理をしていて、実際にこのようなループやListを作る回数などを減らしているということでしょうか。同じ処理を書いた場合にSequenceではどのようなことが起きるかを見ていきましょう。\nLazy evaluationのSequence Collectionは、asSequence()を呼び出すことで簡単にSequenceによる処理に変換することができます。ただ、このコードを実際に走らせるためにはJavaのStreamと同じく終端処理が必要となるのがポイントです。これも「必要とされるまでは実際の処理を行わない」Lazy evaluationの特徴といえます。例えば以下のようなコードを書いたとしましょう。\nlistOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) .asSequence() // Sequenceに変換 .filter { it %2 == 0 } .onEach { println(\u0026#34;Found even: $it\u0026#34;) } .map { it.toString() } .onEach { println(\u0026#34;Now $it is String\u0026#34;) } .take(3) .onEach { println(\u0026#34;$it has taken\u0026#34;) } .toList() // Collectionに再変換（終端処理で処理を走らせる） このコードを実行した結果は以下の通りです。Collectionの場合と結果は同じであるものの、処理の順番が変わっていることを確認できます。\nFound even: 2 Now 2 is String 2 has taken Found even: 4 Now 4 is String 4 has taken Found even: 6 Now 6 is String 6 has taken ここでわかるのは、そもそも8と10に対しての処理は行われてないということです。これはCollectionで全要素に対して一つのFunctional functionの処理が終わったあと、次のFunctional functionが実行される構造に対して、Sequenceは一つの要素に対しての全ての処理が終わったあと次の要素に対して同じ処理を繰り返しているということです。言葉で表現すると複雑ですが、以下のような順になっているということです。\nListの要素にfilterを当てる 要素がfilterのpredicateに当てはまるものなら次の処理に移行する filterされた要素をmapする mapされた要素をtakeする 次の要素に対して同じ処理を繰り返す これを絵で表現すると以下の通りです。\n出典：Kotlin公式ドキュメント - Sequences\n処理の順番や仕組みが違うので、Collectionの時とは実装もかなり違うだろうと予想ができますね。では、こちらの実装を見ていきましょう。\nSequenceでのoperation Collectionと同じく、Sequenceのmap()の実装を覗いてみましょう。先程のコードでSequenceのmap()は中間処理であり、新しいCollectionを作り出すわけではないということはわかりました。実装を見ると、以下のようになっていて、処理結果が反映されたSequenceを返しているのがわかります。\npublic fun \u0026lt;T, R\u0026gt; Sequence\u0026lt;T\u0026gt;.map(transform: (T) -\u0026gt; R): Sequence\u0026lt;R\u0026gt; { return TransformingSequence(this, transform) } ただ、内部でTransformingSequenceという新しいSequenceのインスタンスを作成しているのがわかりますね。このクラスの実装は以下の通りです。ここでループごとにLambdaの実行が行われていますね。\ninternal class TransformingSequence\u0026lt;T, R\u0026gt; constructor(private val sequence: Sequence\u0026lt;T\u0026gt;, private val transformer: (T) -\u0026gt; R) : Sequence\u0026lt;R\u0026gt; { override fun iterator(): Iterator\u0026lt;R\u0026gt; = object : Iterator\u0026lt;R\u0026gt; { val iterator = sequence.iterator() override fun next(): R { return transformer(iterator.next()) } override fun hasNext(): Boolean { return iterator.hasNext() } } internal fun \u0026lt;E\u0026gt; flatten(iterator: (R) -\u0026gt; Iterator\u0026lt;E\u0026gt;): Sequence\u0026lt;E\u0026gt; { return FlatteningSequence\u0026lt;T, R, E\u0026gt;(sequence, transformer, iterator) } } 以上のコードの実行結果と実装でわかるように、Sequenceを使う場合は一つの要素を単位として処理を行っていくので、CollectionでFunctional functionを使う場合に発生し得る不要な処理(毎回Listを生成する、前要素に対してmapを行うなど)を減らせると期待できます。なので元のCollectionが大きい場合やoperationが多い場合はSequenceの方がより良いように見えます。\nただ、性能の観点で考えると、CollectionとSequenceの違いはもう一つ考慮すべきところがあります。それはデータ構造の違いです。\nStateless JavaのStreamでもそうでしたが、Sequenceは状態(State)を持たないのが特徴です。ここで状態を持たないということは、持っている要素の数や順番などに対しての情報がないということを意味します。なぜかというと、SequenceがIteratorに基づいているものだからです。そしてそれが原因で、処理の種類によってCollectionよりも性能は劣る可能性もまたあります。\n先に使っていたサンプルコードを持って考えてみましょう。サンプルコードでは、Sequenceの終端処理としてListを返すためにtoList()を呼び出していました。これは、「状態を持たない」ものから「状態を持つ」ものに変換することですね。簡単なやり方としては、MutableなListを作って、全要素を一つづつadd()していく方法があるでしょう。実際はどうでしょうか？まずはtoList()のコードをみてみましょう。以下がその実装です。\npublic fun \u0026lt;T\u0026gt; Sequence\u0026lt;T\u0026gt;.toList(): List\u0026lt;T\u0026gt; { return this.toMutableList().optimizeReadOnlyList() } まずMutableなListに変換して、さらに読み込み専用（Immutable）のListに変換しているように見えます。さらにMutable Listに変えているところの実装をみてみましょう。\npublic fun \u0026lt;T\u0026gt; Sequence\u0026lt;T\u0026gt;.toMutableList(): MutableList\u0026lt;T\u0026gt; { return toCollection(ArrayList\u0026lt;T\u0026gt;()) } ArrayListのインスタンスを作って、それをtoCollection()に渡していますね。ここでtoCollection()はSequenceをCollectionに帰る時の共通処理で、型を指定にするため引数にListを渡しているようです。さらにtoCollection()の実装をみていきましょう。\npublic fun \u0026lt;T, C : MutableCollection\u0026lt;in T\u0026gt;\u0026gt; Sequence\u0026lt;T\u0026gt;.toCollection(destination: C): C { for (item in this) { destination.add(item) } return destination } ここまでたどり着いてわかったのは、やはりSequenceの要素を一つ一つListの中に入れているということですね。ただ、単純な処理ではありますが、ここでは「Listに要素を足していく」ということ自体に注目する必要があります。\n先に述べた通り、Sequenceは自分が持つ要素の数をわからないので、Listのインスタンスを作る時はサイズを「仮定」して処理するしかないです。そして基本的にMutableなListでは、現在のサイズよりも多くの要素を追加する必要がある時、内部のArrayより大きいサイズのArrayを新しく作り、そこに要素をコピーしていくことを繰り返します。そしてこれを全要素が揃うまで繰り返していきますね。ということは、Sequenceの要素が多ければ多いほどArrayのインタンス作成とコピーが多くなるということになります。\nそしてコピーが全部終わった場合、実際の要素数よりArrayのサイズが大きい場合もありますね。その場合、メモリを無駄に使うだけでなく、実際のサイズもわからなくなるので、サイズを要素数に合わせて再調整する必要があります。toList()の実装で最後にoptimizeReadOnlyList()を呼び出しているのは、おそらくその理由でしょう。optimizeReadOnlyList()の実装は以下の通りです。やはりサイズを再調整していますね。\ninternal fun \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt;.optimizeReadOnlyList() = when (size) { 0 -\u0026gt; emptyList() 1 -\u0026gt; listOf(this[0]) else -\u0026gt; this } これでわかるように、Sequenceを使って処理したあと、Collectionにまとめるなら要素数が多ければ多いほどCollectionよりも性能が劣化する可能性は確かに存在します。CollectionでFunctional functionを呼び出す際にListを作るとしても、すでに要素数はわかっているので、Listのサイズが合わないためのArrayの生成とコピーの処理は不要ですね。なのでCollectionとSequenceのどちらを選ぶかの問題はFunctional functionを呼び出す回数や処理の種類だけでなく、要素の数まで考える必要がありそうです。\nただ、要素数が多い場合でも、終端処理の種類によってはSequenceの方が有利になる可能性もなくはないです。例えばforEach()やonEach()など、個別の要素に対して処理を行うだけの場合は依然としてSequenceの方で良い性能を期待できるでしょう。\n要素数が多い場合に性能に影響する処理としてもう一つ考えられるのは、Sequenceを使う場合でも呼び出せるFunctional functionの中で明らかに「状態を必要とする」ものがあるということです。例えば以下の一覧のようなものです。\nどんな要素が含まれているかわかる必要がある distinct() average() min() max() take() 要素の順番をわかる必要がある indexOf() mapIndexed() flatMapIndexed() elementAt() filterIndexed() foldIndexed() forEachIndexed() reduceIndexed() scanIndexed() これらの処理をSequenceではどうしているのでしょうか。まずはその実装を覗いてみる必要がありそうですね。ここではsort()の方をみていきたいと思います。実装は以下のようになっています。\npublic fun \u0026lt;T : Comparable\u0026lt;T\u0026gt;\u0026gt; Sequence\u0026lt;T\u0026gt;.sorted(): Sequence\u0026lt;T\u0026gt; { return object : Sequence\u0026lt;T\u0026gt; { override fun iterator(): Iterator\u0026lt;T\u0026gt; { val sortedList = this@sorted.toMutableList() sortedList.sort() return sortedList.iterator() } } } 単純ですが、Sequenceを一度Listに変換してsortした後、またSequenceに変えて返していますね。ここでListに変えるために呼び出している関数はtoMutableList()なので、結局toList()を呼び出す場合と同じようなことが起きるということです。なので、状態を必要とする操作の場合は要素数が多ければ多いほど性能はCollectionより劣化しやすい、ということがわかります。\nただ、逆に状態が必要にならない場合は、Collectionと違って中間結果のListを作成しなくなるので、依然としてSequenceが良い性能を見せるだろうと思えます。\n最後に だいぶ話が長くなりましたが、性能の観点でどれを選ぶべきか、という話の結論としては、「どんな処理をするか」によるということになりますね。簡単に整理すると、以下のようになるかと思います。\n条件 おすすめ 処理が複雑 Sequence 処理した結果としてCollectionが必要 Collection ループするだけ Sequence 処理に状態が必要 Collection 要素数が多い Sequence 要素数が少ない Collection もちろんこれらの条件が複数ある場合も十分考えられるので、必要な処理が何かをよく考えてどちらを使うかを慎重に考える必要がありそうです。多くの場合とりあえずCollectionを使うという方針だとしても特に問題はなさそうな気はしますが…\nこの度はKotlinにおいてのSequenceを紹介しましたが、実はイラストを含めてわかりやすく説明しているいつSequenceを使うべきかという良い記事があるので、Sequenceについてより深く理解したい方にはこちらを参考した方が良さそうな気がします。\nまた、ここではKotlinのAPIでの処理のみを紹介しましたが、JavaのStreamを使う場合、Sequenceと違ってparallelStream()を呼び出すことができます。なので並列で処理しても良い場合には、CollectionとSequenceのみでなく、Streamを使うことを検討するのもありですね。\nでは、また！\n","date":"2021-06-13T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-collection-and-sequence/","title":"Sequenceは常にいいか"},{"content":"以前、Spring WebFluxに関するポストを書いたことがあって、そこで少しだけMVCパターン(Controller/Service)とFunctional Endpoint(Router/Handler)に関して触れました。結論だけ先に述べますと、Functional Endpointの導入はMVCパターンは長く使われている良いパターンでありますが、性能や関数型プログラミングには適してないのという問題があるので、それを改善するためのものだといえます。\nさて、その説明だけだと、Spring WebFluxを使う際にはなるべくFunctional Endpointを使うべきな気もします。しかし、実際はどうでしょうか？例えば、従来のSpring MVCと同じくController/Serviceを使う場合は本当にRouter/Handlerを使う時と比べ性能が劣るのか？また、Functional Endpointを使う際に考慮すべき、「MVCパターンにはなかった問題」はないか？といったことを考えられます。\nなので、今回はその二つのパターンを用いて、Spring WebFluxによるサーバサイドアプリケーションを実装するときに考えたいことを少しまとめてみました。\nプログラミングのパラダイムとして SpringのMVCパターンは、アノテーションによるメタプログラミングとオブジェクト指向といった昔ながらの考え方に基づいたパラダイムに近いといえます。もちろん、AOPやDIといったSpring Framework特有の特徴はありますが、Reactive Streamsの実現体であるMono/Fluxで書かれたWebFluxのコードと比べたら、まだ伝統的な書き方に近いという感覚はありますね。\nここでオブジェクト指向と関数型のうち、どれが良いかという議論はしません。また、Javaは元々オブジェクト指向の言語としてデザインされましたが、1.8以降はFunctional Interfaceの導入である程度関数型プログラミングができるようになりましたし、Kotlinでもそれは大きく変わらないことです。なので、Spring MVCとSpring WebFluxのうちどれかを選ぶということがコードをオブジェクト指向として書くか、関数型として書くかという結論にもなりません。\nしかし、Spring WebFluxでは、MVCパターンとFunctional Endpointのどれかを選べるという点からして、どちらかのパラダイムに寄せた書き方はできるというのも事実です。ここでどれを取るかを判断するには、コードを書く人同士の合意がもっとも重要なのではないかと思います。なぜかというと、結局プログラミングのパラダイムというものは何よりもプログラミングの「効率」のために発展してきたからです。\nなので、ここでの判断基準は「如何に読みやすいか」「如何に早く成果を出せるか」など、実利的なものとなるべきでしょう。例えば、すでにサービスとして機能しているアプリケーションの同時実行性能を向上させたい場合は、MVCパターンとして書いた方がすぐにサービスを立ち上げられるので良いと思ったら、それで理由は十分かと思います。もしくは、すでにFunctional Endpointに慣れているプログラマが多い場合は積極的にそれを導入するとかですね。つまり、私の観点からすると、プログラミングのパラダイムは実務者の立場からすると効率により選択するべきものではないかと思います。\nでは、Contorller/ServiceのパターンとRouter/Handlerのパターンの実際はどう違うのかを、コードを通じて見ていきたいと思います。\nMVCパターンで書く場合 Spring WebFluxのMVCパターン、つまりContoller/Serviceのパターンは、その名の通り既存のSpring MVCと比べあまり変わらない感覚で書くことができます。なので、例えば以下のようなコードを書くとしたら、これだけではSpring MVCとの違いがあまりわからないくらいですね。\n@RestController class HelloController(private val service: HelloService) { @GetMapping(\u0026#34;/hello\u0026#34;) fun hello(): ResponseEntity = ResponseEntity .ok() .contentType(MediaType.APPLICATION_JSON) .body(service.getHello()) } @Service class HelloService { fun getHello(): String = \u0026#34;Hello Spring WebFlux\u0026#34; } ただ、Spring WebFluxでは、DB接続を含め完全なノンブロッキングを実現するためには[R2DBC]のようなノンブロッキング対応のAPIを使う必要があります。これはつまり、Reactive Streamを使う必要があるとのことであって、必然的にその実現体であるMono/Fluxを使う必要があるということです。\nなので、とりあえずRepositoryからMono/Fluxを取得し、Reactive Stream固有の書き方に合わせてコードを書いていくしかないということになります。問題は、Reactive Streamはその名前から普通にJavaのStreamの感覚で扱えば良い印象ですが、実際の処理はそう簡単じゃないということです。例えば、JPAやMyBatisのような既存のブロッキングベースのAPIを使う場合は、Serviceのメソッドでは以下のようなコードを書くことになりますね。\n// ユーザIDでユーザ情報とメール送信履歴を取得する fun getMemberWithMailRecord(memberId: Int): MemberWithMailRecord { // ユーザ情報を取得する val member = memberRepository.getMember(id = memberId) ?: throw RuntimeException(\u0026#34;Can\u0026#39;t find member\u0026#34;) // ユーザが作成したメール送信履歴を取得する val mailRecord = mailRecordRepository.getMailRecord(memberId = memberId) ?: throw RuntimeException(\u0026#34;Can\u0026#39;t find mailRecord\u0026#34;) // ユーザ情報とメール送信履歴を合わせて返却 return MemberWithMailRecord( member = member, mailRecord = mailRecord ) } しかし、Mono/Fluxを返すAPIを使う場合は、以下のようなコードになります。\nfun getMemberWithMailRecord(memberId: Int): Mono\u0026lt;MemberWithMailRecord\u0026gt; = memberRepository.getMember(id = memberId) .switchIfEmpty(Mono.error(RuntimeException(\u0026#34;Can\u0026#39;t find member\u0026#34;))) .zipWith(mailRecordRepository.getMailRecord(memberId = memberId).switchIfEmpty(Mono.error(RuntimeException(\u0026#34;Can\u0026#39;t find mailRecord\u0026#34;))) .map { MemberWithMailRecord( member = it.t1, mailRecord = it.t2 ) } やっていることは同じでも、果たしてこれが書きやすく、読みやすいコードであるかどうかは悩ましいですね。他にもFluxで取得したデータをListに変えたい場合、取得したMonoのデータでさらにMonoを取得したい場合など、より複雑な処理が必要な場面ではますます書き方は複雑になります。\n幸い、KotlinにはCoroutinesがあるので、このような複雑な書き方をより簡単に書くことはできます。Corutinesを適用したら、Mono/Fluxを使う場合でも上記のコードは以下のようになりますね。\nsuspend fun getMemberWithMailRecord(memberId: Int): MemberWithMailRecord { val member = memberRepository.getMember(id = memberId).awaitFirstOrNull() ?: throw RuntimeException(\u0026#34;Can\u0026#39;t find member\u0026#34;) val mailRecord = mailRecordRepository.getMailRecord(memberId = memberId).awaitFirstOrNull() ?: throw RuntimeException(\u0026#34;Can\u0026#39;t find mailRecord\u0026#34;) return MemberWithMailRecord( member = member, mailRecord = mailRecord ) } Coroutinesを使う場合はスコープの指定が必要となるのが一般的ですが、実際はControllerのメソッドまでをsuspendとして定義しておくと良いみたいです。ただ、既存のプロジェクトをSpring MVCからWebFluxに移行する場合にこうやって多くの処理をsuspendメソッドにすると、ユニットテストの方を直すのが大変になる可能性もあるのでそこは要注意です。\nFunctional Endpointで書く場合 続いて、Functional Endpontを使う場合のコードです。MVCパターンの問題としてアノテーションがあげられていますが、Router/Handlerでもアノテーションを使うことはできますし、アプリケーションのアーキテクチャによっては必然的にクラスを分けてアノテーションで管理することになるのが一般的かなと思います。なので、そのようなケースではRouterを@Beanとして登録したり、Handlerも@Componentとして定義する場合もあります。そういう場合は、以下のようなコードになります。\n@Configuration class HelloRouter(private val handler: HelloHanlder) { @Bean fun hello(): router { GET(\u0026#34;/hello\u0026#34;, handler::getHello) } } @Component class HelloHandler { fun getHello(request: ServerRequest): Mono\u0026lt;ServerResponse\u0026gt; = ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(Mono.Just(\u0026#34;Hello Spring WebFlux\u0026#34;)) } Functional Endpointを使う場合の特徴は、RouterはあくまでエンドポイントとHandlerをつなぐ役割をするだけなので、HandlerでServerRequestを受け取りServerResponseを返す処理までを書くことになるということです。MVCパターンではRestControllerの戻り値としてResponseEntityや自分で定義したクラスを自由に指定でるし、Serviceではビジネスロジックだけを担当するパターンが多いのを考えるとかなり独特であるといえます。\nこのようにServerRequestとServerResponseを使うため、HandlerはServiceと比べビジネスロジック部分が一回層が深くなった形になります。ServerResponseのbodyでロジックを書いて、それを返す形ですね。例えば以下のようなコードになります。\nfun getMember(request: ServerRequest): Mono\u0026lt;ServerResponse\u0026gt; = ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(memberRepository.getMember(id = request.PathVariable(\u0026#34;id\u0026#34;)) .switchIfEmpty(Mono.error(RuntimeException(\u0026#34;Can\u0026#39;t find member\u0026#34;))) .zipWith(mailRecordRepository.getMailRecord(memberId = request.PathVariable(\u0026#34;id\u0026#34;)).switchIfEmpty(Mono.error(RuntimeException(\u0026#34;Can\u0026#39;t find mailRecord\u0026#34;))) .map { MemberWithMailRecord( member = it.t1, mailRecord = it.t2 ) } ) この場合でもでもCoroutinesを使うことはもちろんできます。Corutinesを使う場合は、以下のような書き方ができるでしょう。\nsuspend fun getMember(request: ServerRequest): Mono\u0026lt;ServerResponse\u0026gt; { val member = memberRepository.getMember(id = memberId).awaitFirstOrNull() ?: throw RuntimeException(\u0026#34;Can\u0026#39;t find member\u0026#34;) val mailRecord = mailRecordRepository.getMailRecord(memberId = memberId).awaitFirstOrNull() ?: throw RuntimeException(\u0026#34;Can\u0026#39;t find mailRecord\u0026#34;) return ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .bodyValueAndAwait( Mono.just( MemberWithMailRecord( member = it.t1, mailRecord = it.t2 ) ) } } 性能 MVCパターンの問題としてあげられるものの一つとして、アノテーションがあります。アノテーションを使うということは、必然的にリフレクションを使うことになるので、自然に性能の低下にもつながるという話ですね。これだけみると、WebFluxではMVCパターンよりもFunctional Endpointを使ったほうが性能でも有利であるように見えます。しかし実際はどうでしょうか？\nSpringの公式ドキュメントでは、Functional Enddpointのことをlightweight(軽量)とは表現しているものの、それ以外に性能がどうという話は一切述べてないです。多くの場合、性能の比較はSpring MVCとSpring WebFluxを対象としていて、WebFluxでのMVCパターンとFunctional Endpointのケースはあまり探せませんでした。なので、ここでは簡単にベンチマークを行うことで二つのパターンでの性能の違いを検証してみました。\nベンチマークツールとしては、Jmeterを使うこともできましたが、短いコマンドで測定ができるのもあり、今回はApache HTTP server benchmarking toolを使ってテストを実施しています。\n使ったコード 性能測定として知りたいのは「実装パターンで性能が変わるか」ということなので、あえてDB接続は排除しました。比較のために作ったサンプルアプリケーションでは、単純にデータを生成する共通のロジックと、それを返すだけのContoller/Service, Router/Hanlderのセットで構成されています。\n共通 データを生成するロジックそのものは共通で、単純にループでListを生成するようにしています。\n// 固定値のデータを生成し返すクラス object DataCreationUtil { // 1970年1月1日から2021年12月31日まで var data: List\u0026lt;Person\u0026gt; = (0..18992) .map { Person( id = it.toLong(), name = \u0026#34;Person${it}\u0026#34;, birth = LocalDate.ofEpochDay(it.toLong()), joined = LocalDate.ofEpochDay(it.toLong() + 10000) ) }.toList() } // 生成されるデータ data class Person( val id: Long, val name: String, val birth: LocalDate, val joined: LocalDate ) Controller/Serviceの実装 MVCパターンについてはコードだけでも十分わかると思いますので、説明は割愛します。\n@RestController class PerformanceTestController( private val service: PerformanceTestService ) { @GetMapping(\u0026#34;/performance-controller\u0026#34;) fun getData(): ResponseEntity\u0026lt;List\u0026lt;Person\u0026gt;\u0026gt; = ResponseEntity .ok() .contentType(MediaType.APPLICATION_JSON) .body(service.getData()) } @Service class PerformanceTestService { fun getData(): List\u0026lt;Person\u0026gt; = service.getData() } Router/Handlerの実装 Functional Endpointでは、MVCパターンと比べ処理と言えるものは全部Handlerの方に書かれてある、という違いがあります。\n@Configuration class PerformanceTestRouter(private val handler: PerformanceTestHandler) { @Bean fun route() = router { GET(\u0026#34;/performance-router\u0026#34;, handler::getData) } } @Component class PerformanceTestHandler { fun getData(request: ServerRequest): Mono\u0026lt;ServerResponse\u0026gt; = ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(Flux.fromIterable(DataCreationUtil.data)) } テスト結果 テストは以下のような条件で実施しました。\nユーザ数は5000, ユーザごとのリクエストは50に設定 ワームアップ時間を考慮して、パターンごとにテストを分ける サーバの再起動後にテストを実施 テストは10回ループ 実際に使ったスクリプトは以下のようなものです。サーバの起動後にこれを実行し、10回のループが終わったら再起動後にFunctional Endpointのテストを実施しています。\n#!/bin/bash for i in {1..10} do ab -n 5000 -c 50 http://localhost:8080/performance-controller done ただ、こうやってもやはりテスト結果では周回ごとに偏差があったので、ここでは中間値に当てはまる結果を紹介します。その結果は、以下の通りになりますが、あらかじめ結論だけ先に言いますとMVCパターンでもFunctional Endpointでもその性能の違いというものは「誤差範囲以内」と表現しても良いかなと思います。\nController/Serviceの結果 Server Software: Server Hostname: localhost Server Port: 8080 Document Path: /performance-controller Document Length: 1440242 bytes Concurrency Level: 50 Time taken for tests: 24.989 seconds Complete requests: 5000 Failed requests: 0 Total transferred: 7201590000 bytes HTML transferred: 7201210000 bytes Requests per second: 200.09 [#/sec] (mean) Time per request: 249.892 [ms] (mean) Time per request: 4.998 [ms] (mean, across all concurrent requests) Transfer rate: 281433.26 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 1 1.4 0 11 Processing: 29 248 79.8 242 516 Waiting: 24 192 60.8 185 442 Total: 29 249 79.4 243 516 Percentage of the requests served within a certain time (ms) 50% 243 66% 275 75% 295 80% 309 90% 354 95% 394 98% 430 99% 449 100% 516 (longest request) Router/Handlerの結果 Server Software: Server Hostname: localhost Server Port: 8080 Document Path: /performance-router Document Length: 1440257 bytes Concurrency Level: 50 Time taken for tests: 25.541 seconds Complete requests: 5000 Failed requests: 0 Total transferred: 7201775000 bytes HTML transferred: 7201285000 bytes Requests per second: 195.76 [#/sec] (mean) Time per request: 255.410 [ms] (mean) Time per request: 5.108 [ms] (mean, across all concurrent requests) Transfer rate: 275360.22 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 1 3.2 0 151 Processing: 33 253 80.4 246 612 Waiting: 28 194 59.8 184 475 Total: 39 254 80.0 247 613 Percentage of the requests served within a certain time (ms) 50% 247 66% 286 75% 302 80% 312 90% 361 95% 398 98% 441 99% 459 100% 613 (longest request) ドキュメンテーション 次にドキュメンテーションの観点からすると、Functional Endpointはまだ導入するには早い気がします。ここでいうドキュメンテーションは、JavaDocやKdocのようなコメントのことではなく、最近よく使われるSwaggerのことを指します。\n最近はSpringFoxなどを使うと簡単にAPIのドキュメンテーションが可能ですが、Functional Endpointだとそう簡単にはできません。すでに理由がわかる方もいらっしゃるかと思いますが、Routerには引数としてエンドポイントとHandlerの処理を渡しているだけで、Handlerは引数がServerRequest、戻り値はServerResponseに固定されてあるのが理由です。\nもちろん、ServerRequestとServerResponseを使う場合でもそれを自分の欲しいデータとして扱うことはできます。例えば、リクエストからパラメータを取る方法は以下のようになります。\n// Path Variableで渡されたIDを持ってユーザ情報を取得する suspend fun getMember(request: ServerRequest): ServerResponse { // Path Variableを取得する val id = request.pathVariable(\u0026#34;id\u0026#34;) // ... } // Request Bodyで渡されたデータを元に新しいユーザを作成する suspend fun createMember(request: ServerRequest): ServerResponse { // Request Bodyをクラスにマッピングする val form = request.bodyToMono(MemberCreateForm::class.java).awaitFirst() // ... } ただ、SpringFoxのように自動でAPIのドキュメンテーションを行ってくれるような便利なAPIは、おそらくリフレクションを使っています。なので、Handlerで実際はどんな処理が行われているかを判断するのは難しいでしょう。\n幸い、この問題はSpringの開発者も認識しているようで、springdoc-openapiを使うとFunctional EndpointでもAPIのドキュメンテーションは可能になります。ただ、この場合でも現時点ではやはり問題があります。なぜなら、これはAPIのドキュメンテーションを自動化するものではなく、「ドキュメンテーションのための手段を提供する」だけだからです。なので、以下のように、RouterやHandlerに関して一つ一つアノテーションを指定する必要があります。\n@Bean @RouterOperations( value = [ RouterOperation( path = \u0026#34;/api/v1/members\u0026#34;, beanClass = MemberHandler::class, beanMethod = \u0026#34;listMember\u0026#34;, method = [RequestMethod.GET], produces = [MediaType.APPLICATION_JSON_VALUE] ), RouterOperation( path = \u0026#34;/api/v1/members/{id}\u0026#34;, beanClass = MemberHandler::class, beanMethod = \u0026#34;getMember\u0026#34;, method = [RequestMethod.GET], produces = [MediaType.APPLICATION_JSON_VALUE] ), RouterOperation( path = \u0026#34;/api/v1/members\u0026#34;, beanClass = MemberHandler::class, beanMethod = \u0026#34;createMember\u0026#34;, method = [RequestMethod.POST], produces = [MediaType.APPLICATION_JSON_VALUE] ), RouterOperation( path = \u0026#34;/api/v1/members/{id}\u0026#34;, beanClass = MemberHandler::class, beanMethod = \u0026#34;updateMember\u0026#34;, method = [RequestMethod.PUT], produces = [MediaType.APPLICATION_JSON_VALUE] ), RouterOperation( path = \u0026#34;/api/v1/members/{id}\u0026#34;, beanClass = MemberHandler::class, beanMethod = \u0026#34;deleteMember\u0026#34;, method = [RequestMethod.DELETE], produces = [MediaType.APPLICATION_JSON_VALUE] ), ] ) fun routeMember() = coRouter { GET(\u0026#34;/api/v1/members\u0026#34;) { handler.listMember() } GET(\u0026#34;/api/v1/members/{id}\u0026#34;, handler::getMember) POST(\u0026#34;/api/v1/members\u0026#34;, handler::createMember) PUT(\u0026#34;/api/v1/members/{id}\u0026#34;, handler::updateMember) DELETE(\u0026#34;/api/v1/members/{id}\u0026#34;, handler::deleteMember) } ご覧の通り、ドキュメンテーションのためのアノテーションが実際のコードよりも長くなっています。Functional EndpointでもSwaggerを利用できる手段ができたのは良いことですが、MVCパターンと比べやはり不便ではありますね。なので、ドキュメンテーションが大事であるなら、まだFunctional Pointを使うべきではないかもしれません。\n最後に 今回は、Spring WebFluxのMVCパターン及びFunctional Endpointをコードの書き方、性能、ドキュメンテーションという観点から比較してみました。Spring WebFluxも発表されたのが2017年なので、もう今年で5年目になりますが、まだまだMVCパターンに比べては色々と補完すべき点が多い印象です。自分の場合はWebFluxのメインコンセプトであるノンブロッキングや関数型プログラミングを活かすためにはやはりFunctional Endpointを選んだ方が良さそうな気はしていますが、まだあえてそうする必要はないのではないか、という感覚です。特にエンタプライズのアプリケーションを書くことになるとますますそうでしょう。もちろん、そもそもWebFluxそのものを導入すべきかということから考える必要がありますが。\nそれでも、やはりFunctional Endpointという実装方式には色々と可能性があると思います。Spring WebFluxでなくても、最近のウェブフレームワークでは多く採用されているものですからね。例えばTechEmpowerのベンチマークでJavaのフレームワークのうちではもっとも性能がよかったjoobyでもMVCパターンとFunctional Endpointとよく似たScript Routeパターンに対応していますし、JetBrainsで開発しているKotlin用のウェブフレームワークであるKtorではMVCパターンなしで、同じくFunctional Endpointとよく似たRoutingにのみ対応しています。なので、他にもExpressやGinのようなフレームワークでも似たようなAPIの実装方法を提供しているので、余裕があったら個人的に試してみて慣れるのも良い勉強になるかもしれません。また、関数型プログラミングはこれからも幅広く使われそうなので、これを持って練習してみるのも良いかもしれませんね。\nでは、また！\n","date":"2021-05-30T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-webflux-router/","title":"WebFluxではFunctional Enpointを使うべきか"},{"content":"Kotlin(JVM)は、コンパイルした結果がJVMのバイトコードになります。Javaで書かれたライブラリをそのままKotlinで利用できるのはそのためですね。これはKotliのライブラリに対しても同じなので、スタンダードライブラリを覗くとJavaの機能に依存しているところも少なくないです。\nただ、KotlinがコンパイルしたらJVMのバイトコードになるということは、単純にKotlinが「書き方が違うJava」であるという意味ではないです。これはKotlinがJavaと言語スペックが違うという点もありますが、JVMだけでなく、JavaScriptやネイティブコードにコンパイルされることも想定してデザインされているので、スタンダードライブラリスタンダードライブラリはプラットフォームに合わせて違う実装になっています。そしてJVMだとしてもJavaのAPIをそのまま利用しているわけではありません。\nKotlinのこういう構造は、内部のソースコードを見るとはっきりします。スタンダードライブラリの一部メソッドやクラスにはexpectとactualというキーワードが使われていますが、これらはJavaのinheritanceと似ているようなものです。Javaではinterfaceで定義したメソッドを、それを継承したクラスでoverrideで実装して使うことになりますね。同じく、Kotlinではexpectとして定義された機能をプラットフォームに合わせてactualで実装しているわけです。\nまた、KotlinのスタンダードライブラリはJavaと一見同じようなものに見えるとしても、実際は違うケースもあります。actualによって実装されたコードがKotlinに合わせて、書かれているからですね。なので、Kotlinのスタンダードライブラリに対しては「Javaと同じだろう」という認識をするのは危険な可能性もあります。\n今回はそういうことで、文字列のwhitespaceに関しての機能を、スタンダードライブラリのソースコードを中心に見ていきたいと思います。\nwhitespaceの判定 とある文字列が意味のある(有効な)データであるかどうかを判定する方法の一つは、その文字列がただの空白であるかどうかを判定することです。つまり、そもそもなんのデータもなかったり、whitespaceだけでないかというチェックをするということですね。\nこういう場合の判定はKotlinのスタンダードライブラリで簡単に行うことができます。KotlinではStringのメソッドとして基本的に以下の二つを提供しています。\nisEmpty() isBlank() Java 11以降でもこれらと同名のメソッドが存在しているので、一見そのままの感覚で良さそうにも見えます。しかし、Kotlinではこれらのメソッドがまずkotlin.text.Stringsから呼ばれるものとされています。JavaのAPIをそのまま使っているわけではないので、処理も違う可能性があるという推測ができますね。\nここで前者の場合、文字列が単純になんのデータも持ってないかどうかに対する判定をおこないます。実際のソースコードを見ると、文字列の長さだけをチェックしているのを確認できます。\nちなみにJavaではStringはCharSequenceを継承していますが、Kotlinとしてもライブラリは違えどそういう継承関係は一緒です。なので、KotlinではStringのメンバーでありながらもCharSequenceの関数として書かれています。\npublic inline fun CharSequence.isEmpty(): Boolean = length == 0 後者の場合は、文字列にwhitespaceまで含めているのかを判定します。以下のコードを見ると、何をやっているかが明確でしょう。\npublic actual fun CharSequence.isBlank(): Boolean = length == 0 || indices.all { this[it].isWhitespace() } isBlank()で呼び出しているisWhitespace()は、以下のような実装となっています。\npublic actual fun Char.isWhitespace(): Boolean = Character.isWhitespace(this) || Character.isSpaceChar(this) KotlinのChar.isWhitespace()は最終的にCharacter.isWhitespace()とCharacter.isSpaceChar()を使って判定することになります。前者の場合はUnicodeのwhitespaceに当てはまるか、後者の場合はUnicodeのspace(改行コードなど)に当てはまるかを判定するJavaのAPIです。ここでわかるように、特集なケースでなければなるべくisEmpty()を使った方が文字列をチェックする時に良いでしょう。\nwhitespaceの削除 文字列が単純に意味のあるデータを持っているかどうかを判定するには、前述通りisEmpty()を使うと良いですが、文字列にwhitespaceだけでなく、意味のあるデータも混在する場合もありますね。こういう時は前後のwhitespaceを取り除きたくなります。\nJavaでは、文字列の前後のwhitespaceを消去する方法としてtrim()とstrip()がありました。前者は昔ながらのもので、全角のwhitespaceを検知できなく、性能の問題もあるのでJava 11以降は後者を使うことが推奨されています。\nただ、Kotlinの場合は少し都合が違います。Kotlinでは基本的にtrim()だけを使うことになります。まずはtrim()の実装をみていきましょう。\npublic inline fun String.trim(): String = (this as CharSequence).trim().toString() まずStringとしては、CharSequenceにアップキャストしてそのtrim()を呼び出すことにしています。そのあとは単純にtoString()で返すだけですね。\n続いて、Stringで呼ばれているCharSequence側のtrim()をみていきましょう。\npublic fun CharSequence.trim(): CharSequence = trim(Char::isWhitespace) ここでは、オーバロードした他のtrim()にisWhitespace()をメソッドレファレンスとして渡しているのがわかります。Booleanが戻り値なので、引数はPredicateであると推測できますね。続けて、こちらで呼び出しているtrim(predicate)の方を確認します。こちらのコードは以下の通りです。\npublic inline fun CharSequence.trim(predicate: (Char) -\u0026gt; Boolean): CharSequence { var startIndex = 0 var endIndex = length - 1 var startFound = false while (startIndex \u0026lt;= endIndex) { val index = if (!startFound) startIndex else endIndex val match = predicate(this[index]) if (!startFound) { if (!match) startFound = true else startIndex += 1 } else { if (!match) break else endIndex -= 1 } } return subSequence(startIndex, endIndex + 1) } ここまできてやっと実際の処理がでました。CharSequenceをループしながら左(start)から右の方にwhitespaceを探し、初めてwhitespaceでない文字を見つけたら右(end)から左の方にループしながら繰り返すという処理ですね。意外と単純ですが、効率的な処理です。\nそしてその処理での判断基準がisWhitespace()になっているわけですが、先に確認している通りこちらは最終的にJavaのAPIを呼ぶことになっているので、trim()でも十分Unicodeに定義されてあるwhitespaceやspaceまでを削除してくれると推論できます。なので、Javaとは違ってあえてstrip()を使う必要はなさそうです。\nまた、trim()は文字列の前後のwhitespaceを削除しますが、場合によっては前方のみ、後方のみで分けて使いたい場合もあるかもしれません。その時は、以下のようなことができます。\nval string = \u0026#34; string \u0026#34; // 左のみtrim println(string.trimStart()) // \u0026#34;string \u0026#34; // 右のみtrim println(string.trimEnd()) // \u0026#34; string\u0026#34; これらのメソッドは引数としてPredicateを渡すこともできるので、他の条件を自前で書く必要がある場合にはそちらを使えますね。\n他にも、whitespaceではない、前後の特定の文字(prefix、suffix)を削除してたい場合は以下のメソッドが提供されています。\nval string = \u0026#34;--hello--\u0026#34; // prefixのみ削除 println(string.removePrefix(\u0026#34;--\u0026#34;)) // \u0026#34;hello--\u0026#34; // suffixのみ削除 println(string.removeSuffix(\u0026#34;--\u0026#34;)) // \u0026#34;--hello\u0026#34; // 前後を削除 println(string.removeSurrounding(\u0026#34;--\u0026#34;)) // \u0026#34;hello\u0026#34; 改行を削除 改行が文字列の前後に入っていればtrim()で十分ですが、文字列の中に改行が含まれていて、それを変えたい場合もありますね。例えばJSONをログに一行で出力したいだったり、以下のようなMulitiline Stringを一行にまとめたい場合です。\nval string = \u0026#34;\u0026#34;\u0026#34; Hello World \u0026#34;\u0026#34;\u0026#34; Intellijだと自動的にtrimIndent()をつけてくれますが、これはあくまでインデントに関与するものであって、中の改行まではtrimしてくれないです。こういう場合は、KotlinでもJavaでも対応するメソッドは特にないので、自分で処理を書くしかないですね。例えば、以下のようなコードが使えるでしょう。\nfun String.stripLine() = replace(System.lineSeparator(), \u0026#34; \u0026#34;) ただ、Javaでも13からText Blockが導入されているので、今後はJavaのAPIの方で上記のようなメソッドが追加されることを期待できるかもしれません。\n最後に 最初にexpectとactualの話をしましたが、これらのキーワードはKotlin Multiplatformでもっとも重要な概念です。Kotlinで書いたコードをさまざまなプラットフォームで共有できるようにすることを目的としているので、こういう構造になっているのは自然ですね。なので、Kotlin/JVMだけでなく、他のことを試したい方にはとりあえず理解しておく必要があるキーワードだと思います。ちょっと独特なだけで、実体は単純なので、理解は簡単でしょう。\nまた、KotlinのStringに関しては、JetBrains公式YouTubeチャンネルの動画で簡単に説明しているので、Kotlinで開発をしている方なら一度は参考にした方が良いかもしれません。\n他に、strip()をあえて使う必要はないと言いましたが、実際Kotlinの最新バージョンである1.5.0でもstrip()はdeprecatedになっていて、以下のようなコメントがついているので、次のバージョンで正式対応するまでは使わない方が良いですね。\n\u0026lsquo;strip(): String!\u0026rsquo; is deprecated. This member is not fully supported by Kotlin compiler, so it may be absent or have different signature in next major version\nこういうケースでもわかるように、KotlinがJavaと100%互換性があると言い切れない側面もあるのではと思います。なので、JavaからKotlinに移行した場合(実際のコードであれ、開発者自身のスキルであれ)には、一度注意深くスタンダードライブラリの説明を読む必要があるかもしれません。\nでは、また！\n","date":"2021-05-08T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-whitespace/","title":"KotlinのString実装を覗く〜whitespace編〜"},{"content":"Kotlin(Java)では、java.timeパッケージのクラスで日付や時間を処理することができます。例えばLocalDateTimeやLocalDateなどがありますね。サーバサイドではこれらのクラスを使ってDBに日付や時間を入力したり、認証用のトークンの有効期間を設定したりの処理ができるようになります。他にもPeriodやDurationがあって、「期間」を扱うこともできますね。\nただ、「年月」という単位を扱いたい場合はどうしたらいいでしょうか。例えば、口座の入出金明細などを照会する時に、「2月から4月まで」という風に期間を設定するケースなどがあるとしたら、いらない「日」や「時間」まで含めるのはあまり効率的でなく、場合によってはバグの原因になるかもしれません。こういった場合は確かな「年月」としてデータを扱うか、数字として表現するかなどどちらかの方法を考える必要があるでしょう。\nということで、今回はこの年月を扱う方法について少し述べたいと思います。\n年月を年と月に 年月を扱うということは、つまり、いつでも「年」と「月」という二つのデータとして分離できるようにしたいということにもなりますね。ここでは二つの方法で、「年月」を「年」と「月」の二つに分けて扱う方法について説明します。\nYearMonthとして LocalDateやLocalDateTimeでは、基本的にISO-8601形式で日付を扱うことができます。もちろん、DateTimeFormatterを使って他の形式を指定することもできますが、扱うデータの形が違うだけで、本質的には「年月日」が基本となりますね。\nISO-8601の「年月日」形式で日付を扱っているということは、つまり、SpringでREST APIを作っている場合、リクエストの値がISO-8601の形式を守っていればLocalDateTimeやLocalDate形式に自動変換されるということでもあります。例えば以下のようなリクエストのJSONがあるとしましょう。\n{ \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2021-04-01\u0026#34; } Spring側では以下のようなコードで、リクエストのdateをLocalDateに変換することができます。\n// リクエストボディ data class DateRequest(val id: Int, val date: LocalDate) // コントローラ @PostMapping(\u0026#34;/date\u0026#34;) fun date(@RequestBody request: DateRequest) { // ... } そして全く同じやり方で、LocalDateをYearMonthに変えることで年月に対応することができます。例えば以下のようなリクエストがあるとします。\n{ \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;yearMonth\u0026#34;: \u0026#34;2021-04\u0026#34; } ここでyearMonthをYearMonthに変えるだけです。以下のようになります。\n// リクエストボディ data class YearMonthRequest(val id: Int, val yearMonth: YearMonth) // コントローラ @PostMapping(\u0026#34;/year-month\u0026#34;) fun yearMonth(@RequestBody request: YearMonthRequest) { // ... } YearMonthを使うことのメリットは、LocalDateTimeやLocalDateと同じくjava.timeパッケージに属するオブジェクトなので、それらと互換性があり、相互変換が自由ということでもあります。例えば以下のように使えます。\n\u0026gt;\u0026gt;\u0026gt; val yearMonth = YearMonth.now() // 現在の年月を取得 \u0026gt;\u0026gt;\u0026gt; println(yearMonth) 2021-04 \u0026gt;\u0026gt;\u0026gt; val localDate = yearMonth.atDay(1) // 年月に日を指定してLocalDateにする \u0026gt;\u0026gt;\u0026gt; println(localDate) 2021-04-01 また、YearMonthは時間に関する便利なメソッドを多く提供しているので、単純に数値としての年月を扱うだけでなく、色々な要件に合わせて日付関連の処理が必要な場合に便利かもしれません。例えば以下のような機能が提供されます。\n\u0026gt;\u0026gt;\u0026gt; val yearMonth = YearMonth.of(2021, 5) \u0026gt;\u0026gt;\u0026gt; println(yearMonth) 2021-05 \u0026gt;\u0026gt;\u0026gt; println(yearMonth.getYear()) // 年を取得 2021 \u0026gt;\u0026gt;\u0026gt; println(yearMonth.getMonth()) // 月(Enum)を取得 MAY \u0026gt;\u0026gt;\u0026gt; println(yearMonth.getMonthValue()) // 月(数字)を取得 5 \u0026gt;\u0026gt;\u0026gt; println(yearMonth.isLeapYear()) // うるう年であるかどうか false \u0026gt;\u0026gt;\u0026gt; println(yearMonth.atEndOfMonth()) // 月の最後の日(LocalDate) 2021-05-31 数字として YearMonthで受け取って処理した方がもっとも綺麗な方法に見えますが、状況によっては素直にInt型で受け取った方が良い(もしくはそうするしかない)ケースもあるはずです。例えば以下のようなリクエストが送らられて来るようなケースですね。\n{ \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;yearMonth\u0026#34;: 202104 } そもそもyearとmonthのように別の項目になっていたとしたらもっとやりやすいのですが、このように年月が一つのInt型のデータとして送られてくる場合は自分で年と月を抽出する処理を作るしかないですね。例えば以下のようなextension functionを書くことができるでしょう。\n// 年を抽出する fun Int.extractYear(): Int = this / 100 // 月を抽出する fun Int.extractMonth(): Int = this % 100 実際のコードを動かしてみると、ちゃんと意図通り動くのを確認できます。\n\u0026gt;\u0026gt;\u0026gt; fun Int.extractYear(): Int = this / 100 \u0026gt;\u0026gt;\u0026gt; 202104.extractYear() res4: kotlin.Int = 2021 \u0026gt;\u0026gt;\u0026gt; fun Int.extractMonth(): Int = this % 100 \u0026gt;\u0026gt;\u0026gt; 202104.extractMonth() res6: kotlin.Int = 4 しかし、パラメータとして渡されたものはただのInt型なので、期待した通りの値ではない可能性もあるという問題があります。常にYYYYMMという形でデータが送られてくるかどうかをチェックする必要がありますね。\nそういう場合に、上記のコードだとリクエストのyearMonthが正しい年月の形式になっているかどうかがわかりません。なので、正規式を用いたバリデーションチェックを挟むことにしたらより安全になるでしょう。例えば、以下のようなコードを使えます。\nfun Int.toYearMonth(): Pair\u0026lt;Int, Int\u0026gt; = if (Regex(\u0026#34;^(19|20)\\\\d{2}(0[1-9]|1[012])\u0026#34;).matches(this.toString())) this / 100 to this % 100 else throw IllegalArgumentException(\u0026#34;cannot convert\u0026#34;) 上記の関数は、以下のような使い方ができます。簡単に使えるのでいい感じですね。\n\u0026gt;\u0026gt;\u0026gt; val (year, month) = 202104.toYearMonth() \u0026gt;\u0026gt;\u0026gt; println(year) 2021 \u0026gt;\u0026gt;\u0026gt; println(month) 4 元の値を二つのIntに分けるために戻り値としてPairを使いましたが、場合によってはYearMonthの方が良いかもしれません。そういう場合は、以下のようなコードが使えます。\nfun Int.toYearMonth(): YearMonth = if (Regex(\u0026#34;^(19|20)\\\\d{2}(0[1-9]|1[012])\u0026#34;).matches(this.toString())) YearMonth(this / 100, this % 100) else throw IllegalArgumentException(\u0026#34;cannot convert\u0026#34;) 年と月を年月に さて、今回は逆に「年」と「月」を繋げて「年月」にする場合の処理を考えてみましょう。二つのIntを合わせて、一つのInt(YYYYMM)にする形です。ここでまず考えられる方法は二つです。YearMonthを使った方法と、文字列に変換してから処理するという方法です。\nYearMonthで まずYearMonthを利用する場合は、年と月をそのまま引数として渡した後、Intに変換すれば良いですね。ただ、YearMonthは基本的にISO-8601形式なので、2021年4月だと2021-04となるのでIntへ変換ができません。なので、まずStringに変えてから、-を消してIntに変換することにします。以上の処理は、以下のようなコードになります。\nfun toYearMonth(year: Int, month: Int): Int = YearMonth.of(year, month).toString().replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;).toInt() 文字列で 文字列で処理する場合は、単純にString templatesを使うことでも可能ですが、注意したいのは、月は1~12という範囲を持つので、単純にtemplateで年と月を繋げると20214のような形になり得る可能性もあるということですね。なので、padStart()を利用して、月が1~9の場合は先頭に0をつけるようにします。そのあとはIntに変換するだけですね。これは以下のようなコードになリます。\nfun toYearMonth(year: Int, month: Int): Int = \u0026#34;${year}${month.toString().padStart(2, \u0026#39;0\u0026#39;)}\u0026#34;.toInt() これらの方法は、引数が二つなので、infixとして定義することもできます(好みの問題かと思いますが)。\n\u0026gt;\u0026gt;\u0026gt; infix fun Int.toYearMonthWith(month: Int): Int = \u0026#34;${this}${month.toString().padStart(2, \u0026#39;0\u0026#39;)}\u0026#34;.toInt() \u0026gt;\u0026gt;\u0026gt; 2021 toYearMonthWith 5 res10: kotlin.Int = 202105 最後に いかがだったでしょうか。あまり難しいコードではなかったので、あえて記事にまでする必要があったのか、という気もしましたが、個人的にはYearMonthというクラスの存在を初めて知ったのもあり、Kotlinならではのコード(extension function)を書いてみたく試したことを共有したいと思った次第です。もしKotlinやJavaで年月を扱う必要がある方には、少しでも役に立てるといいですね。\nでは、また！\n","date":"2021-04-27T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-year-month/","title":"年月を扱ってみる"},{"content":"サーバサイドの機能を作っていると、ファイルダウンロード機能が必要な時があります。ただ、ストレージに保存されてあるファイルをそのまま返すということだけでなく、場合によってはファイルを生成してそのまま返したり、複数のファイルをまとめて転送する必要もありますね。\nリクエストごとに一つのファイルをダウンロードさせるとしたら、実装はそう難しくないものですが、複数のファイルをダウンロードさせるという場合は少し複雑になりますね。ファイルを一つにまとめて送るとしたら、ZIPに圧縮した方が良いでしょう。幸い、Javaでは基本的にZipOutputStreamというAPIを提供しているので、エントリに圧縮対象のファイルを追加したあとZIPファイルを出力だけで良いです。\nただ、単純にファイルが複数だるだけでなく、ディレクトリが多重にネストされてあったりする場合は、ディレクトリ構造を維持しつつそのまま圧縮するとかの追加的な処理が必要となります。そして場合によっては含めたくないファイルがあるケースもあったりしますね。そしてなるべくファイルの数に関係なく(ファイルが一つであれ、ディレクトリであれ)一つの機能で済ませたいものです。なので、今回はそのようなユースケースに合わせた簡単なメソッドを作る方法を、JavaのコードからKotlinへ移行していく過程を簡単に紹介したいと思います。\n今回紹介しますコードは、はBaeldungのJavaでZipを圧縮する方法に関する記事に紹介されてあるものをベースにしています。\nJava まずJavaのコードを見ていきましょう。上記の記事には、以下のようなコードが紹介されています。\npublic class ZipDirectory { public static void main(String[] args) throws IOException { String sourceFile = \u0026#34;zipTest\u0026#34;; FileOutputStream fos = new FileOutputStream(\u0026#34;dirCompressed.zip\u0026#34;); ZipOutputStream zipOut = new ZipOutputStream(fos); File fileToZip = new File(sourceFile); zipFile(fileToZip, fileToZip.getName(), zipOut); zipOut.close(); fos.close(); } private static void zipFile(File fileToZip, String fileName, ZipOutputStream zipOut) throws IOException { if (fileToZip.isHidden()) { return; } if (fileToZip.isDirectory()) { if (fileName.endsWith(\u0026#34;/\u0026#34;)) { zipOut.putNextEntry(new ZipEntry(fileName)); zipOut.closeEntry(); } else { zipOut.putNextEntry(new ZipEntry(fileName + \u0026#34;/\u0026#34;)); zipOut.closeEntry(); } File[] children = fileToZip.listFiles(); for (File childFile : children) { zipFile(childFile, fileName + \u0026#34;/\u0026#34; + childFile.getName(), zipOut); } return; } FileInputStream fis = new FileInputStream(fileToZip); ZipEntry zipEntry = new ZipEntry(fileName); zipOut.putNextEntry(zipEntry); byte[] bytes = new byte[1024]; int length; while ((length = fis.read(bytes)) \u0026gt;= 0) { zipOut.write(bytes, 0, length); } fis.close(); } } zipFileメソッドをみると、引数のfileToZipにZIPで圧縮したいファイルやディレクトリのパスを指定して、fileNameにはファイルもしくはディレクトリ名、zipOutには圧縮後のZIPのファイル名を指定するようになっています。\nそして実装としては、指定したファイルやディレクトリにhidden属性がある場合は圧縮しなく、圧縮元のファイルがディレクトリである場合は中のファイルを全部ZIPに含ませるという処理が含まれてありますね。対象のファイルとディレクトリを全部エントリに追加した後は、圧縮元を読み込んでZipOutputStreamに書き込むという処理となっています。これをKotlinのコードに変えてみましょう。\nKotlinのコードに変えてみる JavaのコードをKotlinのコードに変えるのはそう難しくありません。Intellijの場合、すでにJavaのコードを貼り付けると自動でKotlinのコードの変換してくれる機能を搭載していますので。ただ、それだけでは十分ではないですね。簡単に変換ができるとしても、それが本当にKotlinらしいコードになっているとはいえない場合があります。\nそして、処理自体もより単純に、もしくは読みやすいコードにする方法もあるはずですね。上記のJavaコードをまずKotlinに変えて、色々改善したいところを含めて変えていきます。\nKotlinらしいコードに変える Intellij 2021.1を基準に、Javaのコードをそのまま貼り付けると以下のようなコードに自動変換されます。\n@Throws(IOException::class) private fun zipFile(fileToZip: File, fileName: String, zipOut: ZipOutputStream) { if (fileToZip.isHidden) { return } if (fileToZip.isDirectory) { if (fileName.endsWith(\u0026#34;/\u0026#34;)) { zipOut.putNextEntry(ZipEntry(fileName)) zipOut.closeEntry() } else { zipOut.putNextEntry(ZipEntry(\u0026#34;$fileName/\u0026#34;)) zipOut.closeEntry() } val children = fileToZip.listFiles() for (childFile in children) { zipFile(childFile, fileName + \u0026#34;/\u0026#34; + childFile.name, zipOut) } return } val fis = FileInputStream(fileToZip) val zipEntry = ZipEntry(fileName) zipOut.putNextEntry(zipEntry) val bytes = ByteArray(1024) var length: Int while (fis.read(bytes).also { length = it } \u0026gt;= 0) { zipOut.write(bytes, 0, length) } fis.close() } ここでもっとKotlinらしいコードに変えたい部分は、InputStreamやOutputStreamの使い方です。Javaでもtry-with-resourceがあって、Kotlinにはuse()があるのでそちらを使った方がcloseよりも良い気がします。\nまた、ifはwhenに変えたり、forをforEach()に変えたりなどでよりやりたいことを明確にすることができるようにも見えます。個人的にはスコープをあえて分けたほうが責任が明確になり、処理を追うときに混乱しないのでなるべくスコープ関数やCollection専用のオペレーションを積極的に使用して処理の単位を分けられるところはきちんと分けたいと思います。Javaのやり方をとっても処理としては全く問題がありませんが、せっかくなのでKotlinならではのコードを描きたいものです。\nあえてIOExceptionを投げるという表示をしておくというのも、ランタイム時の例外の処理を強制してないKotlinには相応しくないのではないかという気もするので、アノテーションは削除することとします。\nIOからNIOに変える NIOに関しては以前のポストで何回か言及したことがありますが、サーバのように頻繁かつ同時実行数が多いケースは積極的に採用した方が良いと思います。また、Java 1.8以降から追加されたメソッドでかなり便利に使える機能が多いので、IOをNIOに変えるだけでコードの量をかなり減らせる可能性もあります。\n特にディレクトリを指定した場合、そのディレクトリの子要素を循環するにはNIOのFilesが提供する機能が強力なので、今回はそれを積極活用することにします。\nシグニチャーを変える 上記のメソッドでは、三つの引数を取っていますが、実際に必要なのは圧縮元のパスと、圧縮先のパスのみですね。ZipOutputStreamを呼び出し元で渡す理由は特になく、むしろこのメソッドを利用する度に定義する必要があるので不便ですね。そして、メソッドの中で単純にエントリを追加していて、呼び出し元とオブジェクトに対する処理の職務を分担するという構造もあまりよくないかと思います。なので、ZipOutputStreamの生成と使用はメソッドの中で完結するように変えることにします。\nこうすることで、メソッドの外側(呼び出し元)での使い方はもっと簡単になりますし、圧縮元のデータを読み込む際に使うInputStreamは中で閉じているのに引数のOutputStreamは外で閉じるという複雑な状況は避けられます。\n再帰を無くす 圧縮元のパスがディレクトリである場合は、さらにネストされたディレクトリやファイルもまとめて圧縮するために再帰を使うようになっています。再帰はアルゴリズムとしては重要ではあるものの、処理が全部終わるまでメモリに全データと処理を詰めておくので処理の効率という面ではあまりよくない場合もありますね。やりたいのは単純にhidden属性を持つファイルやディレクトリを除外すること、そしてそれ以外のファイルやディレクトリは全部ZipOutputStreamのエントリに入れたいという単純な事です。\n幸い、NIOを使うことでディレクトリの子要素を全部取得することができますし、取得した子要素はStream\u0026lt;Path\u0026gt;として取得できるので、filter()やforEach()のようなメソッドが使えます。これで十分、再帰を使わずに目的を達成できそうですね。\n完成したコード 以上のことを反映し、修正したコードは以下の通りになります。\nobject ZipService { fun archive(source: Path, target: Path): Unit = ZipOutputStream(Files.newOutputStream(target)).use { zos -\u0026gt; Files.walk(source) .filter { Files.isHidden(it).not() } .forEach { if (Files.isDirectory(it)) { zos.putNextEntry(ZipEntry(\u0026#34;$it/\u0026#34;)) zos.closeEntry() } else { zos.putNextEntry(ZipEntry(it.toString())) Files.copy(it, zos) } } } } 簡単に説明しますと、objectとして宣言したSingletonクラスにおくことでどこでも活用できるユーティリティクラスにして、メソッドのシグニチャはより単純なものにしました。引数のsourceには圧縮元のファイルやディレクトリを、targetには圧縮先のZIPファイルを指定する事になっています。ZipOutputStreamはメソッドの中で生成して、use()を使って自動にクローズされるようにしています。\nまず優先的にFiles.walk()を使って子要素を全部取得するようにしています。取得した子要素はfilter()でhiddenでない場合を選別しているので、分岐は無くなりますね。また、子要素がディレクトリである場合ディレクトリ名であることを表すために/をつけてZipEntryを追加とクローズします。子要素がファイルの場合はZipEntryの追加とコンテンツのコピーを行います。これでより短く、単純なコードの出来上がりです。\n最後に Kotlinらしいコードと述べましたが、上記のコードはあくまでKotlin/JVMでのみ有効ですね。なのでもしKotlin/NativeやKotlin/JSなどで使うには、別の方法を探す必要があるはずです。また、Files.walk()はJava 1.8から追加されたメソッドなので、1.7の場合はFiles.walkFileTree()を、その以前なら仕方なくNIOではない別の方法を使う必要があると思います。\nなので、Kotlin/JVM(Java 1.8以上)ではこれが最善なのかもしれませんが、また色々と研究の余地はありそうですね。こうやってJavaのAPIをKotlinの作法で切り替えていくのも、それなりに価値のあることではないかと思います。\nでは、また！\n","date":"2021-04-14T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-zip-files/","title":"KotlinでZIP圧縮してみる"},{"content":"こないだはGoに関するポストを作成しましたが、やはり本業はKotlinなので、Kotlinに関しても何かわかったことや閃いたことなどあれば、順次に書いていこうと思っています。今回はKotlinでAPIを作りながら、業務での要件をどんなコードで満たしたかを簡単に説明させていただきたいと思います。\nサーバサイドエンジニアをやっていると、要求される機能を以下に実現している方法がどんなものあれ(GraphQL、REST API、マイクロサービスみたいな技術やアーキテクチャの観点の以前の話として)、業務としてはある程度パターン化しているように感じることがあります。こういう場合には、コードよりもロジックが大事であるかのように見える場合もありますね。でも逆に、むしろ似たようなロジックが多いので、より良いコードを書くために工夫できる余地もまた多いのではないか、とも思います。\n正直自分はアルゴリズムに強いわけでもないので、効率的なコードを書くとしたら限界はあるだろうなという気はしています。とりあえず動くコードを書いて、それをリファクタリングしながら少しづつ整える感じのことしかできないのかもしれません。\nしかし、そんな自分にも良いコードを書くためにできることが全くないわけでもないと思います。例えば、Javaでコードを書くときは、参照の問題などからなるべくfinalをつけてオブジェクトをimmutableにするようにと教わりましたが、実際はベンチマークで比較した結果でもわかるように、性能の改善にも繋がっています。また、JavaでもKotlinでも色々と便利なAPIを提供していて、バージョンアップの度にまた新しいAPIが追加されるので、それらの用途と使い方をよく理解した上で、積極的に使用するだけでも読みやすく、性能も良いコードを書くことができます。\nということで、今回はKotlinのAPIを使って書いていたコードを一部紹介したいと思います。\nリストのグループ化 DBに商品情報テーブルがあって、さらに商品属性テーブル、生産地や販売店テーブルなどがある場合に、業務によっては「販売店ごとにどんな商品が販売されているかを確認したい」とか、「特定の商品属性に当てはまる商品だけみたい」とかのケースがあるはずですね。\nそういった場合、APIとしてはテーブルから取得したデータを、特定のカラムを基準にまとめたもの返す必要があります。これをコードに書くとしたらListで取得したデータを、中の一つの属性をキーにMapにまとめて返すということになりますね。Javaだと、以下のような形になるかと思います。\n// DBのデータの例 List\u0026lt;User\u0026gt; list = List.of( new User(\u0026#34;John\u0026#34;, 20, \u0026#34;USA\u0026#34;, \u0026#34;Programmer\u0026#34;), new User(\u0026#34;James\u0026#34;, 30, \u0026#34;Canada\u0026#34;, \u0026#34;Sales\u0026#34;), new User(\u0026#34;Jack\u0026#34;, 35, \u0026#34;UK\u0026#34;, \u0026#34;Programmer\u0026#34;) ); // UserのJobを基準にまとめる Map\u0026lt;String, List\u0026lt;Pair\u0026gt;\u0026gt; map = list.stream() .collect(Collectors.groupingBy(User::getJob, Collectors.mapping(user -\u0026gt; new Pair(user.getAge(), user.getName()), Collectors.toList()))); // {James=[Pair(first=30, second=Sales)], John=[Pair(first=20, second=Programmer), Pair(first=35, second=Writer)]} @Data @AllArgsConstructor static class User { private String name; private int age; private String address; private String job; } @Data @AllArgsConstructor static class Pair { private Object first; private Object second; } KotlinでもJavaのAPIをそのまま使うことができるので、上記のStreamとCollectorを使って同じことはできます。ただ、せっかく違う言語と使っているわけなので、できればKotlinが提供するAPIを活用して同じことをしたいものです。\nKotlinはCollectionで提供する機能だけでもStreamとCollectorを組み合わせたものと似たような処理ができる場合が多いので、JavaのAPIに対応した機能があるかどうかを探すだけで事足りるケースが多いです。ということは、上記の処理でキモになっているCollectors.groupingBy()とCollectors.mapping()と似たようなものがあればいいというわけですが、groupBy()でそれらの処理をまとめることができます。なので、上記のコードをKotlinで変えると、以下のようになります。色々とスッキリしますね。\n// DBデータの例 val list = listOf( User(\u0026#34;John\u0026#34;, 20, \u0026#34;USA\u0026#34;, \u0026#34;Programmer\u0026#34;), User(\u0026#34;James\u0026#34;, 30, \u0026#34;Canada\u0026#34;, \u0026#34;Sales\u0026#34;), User(\u0026#34;Jack\u0026#34;, 35, \u0026#34;UK\u0026#34;, \u0026#34;Programmer\u0026#34;) ) // Jobを基準にMap\u0026lt;String, List\u0026lt;Pair\u0026lt;Int, String\u0026gt;\u0026gt;\u0026gt;にまとめる val map = list.groupBy({ it.job }, { it.age to it.name }) // {Programmer=[(20, John), (35, Jack)], Sales=[(30, James)]} data class User( val name: String, val age: Int, val address: String, val job: String ) Mapのvalueだけを変える 上記の処理に加えて、もっと条件がつく場合もあるかと思います。例えば、金額計算とかの例があるとします。従業員が案件ごとに賃金をもらうということになっていて、案件はコードで管理されている場合、賃金を払う側としては同じ案件に対しては合算した金額のみが知りたいとかのケースもあるでしょう。こういう場合には、従業員ごとにデータをまとめた上で、さらにその人が担当した案件のリスト野中で重複するものがあれば、金額だけを合算するようにする必要がありますね。\nこういう場合は、グルーピングの段階からそういう処理を入れるのがもっとも効率的ではあるとは思いますが、スレッドの問題もあるので(生成中のMapの中を巡回するという)、実際のコードに書くとするとかなり複雑になる可能性もあります。なのでここではまず、ListをMapにまとめた結果を持ってさらに処理を加えるという形を取ります。\nKotlinのMapには、map()以外にもmapKeys()やmapValues()のような関数があって、必要な部分だけをマッピングできます。今回はvalueだけを変えたいので、mapValues()を使った方が無駄がなく、コードを読む側としても意図が明確になって良いと思います。mapValues()を使ってさらにマッピングを行うコードは、以下のようになります。\ndata class User(val name: String, val id: Int, val amount: Int) // DBデータの例 val list = listOf( User(\u0026#34;A\u0026#34;, 1, 1000), User(\u0026#34;A\u0026#34;, 1, 2000), User(\u0026#34;A\u0026#34;, 2, 4000), User(\u0026#34;B\u0026#34;, 3, 5000) ) // nameでまとめた後、重複するidを一つにまとめる(amountを合算) val map = list.groupBy({ it.name }, { it.id to it.amount }) .mapValues { // idでグルーピング it.value.groupBy { pair -\u0026gt; pair.first } // keyはそのまま、valueだけを合算する .map { map -\u0026gt; map.key to map.value.sumBy { pair -\u0026gt; pair.second } } } // {A=[(1, 3000), (2, 4000)], B=[(3, 5000)]} ListをMapにまとめるもう一つの方法は、groupingBy()があります。この関数を使うと、CollectionがGroupingというオブジェクトに変わって、aggregate()・reduce()・fold()・eachCount()のような関数を使うことで後続の処理ができます。上記のコードをGroupingを使ったものに変えるとしたら、以下のようになります。\n// Groupingのaggregateを利用してMapに変えた後から、valueの処理を行う val map = list.groupingBy { it.name } .aggregate { _, accumulator: MutableList\u0026lt;Pair\u0026lt;Int, Int\u0026gt;\u0026gt;?, element, first -\u0026gt; // 新しいキーなら、MutableListを作る if (first) mutableListOf(element.id to element.amount) // そうではない場合は、存在するListに要素を追加する else accumulator?.apply { add(element.id to element.amount) } }.mapValues { it.value?.groupBy { pair -\u0026gt; pair.first } ?.map { pair -\u0026gt; pair.key to pair.value.sumBy { pair -\u0026gt; pair.second } } } 一見、groupingBy()の方が複雑にも見えますが、accumulatorを使ってマッピングした値を積み重ねることができるので、場合によっては考慮する価値があるかもですね。\nMapを使ったキャッシュ DBの参照が頻繁であり、なお参照されるデータそのものは更新される頻度が高くない場合は、アプリケーション内にキャッシュして置くのが良いケースもたまにありますね。こういう場合には、パラメータをキーとして持つMapを宣言しておいて、そのキーがない場合だけDBにアクセスする(そしてMapに追加する)という形にすれば良いでしょう。Javaでは1.8からcomputeIfAbsent()というメソッドを提供しているので、簡単に実装ができます。例えば以下のようになります。\n// DBデータの例 List\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); // キャッシュのMap Map\u0026lt;String, Boolean\u0026gt; map = new ConcurrentHashMap\u0026lt;\u0026gt;(); // パラメータ String element = \u0026#34;A\u0026#34;; // キャッシュにパラメータがない場合はDBデータを参照して、追加した後に返す Boolean exists = map.computeIfAbsent(element, key -\u0026gt; list.contains(element)); // Method Referenceを使った例 exists = map.computeIfAbsent(element, list::contains); Javaで提供する機能なので、もちろんKotlinでも全く同じ形で実装できます。ただ、Kotlinの仕様上computeのコードがLambdaかMethod Referenceかによって書き方が違うので、そこだけ注意する必要があります。これはKotlin自体の仕様によるものですが、Javaの書き方に慣れていると最初はなかなかわかりにくいところかもしれません。\n// DBデータの例 val list = listOf(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;) // キャッシュのMap val map = ConcurrentHashMap\u0026lt;String, Boolean\u0026gt;() // パラメータ val element = \u0026#34;A\u0026#34; // Lambdaの場合 var exists = map.computeIfAbsent(element) { list.contains(element) } // false // Method Referenceの場合 exists = map.computeIfAbsent(element, list::contains) ちなみに、似たような機能をするメソッドとしてputIfAbsent()がありますが、computIfAbsent()の場合Mapにキーがなかった場合にだけ後続の処理が行われるに対して、putIfAbsent()はキーがあるかないかに関係なく処理が走ってしまうという違いがあるらしいです。なのでキャッシュとして使う場合は、computeIfAbsent()を使った方が良いでしょう。\n最後に 自分が書いたコードをいくつか紹介しましたが、いかがだったでしょうか。まだKotlinに移行したばかりなので色々とわからないことが多く、本当はもっとスマートな方法があるのかもしれませんが、自分的には、こうやって実際の業務の要件に合わせて違う言語とコードを比べながら、APIのソースをみたりで自分なりにどうやって書くかを考えてみるのは意味のあることで、楽しいとも思います。\nというわけで、これからもKotlinでの書き方に対する研究はこれからも続きます。そろそろGoでも簡単なAPIでも作ってみたりで勉強をしないとやばそうな気もしていますが…まぁ、なんとかなるでしょう。\nでは、また！\n","date":"2021-03-28T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-code-in-my-style-1/","title":"Kotlinで書いてみた〜その一〜"},{"content":"このブログのポストとしてはいきなりですが、転職してからは仕事の都合上Goも少し触ることとなりました。以前からGoかRustに触れてみたいなとは思っていたのものの、いざ全く触れてみたことのない言語で書かれてあるアプリを修正するようなことになると少し怖くもなります。なので、少しでもGoのことを知っておくべきではありますね。というわけで、今回はGoに少し触れてみて感じたことを、Javaプログラマーの観点から述べてみたいと思います。\nGoの特徴は、色々とあると思いますが、正直GCがあって、VMがない言語だという特徴は、実際はあまり肌で感じる違いではないです。VMがないから、ある言語よりは早いだろうなというとしか言えませんね。\n実際その言語を持ってコードを書くという仕事をしている身からしては、そのような特徴よりも、コードを書く場面で気にしなければならないことの方に注目したいものです。まずは自分が今まで触れてきた言語とはどう違うかですね。例えば、ループや条件などを書くときはどうなのか、今までの習慣通りコードを書いても問題ないだろうか、注意すべきところは何かなど。今回はそのような観点から本当の少しだけGoに触れてみた感想を書いていきたいと思います。\n考え方を変える必要があるかも Goに少し触れてみて考えたことは、もっとも基本的な部分でもJavaを書くときとはかなり違うアプローチが必要ではないかということでした。私の場合はJava意外にPython、JavaScript、TypeScript、Kotlinに触れてみたことがあるのですが、JavaScriptやTypeScriptはJavaの書き方とそう変わらない感覚で書くことができて、Kotlinも基本はJavaを簡略化した感覚でコードが書けます。Pythonがかなり違うのですが、どちらかというと書きたいコードを文法の制約なしで書けるという感覚に近いので、文法の差が気にならないものでした。\nしかし、Goの場合は少しわけが違います。Javaと比べ、書き方が少し違うだけでなく、機能レベルで違いがあるからです。機能レベルで違うということは、単純にJavaのコードを少し変更したくらいのコードを書くことはあまりよくないということになると思います。なので、そもそもの考え方を変える必要があるのではないかと思いました。そういう観点から感じたGoの印象は、以下の通りです。\n似ているようで、似ていない まず目立つのは、文法です。もちろん、大枠はいわゆるCファミリープログラミング言語とそう変わらないですが、Javaと比較すると文法の構造以外の部分でかなり変わった部分があります。例えば、Walrus Operatorとも呼ばれるPythonの代入式に似たような表現があったり、if文の条件式を括弧なしで書けたり、importを文字列で書いたり、クラスやpublic・privateのようなキーワードが無かったりの違いがあるので、コードを書くときの感覚が違うだけでなく、パッケージ構造やアプリケーションのアーキテクチャ設計のレベルで今まで自分が経験していたJavaやKotlinとは違うアプローチが必要ではないかと思えるくらいです。\n色々と違う点を述べましたが、単純にコードを持って比較してみましょう。例えば、以下のようなコードがあるとしましょう。数字に関する計算を担当するクラスがあって、中には渡された引数が奇数か偶数かを判別して、結果を標準出力する形です。\npackage simple.math; public Class Calculator { public void judge(int number) { boolean result = number %2 == 0; if (result) { System.out.println(number + \u0026#34;は偶数です\u0026#34;); } else { System.out.println(number + \u0026#34;は奇数です\u0026#34;); } } } これをGoのコードに変えてみます。例えば以下のようになると思います。\npackage simple.math import \u0026#34;fmt\u0026#34; func Judge(number int) { condition := number % 2 == 0 if condition { fmt.Println(number, \u0026#34;は偶数です\u0026#34;) } else { fmt.Println(number,\u0026#34;は奇数です\u0026#34;) } } 一見あまり変わらないように見えるかもしれませんが、細かいところが違うので注意しなければならないところがあります。いくらIDEの性能がよくなったとはいえ、その言語の仕様と全く違うようなコードを書いてしまっては、正しいコードを提示してくれませんので。例えばインポートは複数になると、以下のようになりますね。\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) Pythonの場合も一行でimportをまとめたり、fromとasでAliasを指定するようなことができたりもします。しかし、Goで根本的に違うのは、GoそのものがMavenやGradleのようなパッケージ管理もできるので、インポートにgithubのパッケージを描くこともできるということです。例えば以下のようなコードで、GoのウェブフレームワークであるGinをインポートすることができます。\nimport \u0026#34;github.com/gin-gonic/gin\u0026#34; また、変数を:=を使って宣言する場合は、関数内でのみ可能であるので、パッケージレベルで宣言する場合は普通にvarを使う必要があるという仕様も理解する必要があったりします。そしてそれに対して、関数の引数としてはvar宣言がいらなく、型を宣言する必要があります。どんな場合でも(初期化とともにvarを使うことは最近のJavaでも可能になりましたが)変数の型を書く必要のあるJavaとは大きく違うところですね。こういう細かい違いがあるので、Goの作法に対する理解なしでJavaの感覚のままコードを書くのは大変なことになるかもしれません。\n大文字には機能がある 会社ごとにルールは違うかと思いますが、今までの自分が経験では、言語がJavaであれJavaScriptであれ以下のようなルールで書く場合が多かったです。\nクラス、インタフェース名はPascalCase フィールド、メソッド、変数、引数はcamelCase たまにPythonでコードを書く場合はsnake_caseだったり、URLはkebab-caseだったりもしたのですが、プライベートでも多くの場合このルールに従ってコードを書いています。そしてこれはあくまで人間が定めたルールなので、守らなくても\nしかし、GoではPascalCaseかcamelCaseかによって意味が変わる部分があります。正確には頭が大文字か小文字かによる違いですね。publicとprivateの代わりになってくるのが、この仕様です。簡単に説明すると、他のパッケージからも参照できるのは大文字から始まるフィールドや関数であり、そうでない場合は小文字から始まるものということです。\n例えば以下をみてください。A Tour of Goで提示しているコードです。ここではmathパッケージをインポートして、あらかじめ定義されてあるπを標準出力している例です。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func main() { fmt.Println(math.pi) } 上記のコードは、なんの問題もないかのように見えますが、実行すると以下のようなエラーメッセージが出力されます。\n./prog.go:9:14: cannot refer to unexported name math.pi これはつまり、外部から参照できないということですね。なので、正しいコードに変えるとmain関数を以下のように直す必要があります。\nfunc main() { fmt.Println(math.Pi) } 先端が大文字であり、外部から参照できるように定義された名前のことをExported Namesというらしいです。Goにはクラスがないので、パッケージをインポートして、そのパッケージ内に存在する.goファイルの中にExport Namesで定義された項目のみを参照することになります。なのでクラスを作って、そのクラスのインスタンスを生成し、小文字から始まるフィールドやメソッドを呼ぶというJavaの作法とは感覚が大きく違いますね。\nポインターの存在 プログラマーだと誰もが知っていることだと思いますが、ポインターがあるかどうかの問題は、コードを各感覚にかなりの影響を与えるようなものです。JavaやKotlinなど、ポインターがない言語ではクラスやメソッド間にただ、GoにはGCがあるので、CやC++のようなメモリー問題はないかとも思いますが、とにかく直接使ってみないとわからないところですね。\nもちろん、Javaでもpublic staticで宣言したり、SpringだとAutowiredアノテーションをつけることでどこでもアクセスできるオブジェクトを作ることは可能です。Kotlinだとcompanion objectという、クラスに似たようなものをまず定義する必要がありますが、呼び出し元としてはJavaとあまり変わらないコードになりますね。\nしかし、こう言ったstaticなものは、JavaやKotlinだと定数として使われるのが一般的です。AutowiredでもSingletoneと使い方はあまり変わらず、固定値を格納するか、常に同じ動作(冪等に違い)をすることを期待するのが一般的ですね。それに比べ、ポインターはやはりその値を直接書き換えたりすることを期待する場合もあるので、やはり違うものです。\nまだ私はポインターを本格的に扱う言語に触れてみたことがなく、Goでもポインターを活用するようなコードは書いたことがあまりないので、ここで言えるのは上記で述べた内容だけですが、私と同じくポインターのない言語の経験しかない方にはやはり慣れるまで時間がかかるのではないかと思います。試行錯誤もかなりありそうですね。\n例外処理が独特 Goで書かれてあるコードをみて、何が一番目立つかというと、例外処理の部分ではないかと思います。私が経験したことのある言語(Java、Python、JavaScript、TypeScript、Kotlin)では、例外処理のためにtry-catchブロックという仕様がありました。言語ごとに少しづつ違うところはありましたが、基本的に例外が発生しうる場所をそのブロックで囲んでから処理する、という発想自体は変わりがありません。例えば以下のようなコードで例外を処理することが一般的でしょう。\npublic static void main(String[] args) { int result = 0; try { result = divide(1, 0); } catch (Exception e) { if (e instanceof ArithmeticException) { System.out.println(\u0026#34;0で分けません\u0026#34;); return; } } System.out.println(result); } private static int divide(int numerator, int denominator) { return numerator / denominator; } しかし、Goではそのような機能はありません。代わりに、どんな関数でも戻り値として期待する値と発生したエラーを定義し、呼び出し元では関数の実行結果としてエラーが発生したを確認して、エラーが発生していたら(エラーがnilではない場合)にそのエラーの対応をする、ということが一般的な作法のようです。言葉で説明すると難しいので、実際のコードをみていきましょう。上記のコードをGoの作法に合わせて書き直すと、以下のようになります。\nfunc main() { result, err := divide(1, 0) if err != nil { fmt.Println(err) return } fmt.Println(result) } func divide(numerator int32, denominator int32) (int32, error) { if (denominator != 0) { return numerator / denominator, nil } else { return 0, errors.New(\u0026#34;0で分けません\u0026#34;) } } このように、関数ではエラーが発生した場合にそれをそのまま返し(上記のコードでは、あえてエラーを作っていますが)、呼び出し元ではエラーがあったかどうかを確認して分岐します。こうすることで「エラーが発生した場所が明確になる」メリットがあるらしいです。確かに、try-catchブロックが囲んでいるコードが多ければ多いほど、例外が発生し得るコードがどれなのかわからなくなる場合もありますね。例外を処理するための機能が、例外を起こさないコードと混り、わけがわからなくなります。そういう観点からすると、Goのアプローチはエラーとロジックを分離できるというメリットがあると言えるでしょう。\nただ、Goの作法では関数を呼び出す度に後続でエラーチェックが入るので、毎回同じ様なコードを書く場合があるのは少し違和感があります。例えば以下のようなコードを見かけるのですが、皆さんはどう思われるのでしょうか。もっとスマートな方法があって、自分が知らないだけなのかもしれませんが…\n// いくつかの関数を呼び出して処理をする関数 func doSomething() (string, error) { // 関数1を呼び出す result1, err1 := someFunction1() // 関数1でエラーが発生した場合はエラーを返却する if err1 != nil { return \u0026#34;\u0026#34;, err } // 関数1でエラーが発生していない場合は関数2を呼び出す result2, err2 := someFunction2(result1) // 関数2でエラーが発生した場合はエラーを返却する if err2 != nil { return \u0026#34;\u0026#34;, err } // 関数2でエラーが発生していない場合は関数2を呼び出す result3, err3 := someFunction3(result2) // 関数3でエラーが発生した場合はエラーを返却する if err3 != nil { return \u0026#34;\u0026#34;, err } // 関数3でエラーが発生していない場合は関数2を呼び出す result4, err4 := someFunction4(result3) // 関数4でエラーが発生した場合はエラーを返却する if err4 != nil { return \u0026#34;\u0026#34;, err } // ...続く } コンパイラーが厳しい コンパイルエラーが発生した場合はIDEで知らせてくれるので気にすることはあまりないかと思いがちですが、意外と気になるのはコンパイラーの厳しさです。個人的にはjShellの様なインタラクティブツールを使ってよくコードの検証をするのですが、GoにはそれがないのでVimで書いたコードをターミナルで動かしてみたり、The Go Playgroundを使ってみています。そしてこういう方法ではIDEの様なサポートをあまり期待できませんので、コンバイルエラーになることが多かったです。\nただ、コンパイルエラーといっても色々な原因がありますが、Goは特に他の言語と比べてもかなり厳しいのではないかと思います。例えば以下の様なコードがあるとします。\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { } ターミナルやThe Go Playgroundでこれを実行すると、以下の様なエラーメッセージが出ます。\n./prog.go:4:2: imported and not used: \u0026#34;fmt\u0026#34; インポートしたパッケージが使われていないというエラーですね。さらに、以下の様なコードを実行したとします。\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { var result string fmt.Println(\u0026#34;this is test program\u0026#34;) } 上記のコードを実行した結果は、以下の通りです。\n./prog.go:8:9: result declared but not used 今回はresultという変数が使われてないというエラーです。この様に、Goでは使われてないインポートや変数などがあればエラーとなるので、他の言語と比べ厳しい方と言えますね。なので、プラグインなしのVimを使って修正するなどの場合には十分に気をつける必要がありますね。IDEでも少しはめんどくさいかもしれません。(リンティングと同時に使われてないパッケージや変数を削除してくれるとかの設定をすれば良いかもですが)\n最後に 他にも細かい違いはまだまだ山ほどありますが、今の時点で言えるものはこのくらいです。ここで述べた、Javaプログラマーの観点からみたGoの特徴というものは、実は「慣れれば問題ない」だけなのかもしれません。しかし、「慣れる」ということは、「すでに慣れている」ものがあったらまたなかなか難しいものにもなりますね。\n例えば、人間の言語でいうと、同じ系列のドイツ語の話者が英語を学ぶのは簡単と言われていますが、それは二つの言語がよく似ているからですね。逆に英語が母国語である人には中国語・日本語のような言語がもっとも難しい言語らしいのですが、これは語彙、文字、文章構造の全てが違うのが原因だそうです。プログラミング言語も本質は人間の言語を模倣しているものなので、新しい言語を学ぶ際には、それが自分の母国語と言える言語と似ていれば似ているほど学びやすく、そうでない場合は難しく感じるものではないかと思います。そういう観点からすると、JavaからGoへの移行は、簡単そうで難しそうな側面があるかなという気がします。\nもちろん、世の中には元Javaプログラマーでも、Goの方が簡単だった！という方もたくさんいらっしゃるかと思います。ただ自分がついていけないだけなのかも…ですが。\n","date":"2021-03-21T00:00:00Z","image":"https://retheviper.github.io/images/go.jpg","permalink":"https://retheviper.github.io/posts/go-first-impression/","title":"JavaプログラマーがみたGo"},{"content":"この度は、転職することとなり、仕事で使われる言語もJavaからKotlinに変わることになりました。個人的にKotlinで簡単なSpring WebFluxプロジェクトを作ってみたことはあり、もともとJavaプログラマがKotlinへ移行するのは難しいことではないと言われてはいるものの、やはり仕事で使われる言語が変わるというのはかなりのチャレンジではあると思います。なので、今まではJavaに関してのポストを主に載せていたのですが、これからはKotlinに関してのポストを増やしていきたいと思います。\nまず、よく知られているように、KotlinはJavaと完璧な互換性を持つものです。それはJVM言語であり、コンパイルしたらJavaと同じくバイトコードになるからですね。ただ、だからと言って「Javaの感覚で」コードを書くということは、Kotlinという「違う言語」に移行する意味を無くす行為な気がします。なぜなら、Kotlinは触れば触るほどJavaとは根本的に違う考え方で設計されている言語だということが伝わってくるからです。最初はJavaの冗長さ(Verbose)を減らすことが第一の目標ではないかという印象を受けましたが、本格的に勉強を始めるとそれだけではないような気がしているのです。\n今回のポストは、CourseraのKotlin for Java Developersの講義の内容に基づいて作成されました。\n冗長さを減らすということ Javaは今でも良い言語であり、多くの言語が発表されエンタープライズレベルで使われるようになった今でも、幅広い分野で使われていますね。Javaが依然としてよく使われる言語であることは、TIOBE indexやJetBrainsのThe State of Developer Ecosystem、StackoverflowのDeveloper Surveyからも読み取れることです。\nただ、Javaが依然として人気の言語だとしても、それは圧倒的にJavaが他の言語に比べ優秀だとか、使いやすい言語であるという意味ではないでしょう。どの言語でもそうであると思いますが、Javaでよく指摘されている問題の一つは、「冗長すぎる」というところです。数多くのライブラリがあり、MavenやGradleのような優秀なビルドツールを使えながらも、やはり言語の仕様は変わってないですからね。Java 9からはこの問題を解消するため、他の言語から影響を受けたような機能を多く導入していますが(例えば、instanceofのパターンマッチングやrecordなど)、言語そのものの設計思想が変わるというよりは「違う言語の特徴をJavaに合わせた仕様で導入する」ことに近いので、根本的な変化とは言えないものです。なので今まで書かれている冗長なコードは残るもので、またこれからも使われることになるはずです。\nコードが短くなる 冗長さを減らすということは、簡単にいうと「より短いコードで、同じ結果を得る」と定義できるでしょう。そういう観点からすると、KotlinはJavaの冗長さを減らすために工夫した痕跡が言語の仕様から感じ取れるようなものです。例えば以下のようなコードがあるとしましょう。\npublic void updateWeather(int degrees) { String description; Color color; if (degrees \u0026lt; 10) { description = \u0026#34;cold\u0026#34;; color = BLUE; } else if (degrees \u0026lt; 25) { description = \u0026#34;mild\u0026#34;; color = ORANGE; } else { description = \u0026#34;hot\u0026#34;; color = RED; } } これをKotlinで書き換えると、以下のようになります。\nfun updateWeather(degrees: Int) { val (description, color) = if (degrees \u0026lt; 10) { Pair(\u0026#34;cold\u0026#34;, BLUE) } else if (degrees \u0026lt; 25) { Pair(\u0026#34;mild\u0026#34;, ORANGE) } else { Pair(\u0026#34;hot\u0026#34;, RED) } } まず二つの変数を、戻り値がPairのオブジェクトの表現式でより短くできることがわかります。そしてこのコードは、when句を使ってより短い形で変えることもできます。その結果が以下です。\nfun updateWeather(degrees: Int) { val (description, color) = when { degrees \u0026lt; 10 -\u0026gt; Pair(\u0026#34;cold\u0026#34;, BLUE) degrees \u0026lt; 25 -\u0026gt; Pair(\u0026#34;mild\u0026#34;, ORANGE) else -\u0026gt; Pair(\u0026#34;hot\u0026#34;, RED) } } さらにPairは、toを使うことでもっと簡単に表現することもできます。そうすると、以下のようなコードになります。\nfun updateWeather(degrees: Int) { val (description, color) = when { degrees \u0026lt; 10 -\u0026gt; \u0026#34;cold\u0026#34; to BLUE degrees \u0026lt; 25 -\u0026gt; \u0026#34;mild\u0026#34; to ORANGE else -\u0026gt; \u0026#34;hot\u0026#34; to RED } } 最初のJavaのコードと比べ、かなり簡単かつ明瞭なコードになっているのがわかります。他の言語を使っていた人だとしても、一瞬ですぐに何をしているのかがわかって、より短く効率なコードになっているのがわかりますね。こういうところこそ、KotlinがJavaの冗長さ−無駄を減らすことに力を入れている部分ではないかと思います。\nコードを簡単に書ける 自分は最初、Kotlinの文法を簡単にみながらswitchがここではwhenに変わって、caseを書く必要がないんだな、くらいの印象しか受けてなかったです。しかし、よくよく見ると、他にもJavaと違うところが良く見えます。ここで読み取れるものは、例えばさっきのコードだと以下がありますね。\nwhen句が表現式として使える when句の条件の対象は条件式の中のみで良い 表現式の中で複数の値を戻り値として定義し、それを toで二つのオブジェクトをPairにまとめることができる 他にも、Javaのswitchに比べKotlinのwhen句は以下のよう活用ができるというところもあります。オブジェクトの比較がより簡単ですね。例えば以下のようなコードで、簡単に二つのオブジェクトに対しても比較が可能になります。\nfun mix(c1: Color, c2: Color) = when (setOf(c1, c2)) { setOf(RED, YELLOW) -\u0026gt; ORANGE setOf(YELLOW, BLUE) -\u0026gt; GREEN setOf(BLUE, VIOLET) -\u0026gt; INDIGO else -\u0026gt; throw Exception(\u0026#34;Dirty Color\u0026#34;) } これをあえてJavaのコードで書くとしたら、おそらく以下のようになるでしょう。個人的に、たくさんのelse ifはあまり読みやすいコードではなく、書く立場としても綺麗ではないと思います。\nprivate Color mix(Color c1, Color c2) { if (c1 == Color.RED \u0026amp;\u0026amp; c2 == Color.YELLOW) { return Color.ORANGE; } else if (c1 == Color.YELLOW \u0026amp;\u0026amp; c2 == Color.BLUE) { return Color.GREEN; } else if (c1 == Color.BLUE \u0026amp;\u0026amp; c2 == Color.VIOLET) { return Color.INDIGO; } else { throw new RuntimeException(\u0026#34;Dirty Color\u0026#34;); } } ここでわかるのは、KotlinではJavaと同じことをするとしても、短いだけでなく、より簡単にコードをかけるということですね。もちろん、別のメソッドを作ったり、Comparableなオブジェクトを作ったり、Comparatorクラスを実装することでJavaでも似たようなことはできるかもしれません。しかし、そこまでしたいかというと微妙ですね。\nもちろん、Java 12からはKotlinのwhenに近い感覚でコードを書くこともできるようになっています。表現式としても使えて、複数の条件を指定することができ、Lambdaの感覚で書けるということも良いですね。\nvar result = switch (month) { case JANUARY, JUNE, JULY -\u0026gt; 3; case FEBRUARY, SEPTEMBER, OCTOBER, NOVEMBER, DECEMBER -\u0026gt; 1; case MARCH, MAY, APRIL, AUGUST -\u0026gt; { int monthLength = month.toString().length(); yield monthLength * 4; } default -\u0026gt; 0; }; このような変化を見ると、この「冗長さを減らす」という面では、Javaもまたバージョンアップとともに新機能を次々と導入してきているので、Kotlinの魅力が半減しているように見えるかもしれません。しかし、Kotlinではもっと重要なポイントがまた一つあります。言語自体の拡張性です。\n拡張ができるということ 言語自体の拡張性と言いましたが、簡単にいうと、以前にも紹介したことのある拡張関数、つまりextensionのことです。Kotlinの仕様としてもこれは大きい部分として紹介されているものですね。これをよく使うと、ただ「継承しなくてもそのクラスにメソッドを追加できる」だけでなく、infixと組み合わせることでまるで予約後であるように使うことができます。\n実際、Kotlin for Java Developersのコーディング問題では、infixで書かれた以下の拡張関数を持って結果の確認を行っていると言われています。\ninfix fun \u0026lt;T\u0026gt; T.eq(other: T) { if (this == other) println(\u0026#34;OK\u0026#34;) else println(\u0026#34;Error: $this != $other\u0026#34;) } このinfixを使うと、以下のようなコードが書けるようになります。\n\u0026#34;ABC\u0026#34; eq \u0026#34;ABC\u0026#34; このような特徴があるということは、使う側からしても便利ですが、これから言語そのもののバージョンアップにしたがってより便利な機能が追加され安いことにもなっていると思います。例えば先ほどのPairオブジェクトを作るtoが、このようにinfix関数として作られているものです。これからもこういった便利な機能が追加され、追加しやすくなるのは確かに開発のコストの削減をという面でも良いことですね。\nもう十分便利であること 冗長さを減らし、拡張性がある言語だという特徴は、おそらくKotlinを作っているJetBrainsにとっても十分有効な特徴であるかと思います。Kotlinのスタンダードライブラリを見ると、すでに便利な関数が多く存在しています。例えば、簡単なループでは以下のようなことができます。\nval list = listOf(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;) // 一般的なfor文 for (element in list) { println(element) } // インデックスを含むfor文 for ((index, element) in list.withIndex()) { println(\u0026#34;$index: $element\u0026#34;) } // インデックスのみのfor文 for (index in list.indices) { println(index) } 以前もJavaのfor文の性能についてのポストで簡単に述べたことがありますが、そこではJavaならインデックスが必要な場合は伝統的なfor文を使い、そうではない場合は一般的に拡張for文を使った方がいいという結論をJMHでのベンチマークで出していました。しかし、こうやってすでに言語から便利な方法を提供していると、性能を気にすることなく便利な方法を取れるという面でも魅力的です。\nそして、forEachでもインデックスが必要であるなら、forEachIndexedを使えるという良い点もあります。例えば、以下のような書き方ができます。\n// 一般的なforEach文 list.forEach(::println) // インデックスを含むforEach文 list.forEachIndexed { index, element -\u0026gt; println(\u0026#34;$index: $element\u0026#34;) } インデックスを簡単に取得できるということは、ループ対象のオブジェクトが持つ全インデックスを取得したい場合に、あえて0のような、マジックナンバーにありえる数値を指定する必要がないというところでも良いですね。Javaだと毎回、static finalなフィールドとして宣言したり、別の定数として管理したりするケースが多いので…\n他にも、正規表現なしでも文字に関して簡単にチェックできる関数が事前に提供されているとか(Char.isLetter()やChar.isDigit()など)、MapにはPairで要素を入れることができるとか、iterableなオブジェクトからStream APIのような操作がすぐできるなど、確かにJavaに比べ「悩む必要がない」のが魅力的と思います。まぁ、人によってはこれはデメリットと認識する可能性もあるのでは、といは思いますが…\n最後に 色々とKotlinの特徴・メリットについて述べましたが、こういう自分もまだ実際に業務でKotlinを使っているわけではないので、まだまだ表面的な知識のみに止まっていると思います。しかし、ここで紹介したことだけでも、Kotlinの魅力は感じ取れるのではないかと思います。\n言語自体も魅力的なのですが、他にもKotlinを扱うことで得られるメリットは多いです。例えば、JetBrainsが開発しているので、Intellijとの相性が良いこと。JVM言語でありJavaとの互換性があるので、Javaの発展をそのまま吸収できるということ。NativeやJavaScriptへのコンパイルもできるということ。他の言語も十分魅力的なポイントはありますが、Javaプログラマーであるなら、一度Kotlinに触れてみる価値はあると信じています。皆さんもまだKotlinに触れたことがないのであれば、この度ぜひ軽い気持ちで挑戦してみてください。\nでは、また！\n","date":"2021-02-28T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-basics/","title":"JavaプログラマーがみたKotlin〜その二〜"},{"content":"もうこれで3回目の、「今更なシリーズ」です。このシリーズ自体は、またベンチマークとともに戻ってきました。さて、今回のテーマはJavaによる文字列の操作となりますが、その中でも連結(Join)と分割(split)について述べたいと思います。最初は単純に、文字列の分割はString.split()でやるしかないのに、連結の場合はString.join()とかCollectors.joining()とか、色々あるなと思ったのがきっかけです。同じことが複数のAPIでできるのは、単純にシンタックスシュガーな場合もありますが、実際は全く実装が違うケースもありますね。特に、Javaのように長い間使われてきた言語こそそのようなケースが多いかと思います。\nまた、単純なシンタックスシュガーに近い場合でも、その前後のコードや可読性など、周りの様相を考慮して適切なものを選ぶ必要がある場合もあります。例えば、以前紹介したInputStreamのtransferTo()がそのようなケースですね。なので、一つのAPIを使う場合は、できればその実装がどうなっているかを確認してみるのも、良いコードを書くための工夫となるのではないかと思います。\nではでは、早速本題に入りましょう。まずは文字列の連結からです。\nConcatenating 文字列の連結といっても、色々なケースがありますね。そしてそういった場合は、String.concat()、String.format()などさまざまな方法があって、それら全部に対してシナリオを想定し検証するということは難しいと思います。なので今回は、「文字列の配列もしくはCollectionを、区切り文字でつないで一つの文字列にする」というケース一つに限定して述べたいと思います。\nJavaでの区切り文字を使った文字列の連結には、主に以下のような方法が考えられます。これら一つ一つのAPIの特徴と、実際の使い方を持って比較して見た後、いつもの通りベンチマークをするということで性能を測定することとします。(+を使って文字列を繋ぐケースは、あまりよろしくないと思うのでケース外としています)\nString.join() StringJoiner StringBuffer StringBuilder Collectors.joining() StringBuffer || StringBuilder 純粋に、Collectionや配列になっている複数の文字列を連結する場合もあるとは思いますが、普通、文字列の連結が必要となる場合では、「とある規則によって」という条件がつくケースが多いかなと思います。例えば、ダッシュ(-)、アンダースコア(_)、カンマ(,)などで並ぶようにですね。そしてこのような規則がある場合、StringBufferやStringBuilderを使った方法は他と比べて少し不利です。なぜなら、最後に区切り文字(delimiter)が付かないように制御するにはかなりコードの書き方に注意しなければならないからです。以下のコードが、そのようなケースです。\n// StringBufferを使う例 List\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); String delimiter = \u0026#34;, \u0026#34;; StringBuffer buffer = new StringBuffer(); // Listの要素と区切り文字を足す for (String string : list) { buffer.append(string); buffer.append(delimiter); } String result = buffer.toString(); // A, B, C, あえて、文字列の末尾に区切り文字が付かないようにするとしたら、おそらくこういうコードを書く必要があるでしょう。\nList\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); String delimiter = \u0026#34;, \u0026#34;; int limit = list.size() - 1; StringBuffer buffer = new StringBuffer(); // Listの要素と区切り文字を足す(最後のインデックスの前まで) for (int i = 0; i \u0026lt; limit; i++) { buffer.append(list.get(i)); buffer.append(delimiter); } // 最後の要素を足す buffer.append(list.get(limit)); String result = buffer.toString(); // A, B, C こういう問題があるのに比べて、他の方法(String.join()、StringJoiner、Collectors.joining())は、区切り文字が最後の要素の後に付かないので、よりシンプルなコードで書けるというメリットがありますね。なので、結論としてStringBufferやStringBuilderは、少なくとも「とある規則によって」複数の文字列を連結する場合には可読性という観点からしてあまり良い選択肢ではないということがわかります。\nStringJoiner StringBufferとStringBuilderではループで文字列を連結して行くので、ループの中で条件分岐など他の処理も必要な場合に使えるのでは？と思われるかもしれません。しかし、そういう場合でも、StringJoinerを使ったほうが良いですね。なぜなら、使い方はほぼ変わらなく、特に操作をしなくても常に末尾に区切り文字が付かないからです。以下は、StringJoinerのもっともベーシックな使い方のコードとなります。\nList\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); // 区切り文字を指定してインスタンスを作る StringJoiner joiner = new StringJoiner(\u0026#34;, \u0026#34;); // あとは要素を足していく for (String string : list) { joiner.add(string); } String result = joiner.toString(); // A, B, C また、StringJoinerを使った場合は、PrefixとSuffixの指定も可能です。これらを指定した場合、文字列の先頭と末尾に指定したPrefixとSuffixが付くようになります。\nList\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); // 区切り文字とPrefix、Suffixまで指定する StringJoiner joiner = new StringJoiner(\u0026#34;, \u0026#34;, \u0026#34;[\u0026#34;, \u0026#34;]\u0026#34;); for (String string : list) { joiner.add(string); } String result = joiner.toString(); // [A, B, C] 使い方だけ見ても、区切り文字を持って文字列を繋ぐ場合はStringBufferやStringBuilderよりStringJoinerの方がより簡単であるということが分かります。\n番外：StringJoinerの実装 ついでに、StringJoinerはどんなコードで書かれているかを見ていきたいと思います。まずはadd()ですが、これは面白くも、ArrayListの実装と似たような感じになっています。StringJoinerクラスはフィールドとしてString[]を持っていて、add()がよばれる度にそれより大きいコピーを作っていく形です。\npublic StringJoiner add(CharSequence newElement) { final String elt = String.valueOf(newElement); if (elts == null) { elts = new String[8]; } else { if (size == elts.length) elts = Arrays.copyOf(elts, 2 * size); len += delimiter.length(); } len += elt.length(); elts[size++] = elt; return this; } そしてtoString()では、フィールドのString[]をループしながら、区切り文字とともに繋げて行くのが分かります。少し変わっているのは、性能を意識しているからか、char[]として文字列をつめた後から新しくStringのインスタンスを作って返しているというところですね。\npublic String toString() { final String[] elts = this.elts; if (elts == null \u0026amp;\u0026amp; emptyValue != null) { return emptyValue; } final int size = this.size; final int addLen = prefix.length() + suffix.length(); if (addLen == 0) { compactElts(); return size == 0 ? \u0026#34;\u0026#34; : elts[0]; } final String delimiter = this.delimiter; final char[] chars = new char[len + addLen]; int k = getChars(prefix, chars, 0); if (size \u0026gt; 0) { k += getChars(elts[0], chars, k); for (int i = 1; i \u0026lt; size; i++) { k += getChars(delimiter, chars, k); k += getChars(elts[i], chars, k); } } k += getChars(suffix, chars, k); return new String(chars); } String.join() String.join()は、InputStream.transferTo()のように、あくまでシンタックスシュガーとして存在するものだと言えます。以下は実際のコードです。\npublic static String join(CharSequence delimiter, Iterable\u0026lt;? extends CharSequence\u0026gt; elements) { Objects.requireNonNull(delimiter); Objects.requireNonNull(elements); StringJoiner joiner = new StringJoiner(delimiter); for (CharSequence cs: elements) { joiner.add(cs); } return joiner.toString(); } 引数に対するNullチェック以外は、Prefix・SuffixなしのStringJoinerでの連結になっているということを確認できます。なので、より短いコードを書きたい場合はStringJoinerを使うよりも、こちらの方が便利ではありますね。\nCollectors.joining() 文字列の連結でStreamを利用する場合、他にもfilter()、map()、peek()など、さまざまな処理をメソッドチェイニングで書けるというところが魅力的ですね。個人的には、処理の役割と目的・影響範囲が明確に見えるので、Streamによる処理を好んで使っています。ただ、以前のポストでも書いたことがありますが、多くの場合にStreamは伝統的なループより性能面で不利ですので、時と場合によって適切に選ぶべきでしょう。\nさて、そんなStreamですが、中の実装はどうなっているのでしょうか。Collectors.joining()の場合、以下のような実装となっています。結局は、StringJoinerを内部で使っているだけですので、String.join()・StringJoinerと比べては、Streamによるコードの変化や性能に影響されるだけと言えるでしょう。\npublic static Collector\u0026lt;CharSequence, ?, String\u0026gt; joining(CharSequence delimiter, CharSequence prefix, CharSequence suffix) { return new CollectorImpl\u0026lt;\u0026gt;( () -\u0026gt; new StringJoiner(delimiter, prefix, suffix), StringJoiner::add, StringJoiner::merge, StringJoiner::toString, CH_NOID); } toString() 実は、Collectionの場合(List\u0026lt;String\u0026gt;)は、もっと簡単に文字列を作る方法がありますね。toString()を呼ぶことで、簡単にカンマ区切りの文字列が出来上がります。ただ、そうして文字列を作った場合、先頭と末尾に[]が入ってしまうので、場合によってはそれらを取り消すか、substring()で抽出するかの追加的な処理が必要となりますね。以下は、substring()を利用して[]の中の文字列だけを切り取るサンプルとなります。\nList\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); String toString = list.toString(); // [A, B, C] String result = toString.substring(1, toString.length() - 1); // A, B, C また、もし区切り文字がカンマではない場合は、とりあえずtoString()で文字列に変換した結果の文字列から、更にreplace()を呼び出し、区切り文字だけを入れ替えるというやり方でも対応はできます。ただ、これは非常に非効率的なやり方ではあります。なぜなら、replace()のコードをみると、結局はループの中でStrinbBuilderを使って新しく作り出すような構造となっているからです。実際のコードは、以下の通りです。\npublic String replace(CharSequence target, CharSequence replacement) { String tgtStr = target.toString(); String replStr = replacement.toString(); int j = indexOf(tgtStr); if (j \u0026lt; 0) { return this; } int tgtLen = tgtStr.length(); int tgtLen1 = Math.max(tgtLen, 1); int thisLen = length(); int newLenHint = thisLen - tgtLen + replStr.length(); if (newLenHint \u0026lt; 0) { throw new OutOfMemoryError(); } StringBuilder sb = new StringBuilder(newLenHint); int i = 0; do { sb.append(this, i, j).append(replStr); i = j + tgtLen; } while (j \u0026lt; thisLen \u0026amp;\u0026amp; (j = indexOf(tgtStr, j + tgtLen1)) \u0026gt; 0); return sb.append(this, i, thisLen).toString(); } 少なくとも区切り文字がカンマではない場合は、toString()とreplace()での文字列の生成よりは、他の方法をとったほうが性能面では有利ではないか、という推測が可能です。もちろん、要素数という変数があるので、実際の性能は測ってみないとわからないものですが…\nベンチマークしてみる(1) では、文字列を連結するために使える色々なAPIと、その特徴を簡単に把握できたので、次に確認したいのは、やはり性能です。特に気になるのは、String.join()やCollectors.joining()でも結局は内部でStringJoinerを使っているというところです。それはつまり、StringBufferやStringBuilderよりもStringJoinerが性能で有利だから、でしょうか。\nこれらのAPIを利用して、実際のアプリケーションに使われるビジネスロジックのコードを書く立場としては、それはコードを簡単に書ける方が良いのは当然ですが、そもそもこういうAPIの場合は、手間を省けるために性能は良くても複雑なコードで実装する可能性もあるのですので、疑問になります。しかも、多くの場合、文字列の操作ではStringBuilderが早いと言われていますので、ますます性能差というのが気になってきます。なので、いつもの通りにベンチマークを実施してみました。\nベンチマークは、カンマ区切りで文字列を連結する例として作成しています。以下がそのコードです。\n@State(Scope.Benchmark) public class StringConcatTest { private static final String DELIMITER = \u0026#34;, \u0026#34;; private List\u0026lt;String\u0026gt; target; @Setup public void init() { final DecimalFormat format = new DecimalFormat(\u0026#34;0000000\u0026#34;); this.target = IntStream.rangeClosed(0, 1000000).mapToObj(i -\u0026gt; format.format(i)).collect(Collectors.toList()); } @Benchmark public void toString(final Blackhole bh) { final String toString = target.toString(); bh.consume(toString.substring(1, toString.length() - 1)); } @Benchmark public void stringJoin(final Blackhole bh) { bh.consume(String.join(DELIMITER, target)); } @Benchmark public void collectorsJoining(final Blackhole bh) { bh.consume(target.stream().collect(Collectors.joining(DELIMITER))); } @Benchmark public void stringBuffer(final Blackhole bh) { final StringBuffer buffer = new StringBuffer(); final int limit = this.target.size() - 1; for (int i = 0; i \u0026lt; limit; i++) { buffer.append(this.target.get(i)); buffer.append(DELIMITER); } buffer.append(this.target.get(limit)); bh.consume(buffer.toString()); } @Benchmark public void stringBuilder(final Blackhole bh) { final StringBuilder builder = new StringBuilder(); final int limit = this.target.size() - 1; for (int i = 0; i \u0026lt; limit; i++) { builder.append(this.target.get(i)); builder.append(DELIMITER); } builder.append(this.target.get(limit)); bh.consume(builder.toString()); } } そして、結果は以下の通りです。\nBenchmark Mode Cnt Score Error Units StringConcatTest.toString thrpt 25 41.445 ± 0.461 ops/s StringConcatTest.stringJoin thrpt 25 28.396 ± 0.447 ops/s StringConcatTest.collectorsJoining thrpt 25 31.024 ± 1.313 ops/s StringConcatTest.stringBuffer thrpt 25 30.570 ± 1.205 ops/s StringConcatTest.stringBuilder thrpt 25 45.965 ± 1.736 ops/s この結果からわかるのは、やはりStringBuilderの性能は優秀ということですね。ただ、よく知られているように、StringBuilderはマルチスレッドを考慮したAPIではないので、スレッドセーフなAPIを使う必要のある環境であるなら、他のAPIを考慮すべきですね。そのような観点からすると、意外と、誤差範囲を踏まえて考えるとString.join()がCollectors.joining()と大差ない性能を見せるという結果となりましたが…このような結果だとすると、気軽にStreamを使っても良さそうな気がします。\nまた、toString()の結果は、やはり早いものとなっていますが、ここでreplace()を挟んだ瞬間性能は半分以下という結果となっています。なので、無理してtoString()を使う必要はあまりないかな、と思いますね。文字列の連結という目的に合うコードかどうかもすぐわからないし…\nもう一つ確かなのは、StringBufferはもう使わなくても良さそうということですね。もうレガシーなコードとして残しておいて、これからはなるべく違うAPIを使うべきなのではないかと思います。\nSplit 次に検証したいのは、文字列の分割です。先に述べたのように、文字列の分割は実質、String.split()しかない状態と言えますね。substring()でもなんとか分割はできるかもしれませんが、その場合はループと条件分岐なしでは話にならないので、そもそも論外かと思います。\nただ、ここで注目したいのは分割した後のことです。String.split()の戻り値はString[]なので、場合によってCollectionに変えたくなりますね。なので、どちらかというと「配列をListに」する方法の検証ということとなりますが…とりあえずListをStringに変えてみたので、その逆の場合を考えてみるということで受け止めてくださると幸いです。\nArrays.asList() 配列をListに変えるもっとも簡単な方法は、Arrays.asList()だと思います。コードも簡単ですね。\nString string = \u0026#34;A, B, C\u0026#34;; // まずは分割する String[] array = string.split(\u0026#34;, \u0026#34;); // Listに変える List\u0026lt;String\u0026gt; list = Arrays.asList(array); ただ、こうやって生成したListのインスタンスは、Immutableとなってしまいます。中の要素を操作できないということですね。\nもちろん、これは新しいListのインスタンスに要素をコピーすることで解決できます。もっとも簡単なのは、コンストラクタの引数としてListを渡す方法ですね。なので、「配列をMutableなListにする」もっとも簡単な方法は、おそらく以下のようになります。\nString string = \u0026#34;A, B, C\u0026#34;; // まずは分割する String[] array = string.split(\u0026#34;, \u0026#34;); // Listに変える List\u0026lt;String\u0026gt; list = Arrays.asList(array); // MutableなListのインスタンスを作成する List\u0026lt;String\u0026gt; mutableList = new ArrayList\u0026lt;\u0026gt;(list); Arrays.stream() 配列をListにするまたの方法は、Streamを利用することです。文字列の連結でも言及したことなのですが、Streamの場合は、map()やfilter()のような中間操作のメソッドを使えるというメリットがありますね。また、Collectorsのどのメソッドを呼ぶかによって結果として生成されるListがImmutableか、Mutableかを決定できるという面もメリット(可読性という観点で)ではないのかと思います。コードはArrays.asList()と比べて少し複雑になっているように見えるかもしれませんが。\nString string = \u0026#34;A, B, C\u0026#34;; // まずは分割する String[] array = string.split(\u0026#34;, \u0026#34;); // Listに変える(Mutable) List\u0026lt;String\u0026gt; mutableList = Arrays.stream(array).collect(Collectors.toList()); // Listに変える(Immutable) List\u0026lt;String\u0026gt; mutableList = Stream.of(array).collect(Collectors.toUnmodifiableList()); ベンチマークしてみる(2) では、次にまたベンチマークとなります。コード自体は明らかにArrays.toList()の方が簡単だったのですが、MutableなListを作るためにはListを生成した後にさらにインスタンスを作成する必要があるということで、性能面で損する可能性もあるのかなという気がします。なので、以上で紹介したArrays.asList()とStreamによるListのインスタンスの作成を、Immutable・Mutableという二つのケースに分けて検証してみました。以下がそのベンチマークのコードです。\n@State(Scope.Benchmark) public class StringSplitTest { private static final String DELIMITER = \u0026#34;, \u0026#34;; private String target; @Setup public void init() { final DecimalFormat format = new DecimalFormat(\u0026#34;0000000\u0026#34;); this.target = IntStream.rangeClosed(0, 1000000).mapToObj(i -\u0026gt; format.format(i)).collect(Collectors.joining(DELIMITER)); } @Benchmark public void arraysAsListImmutable(final Blackhole bh) { bh.consume(Arrays.asList(target.split(DELIMITER))); } @Benchmark public void arraysAsListMutable(final Blackhole bh) { bh.consume(new ArrayList\u0026lt;\u0026gt;(Arrays.asList(target.split(DELIMITER)))); } @Benchmark public void streamCollectImmutable(final Blackhole bh) { bh.consume(Arrays.stream(target.split(DELIMITER)).collect(Collectors.toUnmodifiableList())); } @Benchmark public void streamCollectMutable(final Blackhole bh) { bh.consume(Arrays.stream(target.split(DELIMITER)).collect(Collectors.toList())); } } そして結果は以下の通りです。\nBenchmark Mode Cnt Score Error Units StringSplitTest.arraysAsListImmutable thrpt 25 8.316 ± 1.085 ops/s StringSplitTest.arraysAsListMutable thrpt 25 8.133 ± 0.435 ops/s StringSplitTest.streamCollectImmutable thrpt 25 6.086 ± 0.312 ops/s StringSplitTest.streamCollectMutable thrpt 25 7.247 ± 0.262 ops/s ここでは、Arrays.asList()の方が、性能が高い結果となっていますね。途中で何かしらの操作が必要な場合はStreamの方が良いかと思いますが、そうではなく、単純に配列をListに変えたい場合はやはりArrays.AsList()を使った方がコードもより簡単で、性能面でも少し優勢ということがわかりました。なので、(いつもそうですが)何をしたいかによって適切なコードを選ぶべきかんと思います。\n最後に 他にも、文字列の操作に関してはBaeldungさんの記事がかなり良かったので、皆さんにもおすすめしたいと思います。最近は特に、アプリケーションでもっともよく扱うデータ型が文字列となっているので、文字列の操作に関してはなるべく性能と可読性という観点から良い書き方を取りたいものです。個人的にはStreamが大好きなので、なるべくなんでもStreamで解決したいものですが…Javaだけでなく、プログラミング言語にとって「どんなケースでも正解」というものはないので。\nしかし、Javaに触れてからもう3年も過ぎていますが、今更こんなことを考えるということが恥ずかしい限りですね…次からは、もっと興味深い(そしてこのブログを読まれる方々にも役立つような)ネタを探したいと思います。うまくいくかは少しわからない状態なのですが…！\n","date":"2021-02-08T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-string-concat-and-split/","title":"今更な文字列操作の話"},{"content":"エンジニアとして働いていると会社の方針・クライアントの要求・経歴のような、自分の意思以外のところから自分の技術スタックを決まってしまう場合が決して少なくないと思います。会社は利益集団なので、致し方ないのですが、個人としてはどうでしょうか。私は、エンジニアは常にトレンドとともに前に進むべき職種なので、業務としてはあまり機会がないとしても、やはり自分で何かロードマップを立てて、独学でもスキルアップを図るべきたと思っています。\n例えば私は、どんな案件でも主にJavaとSpringのサーバサイドエンジニアか、Jenkins・Shell・Linuxを触るインフラエンジニアとして働いたことが多いのですが、何か一つは自分のアプリやサービスを作ってみたいと思っています。このような目標ないしは願望がある場合、それを成すためには何が必要か、と考えるようになり、そこから適合なプラットフォームは？言語は？フレームワークは？という風に考え始めて、そのうちでもっとも自分にとって合理的な道を選ぶようになります。何が合理的か、という基準は人それぞれですが(主観だけで決められるものでもないし)、会社が自分のエンジニアとしての目標を考えてくれる可能性は低いので、とにかくこういう目標設定は自分でなすべきでしょう。\nそういう意味で、今年の自分のロードマップを、「やりたいこと」と「良さそうなこと」という基準からいくつか立ててみました。まだロードマップとしては何一つ計画を具体化してないので、ただの興味に近いものなのかもしれませんが…とにかく、今の時点で興味を持っているものや考えていることについて、Google Trendを持って語ります。軽く、「こいつは2021年にこういうものに注目するんだな」と思ってください。\n言語 TypeScript いつまでになるかはわかりませんが、少なくともここ数年はJavaScriptの天下が続きそうですね。ただ、なぜそうかというと、Webの標準であるという強力な基盤がある上に、今はNode.jsやElectronのおかげでブラウザ以外でも使える場面が多いから、ということだけでは説明が難しくなりつつあるという側面もある印象です。今はもはやコーディングを学び始めるきっかけや入門の言語としてJavaScriptに触れるケースが多いし、SPAの登場以来からサーバサイドよりもフロントエンドの重要さが増してきたという感覚でもありますね。アプリケーションというのは、結局はユーザのためにデザインされるものであるということを考えると、より画面に密接な言語が持つ権限の方が大きくなるのは当然なのかもしれません。\nそしてバックエンドだけをみるとしても、最近はなるべくサーバサイドの役割を減らしていくか、細かく分けていく感覚ですね。マイクロサービス、BFF、サーバレスのようなキーワードが流行っているのがその証拠だと思います。もちろんJavaScriptという言語そのものの発達によるものもあるとは思いますが、アプリケーションのアーキテクチャやデザインの思想そのものが変わっているので、仕方ないことです。\nそこで、少なくともJavaScriptは基礎だけでもできるようにしないと、と思いました。研修などで簡単な文法については学んだことがありますが、本格的なアプリを書いた経験はあまりなかったので、少なくともExpressで簡単なREST APIを作ってみるとかの経験はかなり役立つかもしれません。また、フロントエンドも少しは触れるようになるとよりいいでしょう。\nこのように思ったときに、目に入ってきたのがTypeScriptでした。TypeScriptは以前、Udemyの講座で接したことがあり、気に入っていましたが、最近はかなり人気を得ているらしいですね。実際どうかは、まずGoogle Trendで確認してみました。\n結果をみると、確かにTypeScriptに対する興味は日々増えていっているような気がします。おそらくAngular・React・Vueのような有名フレームワークやライブラリからTypeScriptに公式対応し始めたのも理由と思いますし、やはり静的型付けの方が生産性が上がるというところがわかったきたからなのでしょう。私はJavaから触れた人間なので、静的型付けのできるTypeScriptの方を学んだ方が良いかなと思います。\nGo or Rust 個人的には、JVM言語が好きですが、やはり高水準言語\u0026amp;VMを挟む構造ということもあり、より低水準に近い言語も扱ってみたいと思っています。今すぐに必要なわけではないのですが、やはりハードウェア制御やバイナリデータを扱うなど、低水準言語ならではのことがやってみたいという純粋な好奇心が理由です。最近はIOTなどでC言語の人気も高くなっていたりしますが、組み込み系ならまだしも、いわゆる「応用ソフトウェア」を開発する身としては、CやC++、もしくはそれよりも古い言語よりは、GoやRustのような言語に触れてみた方が良さそうな気もします。\nただ、悩ましいのは、それでGoとRustのうち、どれを選ぶかということです。性能だけを考えたら、当然Rustなのかもしれません。多くの場合、Rustが性能ではGoより優れていると言われていますね。実際の例として、音声チャットツールで有名なDiscordはGoからRustに移行しましたが、これが性能のためだったと言っています。ただ、言語を学ぶこと自体の難易度は、やはりRustの方がGoより高いらしいですね。そして一般的に、生産性の方はGoが優れていると言われています。\nなので、以上のことから、自分は何をやってみたらいいのかなと思ってみた結果、CやC++に近い低水準言語の感覚としてRustを触ってみたらどうかな、と思いました。どちらもマイナーな言語ではありますが、Stack Overflow surveyにて、数回も「もっともエンジニアから愛された言語」として選ばれたこともあるRustの方が、これからコミュニティの成長も期待できるのではないかなと思ったからでもあります。特に2020年の結果は、86.1%というすごい結果になっているくらいですので。\nしかし、まだ実務レベルでよく使われているのはやはりGoの方で、リファレンスの量やエンジニアの興味という面でも(仕事で使うとしたら仕方ないのですが)、Goの方がまだ少し優勢ではないのかという気もします。Rustが最初からCやC++の代替を目標としてデザインされた言語であることに比べ、Goという言語がどこまでそのような役割ができるのかというのはまだあまりわかってないのですが、もし同じようなことできるのであれば、あえてRustにこだわる必要はないのではないかと思ったりもしますね。特に、ある言語の成長性というのは、そのコミュニティの大きさにも関係するので…なので、とりあえずGoogle Trendの方で、二つの言語に対する興味度について調べてみました。\nGoの場合は一般動詞(行く)と区別するため、多くの場合golangで検索するケースが多いらしいです。しかし、Rustもあまり状況は変わってなくて(しかも、ゲーム名としても使われているようですね)、rustlangという検索語はあまり使われてないと思うので、直接的な比較が難しいですね。なので、なるべく価値中立的なキーワードとして、go programmingとrust programmingを選んでみました。そしてその結果が、以下です。\n結果だけをみると、やはりRustの方がすごい人気を得ているように見えますが、まだGoの方が優位にはありますね。なので、こちらの方が(急ぎでもないので)、もう少し観望しながら、ゆっくり決めようと思っています。\nKotlin 今はJavaのいうWrite once, run everywhereが、どの言語でも同じようなことができていて(逆にJavaでできない分野はありますが)、それでもJVM言語は依然として魅力的だと思っています。最初からJVMがヒープを設定するのでメモリ管理という面でも安定的で、パフォーマンスも今流行りの高水準言語に比べても優秀な方ですね。また、10年以上世界でもっとも人気な言語だったので、ライブラリ・フレームワーク・リファレンスも豊富ですね。また、バイトコードだけを生成すればいいので、コンパイルする前の言語はどれでも良いです。なのでJava以外でもScala、Clojure、Groovy、Jython、JRuby、Ceylon、Frege、Eta、Haxeのような幾多の言語がJVMを利用できるようになっているわけですね。つまり、JVMこそ死なないが、Javaという言語そのものはこれらの言語のどれかに代替できるというわけです。\nそしていろんな言語の候補があるわけですが、その中でも個人的にはKotlinを選びました。近年のJavaも急激なバージョンアップを重ねながら改善されてはいるものの、実際エンタープライズレベルでそういったバージョンアップの効果を期待できるのはLTSバージョンがでた時だけですね。なので、いますぐ生産性を上げながらもJVMをそのまま利用できるという面では、Kotlinのようなモダンな言語への転換を考えるにはちょうどいい時期なのではないかと思っています。もちろん、私みたいにモバイルアプリの開発を考えているとしたら、尚更ですね。\n他にもGoogle推しの言語であることや、Kotlin/NativeやKotlin/JSなど他の言語でコンパイルできるという点も良いですね(実際Wantedlyでは、すでにKotlin Multiplatformを導入しているらしいです)。そして何より、Kotlinを開発しているのがJetBrainなので、Intellijでは完璧なサポートができるというところも無視できないメリットです。ほんと少しだけですが、使ってみた鑑賞としても、完成度がかなり高い感じの言語だったので(Swiftよりも)、そのようなところからKotlinの未来はかなり明るいと思っています。\nフレームワーク \u0026amp; ライブラリ Svelte 先ほど少しJavaScriptの話をしましたが、JavaScriptそのものの需要や重要性については語るまでもないとはいうものの、そのJavaScriptのフレームワーク・ライブラリはどれが良いかという課題だけは、少なくとも数年でこれが正解と言えるような状態ではないかと思います。ここ数年で幾多のフレームワークやライブラリが生まれ、消えていってますね。幸い、いわゆるフロントエンド3強のAngular・React・Vueの中ではReactが勝者になりつつある雰囲気ではあります。Google Trendの結果も、それを見せていますね。\nしかし、フロントエンド以外の世界はまた話が違います。まだ多くのフレームワークやライブラリが乱立していて、まるで戦国時代のような様子です。こんななかでは、一体どれを選ぶべきか悩ましいし、判断のための調査だけでもかなりの時間と努力が必要となります。このような状況なので、もう数年前から流行っている言葉なのですが、JavaScript Fatigue(JavaScript疲労)という言葉があるくらいです。それだけ現代のJavaScriptを学ぶということは大変なことでしょう。\n例えば私みたいに、ほとんどJavaScriptの経験がない人がフロントエンドエンジニアとなって、Reactがもっとも人気があるからそれをやる、と決めたら、まずNode.jsから初めて、パッケージ管理としてはnpmを使うか、yarnを使うか、言語はJavaScriptそのままにするかそれともTypeScriptにするかを決め、次に必要なものとしてWebpackやBabel、Reduxを学ぶなどと、知っておくべきものと学ぶべきものが多いです。しかも、それぞれのフレームワークやライブラリがその名前だけでは何が何だかわからなくなります。Nuxt.jsはVue基盤のフレームワークだけど、Nest.jsはNode.js用のフレームワークですね。そしてNext.jsはまた、React基盤のフレームワークです。この中では、一体どれを学んだらいいか、どれが良いかというのは混乱するだけです。なのでJavaScriptを扱うエンジニアが、疲労を感じるのも当然のことでしょう。\n自分の場合はすでにサーバサイドの実装がある程度はできるので、フロントエンドも触れるようになって、いわゆるフルスタックとして自分一人でアプリが書けたらいいなと思っています。ただ、会社で使われているフロントエンドのフレームワークがあればそれに触れたら良いのですが、個人レベルでは何が良いかはまだ悩ましいものですね。Reactがもっとも人気だから、やはりそれを選ぶべきか？それもいい選択なのかもしれませんが、これからも本格的にフロントエンドの開発に関わるつもりではない限り、本格的にフロントエンドに時間を投資するのはもったいない気もします。そこで考えた代案が、Svelteでした。\nSvleteの特徴(メリット)としては、色々とありますが、私がもっとも注目したところはかなりシンプルであるというところでした。コードが短いので、書き方に慣れるのが圧倒的に早そうな気がします。そのほかは付加的なメリットとしてよく、とにかく「必要な時にサクッとかける」ものとしては、かなり良さげなものではないかなと思ったりもします。もちろん、ちゃんとしたフレームワークなので、本格的なアプリケーションを作る時も良いでしょう。\nただデメリットとしては、やはりメジャーな3強に比べてそこまで知られても、使われてもないというところです。幸い、Google Trendで確認したところ、少しづつながら注目を得ているのでこれからな気はします。\nFlutter 今はWebアプリケーションばかり書いている私ですが、モバイルの方にも興味があり、どのような言語とフレームワークがあるかだけは把握しておきたいと思っています。そして最近は、モバイルは多くの場合ネイティブよりもハイブリッド・クロースプラットフォームの方が多くなっているような気がします。正確なデータや統計をみたわけではないのであくまで推測に過ぎないのですが、多くの場合ネイティブアプリに投資する時間や予算の余裕のないスタートアップやベンチャー企業の場合は、とりあえずハイブリッド・クロースプラットフォームを好むような印象です。もちろん、複雑な演算やOS特有の機能を使うとしたらやはりネイティブと言われていますが、個人的な経験からだと、意外とハイブリッド・クロースプラットフォームでもできることは多いのでもうOSレベルでもなく、機器固有の機能を活用する必要がなければ大体ハイブリッド・クロースプラットフォームでも事足りるのでは、と思います。\n(ここで個人的な経験というものは、iOS 14から導入されたウィージェット機能を活かした簡単なアプリを作ってみたいなと思い調べたところ、OS固有の機能なので難しいのではないかと思ったものが、意外とReact NativeやFlutterでも十分できるということがわかったことです)\nそして、昔はただのWebViewでできていたアプリも多かったような気がしますが、それならあえてモバイルアプリとして作る必要がないですね(PWAならわかりますが)。でもそのような形のアプリがあったからか、Webの技術から影響され生まれたバイブリッドモバイルアプリのフレームワークもかなり多いような印象です。なのでJavaScriptでコードを書いたり、JavaScriptのフレームワークを基盤にしてアプリを書けるフレームワークがかなり多いですね。例えばApache Cordova、Ionic、NativeScript、React Nativeがそのようなものです。もちろんJavaScript(Web)とは違う系統、つまり伝統的なデスクトップアプリを継承している印象のフレームワークとしてC#基盤のXamarinとDart基盤のFlutterがありますね。\nこれだけ多いハイブリッド・クロースプラットフォームモバイルアプリ用のフレームワークですが、中でもそろそろ淘汰されてそうな技術はあります。またここでGoogle Trendの結果をみてみましょう。5つの項目しか比較ができないので、Flutterは入れてないです。\n少なくとも、NativeScriptにはあまり興味を持っている人がいなく、XamarinやCordovaの場合もだんだん興味が下がっているのを確認できます。そうすると、残りの結果としてはIonicとReact Nativeが残りますね。先ほどフロントエンドの話を少ししましたが、最近のフロントエンド3強の勝者がReactになりそうという現実からして、Web技術に基盤したハイブリッド・クロースプラットフォームモバイルアプリ用のフレームワークは、やはりReact Nativeが適切かなと思います。\nしかし、問題となるのはFlutterです。FlutterはReact Nativeと比べられる場合が多いですね。なので、FlutterともGoogle Trendで比較してみることにします。結果としてはReact nativeと比べFlutterが優勢な気がしています。\n理由として上げられるのは、どちらもiOSとAndroidアプリを同時開発できるものであるという点を踏まえると、やはりパフォーマンス問題ではなかったのかという気がします。React Nativeでは、JavaScriptからネイティブコードを呼び出すという構造から必然的にボトルネックになるしかないと言われていますので。そして、あくまで推測なのですが、Dartという別の言語を採用していながらも、JavaやC#のような言語とかなり文法が似ていて、HTMLやXMLとは違う宣言型でのUIの実装ができるというところも、Flutterならではのメリットなのではないか、という気もします。\nもし自分がモバイルアプリを作るとしたら、おそらくネイティブになる可能性が高いのではないかとは思いますが(ハイブリッド・クロースプラットフォームが必要であれば、大抵Web基盤のアプリで事足りそうなので)、場合によってはハイブリッド・クロースプラットフォームも良い選択肢になるでしょう。そしてFlutterはモバイルだけでなく、より多くのプラットフォームのためのフレームワークとして成長していく予定なので、もし今から学ぶとしたらFlutterの方が良いかもしれません。もちろん、Reactがすでにできるフロントエンドエンジニアだとしたら、React Nativeの方が良いとは思いますが、それ以外の場合はやはりFlutterの方が良さそうな気がします。なので、当面はFlutterを視野に入れておきたいものです。\nそのほかに、React Nativeに関しては興味深い記事がいくつかあったので、いくつかの事例を以下に記載します。\nReact Native at Airbnb React Native: A retrospective from the mobile-engineering team at Udacity React Nativeを採用すべきか〜Shopifyに学ぶ ハードウェア Apple silicon Mac 私はもともと20年ほどOSはMicrosoftの製品ばかりを使ってきたものです。それがたまたま、iPhoneやiPadから初めてAppleの製品に触れてから思ったよりも自分との相性がよかったので(仕事でLinuxを使っていたので気軽にターミナルを使えるという点が大きいのですが)、今後も引き続きMacを使いたいと思っています。少なくとも、自分の環境ではMacではないと困ることはあっても、Windowsでないと困ることはあまりないですので。\nなので、自然にApple Silicon Macにも興味を持ったわけなのですが、やはりいきなりCPUのアーキテクチャが変わるということは、やはり互換性を担保できない問題があるので、その問題に対してAppleはどのような形で解決策を出すのだろう、という疑問を持っていました。発表直後に色々な記事を読んでみてから予想できたのは、少なくとも「性能(演算・発熱・電力消耗を含め)はIntelより優れている」というところでしたが、それはアーキテクチャがより進んだ工程で作られているからか、それともカスタマイズによるものか、またどれだけ優れているかというのはわからない状態でした。なので「2年で移行する」という話を信じ、まずは様子を見ようとしていました。\nそして今はM1チップのMacが色々とでていて、その性能も検証されていますね。確かなのは性能だけを見ると既存のIntel Macを買う理由はもはやないかのように見えます。さすがに互換性という不安要素があるのに、CPUのアーキテクチャを変えるという宣言をするぐらいのものではあると思います。しかし、やはりエンジニアたる身としては、互換性と安定性にまず目が行くものです。なぜなら、私は最初のメジャーバージョンは必ず、何かわかってない問題を抱えている可能性が高いというのを経験で実感しているからです。実際、Bluetooth問題や初期化が難しい点、スリープモードから起き上がらない問題などが一部で報告されていて、外装ディスプレイも公式的には1台しか対応しないという問題もあります(おそらく、Thunderbolt 3の大域幅の問題なのではないかと思います)。また、続々とUniversalバイナリやM1 Nativeでコンパイルされたアプリも発表されていますが、やはりまだそうではないアプリもたくさんあるので、不安ではありますね。\nしかし、それでもいつかはApple Siliconに全てのMacが転換されるだろうし、いますぐM1チップ搭載モデルを購入しないとしても、十分注目する価値はあるのではないかと思っています。いや、注目だけでなく、今年は16インチMacbook Proのフルチェンジの噂もあるので、もしそれが本当なら自分も乗り換えるのではないかと思っているくらいです。もしそれが出るなら、M1チップ搭載モデルの問題としてあげられたところを改善(少なくとも、外装ディスプレイの件やBluetooth問題は改善されそうです)されるはずで、今のアプリケーションのM1 Native対応の速度を見ると年内には意外と多くのアプリをNativeに使えるのではないかと思われます。まだまだこれからが注目なのですが、JavaScript中心の開発を行う方にとっては今のM1搭載モデルも十分メリットがあるのではないかと思います(AdoptOpenjdkはまだx64のみなので私は見送りですが…)。また、最近M1搭載モデルでLinuxを使えるようになったので、ホームサーバとしてこれらのMacを考慮してみるのも良いチョイスかと思います。\n最後に まだJavaとSpringも全ての機能を使いこなしているとも言えない自分が、今から新しい言語やフレームワークを学ぶという計画を立てるのは無理な話なのかしれません。これはいつも悩ましい主題です。一つの言語に関する知識やスキルを極めていった方が良いか、それとも常にトレンドを追いながら幅広い分野のスキルセットを持つべきか。深い知識も、広い知識も持っていて良いものではありますが、自分がこれから積み上げるキャリアを完成するにはどれがより効率的かという疑問は解消されないものです。\n自分なりの答えを出すとしたら、トレンドを追った方が、より自分の持つスキルセットの深さを増して行く方にも作用するのではないかという気はします。JavaしかできないものなのでJavaのAPIを借りて例え話をすると、Java 1.8では他の言語の持つClosureから影響されてLambdaが導入されましたね。その他にもvarの型推論やテキストブロックなどの改善もまた違う言語から影響されたものです。このような変化は、そもそもJavaの開発者たちが他の言語に注目しなかったら起こらなかったことでしょう。なので、「他と比較することで自分をより深く理解することにもなる」のではないでしょうか。そういう意味からすると、自分がすでに持っているスキルセットのみでなく、業界の動向や流行りを早くキャッチして受け入れることこそ重要ではないかと思ったりもします。\nこの度はだいぶ主観的な意見だけ語る場となってしましましたが、どうでしょうか。またこれから自分の考えも、トレンドも変わっていくかもしれませんが、今は私の結論が紹介できただけでよかったかなと思います。そして、こうやって色々と自分の知らない分野について調べたり勉強したりするほど、自分には何もないなと実感でき、良い刺激になります。これからも色々と勉強していかないとですね。\nでは、また！\n","date":"2021-01-24T00:00:00Z","image":"https://retheviper.github.io/images/map.jpg","permalink":"https://retheviper.github.io/posts/my-roadmap-in-2021/","title":"個人的な2021年のロードマップ"},{"content":"以前、Java 1.7から導入されたNIOに関してのポストを書いたことがありますが、いまだにJavaにおけるファイルのI/Oに関しては悩ましいところが多いです。恥ずかしいことですが、Newsroomのセリフでもあるように、「問題を解決する第一歩はそこに問題があるということを認識すること(First step in solving any problem is recognizing there is one)」ですね。なので、今までの自分が書いたコードを振り返り、どのように書いた方が良いかを反省することにしました。\nなので今回は、今までなんとなく使ってきたコードたちを振り返り、なるべくどのような方法をとった方が良いかを考えてみようと思います。ただ、考えられる全てのケースを網羅するのは難しいと思うので、この度はあくまでJavaのコードでファイルをコピーする場合に限ります。なので、考えてみたいこと(検証対象)は以下の通りになります。\nInputStreamとOutputStreamはどう作った方がいいか FileInputStreamとFileOutputStreamを使うか Filesのメソッドを使うか ファイルコピーはどのような方法を使った方がいいか InputSteamからOutpuStreamへ書くか readAllBytes() transferTo() Filesのメソッドを使うか 今回はこれらの疑問について、よく使われているファイルコピーのコードを一つ一つ見ていきながら、考えてみたいと思います。\nInputStreamとOutputStreamはどう作るべきか まずはInputStreamとOutputStreamです。今も多くの場合、メモリー問題を考えて、ファイルはなるべくStreamとして扱っているのではないかと思います。特に今のJavaがよく使われている分野はWebアプリケーションですが、そのWebアプリケーションを作るための代表的なフレームワークであるSpringでもファイルのアップロードやダウンロードはStreamの形式となっていますし、ローカルのものかネットワーク越しのものかを問わずファイルのデータを取り扱えるという意味ではFileやPathというオブジェクトを使う場合に比べ汎用性という面でも良さそうな気がします。\nただ、ローカルでファイルをコピーするために、InputStreamやOutputStreamを生成する方法は、Java 1.7以降だと二つの方法があります。一つはFileオブジェクトからFileInputStream・FileOutputStreamを生成する方式であり、もう一つはPathオブジェクトとFilesクラスを利用して生成する方法ですね。\nなるべくこれからのコードはNIOを使って書きたいと思っていますが、本当にそれだけで十分か、既存のコード(FileInputStreamとFileOutputStreamを利用する)までもNIOのものに変える必要があるかをまず確認してみたいです。\nコピーの方式 まずは、JavaでInputStreamとOutputStreamを利用して、ファイルをコピーするコードから見ていきましょう。\n私を含め、初めてJavaに触れた多くの方々接することとなるファイルコピーのコードは、おそらく以下のようなものではないかと思います。いわば、最も一般的で、オーソドックスな形とも言えるでしょう。\n// byte[]を利用した例 private void copy(File source, File dest) throws IOException { InputStream is = null; OutputStream os = null; try { is = new FileInputStream(source); os = new FileOutputStream(dest); byte[] buffer = new byte[8192]; int length; while ((length = is.read(buffer)) \u0026gt; 0) { os.write(buffer, 0, length); } } finally { is.close(); os.close(); } } // BufferedStreamを利用した例 private void copy(File source, File dest) throws IOException { int buff = 8192; InputStream is = null; OutputStream os = null; try { is = new BufferedInputStream(new FileInputStream(source), buff); os = new BufferedOutputStream(new FileOutputStream(dest), buff); int length; while ((length = is.read()) \u0026gt; 0) { os.write(length); } } finally { is.close(); os.close(); } } ここでまず、FileInputStreamはFiles.newInputStream、FileOutputStreamはFiles.newOutputStreamに代替できます。まず目立つ違いとしては、FileInputStream・FileOutputStreamは引数としてFileを取り、Files.newInputStream・Files.newOutputStreamは引数としてPathを取るという点がありますね。ただ、この違いは、FileとPathの変換が自由なので、あまり決定的な違いとは言えません。つまり、どちらの方法にも簡単に切り替えができるということですね。\n一見、FilesクラスからInputStreamとOutputStreamのインスタンスを生成した方が、より最新のAPIを使っているので性能の面で良さそうな気はします。しかし、JavaのNIOは、必ず性能面で既存のIOと比べ優位にあるわけではないですね。実際、ファイルのI/Oに関しては、NIOを使ってもBlockingモードとしてしか動かないので、あまり性能は変わらないという話もあります。\nそういう場合は、特に問題を起こしてないのに、あえて既存のコードをNIOに切り替える必要は無くなりそうな気もします。しかし、本当にそれで大丈夫でしょうか。\nFileInputStreamとFileOutputStreamの問題 実際は、そうでもないようです。FileInputStream・FileOuputStreamには性能とは別の問題があります。GCによりアプリケーション全体が長くポーズの状態になる可能性があるということです。\nfinalize()のオーバライド問題 GCによりアプリケーション全体がポーズされるということは、つまり、メモリがフルになるということです。ファイルI/Oで、InputStreamとOutputStreamを使ってメモリがフルになるということは、ちゃんとclose()されてないことですね。なので、単純にclose()すれば良いだけなのでは？と思われます。\nしかし、本当の問題はFileInputStream・FileOuputStreamのソースコードにあります。この二つのクラスは、finalize()メソッドをオーバーロードしていて、ちゃんとclose()してもメモリー上にデータが残ってしまう可能性があるのです。この問題は、こちらの記事に説明されてある通り、Jenkinsでも問題視されたことがあり、OpenJDKでもfinalize()を消す必要があると指摘されたことがあります。\nJDKの対応としては、FileInputStream・FileOuputStreamのfinalize()はJava 9からDeprecatedとなり、Java 10からは別の実装を加えることで問題を解決していますが、Java 1.7や1.8を使う場合は依然として問題が起こり得るということになりますね。\nなので、これからはなるべくFileInputStream・FileOutptStreamの利用は避けるようにする必要があると思います。習慣は怖いですので。\nファイルコピーはどのような方法を使った方がいいか 今までの結論で、InputStream及びOuputStreamのインスタンスはNIOを使うことにします。したがって前述のコードは以下のように直すことができますね。\n// byte[]を利用した例 private void copy(Path source, Path dest) throws IOException { InputStream is = null; OutputStream os = null; try { is = Files.newInputStream(source); // FileInputStreamを使わない os = Files.newOutputStream(dest); // FileOutputStreamを使わない byte[] buffer = new byte[8192]; int length; while ((length = is.read(buffer)) \u0026gt; 0) { os.write(buffer, 0, length); } } finally { is.close(); os.close(); } } // BufferedStreamを利用した例 private void copy(Path source, Path dest) throws IOException { int buff = 8192; InputStream is = null; OutputStream os = null; try { is = new BufferedInputStream(Files.newInputStream(source), buff); // FileInputStreamを使わない os = new BufferedOutputStream(Files.newOutputStream(dest), buff); // FileOutputStreamを使わない int length; while ((length = is.read()) \u0026gt; 0) { os.write(length); } } finally { is.close(); os.close(); } } try-with-resource InputStreamやOutputStreamは最後にclose()しないと、すでに使ったものでもメモリ上にデータが残ってしまいますね。なのでfinallyブロックでクローズするのが一般的かなと思いますが、こうした場合、finallyブロックでも追加の例外処理が必要になるケースもありますし、毎回close()するのは忘れられる可能性もあるので危険です。\nなのでJava 1.7からはAutoCloseableとtry-with-resourceが導入され、以下のようにより簡潔かつ安全なコードを書くことができるようになりました。例えば上記のコードは、try-with-resourceを使うと以下のようなコードに代替できますね。\n// byte[]を利用した例 private void copy(Path source, Path dest) throws IOException { try (InputStream is = Files.newInputStream(source); OutputStream os = Files.newOutputStream(dest)) { byte[] buffer = new byte[8192]; int length; while ((length = is.read(buffer)) \u0026gt; 0) { os.write(buffer, 0, length); } } } // BufferedStreamを利用した例 private void copy(Path source, Path dest) throws IOException { int buff = 8192; try (InputStream is = new BufferedInputStream(Files.newInputStream(source), buff); OutputStream os = new BufferedOutputStream(Files.newOutputStream(dest), buff)) { int length; while ((length = is.read()) \u0026gt; 0) { os.write(length); } } } try-with-resourceでは、既存の方式と比べメリットしかないので、これは必ず使うことにします。\nreadAllBytes() 次に考えられるのは、ファイルコピーでのBufferです。以上の例では、byte[]を使うか、BufferedInputStream・BufferedOutputStreamを使っていますが、これは性能のためのものであるということは皆さんもご存知のはずなので、Bufferについては割愛します。\n我々が知る限り、Bufferのサイズが大きければ大きいほど、性能はよくなります。なら、メモリが許容する限り、できるだけ大きいサイズのBufferを指定したら自然に性能はマシンが出せる最大限となるはずです。\nそして、Java 9からは、InputStreamを一気に全部読み込み、byte[]として返すreadAllBytes()というメソッドができました。このメソッドを使うと、Integer.MAX_VALUEサイズのbyte[]を生成してInputStreamを全部読み込むことができます。理論的にはこれを使ったらファイルコピーもあっという間にできそうですね。\nしかし、考えなくてはならないのが、そうやって読み込んだデータはメモリ上に残ってしまうということです。例えば複数のユーザが使っているWebアプリケーションで、数GBに達するファイルをアップロードする場合が予想されるのに、readAllBytes()を使ったらメモリはすぐ足りなくなるでしょう。いくらファイルコピーが早くなるとしても、同時に複数のユーザがファイルをアップロードする場合があれば、一周でのもメモリ上に大量のファイルデータが詰まってしまう可能性があるので、あまり良くない選択になります。なので、なるべくreadAllBytes()の仕様は控えるべきでしょう。\ntransferTo() Java 9からは追加されたメソッドのうちには、InputStreamにはより簡単にOutputStreamにデータを転送することのできるtransferTo()というメソッドもあります。try-with-resourceに加え、transferTo()を使うとさらに簡潔なコードでファイルのコピーができるようになります。例えば以下のようなものですね。\nprivate void copy(Path source, Path dest) throws IOException { try (InputStream is = Files.newInputStream(source); OutputStream os = Files.newOutputStream(dest)) { is.transferTo(os); } } ソースコードを見るとわかることですが、transferTo()ではデフォルトのBufferサイズで作ったbyte[]を使ってコピーをしているので、デフォルト値のBuffer(8192バイト)を使う場合は、Bufferの指定もいらなくなるのが魅力的です。以下はソースコードです。\npublic long transferTo(OutputStream out) throws IOException { Objects.requireNonNull(out, \u0026#34;out\u0026#34;); long transferred = 0; byte[] buffer = new byte[DEFAULT_BUFFER_SIZE]; // 8192 int read; while ((read = this.read(buffer, 0, DEFAULT_BUFFER_SIZE)) \u0026gt;= 0) { out.write(buffer, 0, read); transferred += read; } return transferred; } ただ気になるのは、transerTo()を使う場合は本当にBufferedが要らないかという点です。例えばInputStreamをBufferedInputStreamでラップすると、せめてファイルを読み込む速度は上がるのではないかという疑問が湧いてきます。とにかく、もしものことなので、簡単なベンチマークも実施してみました(実はやってみたかっただけですが)。10GBほどのファイルを生成し、以下のケースでテストしてみました。\nInputStream → OutputStream BufferedInputStream → OutputStream InputStream → BufferedIOutputStream BufferedInputStream → BufferedIOutputStream そしてコードは以下の通りです。\n@State(Scope.Benchmark) @BenchmarkMode(Mode.AverageTime) public class StreamBufferTest { private Path source; private Path output = Path.of(\u0026#34;/Users/retheviper/temp/benchmarkOutput\u0026#34;); // テスト用のファイルを作成する @Setup public void init() throws IOException { final String path = \u0026#34;/Users/retheviper/temp/benchmarkSource\u0026#34;; final RandomAccessFile file = new RandomAccessFile(path, \u0026#34;rw\u0026#34;); long size = (1024 * 1024 * 1024) * 10L; // 10GB file.setLength(size); this.source = Path.of(path); } @Benchmark public void noBuffer() throws IOException { try (InputStream in = Files.newInputStream(source); OutputStream out = Files.newOutputStream(output, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING)) { in.transferTo(out); } } @Benchmark public void withInputBuffer() throws IOException { try (InputStream in = new BufferedInputStream(Files.newInputStream(source)); OutputStream out = Files.newOutputStream(output, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING)) { in.transferTo(out); } } @Benchmark public void withOutputBuffer() throws IOException { try (InputStream in = Files.newInputStream(source); OutputStream out = new BufferedOutputStream(Files.newOutputStream(output, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING))) { in.transferTo(out); } } @Benchmark public void withBothBuffer() throws IOException { try (InputStream in = new BufferedInputStream(Files.newInputStream(source)); OutputStream out = new BufferedOutputStream(Files.newOutputStream(output, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING))) { in.transferTo(out); } } } そしてベンチマーク結果は、以下の通りです。\nBenchmark Mode Cnt Score Error Units StreamBufferTest.noBuffer avgt 25 13.055 ± 0.583 s/op StreamBufferTest.withInputBuffer avgt 25 13.302 ± 0.460 s/op StreamBufferTest.withOutputBuffer avgt 25 13.663 ± 0.535 s/op StreamBufferTest.withBothBuffer avgt 25 12.668 ± 0.355 s/op 予想通り、transferTo()でのコピーの場合、BufferedInputStreamやBufferedOutputStreamを使わなくても性能はあまり変わりありませんでした。単純なファイルコピーではなかったり、InputStreamからOutputStreamというデータの転送ではない場合はまた必要となりそうな気はしますが、このメソッドが使える場合はあまり意識しなくても良さそうですね。\nFiles.copy()がいい？ Java 1.7では、Files.copy()を通じて以下のファイルコピーができるようになっています。\nInputStream → Path Path → OutputStream Path → Path そして一部では、JavaのNIOはネイティブコードで書かれてあるので、InputStreamからOutputStreamへの書き込みよりはFiles.copy()の方が性能がいいと言われる場合もありました。この話が本当さとすると少なくともローカルのファイルを扱う場合、InputStreamからOutputStreamへの書き込みよりはPathを使ったコピーが良さそうな気がします。\nソースで確認する コードが違うと言われたからには、直接確認した方がいいですね。早速、以上であげた三つのメソッドのソースコードを確認することにします。まずは、InputStream → PathとPath → OutputStreamです。こちらはシンプルに、PathからOuputStreamもしくはInputStreamを生成し、transferTo()を使ってコピーすることとなっています。(ただ、これはJava 11基準のソースコードなので、Java 9以前の場合は違うコードの可能性があります)\n// InputStream → Path public static long copy(InputStream in, Path target, CopyOption... options) throws IOException { // コピー以外の処理は省略 OutputStream ostream; try { ostream = newOutputStream(target, StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE); } catch (FileAlreadyExistsException x) { if (se != null) throw se; // someone else won the race and created the file throw x; } // do the copy try (OutputStream out = ostream) { return in.transferTo(out); } } // Path → OutputStream public static long copy(Path source, OutputStream out) throws IOException { // ensure not null before opening file Objects.requireNonNull(out); try (InputStream in = newInputStream(source)) { return in.transferTo(out); } } ただ、やはりPath → Pathの場合は全く違うコードになっています。コピー元とコピー先が同じファイルシステムの場合はFileSystemProviderを使い、そうではない場合はCopyMoveHelperを使うことになっていますね。\n// Path → Path public static Path copy(Path source, Path target, CopyOption... options) throws IOException { FileSystemProvider provider = provider(source); if (provider(target) == provider) { // same provider provider.copy(source, target, options); } else { // different providers CopyMoveHelper.copyToForeignTarget(source, target, options); } return target; } ここでCopyMoveHelper.copyToForeignTarget()の場合は、結果的にFiles.copy(InputStream, Path)を呼ぶことになるのですが、前者の場合は全く違う方式になるのでやはり性能の差が発生する可能性もありそうですね。整理すると、同じシステム内で、Path → Pathでコピーする場合だけ性能がよくなる可能性があるということですね。\nここはまた検証が必要なところなので、またベンチマークを実施してみました。もちろんファイルシステムの違いにより結果は変わる可能性があるので、これが絶対的だとは言えませんが、何らかの違いがあるかもしれません。他のFiles.copy()メソッドは実質的にtransferTo()と同じものなので、今回の比較はInputStream → OutputStreamとPath → Pathだけになります。また、比較のためのテストケースが少ないので、今回はtransferTo()のベンチマークよりもファイルサイズを大きくしてみました。以下は、そのテストコードです。\n@State(Scope.Benchmark) @BenchmarkMode(Mode.AverageTime) public class StreamCopyTest { private Path source; private Path output = Path.of(\u0026#34;/Users/youngbinkim/Downloads/benchmarkOutput\u0026#34;); // テスト用のファイルを作成する @Setup public void init() throws IOException { final String path = \u0026#34;/Users/youngbinkim/Downloads/benchmarkSource\u0026#34;; final RandomAccessFile file = new RandomAccessFile(path, \u0026#34;rw\u0026#34;); long size = (1024 * 1024 * 1024) * 10L; // 10GB file.setLength(size); this.source = Path.of(path); } @Benchmark public void streamToStream() throws IOException { try (InputStream in = Files.newInputStream(source); OutputStream out = Files.newOutputStream(output, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING)) { in.transferTo(out); } } @Benchmark public void pathToPath() throws IOException { Files.copy(source, output, StandardCopyOption.REPLACE_EXISTING); } } そして、ベンチマークの結果は以下の通りです。\nBenchmark Mode Cnt Score Error Units StreamCopyTest.streamToStream avgt 25 12.128 ± 0.331 s/op StreamCopyTest.pathToPath avgt 25 12.257 ± 0.342 s/op 10GBのファイルでは誤差範囲以内の結果となったので、ファイルサイズだけを100GBに変えて同じくベンチマークを実施してみました。その結果は以下です。\nBenchmark Mode Cnt Score Error Units StreamCopyTest.streamToStream avgt 25 160.046 ± 2.538 s/op StreamCopyTest.pathToPath avgt 25 153.505 ± 2.662 s/op 100GBになってからやっと差が見えてくる、ということになりますが、結論としてはやはり、Path → Pathの方が早いという結果となりました。機会があれば、複数スレッドによるI/OやOSの違いまで考慮したベンチマークを書きたいものですが、とりあえずは予想通りの結果となったということで。\n最後に 以上のソースコードとベンチマークでわかったことをまとめると、Javaでのファイルコピーは、とりあえず以下のようなことで結論づけができそうです。\nJava 1.7以上の場合 FileInputStream・FileOutputStreamの代わりにFiles.newInputStream・Files.newOutputStreamを使う try-with-resourceを使う コピー元とコピー先のどちらも同じファイルシステム上のパスであれば、両方Pathが引数のFiles.copy()を使う Java 9以上の場合 Bufferサイズが8192の場合はtransferTo()を使う transferTo()を使う場合、BufferedInputStream・BufferedOutputStreamは必須ではない 多くの場合、エンタープライズアプリケーションはLTSである1.8や11を使うと思われるので、実質的には以上に並べた項目全てが当てはまると言えましょう。\nかなり今更な感があるポストとなりましたが、個人的には自分の納得できる形で整理でき、スッキリしました。こうやって何気なく、「そう教わったから」使っていたコードを振り返ってみるのも良い勉強になりますね。次もまた、こうやってソースコードやベンチマークによる検証をやってみたいなと思います。\nでは、また！\n","date":"2021-01-12T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-file-copy/","title":"今更なI/Oの話"},{"content":"仕事ではJava 11を扱うことが多いのですが、正直、自分の書いたコードを振り返ってみると、Java 9から新しく追加されたメソッドはあまり使ってないのが現実です。しかし、これら新しいメソッドたちは冗長さを隠してくれるシンタックスシュガーとして存在するだけでなく、性能や機能面でより優れているものもあるので、いますぐ使わないとしても目は通しておきたいものが多いなと思っています。\n2021年は次のLTSバージョンとなる17の登場が予告されている時点なので、今更な感はありますが、そろそろ私もSEになってから2年になるので、今回は今年、自分が書いたコードへの反省を含め、Java 9〜11まで新しく追加されたメソッドたちの中から、良さそうな(よく使えそうな)ものを選別してみました。そして今回のポストは、そう選別したメソッドの簡単な紹介となります。\n多くの場合、これらのメソッドを使える環境だとしたらJava 11を導入しているはずなのであまり意味はないのかもしれませんが、それぞれのメソッド名の右に、該当メソッドがどのバージョンから導入されたかを記入していますので参考にしてください。\nStream StreamこそJava 8のキモではないかと思います。そしてJava 9では、そのStreamの問題を色々と改善したり、より簡単に使えるようなメソッドを用意しています。なので、既存のforループにしか慣れてない人でも、簡単に入門できるようになったのではないかと思います。\nIterate (9) iterate()というメソッド名だけではすぐに意味がわからない場合もあると思いますが、このメソッドは伝統的なFor文と同じような構文でStreamでの処理を書くことができるようになります。つまり、「初期化・ループの継続条件・カウンタ変数の更新」を書くことで、Streaｍの要素数を決めることができるという意味です。例えば、以下のような書き方ができます。\n// 0~9までを出力 Stream.iterate(0, i -\u0026gt; i \u0026lt; 10, i -\u0026gt; i + 1).forEach(System.out::println); これはつまり、以下のコードと同じ意味を持ちます。\nfor (int i = 0; i \u0026lt; 10; i++) { System.out.println(i); } ただ、iterate()で指定できる初期化の値が数字という制限はないので(Tです)、以下のようなこともできます。\n// Aで三角形を出力 Stream.iterate(\u0026#34;A\u0026#34;, s -\u0026gt; s.length() \u0026lt; 10, s -\u0026gt; s + \u0026#34;A\u0026#34;).forEach(System.out::println); また、ループの継続条件を指定しないこともできます。\n// Aで三角形を出力 Stream.iterate(\u0026#34;A\u0026#34;, s -\u0026gt; s + \u0026#34;A\u0026#34;).forEach(System.out::println); 継続条件を指定しないと、無限ループになってしまうのでは？と思われそうですね。確かにそうですが、同じくJava 9でStreamの要素数の上限を指定できる新しいメソッドが追加されています。次に紹介するものがそれです。\ntakeWhile (9) 以前、Streamの問題として「途中でやめられない」と書きましたが、Java 9から導入されたtakeWhile()メソッドを使うと、途中で処理を終了するようなことができるようになりました。既存にあったlimit()の場合は、「指定された回数分」という限界がありましたが、こちらはPredicate型の条件を指定できるというところが違います。\n// AAAAAAAAAまで出力する Stream.iterate(\u0026#34;A\u0026#34;, s -\u0026gt; s + \u0026#34;A\u0026#34;) .takeWhile(s -\u0026gt; s.length() \u0026lt; 10) .forEach(System.out::println); なので、iterate()の継続条件を書いてない場合にはtakeWhile()を使ってどの条件で処理が終わるかを明示した方が良いですね。\ndropWhile (9) dropWhile()は、その名からも推測できますが、takeWhile()と真逆の機能をするメソッドです。このメソッドはStreamから与えられた条件と一致する要素を除いて、残りの要素を返却します。\n// AAAAAから出力する Stream.iterate(\u0026#34;A\u0026#34;, s -\u0026gt; s.length() \u0026lt; 10, s -\u0026gt; s + \u0026#34;A\u0026#34;) .dropWhile(s -\u0026gt; !s.contains(\u0026#34;AAAAA\u0026#34;)) .forEach(System.out::println); ofNullable (9) Java 1.8のStreamでは、Null要素を追加するためにはまずその要素がNullかどうかをチェックして、Nullの場合にStream.empty()を呼ぶような形にする必要がありました。いつものJavaのNullチェックですね。例えば以下のようなものです。\n// 要素のNullチェックを含むStreamのCollect keyList.stream() .flatMap(k -\u0026gt; { Object value = map.get(k); return value != null ? Stream.of(value) : Stream.empty(); }) .collect(Collectors.toList()); これを、Java 9ではより簡単なコードで書くことができます。OptionalのofNullable()とあまり変わらない感覚ですね。\nkeyList.stream() .flatMap(k -\u0026gt; Stream.ofNullable(map.get(k))) .collect(Collectors.toList()); Collectors Streamの要素を集約するためのCollectorを提供するCollectors APIですが、こちらの変化は主にシンタックスシュガーなものが多い印象です。主にStreamでしかできなかったことや、既存のCollectorsのみだとかなり長くなるコードを簡潔に書くことができるようになっています。\nfiltering (9) Streamのfilter()と同じ処理を、Collectorでもできるようになりました。どちらを使うかは好みの問題な気がしますが、Collectorそのものを共通化するなどの処理ができそうな気はしますね。\n// 0~9までのリスト List\u0026lt;Integer\u0026gt; numbers = Stream.iterate(0, i -\u0026gt; i \u0026lt; 10, i -\u0026gt; i + 1).collect(Collectors.toList()); // Stream.filter() numbers.stream() .filter(e -\u0026gt; e \u0026gt; 5) .collect(Collectors.toList()); // 6, 7, 8, 9 // Collectors.filtering() numbers.stream() .collect(Collectors.filtering(e -\u0026gt; e \u0026gt; 5, Collectors.toList())); // 6, 7, 8, 9 flatMapping (9) これもまた名前から推測できると思いますが、CollectorsでCollectionに変えるとき、要素のflatMappingをできるようにしてくれるようなものです。具体的には、以下のサンプルコードを参照してください。\n例えば、以下のようなクラスがあるとします。\npublic class Selling { String clientName; List\u0026lt;Product\u0026gt; products; } public class Product { String name; int value; } そして、このSellingのリストを、「clientNameをKeyに、productsをValueにしたMapにしたい」場合はどうしたら良いでしょうか。例えば以下のような方法を考えられます。\nMap\u0026lt;String, List\u0026lt;List\u0026lt;Product\u0026gt;\u0026gt;\u0026gt; result = operations.stream() .collect(Collectors.groupingBy(Selling::getClientName, Collectors.mapping(Selling::getProducts, Collectors.toList()))); しかし、問題は、List\u0026lt;Product\u0026gt;をさらにListの中に入れてしまうことになります。これは本来の目的ともズレていて、無駄な処理が発生し、Valueを持ち出すときも不便なはずです。\nこれをMap\u0026lt;String, List\u0026lt;Product\u0026gt;\u0026gt;の形に変えるとしたら、以下のような方法が使えます。自作のCollectorを作るのですね。\nMap\u0026lt;String, List\u0026lt;Product\u0026gt;\u0026gt; result = operations.stream() .collect(Collectors.groupingBy(Selling::getClientName, Collectors.mapping(Selling::getProducts, Collector.of(ArrayList::new, List::addAll, (x, y) -\u0026gt; { x.addAll(y); return x; })))); ただ、毎回このような自作Collectorを作るというのはあまり効率的ではない方法ではないかと思います。それに、自作のCollectorを普段から使ってない場合はコードだけみても少しわかりづらくもありますね。なので、ここは新しく追加されたflatMapping()で変えてみると以下のようになります。より簡潔ですね。\nMap\u0026lt;String, List\u0026lt;Product\u0026gt;\u0026gt; result = operations.stream() .collect(Collectors.groupingBy(Selling::getClientName, Collectors.flatMapping(selling -\u0026gt; selling.getProducts().stream(), Collectors.toList()))); toUnmodifiable (10) Java 10ではCollectorsに以下の三つのメソッドが追加されています。\ntoUnmodifiableList() toUnmodifiableSet() toUnmodifiableMap() これらのメソッドを使うと、既存のCollectionsを呼ぶ必要なく、簡単に(もっと短いコードで)UnmodifiableなCollectionを作ることができます。\n// Collections.unmodifiableList List\u0026lt;Integer\u0026gt; collectionsUnmodifiable = Collections.unmodifiableList(Stream.iterate(0, i -\u0026gt; i \u0026lt; 10, i -\u0026gt; i + 1).collect(Collectors.toList())); // Collectors.toUnmodifiableList List\u0026lt;Integer\u0026gt; collectionsUnmodifiable = Stream.iterate(0, i -\u0026gt; i \u0026lt; 10, i -\u0026gt; i + 1).collect(Collectors.toUnmodifiableList()); 引数は、既存のtoList()・toSet()・toMap()と同じなので(toMap()だけ、KeyとValueのマッピングを指定する必要がありますね)、既存のメソッドと同じ感覚で使うことができます。\nCollections Collections APIの新しいメソッドは、かなり現代的な書き方を可能にします。Kotlinのような言語がJavaの冗長さを回避するための工夫をしているのであれば、Java側に新しく追加されたメソッドはそれをさらにJavaに似合うような形で受け入れたような印象です。(というか、それしか方法はなかったかもしれませんが…)\nFactory Method (9) Java 9では、ファクトリーメソッドでCollectionの作成ができるようになりました。使い方としては、既存のArrays.asList()と似ているような感覚です。\n// Listの作成 List\u0026lt;String\u0026gt; list = List.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;); // Setの作成 Set\u0026lt;Integer\u0026gt; set = Set.of(1, 2, 3); Mapの場合は、KeyとValueを順番に並ぶことでインスタンスを作成できますが、エントリーを定義することもできます。\n// KeyとValueのセットで定義する Map\u0026lt;String, String\u0026gt; map = Map.of(\u0026#34;foo\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;baz\u0026#34;, \u0026#34;c\u0026#34;); // エントリーを定義する Map\u0026lt;String, String\u0026gt; map = Map.ofEntries( Map.entry(\u0026#34;foo\u0026#34;, \u0026#34;a\u0026#34;), Map.entry(\u0026#34;bar\u0026#34;, \u0026#34;b\u0026#34;), Map.entry(\u0026#34;baz\u0026#34;, \u0026#34;c\u0026#34;)); これらのファクトリーメソッドで作成したCollectionの特徴は、最初からUnmodifiableなオブジェクトになるということです。なので、例えばアプリケーションの起動時にフィールドに定数をCollectionとして定義する、という場合に使えます。つまり、以下のような既存のコードを代替できるようなものです。\n// もっとも基本的な方式 Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); set.add(\u0026#34;foo\u0026#34;); set.add(\u0026#34;bar\u0026#34;); set.add(\u0026#34;baz\u0026#34;); set = Collections.unmodifiableSet(set); // Double-brace initialization Set\u0026lt;String\u0026gt; set = Collections.unmodifiableSet(new HashSet\u0026lt;String\u0026gt;() { { add(\u0026#34;foo\u0026#34;); add(\u0026#34;bar\u0026#34;); add(\u0026#34;baz\u0026#34;); } }); また、このファクトリーメソッドで作ったCollectionは以下のような特徴を持ちますので、必要に応じて使うのが大事ですね。\nImmutable(Unmodifiable)になる Null要素を指定できない 要素がSerializableだとCollectionもSerializableになる copyOf (10) List, Set, MapにcopyOf()というメソッドが追加されています。引数にそれぞれのCollectionを渡すと、Unmodifiableなコピーすることができます。\n// コピー元のリスト List\u0026lt;String\u0026gt; original = ... // コピーする List\u0026lt;String\u0026gt; copy = List.copyOf(original); Optional Optionalは積極的に使われていますか？私の場合は、Streamが返すもの以外で、自分でOptionalを使う場合はあまりないです。色々制約が多いので、複雑なNullチェックが必要な場合ではないと使いづらい気もしますね。ただ、9と10で追加されたメソッドでかなり便利に使えるものとなったので、たまには良いのかもしれません。\nor (9) Optionalの中身がNullの場合に実行されるメソッドです。既存のorElse()やorElseGet()と何が違うかというと、こちらはOptionalの中身ではなく、またのOptionalを返すということです。引数としてはSupplierをとります。\nString string = null; Optional\u0026lt;String\u0026gt; optional = Optional.ofNullable(string); System.out.println(optional.or(() -\u0026gt; Optional.of(\u0026#34;default\u0026#34;)).get()); // \u0026#34;default\u0026#34; orElseThrow (10) Optionalの中身がNullの場合は例外を投げる分岐です。NullのOptionalはもともとNoSuchElementExceptionを投げますが、ビジネスロジックなどによりカスタマイズした例外を投げたい場合などはこちらを使えますね。引数としてはSupplierをとります。\nString string = null; Optional\u0026lt;String\u0026gt; optional = Optional.ofNullable(string); String throwing = optional.orElseThrow(RuntimeException::new); // RuntimeException ifPresentOrElse (9) Optionalの中身がNullかどうかによって二つのアクションを指定して、分岐処理ができるようなメソッドです。第一引数としてはConsumerを指定することで中身がNullではない場合の処理を、第二引数としてはRunnableとして中身がNullだった場合の処理を書きます。\nOptional\u0026lt;String\u0026gt; hasValue = Optional.of(\u0026#34;proper value\u0026#34;); hasValue.ifPresentOrElse(v -\u0026gt; System.out.println(\u0026#34;the value is \u0026#34; + v), () -\u0026gt; System.out.println(\u0026#34;there is no value\u0026#34;)); // the value is proper value Optional\u0026lt;String\u0026gt; hasNoValue = Optional.empty; hasNoValue.ifPresentOrElse(v -\u0026gt; System.out.println(\u0026#34;the value is \u0026#34; + v), () -\u0026gt; System.out.println(\u0026#34;there is no value\u0026#34;)); // there is no value stream (9) Optionalを要素が一つか、Null(Stream.empty())のSteamに変えるメソッドです。もともとStreamから要素を取得するときもOptionalになっていたので、このようなメソッドが追加されたのも当たり前といえば当たり前ですね。要素が多くて一つなのにStreamに変える意味があるかというと、他のStreamと結合ができたりもするので色々と活用できる余地はありそうです。\nOptional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;value\u0026#34;); Stream\u0026lt;String\u0026gt; stream = optional.stream(); String String APIの場合は、主にJava 11でかなりの変化がありました。Webアプリケーションのみならず、最近のアプリケーションは文字列を扱う場合が多いので、このような変化はありがたいですね。\nrepeat (11) 指定した数値分、文字列を繰り返します。同じ文字列の単純な繰り返しだとすると、StringBuilderやStrinbBufferなしでも簡単に使えるこちらのメソッドの方が良いですね。\nString a10 = \u0026#34;A\u0026#34;.repeat(10); // \u0026#34;AAAAAAAAAA\u0026#34; strip (11) 文字列の前後の空白を除外するために、今まではtrim()を使うケースが多かったのではと思いますが、Java 11からはstrip()が追加され、trim()を代替できます。この二つが何が違うかというと、まずそれぞれのメソッドで定義している「空白」が違います。trim()はUnicodeを考慮してなかったため、半角スペースのみの対応となっていましたが、strip()はUnicodeで指定されたWhitespace全部を対象とするので、全角スペースや改行にも対応できます。どの文字がWhitespaceとして扱われるかは、Character.isWhitespace()のメソッドが基準となるので、そちらのJavaDocを参照してください。\nString stripped = \u0026#34;\\n hello world \\u2005\u0026#34;.strip(); // \u0026#34;hello world\u0026#34; また、strip()では前後の空白が全部削除されるのですが、文字列の前後を基準にして片方だけ削除したい場合は、前からだけを削除するstripLeading()や後ろからだけを削除するstripTrailing()も使えます。\nString stripLeading = \u0026#34;\\n hello world \\u2005\u0026#34;.stripLeading(); // \u0026#34;hello world \u0026#34; String stripTrailing = \u0026#34;\\n hello world \\u2005\u0026#34;.stripTrailing(); // \u0026#34;\\n hello world\u0026#34; 今までの説明だけでも十分strip()を使う理由はあると思いますが、実はもう一つがあります。性能です。性能という面ではstrip()の方がtrim()より5倍も早いと言われているので、なるべくtrim()よりはstrip()を使うべきですね。\nisBlank (11) すでにisEmpty()というメソッドがありますが、このメソッドとisBlank()の違いは、trim()とstrip()の関係と似ています。同じく、isEmpty()と比べるとisBlank()の方がUnicodeに対応しているのでより多くのケースのWhitespaceに対応できて、性能でも優れています。\nboolean isEmpty = \u0026#34;\\n \\u2005\u0026#34;.isEmpty(); // false boolean isBlank = \u0026#34;\\n \\u2005\u0026#34;.isBlank(); // true lines (11) 文字列に改行コード(\\n・\\r・\\r\\n)を基準に分けたStream\u0026lt;String\u0026gt;を返却します。\nString multipleLine = \u0026#34;first\\nsecond\\nthird\u0026#34;; long lines = multipleLine.lines().filter(String::isBlank).count(); // 3 Prediacte not (11) LambdaやMethod Referenceで定義したPredicateの結果がFalseかどうかを判断するためのメソッドです。単純にtrueの否定になるだけなのですが、このメソッドの引数はPredicateなので、LambdaやMethod Referenceを使ってより単純に表現できるのがメリットといえますね。\n// 否定の条件式を使う場合 list.stream() .filter(m -\u0026gt; !m.isPrepared()) .collect(Collectors.toList()); // Predicate.not()を使う場合 list.stream() .filter(Predicate.not(Man::isPrepared)) .collect(Collectors.toList()); 最後に 2021年に次のLTSであるJava 17がリリースされると、今のJava 11を使う現場の場合は多くがJava 17に移行するのではないかと思います。12から16まで、さまざまなAPIや機能、JVMの改善などが含まれていて、すでに多くのブログなどで紹介されていますが、また既存のAPIにはどのような変化があるかまでは完全に把握していない状態です。なので、Java 17のリリースに合わせて、もう一度12〜17までの新しいメソッドの整理と紹介を行おうと思います。これだけでもかなり勉強になりますし、業務で使えそうなテクニックも増えていく感覚ですね。\nまた、今年のポスティングはこれで終了となります。色々と大変な一年だったのですが、なんとか年末を迎えることができましたね。その間、このブログにも多くの方々がいらしてくださいました。まだジュニアレベルでしかない駆け出しエンジニアのブログなのであまり情報取集には役立たないかもしれませんが、少しでも私の書いたポストを読んでくださりありがとうございます。来年からは、より面白く、より良い情報を収取してブログに載せたいですね。\nでは、また！\n","date":"2020-12-30T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-new-methods-from-9-to-11/","title":"9からの新メソッドめぐり"},{"content":"今まではずっとSpring MVCを使ってきたので、最近はKotlin + Spring WebFluxという組み合わせで簡単なアプリを作ってみているところです(Spring WebFluxそのものについての紹介は、前回のポストでしているのでここでは割愛します)。Spring WebFluxが紹介されたのももう3年前のことなので(あと少しで、4年になりますね)、もうかなりの時間が経ちますが、実際はあまり幅広く使われてはいないのが現状ではないかと思います。なのでネットで調べてもあまり参考できそうなものがなかったりしますね。\nこれはおそらくSpring WebFluxが本格的に導入するにはまだ色々と考えるべきところがあるからでしょう。例えば、フレームワークとしてまだまだ成熟している技術ではないということがあります。まだ、このフレームワークの核心となるReactorの書き方に慣れていない人が多い(Reactiveといいつつ、RxJavaともまた微妙に違いますしね)という面も考慮しなくてはならないです。会社の立場からしたら、このようにまだ新しい技術をすぐに取り入れるということはリスクもあり、エンジニアの学習コストも考えなくてはいけないという面からあまりメリットがないです。\nそして前回紹介した通り、パフォーマンスの面からしても既存のSpring MVCプロジェクトをSpring WebFluxに変えるだけではあまり得しないという問題があります。なら、新規プロジェクトから導入して良いのでは？と思われる可能性もあると思いますが、スタートアップやベンチャー企業などではそもそもJVM言語を使和ないケースが多いので(勝手なイメージかもしれませんが、そのような企業はやはりPythonやRuby、JavaScriptが多いと思います)、そもそも考慮の対象になっていないのかもしれません。また、もしJVM言語に慣れているエンジニアがいるからSpring WebFluxは導入できるとしても、やはり前述した「検証されてない」と「学習コストがかかる」という問題からは自由ではないですね。\n少なくともこれらの理由からエンタープライズレベルでは、まだSpring WebFluxの導入は難しいかと思います。ただ、おそらくSpringの未来はWebFluxにあるので、これからだんだんWebFluxを中心に開発が行われる可能性もあるので、今からでもReactorの書き方になれる必要があるのかもしれません。また、Springという一つのフレームワークの観点からでなくても、非同期による同時処理性能の向上は、多数のユーザの利用する頻度の高いWebアプリケーションの開発においては重要な要素であるので、少なくともその概念、思想、そしてコードの書き方には慣れる必要があるかもしれません。そういう意味で、自作アプリをSpring WebFluxで書いてみた経験とそこから感じたことを述べたいと思います。\nSpring WebFluxのアプリを書くということ Spirng WebFluxは既存のController + Serviceというパターンでもコードを書くことができます。なので、一見みると既存のSpring MVCで作られたプロジェクトと並行して運用したり、既存のコードを少し書き換えるだけで簡単にWebFluxに移行できそうなイメージを与えていると思います。が、実際はそうでもないような気がします。まず私の場合、簡単なCRUDのサンプル(GitHubのリポジトリはこちら)を作ってみてから、これを応用して調整さんのマイナークロンを作ってみようとしました。\nここで、サンプルではSpring Data JPAを使っていましたが、WebFluxで本格的なノンブロッキングのアプリを書いてみたいと思ったので、R2DBCを導入してDBアクセスも非同期で構成することにしました。なぜかというと、非同期の処理の中に一つでも同期の処理が発生するとしたら、それだけでももはや非同期ではなくなるからです。なのでORMもそれに合わせてR2DBCを使う必要がる、とのことです。\n幸い、R2DBCは使い方としてもSpring Data JPAやSpring Data JDBCとそう変わらない感覚で、Interface形式のRepositoryを作り、DTO形式のオブジェクト(Kotlinなら、Data classで十分でした)を作るだけでDBとのマッピングは簡単にできました。あとJPAに比べ、アノテーションの数が少なくなっているだけなのでテーブルとしてオブジェクトを定義するのも簡単です(@Idをつけるのみで終わります)。そしてSpring Data JDBCとは違って、メソッド名から自動でクエリを生成してくれるというところもあったので、最初は楽だと思いました。でも、やはり初めて触っている技術で、そうなんでもうまくいくはずがありません。一つ、問題にあってしまいました。\nJoinができない 個人的にはどんな技術でも、やはりある程度の時間がたち、安定期に入る前までは既存の技術に比べ圧倒的な優位にあり、すぐに乗り換えるべきと言えなくするところ(つまり、レガシーを捨てがたくする要素)が必ずしも一つ以上は存在すると思います。そういう意味からすると、Spring Data R2DBCが既存のORMを今すぐ代替するには十分ではないと言える部分は、自動でJoinを行う方法がない(テーブル間の関係をあらかじめオブジェクトとして定義することができない)というところと言えるのではと思います。\nSpring Data JPAやJDBCを使う場合、アノテーションを使うことで簡単にテーブル間の関係(@OneToOneなど)を定義できて、テーブルの関係をコード内で簡単に定義できます。しかし、これがまだR2DBCでは対応していない機能となっています。こういう状況では、リポジトリに@Queryアノテーションをつけて直接Joinが含まれたSQLを書いたメソッドを定義するか、二つのオブジェクトをアプリケーションの中で組み合わせるかの方法があるかと思います。\nここで前者の場合、コードカバレッジとして取れない部分になってしまうので(そして、個人的には性能が良いとしても、あまりクエリが複雑になりそうなものはメンテの観点からよく思ってないので)、後者の方法を取ることにしました。オブジェクトとリポジトリが1:1になって、後でテーブルに修正が発生してもそのテーブルに当てはまるオブジェクトを直すだけで済むので、より簡単な方法だと思ったからです。しかし、その決定にも問題はありました。R2DBCが返すSQLの実行結果としてのオブジェクトは、オブジェクトそのままではなく、MonoかFluxであったからです。\nblock()のジレンマ リポジトリから取得したオブジェクトがMonoやFluxなので、またどうやって二つのオブジェクトを組み合わせるか(Joinさせるか)を考える必要があります。\n一番簡単な方法としては、MonoやFluxをブロッキングして使う方法がありますね。すでにMonoやFluxにはblock()というメソッドが用意されてあって、同期のコードの中で使うことも可能になっています。例えばSpring MVCでもRestTemplateよりWebClientを使うことが推奨されているので、非同期と同期のの共存が不可能なわけでも、おかしいわけでもないです。\nただ問題は、そのような方法を取ると非同期のメリットがなくなるということです。なぜなら非同期が見せてくれる素晴らしい同時処理性能は、どこか一つの箇所でもブロッキングが挟むと、結局は同期コードになってしまうからです。それなら今までのSpring MVCとORMでよくて、あえてWebFluxやR2DBCを使う必要が無くなりますね。なので別の方法を試してみることにしました。\nMono + MonoもしくはFlux + Flux やはりここで取るべき方法は、非同期に相応しい処理方法を探すことでしょう。なので調べた結果、二つのMonoもしくはFluxを配列のようなオブジェクトとして結合して扱う方法があるということがわかりました。答えは意外と簡単で、zipWith()というメソッドを使うことで二つのMonoかFluxをつなげることができます。つなげたMonoもしくはFluxは、Kotlin基準でタプルになるのであとはmap()からタプルのインデックス(繋ぎ元がt1、繋ぎ先がt2となります)を指定して使うだけです。例えば、以下のコードはPaticipantというオブジェクトをリポジトリから取得したあと、さらにCandidateParticipantsを取得して結合する例です。\nfun getParticipant(participantId: Long): Mono\u0026lt;ParticipantDto\u0026gt; = repository.findById(participantId) .zipWith(candidateParticipantHandler.getåCandidateParticipantsByParticipantsId(participantId).collectList()) .map { mapDto(it.t1, it.t2) } ただ、このような方式を使ってテーブルをJoinするためには、二つのMonoとFluxを取得できる環境である必要があります。なのでSelectを発行するメソッド(Get系のAPIなど)なら、オブジェクトにテーブル間の関係を上手く設計して反映する必要があります。つまり、一つのキーでJoin対象の全てのオブジェクトを取得できるようにする必要があるということになります。最初のオブジェクトを取得したあと、そのオブジェクトが持つまたのキーで紹介したら良いのでは？と思ったこともありますが、残念ながら私の知る限りは簡単にできそうにないです。なぜなら、そのような方法を取るには以下の手順が必要になるからです。\nキーを持ってJoin元のテーブルを照会、データをMonoもしくはFluxとして取得する 取得したMonoもしくはFluxのmap()を呼び出し、更なるキーを抽出、Join先のテーブルを照会する Join元とJoin先のテーブルを組み合わせる 一見問題なさそうですが、Join先のテーブルを紹介する段階で、またのMonoもしくはFluxを取得してしまうので、それからどうやって元のオブジェクトを取り出すかが問題となります。ここでまたblock()を使うと、今までやってきたことたちが台無しとなってしまうわけです。なので、かなり不便でありながら、コードを持ってのテーブルのJoinは、現時点ではこのような方法しかないのではと思っているところです。\nこれがベストか こうやって、WebFluxで疑問となった問題は、なんとか解決することはできました。しかし、個人的にはこのようなやり方に違和感があります。そして、その理由をProject LoomのリーダであるRon Pressleの説明から探すことができました。彼の話によると、今の非同期プログラミングは以下の三つの問題を持っています。\nコントロールフローを見失いやすい 非同期でコードを書いていると、どんなロジックと目的でコードを書いていたか忘れてしまうような気がする時があります。それはおそらく、アプリケーションの本来の目的を達成するための「ビジネスロジック」よりも、「ノンブロッキングのお作法」の方を気にかけることが多くなるからですね。非同期でコードを書いていると、簡単な条件分岐や繰り返してもかなりコードが複雑になり、一体どのような処理をしようとしたか、その制御の流れを見失いやすくなります。Javaだけやっていた自分にはあまり実感がないですが、JavaScirptで非同期のコードを書いた経験のある方にはこれが理解できるでしょう。(あの有名な、コールバックヘル問題とかがあるし…)\n例えばこのポストでも紹介した通り、二つのテーブルをJoinするために、何をしているかをみてください。同期だったら、最初からJoinしたデータを取得するか、二つのオブジェクトを順番に宣言して処理するだけ済む話ですね。このように簡単な処理でも非同期に変えようとすると、そのコードで何をしたいかよりも、まず非同期の形式に合わせたコードを書くことになるので、「一体何をしようとしているのか」という、そもそもの目的がわからなくなる場合があるという話です。\nコンテキストを見失ってしまう 非同期だと、例外が発生した時に、スタックトレースを追うのが非常に難しくなります。なぜなら、同期の場合は一つのスレッドが一つのリクエストを処理するため、何が実行されどんな結果になったのかを追跡するためにはそのスレッドの残した履歴を見るだけで十分です。しかし、非同期だと、一つのリクエストが複数のスレッドを渡りながら処理されるため、一つのスレッドの履歴を追うだけでは一体どんなことが起こっているかわからなくなるからです。\nコードの伝染 非同期でコードを書くことになると、結局はアプリケーション全体から同期という概念を排除する結論に至るかもしれません。なぜなら、先に述べました通り、非同期の処理の中で一つでも同期の処理が混ざっているとしたら、それだけでも全体の処理が同期になっちゃうからです。なので、同期と非同期を一つのアプリの中で共存させるのはかなり難しくなり、結果的には非同期のコードに他のコードが「伝染」されるようなことが起きてしまうケースが発生します。例えばWebFluxの例では、同期のコードと混ぜて使うこともほぼ不可能に近いので、あえてオブジェクトをMonoやFluxに入れ(非同期に変え)、zipWith()でタプルとして結び、map()やflatMap()で処理するという形になるしかなくなります。そしてその逆の方法を取るとしたら(block()でMonoの中身を持ち出すなど)、もはや最初から同期でコードを書いた方が良いということになってしまうという問題があります。\nそれでも非同期は必要 以上のことは、おそらく非同期に触れてみた人なら誰でも一度は触れてみた問題であり、共感する人も多いのではないかと思います。しかし、このような不便さがあるにもかかわらず、依然として非同期プログラミングの必要性はあります。特に、今のトレンドだと多くのWebアプリケーションで同時処理性能が重要となっていますので、尚更です。実際グーグルアナリティクスは、KissMetricsを引用して「ページのレスポンスが 1 秒遅れると、コンバージョン数が 7% 減少する」、「47% の消費者は 2 秒以内にウェブページが読み込まれることを期待している」と行っているくらいですが、このような要求に対応できるのはやはり非同期としか言えません。なので、JavaScriptやC#のような言語ではasync/await/promiseなどを試し、KotlinでCoroutineというものを導入するなど、なんとしても非同期の短所を補完するという努力をしているところですね。\n特に、Javaの場合はOSのスレッドを直接使用するので、同時に処理できるリクエストは数千くらいにすぎません。なので、このスレッド基盤という言語そのものの限界を克服するため今まで多くの非同期ライブラリが作られてきました。しかし、ライブラリにはやはり限界があったので、JVMレベルでの対応が検討されているところです。それが先ほど紹介しました、Project Loomというものです。\nProject Loomという答え Project Loomでは、既存のスレッドをFiberという仮想・軽量スレッドとして切り分け、同時処理性能をあげると同時に、「非同期プログラミングを同期プログラミングの感覚でできるように」して、今までのように非同期か同期かによってコードが変わるような現象を無くすのが目的です。Fiberは数百万まで生成できるというし、使い方としても既存のスレッドどあまり変わらないので、コードの修正も少なくなります。それに、同期の感覚で書いても内部的にasync/awaitを使ったかのように処理されるので、コールバックヘルのような問題も無くなりますね。\nまた、KotlinのCoroutineはあくまでコンパイラレベルの対応なので、上述した3つの問題のうち、「コンテキストを見失う」、「コードの伝染」という問題は根本的に解決できませんが、Project LoomはJVMレベルの対応なので、これらの問題が全部解消されるというメリットもあります。そして既存の同期APIを非同期に変えることで、クライアントコードとしてはあまり変わることなくすぐに適応できるのも魅力的ですね。\n多くの場合に「非同期の性能」は欲しくても、多くのプログラマが「同期の書き方」に慣れているという現実からして、Proejct Loomが正式リリースされるとしたら既存のCoroutineやasync/await/promise、Reactice Streamを使ったプロジェクトの多くがFiberを使ったコードに変わるのではないかと思います。\n最後に どんな分野でも、過渡期にあるものがもっとも混乱しやすく、辛いものです。そしてプログラミングにおいては、非同期プログラミングというパラダイムが、まさにそうなのではないかと思います。多くの天才たちが言語、ライブラリ、フレームワークとしうさまざまな方面から非同期プログラミングの短所を克服しようと努力してきて、やがてその結実が見えてくるような気がします。Reactive Steamの思想、「反応型」という概念は個人的に嫌いではないですが、それが今までのプログラミングに比べあまりにも変わった書き方になってくるので、個人的には「特定の目的のため、完全に違う言語も学ぶのと同じくらいの努力が必要となるが、その効率の悪さを甘んずるほどのメリットがあるか」とも思います。それはおそらく、まだ自分が本格的に非同期のコードを書いてみたことがないのでそう思うだけなのかもしれませんが。でも、これもProject Loomのように、同期の感覚で非同期プログラミングができるような技術があると解決される問題でしょう。\nただ、まだProject Loomは完成した技術ではなく、開発途中のものなので、のちに限界や問題が見つかる可能性もありますね。そして将来的にWebFluxなどで採用されるとしても、リリースのタイミングがわからないので、それまではReactive StreamやCoroutineを使うしかないので、のちにLoomのリリースに合わせて既存のコードを全部書き換えるというのも無理な話です。なるべく早く出て欲しいのですが、まだ2年以内のリリースはないみたいなので、今の姿とは全く違うものになる可能性もありますね。\nだとしても、JVM自体が代わり、非同期プログラミングがより楽なるということは魅力的なものです。最近はKotlinにハマっていますが、こうやって変わっていくJavaを見るのも楽しいことですね。また新しい世界が見えてきそうな気分です。\nでは、また！\n","date":"2020-12-20T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-webflux-and-nonblocking/","title":"Spring WebFlux、少し触ってみてからの話"},{"content":"Javaはもともと手続き型な言語ですが、賢い方法で関数型な言語の特徴を受け止めていて、言語の中に共存させていますね。個人的には関数型プログラミングというものに憧れているので、Javaの中でも好んでStreamやLambdaを使っていて、個人的にもKotlinとSpring WebFluxで色々試しているところです。\nただ、Java 1.8から続いている話ですが、Streamは果たして全てのForループを代替できるのか？というものがありますね。そしてここでForループを代替ない理由としてよく挙げられているものが、性能・可読性・デバッグの難しさです。つまり内部的により複雑な処理を行っているため性能もよくないし、例外が発生した時も理由を特定するのが難しい上に、多くの人はMethod ChainingとLambdaに慣れてない、ということですね。\n以上の理由から、私も普段はやはりList = ArrayListで、ループは拡張For文(たまに、Listの要素をいじって新しいインスタンスを新しく生成する場合はStream)というルールを当たり前のように守ってきていますが、ふとこれで本当に良いのか、いう疑問が湧いてきました。Javaも16までバージョンアップしていて、そろそろ関数型プログラミングに転換しても良い時期なのでは？だったり、自分の知っているものは正しいのかという検証をしてみたいという風にですね。\nなので今更な感じではあるのですが、ちょっとしたベンチマークを兼ねて色々検証してみたり、考えてみました(本当は、ベンチマークがしてみたかっただけ)。\nループの方法 今更な紹介となりますが、そもそも今回のポストが今更な話をしているので、Collectionに関する4つのループ文の細かい話もして行こうかと思います。\n多くの場合、Collectionや配列のループ処理の方法は、以下の表通りに使われているのではないかと思います。\n種類 使う場面 For インデックスが必要な時 拡張For 他の方法を取る必要がない場合 Iterator 基本的に使わない forEach() 基本的に使わない 上記のケースが成立する基準は、やっぱり性能になっているのではないかと思います。他にも可読性だとか、色々考慮する要素はあると思いますが、何よりも性能が基準として優先されているのは否定できない事実かと思います。なぜなら、他の要素はチューニングが難しいか、できないものであり(例えばセキュリティやバグ防止のためのバリデーションチェックは、効率的なコードに書き換えることはできても、そもそも無くすというのは論外になりますね)、全ての要件が満たされたアプリケーションでリファクタリングにより「目に見える形で」改善できるのは性能しかないからでしょう。そもそも、同じ処理をするなら性能が良い方が絶対いいですし。\nなので、私が初めてループ処理に関して学んだ時は伝統的な形のFor文とWhileなのですが、のちにCollectionや配列だと拡張For文を使った方が良いという風に教わりましたが、その時も根拠としてあげられたのが「Forと拡張Forは性能上あまり違わない上に、拡張Forの方が常に要素数分だけループするのが保証されてあるから」ということでした。やはり性能から考えて、それから他のことも考慮するような話ですね。納得のいく話だったので、私自身もそれを信じて今までずっと拡張For文を使ってきました。\nでも、実際はどうか検証してみたことはあまりなかったですね。ネットなどで調べてみても、拡張For文は既存のループの書き方を向上させたものであるとか、IteratorのSyntax Sugarに過ぎないとかの話もあリました。聞いた話では、もっとも性能が良いのは\nただ一つ、StreamとforEach()はどうでしょう。Javaでこれらが導入されてからもさらに時間が立っています。しかし、上述したとおり、依然としてStreamやforEach()は性能が劣るから多く使われてないような気がしています(他にも、あえて使う理由がわからない・わかりにくいなどの理由があると思いますが)。最初Java 1.8リリース当時にも、多くの人が性能のテストを行い、少なくとも性能面では既存の方式が有利という結論を出していて、今もそれはあまり変わってないようです。Javaのバージョンも16にまで上がったのですが、それまで行われたチューニングを踏まえてもStreamやforEach()が持つ根本的なアーキテクチャ(?)的な理由から、既存の方式よりも性能が劣るのはしょうがない、という風に認識されています。\nしかし、誰かにそう言われたから、そう思うというのはあまり良い考え方ではないでしょう。また、前述のとおり、Javaはすでに16までバージョンアップを重ねていて、大抵の変化というのは新しい機能の追加となっていますが、裏では何かJVMやコンパイラのチューニングなどでなんらかの目に見えない改善があったのかもしれません。関数型としてのコードの書き方に慣れているかどうかは、その人の問題として、性能面で改善されているとしたら、よりモダンな方法を使わない理由がないですね。そして、本当に拡張For文が全ての場合で良いかどうかの検証もあらかじめしておく必要があると思います。\n以上の理由から、まず検証で使う4つのループの紹介と、そのベンチマークについて紹介したいと思います。\nFor文 まずは伝統的な形のFor文です。一部ではc-styleとも呼ぶらしいですね。一番基本となるもので馴染みもありますが、やはり古い、という印象もあります。端的に、最近のいわゆるモダンな言語では、このような形のループは使えない場合もありますね。基本的に以下のような形です。\nfor (int i = 0; i \u0026lt; list.size(); i++) { System.out.println(list.get(i)); } マイクロな最適化として、ループ対象のCollectionや配列の長さをあらかじめ宣言しておく場合もありますね。こうすると、ループ毎にループの対象となるCollectionや配列のサイズを毎回計算する必要がないため、少しは性能が有利になるという話があります。(コンパイラがこれぐらいの最適化は勝手にしてくれるという話もありますが)\nint size = list.size(); for (int i = 0; i \u0026lt; size; i++) { list.get(i); } この伝統的なFor文の良いところは、インデックスを基準にして処理をするため、インデックスが必要な場合はなんでもできるということです。例えば、以下のような場合があるでしょう。\n// 偶数のインデックスのみ処理をしたい for (int i = 0; i \u0026lt; list.size(); i += 2) { System.out.println(list.get(i)); } // 条件と一致する要素のインデックスが知りたい for (int i = 0; i \u0026lt; list.size(); i++) { if (list.get(i).length() == 10) { System.out.println(i); } } // 前後の要素と比較したい for (int i = 1; i \u0026lt; list.size(); i += 2) { System.out.println(\u0026#34;インデックス\u0026#34; + i - 1 + \u0026#34;の長さ：\u0026#34; + list.get(i - 1).length() + \u0026#34;、インデックス\u0026#34; + i + \u0026#34;の長さ：\u0026#34; + list.get(i).length()) } ただし、For文で指定されてあるインデックスが必ずループ対象の範囲内にあるかどうか、わからなくなる場合もあります。0から始まるインデックスでi - 1を指定してしまったり、iの範囲が対象のCollectionや配列よりも大きくなり例外を投げることになることもあるでしょう。また、インデックスを利用した場合、マジックナンバーになってしまう可能性もあるので、バグが発生する可能性が上がったり、可読性が悪くなるなどの問題があります。なので、インデックスを基準に処理をしたい場合は慎重にコードを作成する必要がありますね。\n拡張For文 いわゆるfor-each文ですね。Colleciton/配列内の全要素を巡回しながら処理するには、これほど理解しやすく、安全なものはないかと思います。\nfor (String element : list) { System.out.println(element); } 最近は、Javaのみでなく他の言語でもこれが標準となっているようです(書き方は言語毎に少し違いますが)。それはつまり、インデックスによるループよりも、ループないで扱うオブジェクトを拡張For文で対象のCollection/配列内の要素に確実に制限した方が色々有利だということでしょう。実際、インデックスといいつつ、伝統的なForb文のものはCollection/配列のインデックスと同じものでもないですので、危険なコードでもありますから。\n伝統的なFor文と比べ、拡張For文の中ではインデックスを利用することができないという問題があります。ただ、全く方法がないわけではないです。どうしてもインデックスを拡張For文の中で使いたい場合は、ループの外に定数を宣言するか、Collectionなら利用できるindexOf()か、Collections.binarySearch()を使う方法があります。\n// 定数を利用する方法 int i = 0; for (String element : list) { System.out.println(element + \u0026#34;のインデックス：\u0026#34; + i); i++; } // indexOf()を利用する場合 for (String element : list) { System.out.println(element + \u0026#34;のインデックス：\u0026#34; + list.indexOf(element)); } // Collections.binarySearch()を利用する場合 for (String element : list) { System.out.println(element + \u0026#34;のインデックス：\u0026#34; + Collections.binarySearch(values, value)); } ただ、ループの中でindexOf()を使うのはあまり良い選択じゃないです。以下はArrayList.indexOf()の実装になりますが、結局Collectionの中をループしながらインデックスを探すことになるので、実質的に二重ループになっちゃいます。なのでインデックスがどうしても必要な場合は、なるべく定数を使うか、伝統的なFor文を使うべきですね。\n// ArrayList.indexOf() public int indexOf(Object o) { return indexOfRange(o, 0, size); } int indexOfRange(Object o, int start, int end) { Object[] es = elementData; if (o == null) { for (int i = start; i \u0026lt; end; i++) { if (es[i] == null) { return i; } } } else { for (int i = start; i \u0026lt; end; i++) { if (o.equals(es[i])) { return i; } } } return -1; } CollectionsのbinarySearch()を利用する場合も、結局ループしながらインデックスを探すというのは変わりませんので注意を。以下はその実装です。\n// Collections.binarySearch() public static \u0026lt;T\u0026gt; int binarySearch(List\u0026lt;? extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; list, T key) { if (list instanceof RandomAccess || list.size()\u0026lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key); } private static \u0026lt;T\u0026gt; int indexedBinarySearch(List\u0026lt;? extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; list, T key) { int low = 0; int high = list.size()-1; while (low \u0026lt;= high) { int mid = (low + high) \u0026gt;\u0026gt;\u0026gt; 1; Comparable\u0026lt;? super T\u0026gt; midVal = list.get(mid); int cmp = midVal.compareTo(key); if (cmp \u0026lt; 0) low = mid + 1; else if (cmp \u0026gt; 0) high = mid - 1; else return mid; // key found } return -(low + 1); // key not found } private static \u0026lt;T\u0026gt; int iteratorBinarySearch(List\u0026lt;? extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; list, T key) { int low = 0; int high = list.size()-1; ListIterator\u0026lt;? extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; i = list.listIterator(); while (low \u0026lt;= high) { int mid = (low + high) \u0026gt;\u0026gt;\u0026gt; 1; Comparable\u0026lt;? super T\u0026gt; midVal = get(i, mid); int cmp = midVal.compareTo(key); if (cmp \u0026lt; 0) low = mid + 1; else if (cmp \u0026gt; 0) high = mid - 1; else return mid; // key found } return -(low + 1); // key not found } Iterator Iteratorは、個人的にあまり馴染まない(使いたくない)ものです。どのCollectionでもIteratorとして抽出ができてしまうので、CollectionよりもIteratorが主体になってしまうような感覚であり、定型文な書き方を矯正しているからです。少なくとも拡張ForではどんなCollectionのどんな要素を抽出して使っているのか明確ですが、Iteratorだとそれがわからないですね。\nとにかく、そんなIteratorですが、ForでもWhileでもループをかけるという特徴があります。\n// Forを使う for (Iterator iterator = values.iterator(); iterator.hasNext(); ) { System.out.println(iterator.next()); } // Whileを使う Iterator iterator = values.iterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } Iteratrorを使う場合の問題は、使い方がいまいち直感的ではないということです。例えば以下のような例をみましょう。getFoo()とgetBar()は、同じオブジェクトから呼ばれているように勘違いしやすいのではないでしょうか。\nfor (Iterator iterator = list.iterator(); iterator.hasNext(); ) { System.out.println(iterator.next().getFoo()); System.out.println(iterator.next().getBar()); // 注意！ } 面白いことに、拡張For文のバイトコードは、Iteratorを使うコードになるということです。なので少なくとも拡張For文は、Iteratorよりは発展した形と言えるのかもしれません。\nforEach() モダンな書き方としてのforEach()ですね。拡張For文とあまり違わないのですが、Lambdaやメソッド参照が使えるというメリットがありますね。また、Kotlinのスコープ関数のように、処理の範囲がはっきりするという意味で良いのかもしれません。何よりコードが短くなるのが好きですね。\nlist.forEach(System.out::println) 実装としても、拡張For文の中でLambdaを実行するという単純な構造になっています。なので単純に考えて、拡張For文よりは性能が劣る可能性がありますね。以下はIterableの実装です。\n// IterableのforEach() default void forEach(Consumer\u0026lt;? super T\u0026gt; action) { Objects.requireNonNull(action); for (T t : this) { action.accept(t); } } ただ、ArrayListの場合は実装が大きく違います。なので性能も大きく変わる可能性がありますね。以下はその実装です。\n// ArrayListのforEach() public void forEach(Consumer\u0026lt;? super E\u0026gt; action) { Objects.requireNonNull(action); final int expectedModCount = modCount; final Object[] es = elementData; final int size = this.size; for (int i = 0; modCount == expectedModCount \u0026amp;\u0026amp; i \u0026lt; size; i++) action.accept(elementAt(es, i)); if (modCount != expectedModCount) throw new ConcurrentModificationException(); } static \u0026lt;E\u0026gt; E elementAt(Object[] es, int index) { return (E) es[index]; } ベンチマークで検証してみると この度も、JMHを使って簡単なベンチマークを作ってみました。実はstatic finalなフィールドとして宣言しておくとそのオブジェクトを全てのベンチマークで使い回してくれるのかな、と思っていましたが、どうやらそうではなかったみたいです。なので今回は、ちゃんと@Setupアノテーションを使ってフィールドの初期化をしてみました。実際のコードは以下のとおりです。\n@State(Scope.Thread) public class LoopTest { private List\u0026lt;String\u0026gt; values; @Setup public void init() { final DecimalFormat format = new DecimalFormat(\u0026#34;0000000\u0026#34;); values = IntStream.rangeClosed(0, 9999999).mapToObj(format::format).collect(Collectors.toList()); } @Benchmark public void indexLoop(Blackhole bh) { final int length = values.size(); for (int i = 0; i \u0026lt; length; i++) { bh.consume(values.get(i)); } } @Benchmark public void iteratorLoopFor(Blackhole bh) { for (Iterator iterator = values.iterator(); iterator.hasNext(); ) { bh.consume(iterator.next()); } } @Benchmark public void iteratorLoopWhile(Blackhole bh) { final Iterator iterator = values.iterator(); while (iterator.hasNext()) { bh.consume(iterator.next()); } } @Benchmark public void extendedLoop(Blackhole bh) { for (String value : values) { bh.consume(value); } } @Benchmark public void forEachLoop(Blackhole bh) { values.forEach(bh::consume); } } そしてベンチマークの結果は、以下のとおりです。\nBenchmark Mode Cnt Score Error Units LoopTest.indexLoop thrpt 25 27.737 ± 0.475 ops/s LoopTest.iteratorLoopFor thrpt 25 26.968 ± 0.556 ops/s LoopTest.iteratorLoopWhile thrpt 25 27.250 ± 0.557 ops/s LoopTest.extendedLoop thrpt 25 13.186 ± 0.152 ops/s LoopTest.forEachLoop thrpt 25 12.479 ± 0.104 ops/s やはり、4つのループがそれぞれ違う結果を見せているのがわかります。少なくとも、ここでは伝統的なFor文を使った方がもっとも性能の面では有利のように見えますね。なるべく拡張For文を使った方が良い、という根拠として性能はあまり変わらないからというのはなんだったんだろう、と思うくらいの差があります。\nしかし、本当にこれで、性能が良い方を選べば良いという結論を出して良いのでしょうか？\n考えたいこと 処理としての結果が同じだとしたら、やはり性能の良い方を選びたくなるのは当然です。企業レベルの話だと、性能は費用と直結する問題でもありますしね。しかし、複雑化している現代のアプリケーションで考えるべきは、性能のみではありません。極端的な話だと、性能のためにをC、C++でWebアプリケーションを作るとしたら、他の言語に比べて生産性が下がってしまうでしょう。そして可読性や維持保守を考えず、性能を優先したコードだけを書いていくと、いわゆるスパゲティコードになってしまう可能性もあります。\nなので、性能のみではなく、アプリケーションを開発するときには色々と考慮すべき要素があるのは確かです。例えば、Readability(可読性)、Error-proneness(エラー発生可能性)、Capability(処理能力)などがあるでしょう。今までは性能ばかりの話をしてきましたが、これらの観点から4つのループを比較してみたらどうでしょう。\n可読性とエラー発生可能性の側面から考える 拡張For文(forEach())では、Collectionそのものの要素をことができます。逆に、For文やIteratorでは可能ですね。ならば、Collectionや配列の中でとある条件と一致する要素だけのことしたい場合は、拡張For文よりもFor文やIteratorを使うべきであるようにも見えます。\nしかし、観点を変えてみると、元のオブジェクトそのものが変わることで起こり得るサイドエフェクトが発生する場合も考えられます。こういう場合、元のオブジェクトを直接操作できるということはメリットではなくデメリットになってしまいますね。なので、どちらかというと、与えられたCollection/配列から条件に一致する要素だけを抽出して新しいCollection/配列のインスタンスを生成するのが正解の可能性もあります。そしてそれをよりわかりやすいコードとして実現できるのは拡張For文(forEach())ですね。例えば、以下のようにです。\n// 元のリストが変わってしまう public void filterFor(List\u0026lt;String\u0026gt; list) { for (int i = 0; i \u0026lt; list.size(); i++) { if (list.get(i).length() \u0026gt; 10) { list.remove(i); } } } // 元のリストには影響がない - For文 public List\u0026lt;String\u0026gt; filterFor(List\u0026lt;String\u0026gt; list) { List\u0026lt;String\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; list.size(); i++) { String element = list.get(i); if (element.length() \u0026lt;= 10) { result.add(element); } } return result; } // 元のリストには影響がない - 拡張For文 public List\u0026lt;String\u0026gt; filterForEach(List\u0026lt;String\u0026gt; list) { List\u0026lt;String\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (String element : list) { if (element.length() \u0026lt;= 10) { result.add(element); } } return result; } // 元のリストには影響がない - Stream.forEach() public List\u0026lt;String\u0026gt; filterStream(List\u0026lt;String\u0026gt; list) { return list.stream().filter(element -\u0026gt; element.length() \u0026lt;= 10).collect(Collectors.toList()); } 良いコードは、短く、わかりやすいコードなのではないかと思っています。そしてわかり安いコードは、誰がメンテしてもバグを起こす可能性は低くなるはずでしょう。そういう観点からすると、伝統的なFor文とIteratorは、今は使うべきではないのかもしれません。\n処理能力の側面から考える 処理能力、というのはある程度性能ともつながるものですね。なので、性能という側面でもう一度考えてみます。互換性、汎用性などとも言える物かもしれません。ここで言いたいのは、Collection/配列がどんなものであれ、一定の性能を保証する実装を考える必要があるということです。\n引数としてListをとり、なんらかの処理をループで行うメソッドを実装するとしましょう。今まであげてきた、4つのループのパターンのうちどれを選ぶべきかは、その引数の実装クラスが何になるかわからない、という面も考慮する必要があります。なぜなら、Listは色々な実装クラスを持つインタフェースだからです。\n引数としてListをまず宣言しておくと、言語の仕様としてはListの実装クラスはどれでも許容することになりますね。なので引数として入ってくるのはArrayListになる可能性もあり、LinkedListにな可能性もあり、極端的にはAbstractListで個人がカスタマイズしたものが来る場合もあると予想できます。他にもJava 11を基準に、java.util.Listを継承しているCollectionの実装クラスの場合、例えばAbstractSequentialList、AttributeList、CopyOnWriteArrayList、RoleList、RoleUnresolvedList、Stack、Vectorなどがあって、これらが全部Listになりえるので、どの実装でも対応する必要があります。\nもちろん、Javaでとあるインタフェースを継承するということは、処理の前提条件と結果結果が明確であることと同じ意味なので、実装クラスが変わったって、処理の結果が大きく変わることはないです。ただ、Listの実装クラスが複数存在するのは、それらを使う目的によってどちらかに偏ったパフォーマンスを見せるためであることをまず理解する必要がありますね。これはつまり、同じ条件下でも実装クラスによって処理の性能が大きく変わる可能性があるということです。一般的に多く使われているListの実装クラスはArrayListですが、参照以外の性能が劣るという理由からLinkedListが使われる場面もあると予想できます。だとすると、ArrayListで性能がよかったものがLinkedListでもそうとは限らないものですね。\n上記で実施したベンチマークだけをみて、性能は絶対これが有利だ、と言いきれない理由がここにあります。なぜなら、テストようのデータをCollectors.toList()を使ってListとして作成していますが、以下のコードでわかるように、常にArrayListが生成されているからです。\npublic static \u0026lt;T\u0026gt; Collector\u0026lt;T, ?, List\u0026lt;T\u0026gt;\u0026gt; toList() { return new CollectorImpl\u0026lt;\u0026gt;((Supplier\u0026lt;List\u0026lt;T\u0026gt;\u0026gt;) ArrayList::new, List::add, (left, right) -\u0026gt; { left.addAll(right); return left; }, CH_ID); } なので、ついでに他の実装クラスのベンチマークもしてみることにしました。ただ、Listの実装クラスを全部テストするのは無理があるので(特に、AbstractListやAbstractSequentialListは別途実装が必要ですし、CopyOnWriteArrayListはマルチスレッドでないと意味がないし、RoleListやVectorはほとんど使われてなく、Stackをループで利用するとは思わないので)、LinkedListの場合はどうかだけ確認してみました。まあ、ArrayListと違う反例は一つだけあったら十分ですしね。\n幸い、CollectorsにはtoCollection()でCollectionの実装を指定できます。なので、上記のベンチマークのコードから、以下のような修正を入れるだけでListの実装を変えることができます。\n// LinkedListの場合 values = IntStream.rangeClosed(0, 9999999).mapToObj(format::format).collect(Collectors.toCollection(LinkedList::new)); LinkedListの場合、要素数が増えると急激に性能が低下する傾向があります。なので、ArrayListの時よりも要素数は2桁ほど減らしてベンチマークを実施しました。結果は以下です。\nBenchmark Mode Cnt Score Error Units LoopTest.indexLoop thrpt 25 0.084 ± 0.005 ops/s LoopTest.iteratorLoopFor thrpt 25 854.459 ± 36.771 ops/s LoopTest.iteratorLoopWhile thrpt 25 839.233 ± 18.142 ops/s LoopTest.extendedLoop thrpt 25 659.999 ± 47.702 ops/s LoopTest.forEachLoop thrpt 25 780.463 ± 78.591 ops/s ArrayListとは真逆の結果になったのがわかります。特に、インデックスによるループは使えるものにならないほど性能が低く、拡張For文よりもforEach()の性能が高いという、意外の結果となっています。このベンチマークでの数値が絶対的なものとは言えませんが、結果から推論できるのは、やはりArrayListのインデックスを利用する伝統的なFor文でのループが一番早かったからという理由だけで、全てのListをFor文で処理するというのは危ないということです。なので「どの実装クラスでも、平均的に良い性能を出してくれる」方式を選ぶ必要があるという結論を出せるでしょう。(それがおそらく拡張For文な気がします)\n最後に 全ての場面で最適なコードを書くのは難しいことで、過去に書いたコードはいずれ改善しなければならないものとなりますね。あまりエンジニアとしての歴の長くない自分でも、たまに入社前のコードをみるとびっくりするくらいです。なんとか動くようなものは作ったものの、重複するコードや無駄なインスタンス作りなど、至る所に自分のミスが散らかっています。\nなのでたまには、そのような過去の自分が書いたコードと向かい合って、それを直してみるのも良い経験になるのではないかと思ったりもします。特に今回みたいに、ループ処理は基本の中の基本ですが、その処理すらどれを選ぶかよくわかってないまま(そして副作用などは考えず)、ただひたすら書いてしまったものも多かったので、それに対する反省を兼ねて、そして自分の思うことの根拠を探すための勉強にもなりますので。そしてベンチマーク、意外と楽しいですので。これで自分の理論を証明していくのも良い経験ですね。\nでは、また！\n","date":"2020-11-30T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-collection-loop/","title":"今更なループの話"},{"content":"一般的に、アプリケーションには要求される業務的な要件やセキュリティの観点から考慮すべきことがあるので、なんらかの機能を作るときはそれが「動くか」だけでなく、任意のロジックが必要となる場合がありますね。なのでその機能が動くにはとある場合で、動くときはとある条件に合わせて処理をする、といった制限が要求されることがあります。\n今回のポストも、またそのような業務上の要件から生まれた話です。現在、私が関わっている案件では、EC2で起動するSpring Boot基盤のアプリを作っています。このアプリでは、ファイルのデータとアップロード先のパスを指定すると、S3にアップロードするという単純な機能があり、それは自分の担当となっています。\n単純にアップロードパスとデータがあれば、動く機能を作るのは単純です。SpringにはSpring Cloudというフレームワークがあるので、すでにResourceLoaderというクラスを持ってファイルアップロードを実現できます。Spring Cloudを使わない場合でも、AWS SDKを使うと簡単に実装ができます。事実、こちらの昨日もアップロード先のパスとファイルだけあれば良いものとなっているので、実装というまででもないですね。\nただ、この機能が呼び出されたとき、渡されたアップロード先のパスが「正しいもの」であるかを確認する必要がありました。つまり、業務上ファイルをS3に格納する際に決まったパスのルールがあって、この機能からはパラメータとして渡されたパスが規定のパターンと一致するかどうかを一度チェックする必要がありました。\n渡されたパスが「正しいもの」かどうかをチェックするための機能は、何で作ったら良いでしょうか。そしてどう作った方が良いでしょうか。色々な方法があるかと思いますが、まずここでは自分がどう実装したかを紹介していきたいと思います。\n文字列のパターンは正規表現で まず、ファイルのアップロード先(保存先)パスは文字列であり、特定のパターンである必要があります。文字列が特定のパターンで構成されているかどうかの判定は、正規表現を使いますね。なので、「正しいもの」としてのパスのパターンは、正規表現としてあらかじめ宣言しておいて、渡されたパラメータがそれと一致するかをチェックすることとします。ただ、Javaでは正規表現を使って文字列のパターンを判定する方法がいくつかありますので、それらのうちにどれを選ぶべきかを考える必要があります。例えば、以下の方法がありますね。\nPattern.matches()を使う PatternからMatcherを取得し使う PatternからPredicateを取得し使う String.matches()を使う そして、これらの方法は、以下のコード通りに使うことができます。\n// 正規表現の例 String patternRegex = \u0026#34;^[0-9]*$\u0026#34;; // 正規表現で判定したい文字列 String value = \u0026#34;123456789\u0026#34;; // Pattern.matches()を使う場合 boolean patternMatches = Pattern.matches(patternRegex, value); // Matcherを使う場合 Pattern pattern = Pattern.compile(patternRegex); Matcher matcher = pattern.matcher(value); boolean matcherFind = matcher.find(); // 部分一致 boolean matcherMatches = matcher.matches(); // 完全一致 // Predicateを使う場合 Pattern pattern = Pattern.compile(patternRegex); boolean matcherFind = matcher.asPredicate().test(value); // 部分一致 boolean matcherMatches = matcher.asMatchPredicate().test(value); // 完全一致 // String.matches()を使う場合 boolean stringMatches = value.matches(patternRegex); ここでMatcherやPredicateを使う場合、部分一致を選べられるので、部分一致の場合はこれらを使うしかなさそうです。しかし、完全一致が必要な場合は何を基準に、どれを選ぶべきでしょうか。どれも同じような結果を出すのであれば、より効率的な方法を選びたくなります。そして、この場合、考えられるのは性能です。つまり、どれを使った時にもっとも早く判定の結果を得られるかということです。\nどれも同じなら性能で 前述の通り、文字列が与えられた正規表現のパターンと一致するかどうかを判断する様々な方法があるので、中でももっとも早いのはどれか、測定したいと思います。いわゆるストップウォッチ方式(処理終了時点の時間から、処理開始時点の時間を引く)が簡単ですが、より正確な比較がしたかったためOpenjdkから提供するJMHを使ってベンチマークを作りました。Java特有の起動が遅い問題で測定に影響が出るのを防ぐためか、何回かのウォーミングアップも含めて測定をしてくれるので、良いですね。\n実際にベンチマークを行うため使ったコードは、以下の通りです。\npublic class RegexTest { private static final String PATTERN_REGEX = \u0026#34;^[0-9]*$\u0026#34;; private static final DecimalFormat DECIMAL_FORMAT = new DecimalFormat(\u0026#34;0000000\u0026#34;); private static final Pattern PATTERN = Pattern.compile(PATTERN_REGEX); private static final Predicate PREDICATE = PATTERN.asPredicate(); private static final Predicate MATCH_PREDICATE = PATTERN.asMatchPredicate(); private static final List\u0026lt;String\u0026gt; VALUES = IntStream.rangeClosed(0, 9999999).mapToObj(DECIMAL_FORMAT::format).collect(Collectors.toList()); @Benchmark public void patternMatches(Blackhole bh) { for (String value : VALUES) { bh.consume(Pattern.matches(PATTERN_REGEX, value)); } } @Benchmark public void matcherFind(Blackhole bh) { for (String value : VALUES) { bh.consume(PATTERN.matcher(value).find()); } } @Benchmark public void matcherMatches(Blackhole bh) { for (String value : VALUES) { bh.consume(PATTERN.matcher(value).matches()); } } @Benchmark public void predicate(Blackhole bh) { for (String value : VALUES) { bh.consume(PREDICATE.test(value)); } } @Benchmark public void matchPredicate(Blackhole bh) { for (String value : VALUES) { bh.consume(MATCH_PREDICATE.test(value)); } } @Benchmark public void stringMatches(Blackhole bh) { for (String value : VALUES) { bh.consume(value.matches(PATTERN_REGEX)); } } } そして、測定の結果は以下の通りです。実際の出力はメソッド名の順番が違いますが、コードでの並び順に合わせて変えています。\nBenchmark Mode Cnt Score Error Units RegexTest.patternMatches thrpt 25 0.591 ± 0.013 ops/s RegexTest.matcherFind thrpt 25 1.525 ± 0.022 ops/s RegexTest.matcherMatches thrpt 25 1.481 ± 0.030 ops/s RegexTest.predicate thrpt 25 2.050 ± 0.182 ops/s RegexTest.matchPredicate thrpt 25 1.733 ± 0.236 ops/s RegexTest.stringMatches thrpt 25 0.609 ± 0.005 ops/s この結果からして、性能面では確かにMatcherかPredicateを使った方が良いと言えるでしょう。ただ、ベンチマークの結果としてはPredicateが誤差を含めても性能がもっとも良いこととなっていますが、Pattern.asPredicate()はJava 1.8、Pattern.asMatchPredicate()はJava 11から導入されたので、JDKのバージョンに合わせて適切な方を選ぶ必要があります。\nただ、結果だけでなく理由も知りたくなります。性能がよかったMatcherとPredicateの場合、テストではあらかじめインスタンスを作成しておいたという共通点があります。なので、性能の低いPattern.matches()とString.matches()の場合、メソッドが呼び出されるたびにインスタンスを作成しているため遅くなっているのではないか、という推測もできますね。実際はどうか、コードをみていきましょう。\nまずPattern.matches()ですが、実際のコードは以下の通りです。\n// Pattern.matches public static boolean matches(String regex, CharSequence input) { Pattern p = Pattern.compile(regex); Matcher m = p.matcher(input); return m.matches(); } これをみると、PatternとMatcherのインスタンスがメソッドを呼び出すたびに生成されるということがわかります(実際、Pattern.compile()とPattern.matcher()のコードを追ってみるとインスタンスを作成するのがわかります)。なのでこちらが遅くなるのは当然のことですね。\nそれでは、String.matches()の場合はどうか、同じくコードから確認しましょう。実際のコードは以下の通りです。\n// String.matches public boolean matches(String regex) { return Pattern.matches(regex, this); } これもまた、単にPattern.matches()を呼び出しているだけなので、遅いわけですね。ただ一つ違う点は、比較対象となる自分自身のインスタンスが必要なため、Patternとは違ってstaticメソッドではないというところといえますが、これは性能に影響する部分ではないので、ベンチマークでも誤差範囲の中の結果となったと思います。\n実際のValidatorを作る では、性能でMatcherとPredicateが有利であるということがわかったので、あとはこれを利用して、渡されたパスが許容できるものかどうかを判定するValidatorを作ります。今の案件ではJava 11を使うので、Predicateを選びました。\nパスのパターンは複数あるので、配列やリストとしてパターンを指定して起きます。また、Predicateで判定するので、あらかじめ指定したパターンでインスタンスを作成しておいて、判定が必要なときはパターンの配列やリストをループさせて、一致するものがあるかどうかを返すと良いでしょう。この要件から、実際のコードは以下のようになりました(パスの正規表現は、実際の業務とは違うものとなっていますが)。\n@NoArgsConstructor(access = AccessLevel.PRIVATE) public class StorageValidator { /** * 許容されたパスのパターン */ private static final List\u0026lt;Predicate\u0026lt;String\u0026gt;\u0026gt; PATH_PATTERN_MATCHERS = List.of( createMatcher( \u0026#34;\\\\/contents\\\\/images\\\\/\\\\d{0,4}\\\\/(19[0-9]{2}|20[0-9]{2})(0[0-9]|1[0-2])\\\\/thumbnail\\\\.(?:bmp|jpg|jpeg|gif|png|BMP|JPG|JPEG|GIF|PNG)$\u0026#34;), createMatcher( \u0026#34;\\\\/contents\\\\/images\\\\/\\\\d{0,4}\\\\/(19[0-9]{2}|20[0-9]{2})(0[0-9]|1[0-2])\\\\/thumbnail_backup\\\\.(?:bmp|jpg|jpeg|gif|png|BMP|JPG|JPEG|GIF|PNG)$\u0026#34;)); /** * * 与えられた文字列が、SPLで利用できる有効なファイルアップロードパスであるかどうかを判定する. * * @param path 判定対象の文字列(ファイルパス) * @return 判定結果 */ public static boolean isValidUploadPath(String path) { return PATH_PATTERN_MATCHERS.stream().anyMatch(predicate -\u0026gt; predicate.test(path)); } /** * * 与えられた正規表現から、{@link Predicate}型のパターンマッチャーオブジェクトを返す. * * @param pattern 正規表現 * @return 生成されたパターンマッチャーオブジェクト */ private static Predicate\u0026lt;String\u0026gt; createMatcher(String pattern) { return Pattern.compile(pattern).asMatchPredicate(); } } 以上で、渡されたパスが想定のパターンと一致するものかどうか判定することができるようになりました。簡単ですね。\n番外：Kotlinで書いてみたら？ 今回のポストの内容とはあまり関係のないものですが、ちょっとした興味本位から、Javaで作成したValidatorをKotlinのコードに変えてみました。幸い、intellijには、Javaで書かれたコードをKotlinに変えてくれる便利な機能がありますので簡単にできます。そもそもKotlinを作ったのがJetbrain社なので、Kotlinを普及するための機能であるとは思いますが、おかげでJavaプログラマがKotlinに入門するのも簡単になりますね。\nstatic finalなフィールドをKotlinではcompanion objectとして扱うことになるらしく、コード自体はそう変わらない感覚です。ただ、Kotlinではstream()を呼び出さなくてもCollectionから直接呼び出すことのできるメソッド(any)があったり、List.of()もlistOf()で代替できるのですが、自動変換ではそこまでしてくれなかったので、そういうところ自分で変えるしかないです。それで完成したコードは、以下の通りです。\nclass StorageValidator { companion object { private val PATH_PATTERN_UPLOAD = listOf( // 画像の保存先パスのパターン(正規) Pattern.compile( \u0026#34;\\\\/contents\\\\/images\\\\/\\\\d{0,4}\\\\/(19[0-9]{2}|20[0-9]{2})(0[0-9]|1[0-2])\\\\/thumbnail\\\\.(?:bmp|jpg|jpeg|gif|png|BMP|JPG|JPEG|GIF|PNG)$\u0026#34; ) .asMatchPredicate(), Pattern.compile( \u0026#34;\\\\/contents\\\\/images\\\\/\\\\d{0,4}\\\\/(19[0-9]{2}|20[0-9]{2})(0[0-9]|1[0-2])\\\\/thumbnail_backup\\\\.(?:bmp|jpg|jpeg|gif|png|BMP|JPG|JPEG|GIF|PNG)$\u0026#34; ) .asMatchPredicate() ) } fun isValidUploadPath(path: String): Boolean { return PATH_PATTERN_UPLOAD.any { predicate -\u0026gt; predicate.test(path) } } } 最後に 本当は、正規表現を使って文字列がパターンと一致するかどうかを判定する、という機能を作るのはそう難しいことではないですね。どちらかというと、正規表現そのものの書き方が難しいのでは、と思います。ただ、最近はRegExr、RegEx Testing、regular expressions 101など、その場で入力した正規表現をテストしながら作成できるオンラインのツールなども多いので、じっくり時間をかけるといくらでも必要なパターンに合わせたものを作ることができると思います。\n個人的な感想としては、書いたコードは短く簡単でしたが、久々に色々と考えられる(効率の面で)チャンスとなったのでなかなか面白い作業になったと思っています。これからもこのような要件があるとしたら、また違う方法で挑戦してみたくなりますね。では、また！\n","date":"2020-11-22T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-string-pattern-validator/","title":"パターンと一致する文字列かを判定する"},{"content":"KotlinがJavaと区別される特徴といえば色々あるとは思いますが、そのうちの一つとしてあげられるものがScope Functionではないかと思います。以前のポストでも簡単に触れたことがありますが、これらのビルトイン関数たちは確かに上手く使うと、Javaより簡潔なコードを書くばかりでなく、より作成者の意図が伝わりやすい、強力な武器になり得るのではないかと思います。しかしながら、やはり他の言語と比べて新しい概念のものであるので、どの場面で使った方が良いか、どう使ったらいいかという、いわゆるBest Practiceがどんなものであるか悩ましくもなりますね。\nおそらくこう思っているのは自分だけでなく、ネットから検索してみるとScope Functionに関しての多くの記事を見つけることができますが、大抵が個別の関数の使い方の紹介に過ぎないような気がしています。なので、具体的にこれらの関数はなんのためのものであり、それぞれどう使い分けるか、またどの場面で使うべきかというのが気になっていたところです。調べてみたところ、公式のドキュメントやいくつかのブログの記事などに情報があったので、これらをまとめて整理してみました。\nそもそもScope Functionって？ 最初にScope Functionを言及しましたが、これは一体なんでしょう。まずはなぜこれらの関数にこんな名前がつけられたのかを知る必要がありそうです。公式の説明だと、以下のように述べています。\nThe Kotlin standard library contains several functions whose sole purpose is to execute a block of code within the context of an object. When you call such a function on an object with a lambda expression provided, it forms a temporary scope. In this scope, you can access the object without its name. Such functions are called scope functions.\n要するに、特定のオブジェクトに範囲(Scope)を限定し、Lambdaで書かれた関数を実行させたい場合に使えるもののようです。もちろん、これは全く新しい形の関数ではありません。形(コードの書き方)としては、オブジェクトの関数を呼び出しているだけのように見えるだけですね。しかし、コードをみてみると、コードブロックで警戒を作ることでより特定のオブジェクトに限った処理であることが視覚的に、明確になります。例えば、以下のようなコードはどれもやっていることは同じですが、コードも読む側からすると処理を適用させる範囲は後者の方が明確になりますね。\n// Scope Functionなしのコード var john = Person(\u0026#34;John\u0026#34;, 20, Gender.Male) john.doWarmingUp() john.startToRun() // Scope Functionのletを使った場合 var john = Person(\u0026#34;John\u0026#34;, 20, Gender.Male).let { it.doWarmingUp() it.startToRun() it } どう違う？ 上述した通り、コードの範囲を限定することで、処理の範囲が明確になるのが分かりました。しかし、これでScope Functionを使う準備は整ったかというと、そうでもないです。実はそれ以外でも考えなければならないところがあるのです。なぜなら、Scope Functionにはwith、let、apply、run、alsoの5つの関数が存在するからです。複数の関数が存在していることは、時と場合によって違う関数を選ぶ必要があるということでしょう。\nでは、これらの関数は他と何が違うのでしょう。まず、考えられるのは仕様です。Scope Functionは、内部的には渡したLambdaを実行してその結果を返すこととなっています。だとすると、仕様としてはLambdaで扱うオブジェクトと戻り値の差だけとなります。Scope Functionには処理の対象となるオブジェクト(レシーバーと言います)と、Lambdaをまずパラメータとして渡すようになります。ここで渡したレシーバーへのアクセスをどう書くかと、処理後の戻り値がどうなるかがこの5つのScope Functionでそれぞれ違います。これをテーブルとして表すと、以下のようになります。\n関数名 レシーバーのアクセス 戻り値 with this 最後の結果 let it 最後の結果 apply this T run this 最後の結果 also it T また、withを除いた他の4つの関数たちは、Extension Functionでもあるという特徴があります。 Extension Functionは何か？というと、簡単に、既定のクラスに関数を追加することです。Javaだとクラスの継承やラッパークラスの作成、オーバーライドなどで実現できる機能ですが、Kotlinでは簡単に定義できます。\n「Scope FunctionなのにExtension Functionでもあると？」と思うかもしれませんが、理由は簡単です。クラスの作成時に関数として定義してなくても、もともとそのクラスにあったかのように呼べる関数だから、ということです。どのオブジェクトでも、宣言しなくてもwithを除いたScope Functionをあらかじめ宣言しておいたかのように使うことができます。\n参考：itとthis itは、パラメータが一つのみのLambdaで使われるものです。例えばJavaだと、パラメータが一つだとしても、Method Referenceを使わない限り以下のように書く必要があります。\nList\u0026lt;String\u0026gt; names = List.of(\u0026#34;john\u0026#34;, \u0026#34;jack\u0026#34;); // Predicateの引数は一つだけだが、 Optional\u0026lt;String\u0026gt; filtered = names.stream().filter(name -\u0026gt; \u0026#34;john\u0026#34;.equals(name)).findFirst(); Kotlinでは、同じ状況だとパタメータを省略して単純にitとして表現することができます。\nval names: List\u0026lt;String\u0026gt; = listOf(\u0026#34;john\u0026#34;, \u0026#34;jack\u0026#34;) // itでの省略 val filtered = names.first { it == \u0026#34;o\u0026#34; } 結局、thisと同じではないか？と思いがちですが、itはLambdaのぱらめーたにスコープが限定されて、thisのスコープはローカルでもグローバルでもなれるという点が違います。なぜなら、thisはレシーバーそのものを指していて、パラメータがない場合はLambdaのスコープの外を指すことになるからです。つまり、パラメータのないLambdaではitを使えないですが、thisは使えるということになります。\nいつ使う? では、Scope Functionに5つの関数があり、それぞれどう違うかがわかったので、次はそれぞれをどう使い分けるか知りたいですね。さまざまな意見があったので、(自分の)納得の行くもので整理してみました。\nwith withはExtension Functionではないので、オブジェクトをパラメータとして受け取る一般的な関数として使えます。このような特徴があるので、複数のオブジェクトで同じような処理を行う必要があるときに使えます。例えばforループの中で、わざとメソッドを分離したくない(命名がめんどくさいとか…)の場合に使えるでしょう。また、やはりScope Functionなので、処理の範囲を明確に区分するためにも使えそうですね。\nfor (name in names) { println(name) with(name) { var rev = this.reversed() reversedName.add(rev) } } let オブジェクトをトリガーにして、そのオブジェクトを持って何かの処理を行いたい場合に使うものです。letの意味通り、そのオブジェクトを持って何かをするというイメージですね。また、戻り値も最後の結果となるので、それを持ってまた何かができそうです。また、Safe Callな使い方ができるため、Nullではない場合のみの動作として指定することも可能です。なのでNullになり得るオブジェクトでは、letを使った方が良いでしょう。\nvar name: String? = null name?.let { println(\u0026#34;name is not null\u0026#34;) } apply Lambdaでレシーバーの関数を使わず、レシーバーそのものを返却する場合に使います。つまり、レシーバーのプロパティに新しい値を入れる場合のことです。代表的にはオブジェクトの初期化がありますね。もちろん初期化だけなら、コンストラクタを呼ぶだけで十分ではないかと思われますが、同じオブジェクトの値を入れ替える(例えばConfigurationクラスなど)場合に役立つらしいです。例えば以下のような場合を考えられます。\nif (devMode) { SomeConfig().apply { name = \u0026#34;devMode\u0026#34; } } run 他の4つの関数で事足りるので、なるべくこれは使わないように、という話が多いです。確かに、runはノーマルな関数としても使えるので(run {})、withとの違いがいまいちわからない感覚ではありますね。あえて使う場合は、オブジェクトの値をオーバーライドする時だそうです。でも、これもletでできるので、あえて使う必要はなさそうです。多くのケースで、これは推奨されてませんでした。\nただ、一部ではオブジェクトの初期化として使うと便利と言ってました。確かにthisを使うので、少しながらitを使うletよりコードが短くなり得るというメリットはありますね。\n// runを使う場合 var result1 = server.run { port = 8080 get(\u0026#34;/members:$port\u0026#34;) } // letを使う場合 var result2 = server.let { it.port = 8081 get(\u0026#34;/members:${it.port}\u0026#34;) } also オブジェクトがトリガーとなり、そのオブジェクトとは関係のない別の処理を行います。なので、元のオブジェクトがNullであっても、そのオブジェクトが呼ばれた時点でなんらかの処理を行うことができます。これを応用すると、条件分岐(三項演算子)のように使うことも可能です。例えば、こんな感じですね。\nvar name: String? = null name?.let { println(\u0026#34;name is not null\u0026#34;) } ?: also { println(\u0026#34;name is null\u0026#34;) } まとめ Kotlin Standard Library (let, run also, apply, with)では、5つのScope Functionのどれを使うか判断する基準を、フローチャートとして提示しています。以下は、そのフローチャートを簡単に翻訳したものです。どれを使うか悩ましい時は、こういう基準でコードを書いても良さそうですね。\n応用 Scope Functionが戻り値としてレシーバーそのものを返すというのは、すなわちBuilderパターンとして使えるという意味でもあります。なので、適切な組み合わせで、Scope Functionによるメソッドチェーンもできます。これをよく活用すると、かなり関数型的な感覚でコードを書くことができるでしょう。以下は、その例の一つです。\n// letの連鎖 var three: Int = \u0026#34; abc \u0026#34;.let { it.trim() }.let { it.length } // alsoの連鎖 var jack: Person = Person(\u0026#34;Jack\u0026#34;, 30, Gender.MALE).also { println(it.name) }.also { it.doWarmingUp() } 最後に 実は、このようなKotlinの機能は、そう新しいものでもありません。なぜなら、同じJVM言語であり、同じく機能するwith()のような関数を提供しているGroovyがあるからです。自分の中ではSpockかGradleのイメージしかないのですが…\nただ、このようにKotlinが提供する機能は「新しくはないけど、快適」という感覚を確かに伝えてくれているような気がします。なのでJavaプログラマーがKotlinに転向していく数も増えているのでしょう。最近は特にPythonやJavaScriptのような言語が人気を得ているところですが(KotlinはTIOBEの順位からだと見えもしないし…)、もっと性能と安定性、そして快適な開発のバランスを持ちたいと思う人にはぜひおすすめしたい言語です。なのでもっと、Kotlinの魅力をより多くの人(自分を含め)にわかった欲しいところです。今回のポストで少しはそれが伝わると幸いですね。\nでは、また!\n","date":"2020-11-04T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-scope-function/","title":"Scope Functionはどう使うか"},{"content":"KotlinがAndroidの公式言語になってからもだいぶ長い時間が経ちましたが、まだまだWebアプリケーションの業界ではサーバサイド言語としてはJavaを使う企業も多く(自分の場合がそうです)、モバイル業界でもJavaを使うところが多いようです。Javaも9以降はアジャイル開発でバージョンアップにスピードを出していて、いわゆるモダンな言語の特徴を吸収していってますが、そもそもの言語のデザインが古いし、互換性のために昔ながらの名残を捨てられてないところもあるので根本から違う哲学でデザインされた言語とはかなり違うはずです。また、JVMを利用しないKotlin Nativeも発表されているので、今後Javaよりも活躍できる場面が多いかも知れないなーという気がしたりもします。(GraalVMは使われることあるのかな…)\n取りまとめ、まだ正式の研修とかを受けたわけでなく、あくまでSpring WebFluxを使って簡単なアプリを作ってみるついでに使ってみただけなので、よくわかってない部分も多いかと思いますが、今まで触れてみた感想をJavaプログラマーの観点から簡単に紹介したいと思います。\nこれがよかった まずは使ってみてよかったところから。結論からとなりますが、良いと思ったところはおおよそイメージ通り(期待してた通り)という感覚です。\nやはりモダンな感じ Kotlinで書いたコードを見ると、モダンな言語だとやはりこんなものかなという感覚ではあります。モダンな言語が何か、という定義から必要になるのではという気もしますが、例えばSwift、Kotlin、Goみたいなものですね。あまり他の言語に詳しいわけではないのですが、これらの言語はなんとなくPythonに似ているような気がします。例えばvar、funのように基本文法で略語をよく使っていたり、型の指定はコロンの後につけたり、セミコロンがなかったり、inやRange、isがあるなどの共通点があったりしますので。他には、言語の使用ではないけど、一般的にはGetter/Setterを使わずフィールドに直接アクセスするところですかね。(おかげでLombokを使う必要がないのも便利ではあります)\nただ、そんなモダンな感覚でありながらも、やはりKotlinはJavaよりな感覚ではあります。厳格なJavaをよりゆるくしただけの感覚といえばいいでしょうか。例えばPythonだとelifなのですが、Kotlinではelse ifだったりしますので。JVM言語という理由だけでなく、基本文法からしてもJavaプログラマーならすぐに適応できる言語でもあります。例えばforループにラベルをつけることができたりします。\nあえてJavaとの比較をするとしたら、やはり冗長さを省けただけでなく、Javaという言語のデザインを根本的に変えようとしている気もしていました。例えばNullや、Mutableを扱う方式がそうです。Kotlinでは基本的に変数はNullになれなくて、Nullになれるオブジェクトは最初からそうであると宣言する必要があり、Nullになれるオブジェクトを扱う時もSafe Callを強制することでNullに対してはコンパイラレベルではできるだけサポートしている感覚です(おそらくこれはモダンな言語だと全部がそうですが)。そしてCollectionなどを宣言する時も、あえてMutableという宣言をしない限りは基本的にImmutableなオブジェクトが生成されます。これだけでもJavaでもっともよく見つかるNPEをよく避けられる気がしてしまいます。いちいち宣言して、コールもクエスチョンマークをつける必要があるのは面倒臭いことな気もしますが、コンパイルエラーの方がランタイムエラーよりはずっとマシだというのは我々みんなが知っていることではないでしょうか。\nあとは、個人的にPythonで使ってみてよかったなと思った機能がKotlinにもあってよかったです。例えばMultiple Return(複数の戻り値)だったり、Named argument(名前付き引数)があります。前者は特に、Pair/Tripleという型で明確な戻り値を提示できるのが素晴らしいと思います。こういうところはモダンながらも、Javaの持つ安定性もしくは丈夫さを捨ててなかったなという印象を与えてくれました。\nただ、これらのメリットは最近のJavaもかなり近づいている状態ではあります。(まだ少し遅い感はありますが)\nクラス=ファイルではない Javaの場合は、一つのファイルには一つのクラスというのが常識のようになっています。もちろんInner Classを書く場合もありますが、それだと名前とおりクラスの中に含まれたものになるので、インスタンスを生成するときに複雑だったりしますね。でもKotlinだと、純粋なクラスを一つのファイルに複数書くことができます。\nなので、似たようなクラスを一つのファイルの中に集めておくことができますね。例えばDTO、DAO、Entityなど似たようなクラスが複数損際するパターンでは、一つのファイルの中にそれらを集めておいた方がパッケージの中が複雑にならないような気がします。実際、Kotlinを試しながら好みの領域の話かもしれませんが。\nどちらかを選択できる自由があるというのが、必ずしも良いこととは言い切れませんが、ファイル内にクラスを複数書くかどうかはキメの問題であって実装時のコーディングスタイルに影響を与えるものではないので(今時importを直接書く人もいないだろうし…)、良い点として挙げられるのではないか、と思います。\n拡張関数で自由に関数を追加できる Javaのデメリットとしてよく挙げられているのが、冗長すぎる(verbose)ということです。いわゆるBoilerplateなコードを毎回書かなくてはならないというのは、生産性の面からもよくないです。Javaにこういう面があるので、さまざまなデザインパターンが発達したり、IDEでコードを自動生成してくれたり、Lombokのようにコードの量を減らしてくれるライブラリが人気だったりしますね。自分が開発に参加したフレームワークの開発の案件も、結局は冗長化するコードを減らしたいという目的によるものでした。\nとにかく、Kotlinはこういう問題に対する反発ないしは反省から言語がデザインされているようにも見えます。最近のモダンな言語の特徴をコピーしただけでなく、Javaを改善させたいという強い意志が言語のデザインから感じ取れているような感覚でした。\nスタンダードライブラリがとにかく便利 拡張関数が便利な理由ともつながるようなことですが、Kotlinのスタンダードライブラリに存在する関数たちもまた同じ観点から便利といえます。例えば、すでに有名なのがいわゆるScope Functionsのlet、with、apply、run、alsoのような関数です。\nこれらはJavaだと別途ユーティリティクラスを作るか、プライペートなメソッドを定義するか、特定のクラスを継承してからオーバライドで関数を新しく定義するなどの方法で対応はできるものの、やはり手間がかかるのでやりたくはないものです。これをKotlinでは、より関数型的な方法で解決してくれます。例えばletの例を見ていきましょう。以下のようなdata classがあるとします。\ndata class Member(val region: String, val name: String) このdata classのインスタンスを一つ作成します。そうすると以下のようになるでしょう。\nvar john = Member(\u0026#34;Tokyo\u0026#34;, \u0026#34;John\u0026#34;) あとで、同じくMemberのインスタンスとしてjakeという変数を追加するとします。jakeは常にjohnと同じregionである必要があります。\nvar jake = Member(\u0026#34;Tokyo\u0026#34;, \u0026#34;Jake\u0026#34;) これをJavaの考え方で、コードを整理するとしたら以下のようになります。regionを同じインスタンスを使うようにすることですね。\nvar tokyo = \u0026#34;Tokyo\u0026#34; var john = Member(tokyo, \u0026#34;John\u0026#34;) var jake = Member(tokyo, \u0026#34;Jake\u0026#34;) これをletを使う場合のコードとして書くと、以下のようになります。\nvar jake = john.let { Member(it.region, \u0026#34;jake\u0026#34;) } 共通のregionを別途変数として宣言したくても、jakeのregionはjohnに指定したregionと同じ値となります。そしてある意味、こちらの方が「johnとjakeは同じregionを共有する」という意図がコードの中によく表れているのではないか、という気もします。今は簡単なフィールドを共有しているだけですが、変数の数が増えたり処理すべき項目が多くなった場合はいちいち定数を宣言するよりも、このような書き方の方がより優雅になるのではないか、と思います。そういう意味では、かなり洗練された方法を提供していますね。同じことをJavaでまねるとしたら…あまりやりたくなくなりそうです。\nこれはいまいち KotlinがJavaのさまざまな問題や不便さに注目し、それらの多くを解消してくれたのは事実ですが、果たして全てJavaより発展しているか、というとそうでもないような気もします。ただし、ここであげているKotlinの問題点ないしデメリットは、メリットと同様、個人的な見解なので参考までに。\nvarと型 モダンな言語から接した人なら、変数の宣言がvarだけで集結するのはメリットと言いたくなるかもしれません。実際Kotlinだけでなく、JavaScriptやC#など現代に使われる大体の言語はvarに対応していて、あのJavaすらも10からvarによる変数の表記を導入しています。また、Pythonのようにそもそもvarの宣言すらいらない言語があったりもしますね。varをつけることで変数であることが明確だという考え方から来てるのか、Javaとは違って関数もFirst Class Objectとして扱う言語としては関数と同様に表記したいからそうしてるのか、よくわかってないのですが、どっちかというと流行りのもののようです。\nこういう傾向だけを見ると、「変数は変数であることが分かればいい」というだけの話になっているようにも思えます。しかし、私はこのvarが果たして良いものであるかという疑問を持っています。Javaに慣れすぎていて、新しいのを受け入れられてない、もしくはvarの良さをわかってないだけかもしれませんが、とにかく「型指定で変数であることも、型もわかるからこっちの方が良くないか」と思ってしまいます。\nこう思うまたの理由は、モダンなプログラミング言語の中でもTypeScriptのように、あえて型指定ができるように既存の言語を変えようとする動きもあるからです。Pythonの場合も3.6から型を宣言できるようになっています。これ自体が、「変数は変数であることが分かればいい」から「変数の型もわかった方がいい」に変わっているように見えます。ただ、問題は最初から型指定で変数を指定する方法がなく、varしかない言語に型指定(型注釈)が付く場合です。varのメリットである短くかけるというところが、型指定をすることで台無しになります。\n例えば、Kotlinでのvarだけの宣言だと以下のようになりますね。\nvar a = \u0026#34;this is string\u0026#34; そしてvarに型を指定すると以下のようになります。\nvar b: String = \u0026#34;this is string\u0026#34; Javaの伝統的な書き方だと以下です。こちらの方が、むしろコードは短くなるし、変数であることも明確ではないでしょうか。\nString a = \u0026#34;this is string\u0026#34;; また、厳密にいうと変数ではないところではvarをつけないのは当たり前なのかもしれませんが、Javaだと変数でも戻り値でも引数でも型をつけてしまうのに対して、Kotlinではこれらにvarをつけるか型をつけるか方が省略できるかという場面がそれぞれ区別されてしまうので、これだけはJavaよりも厳格じゃないか？という気になったりもします。例えば以下のような例です。\n関数の引数の場合は、型の指定が必要です。\nfun getMember(request: ServerRequest): Mono\u0026lt;ServerResponse\u0026gt; = ok() .contentType(MediaType.APPLICATION_JSON) .body(repository.findById(request.pathVariable(id).toLong()).map { MemberDto(it.username, it.name) }) .switchIfEmpty(notFound().build()) そして関数の戻り値は、型推論により省略可能です。\nfun getMember(request: ServerRequest) = ok() .contentType(MediaType.APPLICATION_JSON) .body(repository.findById(request.pathVariable(id).toLong()).map { MemberDto(it.username, it.name) }) .switchIfEmpty(notFound().build()) しかし、これはあくまで関数をSingle Expressionで書いた時の話です。明示的にreturnを書く場合は戻り値を省略するとコンパイルエラーになります。例えば以下のような場合がそうです。\n// これはコンパイルエラー(戻り値はUnitとなってしまう) fun getMember(request: ServerRequest) { return ok() .contentType(MediaType.APPLICATION_JSON) .body(repository.findById(request.pathVariable(id).toLong()).map { MemberDto(it.username, it.name) }) .switchIfEmpty(notFound().build()) } また、data classの場合はフィールドにvalかvarをつける必要があります。しかし、一般的なクラスを宣言する場合は要りません。\n// data classではvalかvarが必要 data class MemberDto(val username: String, val name: String) // classなら必要ない class MemberEntity(username: String, name: String) 通常はコンパイルエラーになるので、慣れるまではKotlinのルールに従ってコードを見直すしかないのですが、Javaからの入門だとかなり混乱する部分です。(自分だけかもしれませんが…)\n依存関係 プロジェクトにKotlinを使う場合はスタンダードライブラリを追加して使う必要があります。ただ、ここでkotlin-stdlibだけを追加すると、Java 1.7以降に追加された一部の機能(AutoCloseableなど)を使えなくなります。なので、Java 1.7以降の機能を使いたい場合はkotlin-stdlib-jdk7かkotlin-stdlib-jdk8を依存関係に追加する必要があります。\n個人的にはOracleとGoogleの訴訟沙汰のようなことがあって、わざと著作権を避けるための独自のパッケージを作ったりしらからではないかなと思いましたが、実際はJava 9から導入されたModuleシステムに対応するための理由だそうです。なのでkotlin-stdlib-jre7がkotlin-stdlib-jdk7に、kotlin-stdlib-jre8がkotlin-stdlib-jdk8に代替されたらしいですね。\nとにかく、これらのスタンダードライブラリを使うには、MavenやGradleのような依存関係を管理するパッケージマネージャを使って一回だけ登録しておけばよく、そこまでめんどくさくはないことなのかもしれませんが、例えばkotlin-stdlib-jre8があったりするので、初めはどれを選べば良いか、どれが必要であるかを把握するのにも時間を使ってしまうことになるのでデメリットになるのではないかと思います。例えばkotlin-stdlib-jdk7がなくても、AutoCloseable以外のJDK 1.7の機能は使えたりするのですが、今から作るプロジェクトや既存のプロジェクトにAutoCloseableが使われるかどうかで依存関係をまた追加するかどうかを調べるのもかなり面倒くさそうです。\nそしてJDK7やJDK8対応のスタンダードライブラリが別途存在するということは、今後JavaがバージョンアップしたらまたJDK 17などの新しいスタンダードライブラリが追加される可能性があるということでもあるでしょう。7(1.7)と17はよく勘違いしそうだし…あと、JDK以外の依存関係のパッケージが色々あるので(kotlin-reflectなど)、プロジェクトの構成によってはKotlinの導入にはかなり慎重になる必要がありそうです。ある意味、KotlinがPost Javaとしてのポテンシャルは十分でありながらも、Androidアプリの作成以外にではあまり導入されてないのはこのような理由もあるのではないかという気もしています。\n最後に 簡単にPros・Consに分けて自分が感じたKotlinに対して書いてみました。実はまだ本格的な案件で触れてみたわけでもないので、Type aliasesやinline classのような良さそうな機能に触れてもないです。でもやはり、使えば使うほど魅力的な言語であるなと感じているところです。なので個人的な意見としては、すでにJavaを使っているところなら本格的にKotlinへの移行を考慮しても良いのでは、と思っています。Javaプログラマーなら慣れやすく、より生産性も高いながら、Javaとの100%の互換性も担保されているので…言語の完成度やJVM対応でありながらNative、JavaScriptとの連動も可能なのを見るとまさにPost Javaとして相応しい言語なのではないかと思うくらいです。(他のJVM言語には悪いですが…)\nそういう意味で皆さん、今からでもKotlinやりませんか！\n","date":"2020-10-25T00:00:00Z","image":"https://retheviper.github.io/images/kotlin.jpg","permalink":"https://retheviper.github.io/posts/kotlin-first-impression/","title":"JavaプログラマーがみたKotlin"},{"content":"入社してから自分で立てた目標は、年に1つ以上の資格を取ることです。それは自分が元文系出身であって、転職でエンジニアになったので他の人と比べエンジニアとしてのベースとなる知識が足りないからです。なので「何から勉強したら良いかわからない」場合も多く、資格の勉強をしていると少なくとも問われることに対しての勉強はできるので、資格の勉強を頼りにして自分のスキルアップを狙っています。\nそんな中、今回はAWS認定の資格を受験しました。AWS認定資格の種類は以下の画像の通りですが、この中で自分が受験したのはデベロッパー - アソシエイトです。仕事でAWSの基本的なサービス(EC2とS3など)に触れたことがあり、AzureでもFunctionsとQueueによる簡単なサーバレスアプリを作ってみたり、Bastionなどのサービスを使ってみた経験があったので悩まずデベロッパーを選択。なんとか受かりましたが、少し使ってみただけで気安く挑戦するような資格ではなかったと実感しました…\n出典：AWS公式サイト - AWS認定\nともかく、前回のJava資格の時でもそうであったように、今回も自分の観点からテストはどうだったかについて簡単に述べていきたいと思います。\n試験はどんな感じか 問題で問われる知識の内容や範囲(AWSのサービスの種類)の側面でなく、テストそのものはどうかというと、学生時代の歴史の科目のような感覚のテストでした。もちろん、テストとは受験者の持っている知識の確認が基本なのであって、AWSのサービスの仕様や特徴などを問われるだろうと推測するのは自然なことだと思います。しかし、実際のテストで問われるのは「このようなアプリを開発するためにはどんなサービスを使って、どのような構成にするべきか？」といった物よりも、「このような要件で、入力すべきCLIのコマンドは何か？」、「この状況下のこのサービスで、〇〇を設定するために使うオプションは何か？」のような、暗記の問題が多い感覚です。例えば、特定のサービスにとある設定をするために使うファイルの「ファイル名」を選ぶような問題があったりします。\nAWSには本当に多様なサービスがあって、それぞれの役割や領域が重なる部分もあるので、サービスの仕様を正確に把握していない場合は設計も開発も難しくなるので、自然に試験もこのような形になるのかも、とは思っています。特に資格のレベルが上がり、アプリケーションの設計という面で考えると、開発の効率やコストの問題も考えなくてはならないので、各サービスの仕様の詳細までを知っておく必要があるでしょう。\nただ、このようなクラウドサービスの発達により、インフラもコードで書いて構築する時代となっているし、資格も「デベロッパー」なのに、より開発者よりな形の試験にするのはできなかったのかな？と思ったりもしました。例えばLambdaについての問題だとすると、頻繁なアクセスがある場合と処理量の多い場合はそれぞれどの言語で書いた方がいいか、を聞いたりしたらよかったのではないかなと思いました。\nどうやって準備したか 自分の場合はアソシエイトの3つの資格の概要をまとめた本を購入しましたが、あくまでAWSのサービスを紹介するものでしかなかったので、Udemyで5回分の模擬試験の講座を購入しました。ただ、こちらもデベロッパーをターゲットとしてるよりは、プロフェッショナルレベルの資格の内容も含めているような印象でした。どちらかというとUdemyの模擬試験の方が難しかったと思います。他にもUdemyの模擬試験も複数ありますので、どれを選ぶかによってまた実際のテストで受ける印象は変わってくるのではないかと思います。\n個人的には他の資格もある程度問われる知識に関しては共有する部分が多いと思うので、難易度の高い模擬試験の講座を購入し、繰り返して解いてみることがもっとも効率的ではないかと思います。そうすると他の資格を受験するときに役立つ可能性もあるでしょう。他にもAWSのオンライン動画講座を受講したり(無料ですが、英語版しかないです)、公式のドキュメントを参照したりしていました。AWSの公式のトレーニングコースで動画やPDFの資料を提供していますので、目を通しておくのをオススメします。\nまた、プラクティショナから取得されている方にはあまり当てはまらないことなのかもしれませんが、AWSのサービスは略称で呼ばわれることが多く、名称だけではどんなサービスなのか推測が難しいので、少なくとも模擬試験で言及されるサービスに対しては自分の辞書を作っておくのをオススメします。EBSとEFSはストレージなのに、ECSはコンテナサービスだったりするので…(他にも、SQSとSNSはメッセージングなのにSTSはセキュリティ関連の物だったりします)\nとにかく「覚える」ことがまず大事な資格だと思います。\n何を問われるか それでは、次は試験の内容です。AWS公式サイトのデベロッパーの紹介ページを見ると、推奨される知識と経験としていくつか挙げられている項目がありますが、これはあくまで大まかな概念の話であって、実際のテストで具体的に何を問われるかは提示されていません。サンプル問題があるのでそちらを参照することはできますがね。自分の場合は模擬試験を通じて勉強はしたものの、模擬試験のうちどれがデベロッパーに当てはまるかは分かりませんでしたので、AWの公式の模擬試験を購入するのもありかなと思いました。\nとにかく、実際のテストでよく問われれる(デベロッパーとして重要視される)と感じたのは以下のような物です。\nサーバレスアーキテクチャ もっとも多かった問題のパターンは、サーバレスアーキテクチャでの開発に関するものです。つまり、AWS LambdaとAmazon DynamoDB、もしくはAmazon Kinesisの組み合わせでアプリを構築する場合を提示して、そこで注意すべきことや、障害が起きた場合の対処、各種設定方などを問われます。単純にサーバレスがどんな物であるか、を理解しているだけでは十分ではないと思います。なので以上の三つに関しては、詳細まで勉強する必要があります。Lambdaでの依存関係の設定はどうするか、スケーリングを考慮したシャードの設定方法、RCUとWCUのスペックなどを覚えておくといいでしょう。\nまた、サーバレス開発をする場合だとしても、エンドポイントやセキュリティ周りのことは一般的なアプリケーション開発(サーバあり)と変わらないので、一般的な開発のパターンで考えるべきところの問題もありました。例えば、Amazon API Gatewayの設定やS3、CLIコマンドの使い方などがあります。\nセキュリティ セキュリティに関する問題もあります。主にIAMでのポリシー、ロール、グループの設定や、KMSを使った暗号化キーのマネージメント、などがあります。Lambdaについての問題があるので、もちろんLambdaオーソライザーに関しての問題もありました。\nログ・監視 アプリケーションに障害が起きた場合での追跡方法や、監視したい要件でどのような設定をすべきかを問われます。例えばX-Rayデーモンの設定方法や、CloudWatchとCloudTrailの違いが何かなどをとう問題があります。\nCode兄弟 CI/CD関連で問われる問題です。CodePipeline、CodeDeploy、CodeBuild、CodeStarに関して問われますが、それぞれの役割をちゃんと理解していれば難しくはないです。\nただ、機能的にElastic Beanstalkや、CloudFormationと重なる部分があり、デプロイでどれを使うかを選ぶような問題もあったような気がします。これらの違いはちゃんと覚えておく必要があります。\n自分の場合は、以下のイメージを見てだいぶ理解ができたと思います。\n出典：AWS公式サイトトレーニングコース Exam Readiness: AWS Certified Developer - Associate (Digital)\n以上の内容は、AWS公式の動画でよく説明してくれているので、一回は受講することをオススメします。\n参考にすると良い情報 他に、ちょっと覚えておくと良さげなものは以下の通りです。\n名前だけは知っておきたいサービス テストで深くは問われないですが、名前だけ(そして何をするものか、大まかなイメージだけ)知っておくといいものとしては、OpsWorks、Step Functions、Systems Managerのパラメータストアがあります。ただ、こちらまで覚えるのは流石に余裕がないと思います。他も覚えるべきものは山ほどあるので…\nAWS Elastic Load Balancer ELBについての問題も出ますが、最初はいまいちALB、NLB、CLBのことがわからず苦労をしていたので、以下のことを覚えておくと良いと思います。ELBはクラウドプラクティショナでも問われるものなので、私みたいにAWSの資格が初めてだったり、ELBのことが全くわからない場合に覚えておくといいでしょう。\n特徴 ALB NLB CLB 対応Protocol HTTP, HTTPS TCP, TLS TCP, SSL/TLS, HTTP, HTTPS プラットフォーム VPC VPC EC2-Classic, VPC Health-Check 対応 対応 対応 CloudWatchマトリックス 対応 対応 対応 ロギング 対応 対応 対応 Zonal Fail-Over 対応 対応 対応 Connection Draining 対応 対応 対応 同じインスタンスの複数ポートに負荷分散 対応 対応 - WebSockets 対応 対応 - ターゲット(IPアドレス) 対応 対応 - ターゲット(Lambda) 対応 - - ロードバランサー削除保護 対応 対応 - パスベースルーティング 対応 - - ホストベースルーティング 対応 - - Native HTTP/2 対応 - - Idle Connection Timeout設定 対応 対応 - Zond間の負荷分散 対応 対応 対応 SSLオフロード 対応 対応 対応 SNI(Server Name Indication) 対応 - - Sticky Session 対応 - 対応 バックエンドサーバ暗号化 対応 対応 対応 固定IP - 対応 - Elastic IPアドレス - 対応 - 送信元IPアドレスを保持 - 対応 - リソースベースIAMアクセス許可 対応 対応 対応 タグベースのIAM権限 対応 対応 - スロースタート 対応 - - ユーザ認証 対応 - - リダイレクト 対応 - - 固定応答 対応 - - 受かったあとは 試験に受かったら、しばらくしていくつかの特典をもらえるよ、というお知らせのメールが届きます。詳細はAWS認定のサイトからも確認できる内容ですが、簡単に紹介しますと以下の特典が提供されます。\nAWS認定ロゴのついたグッズを購入できる AWS認定資格の模擬試験を一つ購入できるクーポンをもらえる LinkedInのAWSコミュニティに入れる 次の試験が半額になるクーポンをもらえる イベントがあったら通知をもらえる Acclaimに登録できるデジタルバッジをもらえる 正直デジタルバッジと模擬試験のクーポンがもらえる以外、使うところがあるかどうかは少々疑問になるライナップではあります。ただ、自腹の場合は確かに次の試験が半額になるのはメリットになるでしょう。自分の場合は会社負担のなっているので、半額でも特にうれしくはないのですが…\n最後に それで、受験する価値はあったのか？というと、それはあったと思います。暗記に弱く、あまり好きではないので苦労したのは事実ですが、おかげでAWSというサービスをより深く理解することができたと思っています。違うベンダーのクラウドだとしても、やはり大まかなサービスのコンセプトはやはりAWSと変わらないものなので、AzureやGCPなどを使う場合でも十分役立つのではないかと思います。実際、AWSを使うことで得られた知識でオラクルクラウドのVMを有効活用しているところです。\nまた、クラウドごとに無料Tierで提供されるサービスはそれぞれ違うのですが、AWSの場合はLambdaとDynamoDBが無料で使えます。なので、この資格を勉強しならが得られた知識で何らかのサーバレスアプリケーションを作ることも可能になったような感覚です。オラクルクラウドとGCPでは無料で使えるVMを提供しているので、この組み合わせだと無料プランだけで立派なサービスを立ち上げることも不可能ではないような気がしますね。おそらく、こういうのも資格に挑戦してなかったら得られなかった知識だっただろうと思っています。\n一つ悩ましいのは、AWSの資格は3年という有効期限があるということです。クラウドは常に変化するサービスであり、続々と新しいサービスが登場するものなので仕方ないものですが、資格を持ってどこに活用するか(例えば、就職や転職に使うなど)はよく考える必要がありそうな気がします。あとは、、他の資格とも共有する部分が多いと思うので、連続して違う資格を早めに取っておいたほうが良さげな感じですね。なので自分もいつかまた、ソリューションアーキテクトなどに挑戦したいと思っています。\nま、しばらくは疲れたので全然挑戦する気にならないのですが…\n","date":"2020-10-04T00:00:00Z","image":"https://retheviper.github.io/images/aws.jpg","permalink":"https://retheviper.github.io/posts/aws-certification-associate-developer/","title":"AWS認定デベロッパー - アソシエイトについて"},{"content":"MDNの説明によると、HTTPメソッドは主に「リソースに対する」ものだと定義しています。なので書くメソッドの説明としても、GETならリソースの表現、POSTならリソースの作成、PUTならリソースの置き換え、DELETEならリソースの削除のような表現をしていますね。これを元に、CRUDの基本を説明するような資料も多く、学習者は自然にAPIのデザインはリソースの作成、参照、置き換え、削除を基準に考えることになります。あとはリクエストに対する処理が冪等であるかどうか、というのが重要ですね。\n理論 != 現実 しかし、理論的にはそうだと認識していても、実際の業務をアプリケーションとして表現していくと、悩ましい場合があります。今回がそうです。「リソースを単純に返すだけに見えるけど、実際は内部的にリソースの作成や修正が伴う処理の場合はどうするか」という疑問がありました。なぜなら、このような要件があったからです。\nクライアントにとあるファイルのダウンロードを提供するAPIがある。 サーバからのファイルの取得には以下の二つのケースがある。 ファイルがすでに作成されていて、サーバはそのファイルをクライアントに返すだけ。 リクエストに応じて、サーバがDBのレコードをファイルに書き込み(作成し)クライアントに返す。 クライアントがファイルをダウンロードすると、サーバはDBを更新する。 サーバはDBに「ファイル出力済み」というフラグと、「更新ユーザ」としてログインユーザの情報を登録する。 「ファイルがすでに作成されていて、サーバはそのファイルをクライアントに返すだけ」だと、GETでことは済みます。クライアントの観点からしてもそうであって、サーバもすでにあるリソースを返すだけですね。これは一般的にGETに期待する、「リソースの表現」に合致していて、何の問題もないです。\nしかし、「リクエストに応じて、サーバがDBのレコードをファイルに書き込みクライアントに返す」の場合はどうでしょうか？ファイルを新しく作成するのは、「リソースの作成」に当てはまるのでしょうか？それとも、レコードとしてすでに存在しているので、ただ違う形に加工するだけであって「リソースの表現」に当てはまるのでしょうか。または、ファイルの要請だけでなく、クライアントが提供した情報を持ってDBの更新を行うので、「リソースの作成」に当てはまるのでしょうか。\n誰の観点から見るべきか 以上の問題は、自分の観点、つまり、バックエンドエンジニア及びアプリケーションとしての問題でもあります。ならば、クライアント(ユーザ)からしてみるとどうでしょうか。以下のことを考えられます。\nクライアントはファイルがすでに作成されているか、作成してくれるかに興味がない。 リクエストは「ファイルをくれ」であり、「ファイルを作ってくれ」ではない。 クライアントのリクエストに合わせてファイルを作成するかしないかはサーバ(アプリケーション)の都合である。 クライアントはファイルを要請するだけであり、中の処理についてはわからない。 つまり、クライアント側からすると、ファイルをダウンロードするという行為に関しては、あくまで「リソースの表現」を要請することに過ぎなくなります。サーバサイドとはまた都合が違いますね。こういう場合は、どちらの都合に合わせるかは明白ではないでしょうか。優先すべきは、クライアントですね。なぜなら、アプリケーションはそもそもクライアントの要件のために存在するものであるからです。なので、サーバサイドの都合でリクエストをPOSTにする理由はなくなりますね。\nSpecでは ただ、クライアント側の観点枯らして、こういう場合はGETが正しい、と決めつけるにはまた違う根拠がいるのではないでしょうか。例えばHTTPの標準とかです。ちょうど、HTTPのスペックに関する文には、以下のような章があります。\n4.2.1. Safe Methods\nRequest methods are considered \u0026ldquo;safe\u0026rdquo; if their defined semantics are essentially read-only; i.e., the client does not request, and does not expect, any state change on the origin server as a result of applying a safe method to a target resource. Likewise, reasonable use of a safe method is not expected to cause any harm, loss of property, or unusual burden on the origin server.\n4.2.1 安全なメソッド\nクライアントがサーバの状態を変えない、変わることを期待しない読み込み専用のリクエストは、「安全」とされる。安全なメソッドを正しく使うことによりサーバにとって害や損となるような事情を発生させなくなる。\nこれを見ると、クライアントのリクエストによりサーバに何かリソースの変化をもたらすようなことになる場合は、GETではなさそうが気がします。しかし、重要なのはこの次の内容です。\nThis definition of safe methods does not prevent an implementation from including behavior that is potentially harmful, that is not entirely read-only, or that causes side effects while invoking a safe method. What is important, however, is that the client did not request that additional behavior and cannot be held accountable for it. For example, most servers append request information to access log files at the completion of every response, regardless of the method, and that is considered safe even though the log storage might become full and crash the server.\n安全なメソッドが、完璧な読み込み専用でなかったり、副作用を伴うなどで害を招くような実装を防止するわけではない。大事なのは、その動作はクライアントの要請ではなく、責任もまたクライアントにないということである。例えば、多くのサーバはどんなメソッドに対してもアクセスログを記録するが、ログによってサーバのストレージが足りなくなりサーバに障害が生じることもあり得るが、こういう場合でもリクエストは安全とする。\nつまり、大事なのは実装上の都合はあくまでサーバサイドの責任であって、クライアントからの行為であるということですね。安全なリクエストのためには、クライアントのリクエストがどのようなものであるかを考えるべき、という結論になります。HTTPメソッドはそういう行為の本質を表すものであり、サーバの都合の表現ではない、ということですね。なのであくまでクライアントからして、「リソースの表現」に対する要請であれば、サーバはGETとして応じるベキになるかと思います。\n最後に 実際のウェブサイトを観察してみても、純粋な「リソースの表現」としてのGETはあまりないような気もしています。例えば訪問するだけで「訪問者カウンター」が更新されるサイトがあったりしますね。こういう場合でもクライアントはリソースの作成やリソースを要請してなく、サーバ側で勝手に更新を行っていますが、依然としてGETとされまていて、違和感もないです。むしろこのようなリソースの変更全てをPOSTとして扱うようになるとしたら、GETを使う場面自体がなくなるかもしれません。\nただし、注意しなくてはならないのが、リソースの作成や更新を伴うリクエストを全てGETにするという発想も危険であることです。リソースの扱い方という原則に戻って考えたら、「POSTで作成させた後、GETで取得させる」というやり型もあるのかもしれません。こういう場合は2回のトランザクションが発生するので性能面では劣るかもしれませんが、場合によってはより安全な設計になる可能性もありますね。\nなので「全てのケースに置いて」ではなく、アプリケーション全体の設計や今後の方向性を考慮した設計として適切なHTTPメソッドを選んだ方が良い、というのが結論になるかと思います(原論的な話ですが)。重要なのは、「こういう場合は絶対GETだ」、「こういう場合は絶対POST」だと言わないことですね。\n","date":"2020-09-21T00:00:00Z","image":"https://retheviper.github.io/images/magic.jpg","permalink":"https://retheviper.github.io/posts/get-or-post/","title":"GETとPOSTの間"},{"content":"Springが初めて発表されたのが2002年なので、およそ20年に近い時間が経ちました。今はJavaと言えば当たり前のようにSpringを使っていて、Spring MVCやMaven、propertiesのような煩雑な環境構築と初期設定の問題も、Spring Bootが登場したおかげでだいぶマシになりましたね。特に自分の場合がそうですが、Spring Boot、Gradle、yamlを使ってXMLは一つもないアプリをよく書いていて、なんでも楽と思います。\nこうも発展を成し遂げているSpringですが、実は数年前から、そもそものSpring MVCの問題を改善したいという声があり、Spring 5.0からはMVCとは全く違う、新しいフレームワークが登場していました。それが今回紹介します、Spring WebFluxです。\nSpring WebFluxはMVCと何が違うか 最初から作り直したフレームワークなので、根本的な部分から違うところが多いので、理論的なだと以下のキーワードをあげられますでしょう。\n非同期、ノンブロッキング、反応型 私の(コードモンキーの)レベルからしたら、実際のコードのレベルで体感するMVCとの違いは以下です。\nMonoとFlux Controller/Serviceの代わりにFunction TomcatよりNetty JPA/JDBCよりR2DBC 新しい抽象化クラス では、これらの違いについて、一つ一つみていきたいと思います。\n理論的な話 非同期、ノンブロッキング、反応型 Spring WebFluxはProject Reactorに基づいて作られていて、その根幹となる考え方はReactive Streamだそうです。Reactive Streamは標準APIであり、Java 9からjava.util.concurrent.Flowとして導入されています。\nReactive Streamはデザインパターン的にはObserverと似ています。簡単にいうと、何かのイベントが発生した時に「通知してくれ」と頼んで、データをもらうということです。この「通知してくれ」と頼む行為のことをSubscription(購読)といい、データを発行するPublisherと購読するSubscriberの間でSubscriptionのやり取りで行われます。こういうイベント基盤のプログラムを作ることが、いわゆる「反応型」だそうです。\nそしてReactive StreamではこのSubscriptionのやり取りが、非同期・ノンブロッキングで行われるらしいです。ということは、コードが実行された時点か終わるまでまつ必要がなく、その間に他のことができるということです。なので同じ数の同じタスクを実行するときは同期・ブロッキングと比べあまり性能面での違いはないのですが1、スレッド数がボトルネックとなる場合だと、非同期・ノンブロキングの方が早くなります。\n理論的な話は、深くまで踏み入るとキリが無くなるので、実際のコードではどんな違いがあるのか？をみていきましょう。\nコードのレベルでの話 MonoとFlux Spring WebFluxだと、コントローラのメソッドに戻り値(レスポンス)としてMonoとFluxを使うということです。Spirng MVCなら、文字列でJSPファイルを指定したり、REST APIならJSONとして返すオブジェクトを指定していましたね。もちろんMonoとFluxもJSONオブジェクトとして出力されるのですが、作り方が少し独特です。\nSpring WebFluxの根幹となる考え方がReactive Streamであると先に述べましたね。そしてReactive StreamをWebFlux側で実装したものが、MonoとFluxになります。Monoは0か1か、Fluxは0かNかの違いだそうですが、必ずしもCollection=Fluxにする必要はなくて、Monoとして扱うこともできます。\nReactive Streamは、名前からしてJava 1.8のStream APIとなんらかの関係があるようにも見えます。実際、データの作成と消費の時点が違うのですが2、似たような名のメソッドや、Lambdaで完成していくところは似ています。すでにRxJavaやJOOLなどに慣れているなら、書き方的にはまあり問題なく適応できそうですが、そうでない方には適応が難しいかもしれません。\n例えば、GETで、リクエストを受けたら1秒後にレスポンスを返す簡単なメソッドを実装するとしましょう。Spring MVCによるREST APIだと、以下のようになるでしょう。\n@GetMapping @ResponseStatus(HttpStatus.OK) public String getTest() { try { Thread.sleep(1000); return \u0026#34;task completed by mvc\u0026#34;; } catch (InterruptedException e) { return \u0026#34;task failed by mvc\u0026#34;; } } WebFluxでは、以下のようにMonoを作成して返します。\n@GetMapping @ResponseStatus(HttpStatus.OK) public Mono\u0026lt;String\u0026gt; getTest() { return Mono.delay(Duration.ofMillis(1000)).then(Mono.just(\u0026#34;task completed by Mono\u0026#34;)); } 最近はなんでも宣言的な言語やフレームワークなどが増えているので(例えばFlutterやSwiftUIがそんな感じですね)、こういう書き方は珍しくもないですが、伝統的な命令型プログラミングに慣れている方には少し辛い書き方になるかもしれません。私自身も、StreamやLambdaは好きなものの、ネストしていく命令型とメソッドチェインで長くなる宣言型のどちらがいいか確信がないです…\nController/Serviceの代わりにFunction WebFluxのコード上でのもう一つの特徴は、ControllerとServiceの代替となるクラスを作ることができるということです。もちろん従来通りControllerとServiceクラスを利用することもできますが、どうせなら新しいものが使ってみたくなりますね。\nSpringでController/Serviceを作るということは、つまりアノテーションによる「メタプログラミング」に依存して開発するというです。確かにアノテーションは便利であって、Springではアノテーションでなんでもできるような感じもしますね。しかし、アノテーションによる開発では以下のような問題があります。\nコンパイラで検証できない コードの行為を規定しない 継承、拡張のルールに標準がない 理解できない、誤解しやすいコードをを生み出す可能性がある テストが極めて難しい カスタマイズが難しい なぜかというと、アノテーションを使うということは結局Reflectionに依存するということになるからです。Reflectionを使っているとランタイムでバイトコードが生成されてしまうので、コンパイルタイムにできることがあまりないですね。Reflectionは確かに強力な道具ですが、他にも問題はあります。例えばパフォーマンスは低下し、デバッグも難しいです。こういう問題があるのでJavaのコードをネイティブにコンパイルしてくれるというGraalVMではReflectionに対応していないのかもしれないですね。\nとにかく、このような問題を解決するためにWebFluxで新しく導入されたのは、Functionです。はい、言葉通り、関数です。既存のControllerに対応するRouterとServiceに対応するHandlerを作り、関数型モデルとして(Functinalに)コードを書くことができます。もちろんFunctionalに書くとしても、アノテーションは使えます(むしろアノテーションなしではだめです…)。例えば以下のような書き方になります。\n@Configuration public class Router { @Bean public RouterFunction\u0026lt;ServerResponse\u0026gt; route(final Handler handler) { return nest( path(\u0026#34;/api/v1/web/members\u0026#34;), RouterFunctions.route(GET(\u0026#34;/\u0026#34;), handler::listMember) .andRoute(POST(\u0026#34;/\u0026#34;), handler::createMember) ); } } @Component public class Handler { private final MemberRepository repository; @Autowired public Handler(final MemberRepository repository) { this.repository = repository; } public Mono\u0026lt;ServerResponse\u0026gt; listMember(final ServerRequest request) { return ok() .contentType(MediaType.APPLICATION_JSON) .body(Flux.fromIterable(repository.findAll()), Member.class); } public Mono\u0026lt;ServerResponse\u0026gt; createMember(final ServerRequest request) { return ok() .contentType(MediaType.APPLICATION_JSON) .body(request.bodyToMono(Member.class) .flatMap(member -\u0026gt; Mono.fromCallable(() -\u0026gt; repository.save(member)) .then(Mono.just(member))), Member.class); } } 関数型になってわかりやすくなったような、難しくなったような…\nTomcatよりNetty Spring WebFluxのデフォルトのアプリケーションサーバはNettyです。簡単に推論できる理由ですが、Nettyの方が最初からノンブロッキングという考え方に基づいて作られているからでしょう。Tomcatはもちろん同期、ブロッキングなので、Nettyと比較すると以下のような違いがあるらしいです。\nTomcat：リクエストとスレッドは1:1 Netty：リクエストとスレッドはN:1 もちろん、Spring MVCみたいにNettyの代わりにTomcatを使うこともできます。例えばGradleでは以下のような書きます。\ndependencies { implementation(\u0026#39;org.springframework.boot:spring-boot-starter-webflux:2.3.3.RELEASE\u0026#39;) { exclude group: \u0026#39;org.springframework.boot\u0026#39;, module: \u0026#39;spring-boot-starter-reactor-netty\u0026#39; } implementation(\u0026#39;org.springframework.boot:spring-boot-starter-tomcat:2.3.3.RELEASE\u0026#39;) } JPA/JDBCよりR2DBC これもアプリケーションサーバと同じような話です。JPA/JDBCなど、従来のORMはブロッキングなのでノンブロッキングに対応するR2DBCに変えましょう、ということです。NIOがそうであったように、ブロッキングでもR2DBCを使うと場合によってはパフォーマンスの向上を図ることもできるらしいです。\n新しい抽象化クラス WebFluxでは、Spring MVCで使っていた抽象化クラスも変わっています。これも同じく、関数型としての書き方とノンブロッキングに対応するためですね。\n種別 Spring MVC Spring WebFlux リクエスト HttpServletRequest ServerHttpRequest レスポンス HttpServletResponse ServerHttpResponse 他のAPIをコール RestTemplate WebClient WebClientの場合は、RestTemplateがdeprecatedに変更されるので、Spring MVCを使う場合でも導入を考える必要はあります。実際Spring MVCで使う場合でもResttemplateに比べてパフォーマンスが向上される場合もあると言われています。\nどこに使えばいい？ さて、さっくりWebFluxの特徴をみてみましたが、どうでしょうか。書き方もかなり違っていて、Servlet基盤のMVCとは全く違うReactorに基盤しているので、Spring WebFluxの導入はかなり悩ましいことです。実際Spring MVCと同時に使うこともできず(無理矢理Dependencyを追加しても、MVCが優先され、WebFluxの機能は動作しなかったりもします)、Spring Securityなど他のフレームワークもSpring WebFluxのために作りなおったものに変えなくては行けないので、既存のシステムやSpring MVCに基づいて整備したライブラリなどがたくさんある場合はその影響範囲が測定できません。\nそして性能面でも、ノンブロッキングが強いのは、「指定されたスレッド数を超えるリクエストがある場合」という条件下の話です。ノンブロッキングに変えたからって、単一スレッドでの性能が上がるわけでもないということですね。3\nただし、以下のような場合はWebFluxの導入を考えられます。\n完全新規サービスをはなから作る 複数のサービスがあり、サービス同士での呼び出しが多い場合(マイクロサービス) BFF4の場合 最後に 簡単に整理しようとしていた内容が、いつの間にかかなり長くなりましたが…おかげで色々と勉強はできたと思います。WebFluxが登場してからもすでに数年が経っていて、RestTemplateがdeprecatedになる予定であるように、究極的には全体をWebFlux基盤に移行する必要が生じる日がいつか来るかもしれません。なんでも最近は非同期、関数型、反応型というキーワードがすごく流行っていますし。\n静的タイプの言語が最初に生まれて、動的タイプの言語も生まれ、TypeScriptのように静的な世界にまた戻るような現象が起きているのを見ると、また関数型から命令型に移行する日もいつか来る可能性があるのかな、と思ったりもします。ただ、こういうパラダイムはどれが絶対というわけではないので、優秀なプログラマならどれも適時適切に使いこなせるようにならないと、という気もします。エンジニアとしての勉強の道は本当に終わりがないですね。\n実際は、スレッド数によるボトルネックのない状態だと、関数型の方が少し遅いらしいです。実際は関数型のAPIの実装の方が複雑だからですね。ただ、この違いはコードの読みやすさや実装のしやすさなどを考慮した時は、十分トレードオフとして考慮できるくらいの差のようです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStreamは同期なので、データを生産と消費が一緒に行われます。しかし、Reactive Streamではデータの生産がすぐ消費までつながるとは言い切れません。非同期だからです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nむしろ、単一スレッドでの処理は、WebFluxの方がMVCに比べ少し遅いという話もあります。forループに比べStreamが遅い理由と似ているような気がしますが…\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBack-end For Front-endの略。マイクロサービスの一種で、複数のエンドポイントをまとめて固有のオブジェクトを生み出すバックエンド。フロントエンドは一つのエンドポイントを呼び出すだけでことが済みます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-09-06T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-webflux/","title":"Spring WebFluxって何？"},{"content":"今更な話ですが、主にバックエンドの仕事をしてきたエンジニアとしては(特に、Springが全てを占拠してしまったJavaがメインな立場からは)、昨今のフロントエンドの世界を見ていると、一瞬よそ見しただけでもすぐに時代遅れな人間になってしまうような気になります。それだけさまざまなライブラリ、フレームワーク、設計思想、プラットフォームが現れ消えていくので、これからどれを学んでおくべきかわからなくなります。\n例えばアプリの設計思想だと、SPA1がトレンドになっているものの、WASMの登場でPWA2も登場し始めています。JavaScriptのフレームワークでは、「Virtual DOMを使用するので早い」という、Angular・React・Vue.jsが今の通称「3強」となっていますが、「DOMを直接更新するから早い」というSvelteもあります。Node.jsは失敗したというDenoが登場しているし、今のTypeScriptの位置を目指したが失敗したと思われたDartは、FlutterやFuchsiaで再起できているようにも見えますね。\nこのように、フロントエンド系は新しい概念と技術の氾濫で混乱な状況ですが、面白くも、トレンドと逆行するような形のWebアプリ、いや、Webサイトの構築に関する概念もそれなりの割合を示しているように見えます。本日のポストのテーマである、「静的ウェブページ」です。\n静的ウェブページ(Static Web Page)とは 「静的」であるということは、「動的」と区分される特徴があるということでしょう。ここでいう動的か静的かというのは、入力によって何かの処理が行われるか、もしくは常にサーバに保存されたデータをそのまま返すかががもっとも重要な基準となっているようです。例えばWikipediaでは以下のように定義しています。\nA static web page (sometimes called a flat page or a stationary page) is a web page that is delivered to the user\u0026rsquo;s web browser exactly as stored, in contrast to dynamic web pages which are generated by a web application.\n言い方を変えると、サイトが「アプリケーション」として作られたものならば動的、もしくはただの固定されたデータを表示するための「ページ」であるなら静的といえますね。\nなぜ今更？ サーバに保存されたデータをそのまま返すという概念は、 Webアプリケーションが登場する前から存在していました。当時のウェブサイトはhtmlとcss、イメージなどのリソースをFTPなどでサーバにアップロードしてホスティングを行っていたのですが、これは今の静的ウェブページにとっても同じ方式です。\nつまり、概念だけを見ると静的ウェブページは昔に流行っていた「ホームページ」や「ブログ」などに近いものです。時代に逆行するような、静的ウェブページが今更注目されるのは、なぜか。\n当たり前ながら、当時とは違って、JavaScriptの発展によりサーバに依存しなくても簡単な処理はブラウザでできるようにもなり、適切なAPIがあればバックエンドを実装する必要がなくなるのもあり、AWS LambdaやAzure Functionsのようなサービスによりサーバレスができる環境ができるなど、バックエンドよりはフロントエンドの目的によって選択できる方法が多くなったという点があるでしょう。\nそして、もっとも根本的な、サイトの目的がありますね。ブログ、ポートフォリオ、イベント案内などそもそも「大した処理が必要ない」サイトに関しては、静的にしたほうがリソース使用、費用、パフォーマンス、セキュリティという面で動的ウェブページより有利です。例えば今このブログみたいにですね。\n最近はMarkdownなどで文書を書くと、それを元に静的ウェブページを作成してくれるツールもたくさんあり、そのようなツールで生成したデータを載せるだけでホスティングする方法も多いです。自分でソースを管理できて、簡単にホスティングできるのである意味ではもっとも静的ウェブページを作り安い時代となったとも言えますね。\n静的ウェブページを作られるツール 多くの場合、静的ウェブサイトの生成ツールの使用は難しくありません。ベースになっている言語の実行環境を構築して、テンプレートをダウンロードして適用するか、作っていくかなのですが、ツールを実行するための環境の構築はツールのホームページの手順通りでよく、テンプレートも人の作っておいたものを持ってきて少し修正するくらいです。簡単な修正なら、HTML、CSS、JSON、YAMLくらいのファイルをいじるだけなので(プロパティファイルで色々なオプションを選択できる例もあります)、難しくありません。あとはMarkdownなどで記事を書いていくだけですね。\nただ、自分の経験からだと、既製のテンプレートを大きく変えるか、最初から作り直す場合は自分が触ることのできる言語をベースにしておいた方が良いということですね。例えばこのブログはJekyllで作られていて、テンプレートも一部修正していますが、そもそも私はRubyに慣れてないので根本的な部分を修正してはいないところです。\nJekyll以外には、Nodejs基盤のツールが多いようです。例えばMetalsmithや、Hexoのような物があって、React基盤のGatsby、Vue.js基盤のVuePressもあります。JavaScriptとそのフレームワークの使用に慣れているなら、こちらがいいかもしれませんね。他には、Go言語基盤のHugoもありますので、Goに慣れている方ならこちらを選択することもありですね。\n静的ウェブページを載せられるプラットフォーム ツールを使って、静的サイトを作ったら、次はホスティングが必要となりますね。静的ウェブページのホスティングの場合、このブログでも使用しているGitHub Pagesやnetlifyのようなサービスがあり、データをGitなどでアップロードすると自動でビルドとデプロイが完了される便利さがあります。\n他にはクラウドを利用する方法があります。AWSやAzure、Google Cloud Platformのようなクラウドサービスには、S3やBlobのようにウェブからアクセスできるストレージサービスが必ずありますが、このストレージにツールで作成した静的サイトをアップロードしてホスティングする方法もあります。特にGoogleのサービスの場合、Google Firebaseというアプリ開発に特化したプラットフォームもあるのでAPIとの連携が必要な場合は考慮できる候補ではないかと思います。\n最後に ここでは単にツールとプラットフォームを紹介(というより、言及)しただけですが、静的ウェブページでできることや活用できるものは、他にも多いようです。同じ静的ウェブページだとしても、どのような技術の組み合わせでどのような物が作り上げられるかがかなり変わってくるのですが、実際にどんな物ができるかはJAMstackのExamplesタブから色々発見できますので、参考にしてみてください。\n個人的にはGatsbyに興味があって、Reactの勉強のついでに、いつかポートフォリオのためのサイトをそれで作り上げてみようかなと思っています。Oracleクラウドだと無料のVMやDBも提供されるので、簡単なAPIを作成しておくのも可能になるかもしれませんね。いや、いい時代になったものです。皆さんもぜひ、静的ウェブページで自分だけのサイトを築き上げてみてください。\nSingle Page Applicationの略で、従来のWebアプリケーションが複数の画面で構成されていたことに対して、画面を一つにして開発の効率やパフォーマンスの向上を図ったWebアプリケーションのこと。画面がサーバサイドと分離されているため、バックエンドとして選択できるものの幅が広いという特徴もあります。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nProgressive Web Applicationの略で、Webアプリでありながらもネイティブアプリならではのメリット(パフォーマンスが高い、インストールすることでオフラインでも利用可能)を採用したアプリケーションのことを指します。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-08-23T00:00:00Z","image":"https://retheviper.github.io/images/tools.jpg","permalink":"https://retheviper.github.io/posts/static-web-page/","title":"静的ウェブページに興味を持つ"},{"content":"前回、JWTとともにSpring Securityでの実現方法についてポストを書きましたが、JWTを使ってAccess Tokenのみを発行した場合は、考えられる脆弱性があるらしいです。なので今回は、Access Tokenのみを使う認証の特徴と問題点及びそれを補完するための技術を紹介したいと思います。\nそもそもJWTを使った認証認可は、JWTがISO標準ではあるものの、正解と言った実装方式がないらしいです。なので以下で紹介するAccess Tokenのみの発行に伴う問題と、それを補完するための戦略は概念だけを紹介して行きます。\nAccess Token Access Tokenのみを使う場合は、最初の認証以降、サーバでは秘密キーを利用してリクエストに載せられたTokenを確認するため、データベースやストレージなどのI/Oが必要なくなるため高速で処理ができます。ただ、Tokenはリクエストを送るクライアント側が持っているため、強制的にセッションを終了させることができません。なので、ログアウトの処理をするためにはクライアント側からToken情報を削除するしかないですね。\nAccess Tokenを使った認証方式はこういう特徴を持っているため、クライアント側がTokenを持っているため、これをもし第三者に乗っ取られた時はサーバサイドからそれを判断できる術がありません。こういう問題を防ぐため、Tokenを発行する際はあらかじめ期限を設定することで、一定の時間がすぎるとTokenの有効期間が満了するようにして、再利用できなくしています。\nTokenの期限設定のジレンマ Tokenの期限をあらかじめ設定する場合、有効期限をどう設定するかの問題があります。短くした場合は、Tokenを乗っ取られた場合でもすぐにTokenを使えなくなるため安全性を担保できますが、逆にユーザは頻繁にログインし直さないと行けなくなります。逆に長くした場合は、ユーザはログインし直さなくてもサービスを長期間利用できるものの、乗っ取られたTokenも長く使えるようになりますね。\nRefresh Token Refresh Tokenは、満了したAccess Tokenの有効期限を更新するための別のTokenです。サーバは認証時にクライアント側にAccess TokenとRefresh Tokenをともに発行します。ここでAccess Tokenの期限は30分くらいの短い期間に設定して、Refresh Tokenは1ヶ月ほどの長い期間に設定します。\nクライアント側でログイン後に30分がすぎて、Access Tokenの期限が満了した場合にRefresh Tokenでサーバに新しいAccess Tokenの発行を依頼します。サーバはクライアントのリクエストに乗ってあるRefresh Tokenを検証し、問題なければAccess Tokenを再発行します。これでAccess Tokenを乗っ取られたも、すぐに満了し、ユーザはRefresh Tokenを持って有効期限の自動更新ができるので毎回ログインする必要がなくなります。また、サーバサイドでRefresh Tokenを検証するデータを持っているので、もし問題がある場合はRefresh Tokenを満了させることもできます。\nただ、Refresh Tokenを使った場合でも完璧では言えません。サーバが検証するためにRefresh Tokenに対応するデータを別のデータベースやストレージに保存する必要があるので、追加的なI/Oが発生し得るという問題はあります。また、Refresh Tokenを用いたAccess Tokenの更新のためにはサーバとクライアント側の両方に機能を追加する必要がありますね。あと、Access Tokenと同様にクライアント側がRefresh Tokenを保存する必要があるのでこれをどう保存するかを考える必要があります。\nSliding Session Sliding Sessionは、セッションを継続して利用しているユーザがいる場合、自動でAccess Tokenを更新してくれる仕組みのことです。リクエストごとに新しいAccess Tokenを発行する方法がありますが、特定のリクエストに限って処理する方法もあります。例えば、ユーザが長い時間を送ると予想されるページ(問い合わせの作成)や、次に何かのアクションが予想されるページ(ショッピングカートにアイテムを追加する)の場合があるでしょう。他には、Access TokenのIAT(トークン発行時間)を確認して新しいAccess Tokenの発行を要請する場合もあるらしいですね。\nただ、Sliding SessionでAccess Tokenを更新する場合、そもそもの有効期限が長く設定されていると無限に近くセッションが延長され、ユーザが全くログインしなおさなくてもサービスを利用できることになるので注意が必要です。逆に、ユーザがあまり長い時間の作業を行わない場合はSliding Sessionそのものの必要性がなくなる可能性もあります。\nRefresh Token + Sliding Session 最後は、Refresh TokenとSliding Sessionを同時に使った方法です。普通のSliding Sessionを適用した方法との違いは、Access TokenではなくRefresh Tokenを更新するということです。Refresh Tokenを更新することにより、Access Tokenを頻繁に更新する必要がなくなります。またRefresh Tokenが更新されるためアクセス回数の少ないユーザでも比較的ログインを頻繁にする必要がなくなりますね。Refresh TokenとSliding Sessionのメリットを適切に融合させた戦略と言えます。\nただ、この方式でも問題が全くないとは言えません。サーバからRefresh Tokenの有効期限を満了させない限り、ユーザはリソースにアクセスできるので、もしPCやスマホなどの機器が乗っ取られた場合は対応できません。なのでアカウントのパスワードを周期的にリセットさせるか、特定のリソースにアクセスする場合は再び認証させるなどの方法が必要となります。\n最後に 認証はWebアプリケーションにとってもっとも重要で、当たり前な機能となっていますが、セキュリティのため強化して行くとユーザの使用性に悪影響が出てしまうという問題を持っているものです。なので色々な方法論があり、さまざまなWebアプリケーションではそれぞれ違う方法を使って脆弱性に対応していますが、どれもメリット・デメリットがあるので正解と言える物はないのかもしれません。\nまた、JWTのPayloadは暗号化されないため(ただのbase64文字列)、JWTを乗っ取られた時にその中身を見れるという問題もあるらしいです。これを補完するためにPASETOという物も提案されているらしいですが、これがまたJWTほど亜補給される稼働かはわからない状態ですね。\nあと、そもそもの認証方式の問題ですが、量子コンピュータが一般に補給されたら、従来の方式による暗号の解析はすぐにできるようになるとも言われているので、暗号化という考え方そのものが変わる可能性もあるかもしれません。そうなると既存のアプリも全体を作り直さなくてはならないことになるでしょう。\nセキュリティは難しいものです。暗号化とともに、ユーザが便利に使える設計、戦略を適切に選定するためには色々と工夫が必要と感じます。\n","date":"2020-08-02T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/refresh-token-and-sliding-session/","title":"JWTの補完：Refresh TokenとSliding Session"},{"content":"よく訪問しているサイトに、とある質問がありました。質問の内容とは、List二つを、重複する要素なしで一つにまとめる方法ということでした。SQLなら簡単に解決できそうな問題でもありますが、クエリーを修正できない状態だったり、複数のAPIを呼び出してその戻り値を扱う場合は直接コードを書いて併合するしかないですね。今まであまり経験したことのない状況でしたが、個人的にも興味が沸いたのでいろいろ試しながらコードを書いてみました。\n問題のコード 質問の作成者がやりたいことは、List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;が二つあり、Listの中のMapの要素を確認し、重複する物があれば一つのListとしてまとめたいということです。ここで重複の条件は、MapのKeyとValueでした。\n今はコードは存在するものの、重複チェックのためのロジックが複雑になりすぎて、負荷も高く性能面でも問題があるらしいです。まず彼の後悔しているコードは、以下のようなものでした。\nList\u0026lt;Object\u0026gt; list1 = ... List\u0026lt;Object\u0026gt; list2 = ... for (Object object : list1) { Map\u0026lt;String, String\u0026gt; map = (HashMap\u0026lt;String, String\u0026gt;) object; String a1 = map.get(\u0026#34;a\u0026#34;); for (Object object2 : list2) { Map\u0026lt;String, String\u0026gt; map2 = (HashMap\u0026lt;String, String\u0026gt;) object2; String a2 = map2.get(\u0026#34;a\u0026#34;); if (a1.equals(a2)) { Set\u0026lt;String\u0026gt; keys = map2.keySet(); for (String key : keys) { String v = map2.get(key); map.put(key, v); } } } } コードをみると、3重ループになっていて、KeyとValueが一致する項目がないかを一つ一つチェックしています。そして実際のコードなのかよくわかりませんが、このコードだとキーがリテラルになっているので全体のキーを循環できるようなループがまた追加されるべきではないかと思います。そうするとまたループが追加されたりして、より複雑なコードになりそうですね。これをListの個数分回すとしたら、それは負荷が多いだろうなと思います。なのでこれをなるべく短く、より単純なコードにしてみたいと思います。\nBest Practiceではないのかもしれませんが、ひとまず自分の考えてコードを紹介します。\nStreamを使ってみる Listの要素を一つづつ処理したい場合は、まずStreamを使う方法がないかをまず考えてみました。ネットで調べてみるとやはりStreamを使って複数のListをマージする方法がいくつかあります。それらを使って検証してみました。\n結合と重複の除外 Stream.concat()を使うと、二つのStreamをつなげることができます。また、Streamではdistinct()で重複を除外することができます。これらの組み合わせを使うと、二つのListを重複する要素なしで結合することができます。まず簡単な例題を使うと以下のようなコードになります。\n// 結合したいList List\u0026lt;String\u0026gt; list1 = List.of(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;); List\u0026lt;String\u0026gt; list2 = List.of(\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;f\u0026#34;); // 結合と重複の除外(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;f\u0026#34;) List\u0026lt;String\u0026gt; concat = Stream.concat(list1.stream(), list2.stream()).distinct().collect(Collectors.toList()); Stream.concat()の引数は二つしか指定できないので、3つ以上のListを繋ぐ場合はループを使うのを考慮すると良いです。また、distinct()の場合、equals()がちゃんと定義されてあるという前提ならどんなオブジェクトでも重複検査ができます。なのでLombokの@Dataのようなアノテーションのついたクラスでも重複を除外して一つのListに納めることができます。\n限界 質問の作成者は、Listの中のMapの要素に対して重複チェックを行いたいと行っていますが、この方法ではそのようにはなりません。なぜなら、Mapそのものをequals()で比較してしまい、中の要素一つ一つに対してはチェックしない構造となっているからです。なのでこのようなコードだと、二つのListを繋いだような物ができてしまうだけです。\nForループを使ってみる 今回は質問者のコードを直して、より効率的に変えてみます。あえてForループを使っているのは、条件に一致した場合に一回だけput()を実行するためです。StreamやforEach()は全ての要素に対して処理を行うためのものなので、除きました。\nMapにはput()以外でもputAll()があるので、要素を巡回しながら一つづつ一つでもループは消すことができます。そしてputAll()を実行したら、次の要素までチェックする必要はなくなるのでcontinueを実行して次のループは飛ばすようにして、無駄な処理を無くします。そうするとまず以下のようにコードを帰ることができますね。\nfor (Object object : list1) { Map\u0026lt;String, String\u0026gt; map = (HashMap\u0026lt;String, String\u0026gt;) object; String a1 = map.get(\u0026#34;a\u0026#34;); for (Object object2 : list2) { Map\u0026lt;String, String\u0026gt; map2 = (HashMap\u0026lt;String, String\u0026gt;) object2; String a2 = map2.get(\u0026#34;a\u0026#34;); if (a1.equals(a2)) { // ループを一回無くす map.putAll(map2); continue; } } } ここで、MapのKeyを指定しているところも直します。リテラルで指定しなく、Entryを巡回しながら比較することにするのでループは一つ増えます。そして、EntryはSetで取得できるので、Collectionのメソッドであるcontains()を使って比較することができます。なので比較したいMapのうち、どちらかのEntryを巡回しながら要素が違う方のMapに入っているのかを確認するだけで良いですね。これを反映して直したコードが以下です。\nfor (Object object : list1) { Map\u0026lt;String, String\u0026gt; map = (HashMap\u0026lt;String, String\u0026gt;) object; for (Object object2 : list2) { Map\u0026lt;String, String\u0026gt; map2 = (HashMap\u0026lt;String, String\u0026gt;) object2; // Entryで比較する for (Map.Entry\u0026lt;String, DataClass\u0026gt; entry : map2.entrySet()) { if (map1.entrySet().contains(entry)) { map1.putAll(map02); continue; } } } } あとは最初からあらかじめListの型変換をして、ループ毎に型変換をしないようにすることですね。あまりこれで性能の改善は期待できないのかもしれませんが…\n// あらかじめ型変換をしておく List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; convertedList1 = (List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt;) list1; List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; convertedList2 = (List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt;) list2; for (Map\u0026lt;String, String\u0026gt; map : convertedList1) { for (Map\u0026lt;String, String\u0026gt; map2 : convertedList2) { for (Map.Entry\u0026lt;String, DataClass\u0026gt; entry : map2.entrySet()) { if (map1.entrySet().contains(entry)) { map1.putAll(map02); continue; } } } } あとは場合によってMapのKeyをソートするなどの処理が必要になるかもですが、一旦これで要件は満たしたような気がします。\n条件が違う場合 質問者のコードからは、推論するしかないことですが、もしListのインデックスを基準に比較するという条件があるとしたら、コードはより減らすことができます。list1とlist2の同じインデックスに、同じ要素を持つMapがあるかを確認するということです。もしこの条件があるとしたら、ループは2重に納めることができます。以下はその場合のコードです。\nfor (Map\u0026lt;String, String\u0026gt; map : list1) { for (Map.Entry\u0026lt;String, String\u0026gt; entry : map.entrySet()) { // List2と同じインデックスを比較する Map\u0026lt;String, String\u0026gt; map2 = list2.get(list1.indexOf(map)); if (map2.entrySet().contains(entry)) { map.putAll(map2); continue; } } } ただ、この方法を使うためには二つのListが同じサイズを持っているという前提が必要となるのでそこには気をつけなくではならなくなります。\n最後に たまにこうやって、自分ではまだ遭遇していない場合に対して考えられるチャンスとなるので、コミュニティに注目するのは良い経験となると思います。調べている間に普段は使ってみたことのないメソッドやAPIを調べてみたり、自分が書いていたコードを振り返ってみる良い機会にもなりますね。\n他にもKeyは構わなく、重複するValueがある場合のみチェックするとか、要素のフィールドが重複している場合をチェックするかなどさまざまなバリエーションを考えられると思います。どれも面白い主題なのですが、今回の主題とは少し乖離があるため、機会があればいつかそのようなケースに対してのコードもブログに載せたいと思います。では、また！\n","date":"2020-07-26T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-compare-and-merge-lists/","title":"二つのListを結合する"},{"content":"個人的に、クエリーやDBのプラグインなど、DBそのものによるデータの整形はあまりよくないと思っていて、なんからの「処理」が入る場合にSQL文はなるべく最低限のCRUDに制限し、必要な作業は全てアプリにて処理するような実装をしたほうが設計として望ましい形だと思っています。なぜかというと、システム全体の性能、アプリのDBに対しての独立性、処理の容易さという観点からそちらの方が優れていると思っているからです。\nまず性能ですが、DBサーバとAPサーバが物理的に同じ性能のマシンを使っているとしたら、実際は最適化されたクエリーでパフォーマンスをあげることは十分できますね。しかし、処理すべきデータやリクエストが増えると必ずしもそうとは言えません。Webアプリケーションでユーザ数が増えるとスケールアウト1やスケールアップ2を考慮することになりますが、スケールアウトする場合の場合にDBへの依存が強いアプリは問題を起こす可能性があります。APサーバは複数を利用してもアプリの動作には変わりがありませんが、DBサーバの場合は台数が増えるとデータの整合性という新しい問題が出てきますね。どのDBにも同じデータが保存されていることを保証したいなら、DBサーバをスケールアウトするよりはAPサーバをスケールアウトした方が良いと思います。なのでこの場合はアプリにデータの「処理」を任せた方が望ましいですね。\n次にアプリのDBに対しての独立性ですが、最近は費用の高いOralceから無料のPostgreSQLやMySQLに移行するシステムも増えてきていますね。しかし、ここでDBのクエリーやプラグインに依存した部分が多ければ多いほど移行は難しくなる可能性があります。微妙に違うカラムのデータ型から、互換性のないクエリーや機能などを全面的に検討する必要がありますね。なのでSQL文はなるべく単純にした方が、移植生が上がります。\n最後に、処理の容易さなのですが、現在DBの主流であるRDBSの場合は、徹底的にデータをいかに効率的に保存するかが最も重要な課題となっているものです。正規化を終えたテーブル構造は、データを加工するには適合していません。なのでアプリでは必要に応じてエンティティを作って使いますが、そのエンティティに合わせてSQLを書く際はまた複雑なJoinやWhereなどを描かなければならなくなりますね。エンティティの中に他のエンティティがま他入っていたりする場合はJoinをするか、しないかのケースまで考えてクエリーを描かなければなりません。\nこのような理由からして、自分はなるべく複雑なクエリーを排除し、なるべくアプリ中心的に開発できる方法はないかと思っていました。初めはJDBCに触れ、そのあとはMyBatisを使うようになっていましたが、MyBatisを使ってもケース別にクエリーを書く必要があるのはまた同じでした。(プラスで、xmlが必要となることもありますね)最近はテレワークにより通勤時間がなくなり、仕事が終わった後は勉強をかねて自作アプリを実装していますが、ここでより簡単に、またよりアプリに優しいDBのAPIがないかと思っていましたが、そこで発見したのがJPAでした。\nJPAとは JPAはJava Persistence APIの略で、Javaが提供する標準APIです。ただ、標準APIだとしても、Interfaceとして定義しているだけなので、これを実装したものが必要となります。JPAを実装したものとして有名なのがHibernateで、これをさらに使いやすく整理したモジュールとしてSpring Data JPAが存在しています。\nだいぶ序論が長くなりましたが、結論的に今回言いたいことが何かというと、Spring Data JPAを使うと他のライブラリやフレームワークを使うよりも楽に開発ができるということです。おそらくSQL中心的な開発に関しては自分と同じようなことを考えていた人が他にもいたらしく、JPAは単純なクエリーを自動で作ってくれたり、自動でテーブルのJoinをしてくれるなど全くSQLに触れなくてもDBを扱うことを可能にしてくれています。同じくORMとしてよく使われているMyBatisに比べても、JPAを使った方がより効率的なのではないかな、と個人的には考えています。その理由からか、海外では他のORMよりもJPAを使っている割合の方が圧倒的に高いようでした。なのでもし今もMyBatisを使っている方がいらっしゃるなら、ぜひJPAを一度は使ってみて欲しいところです。\nでは、実際にJPA(Spring Data JPA)の特徴と使用法を、一つづつ紹介しましょう。\nJPAの特徴 まずはJPAの特徴です。正確には特徴というよりは、JPAを使う場合のメリットに近いのではないかと思います。\n自動グラフ探索 まず、このようなクラスがあるとします。\nclass Team { private long id; private String name; } そしてチームには所属された選手がいます。\nclass Player { private long id; private String name; private Team team; } 既存のSQLでこの選手オブジェクトをDBから照会する場合は、以下の手順が必要となります。\nTeamテーブルをSelectする TeamオブジェクトにSelectしたデータを埋め込む PlayerテーブルをSelectする PlayerオブジェクトにSelectしたデータを埋め込む PlayerオブジェクトにTeamオブジェクトをセットする この手順は、テーブルのJoinが多くなればなるほど増えます。また、SQLもさらに長くなりますね。例えばTeamクラスにRegionというクラスがフィールドとして入り、RegionにはさらにCountryというクラスが入るとしたら？それぞれのテーブルを照会してJoinするようなクエリーを作る必要があって、またオブジェクトも全部マッピングする必要がありますね。もしこの手順に何らかの抜け漏れなどがあったら、コードは思い通りに作動しないはずです。\nJPAでは、このような参照関係を自動で解決してくれます。例えば、何もしなくても以下のようなことが可能です。\n// RepositoryからPlayerオブジェクトを取得する Player player = repository.findById(1); // Playerが所属したTeamを取得する Team team = player.getTeam(); 性能の最適化 自動的にグラフ探索をしてくれるところをみると、まさかPlayerテーブルを照会するたびにTeamテーブルも照会しているような無駄なことをしているわけではないか、と思われそうですが、実際はLazy Loadingにより、TeamはgetTeam()を実行した時点でまたのSelect文を発行して照会していることになります。なので、Playerだけの情報が必要な場合はその情報しか照会しないことになります。\nでは、場合によっては数回もトランザクションが発生するのではないか、という疑問もあり得ると思いますが、これをJPAではキャッシュとバッチ、フェッチタイプの指定で解決しているようです。これらは以下のように作動します。\nキャッシュ：同じエンティティに対して(同一トランザクションならば)2回目以降のSelectはキャッシュから照会する バッチ：バッチメソッドを提供し、一回のコミットで複数のクエリを転送する機能を実装している フェッチタイプ：アノテーションによりLazy Loadingをするか、最初からJoinして照会するかを選択可能 自動クエリ生成 先にRepositoryからオブジェクトを取得する例を照会しましたが、これが可能になるのはJPAが自動的に生成してくれるクエリーがあるからです。JPAが提供するアノテーションやインタフェースを使うとこで、自動生成されるクエリーを利用することができます。\nつまり、エンティティとして利用するクラスとJPAをアノテーションとインタフェースに紐づけておくといちいちクエリーを作成してオブジェクトにマッピングする必要がなくなるということです。もちろん自作クエリーを使うこともできるので、自動生成されるクエリーを使いたい場合も問題ないです。\nこのクエリーの生成は、エンティティのCRUDだけに限ることではありません。エンティティに変更が生じた場合(フィールドを追加する必要がある場合)は、DDLも自動的に作成してくれます。ここで自動生成されるDDLをDrop-Createにするか、AlterにするかはアプリケーションプロパティのHibernate設定でできます。\nJPAの使用法 では、実際のJPAをコードとしてはどう使えるかをみていきましょう。\nEntity JPAでは、エンティティとそのフィールド(テーブルのカラムや参照関係)をアノテーションによって簡単に定義することができます。例えば以下のようになります。\n// Entityとして指定する @Entity class Car { // 自動的に増加するPK @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private long id; // UniqueかつNonNull設定 @Column(nullable = false, unique = true) private String name; // Companyとは多対一の関係 @ManyToOne @JoinColumn(name = \u0026#34;company_id\u0026#34;) private Company company; // Insuranceとは一対多の関係で、Lazy Loadingをする @OneToMany(fetch = FetchType.LAZY) @JoinColumn(name = \u0026#34;insurances_id\u0026#34;) private List\u0026lt;Insurance\u0026gt; insurances; } こうやってEntityにはGetter・Setterをつけておくことで、Selectした時点でこのクラスに紐づくCompanyはInsuranceのようなテーブルの情報も参照できるようになります。\nRepository JPAでは、いくつかのRepositoryというインタフェースを提供しています。よく使われているものは、JpaRepositoryやCrudRepository、PagingAndSortingRepositoryなどがあって、それぞれ基本的に提供しているメソッドが異なります。例えばCrudRepositoryは普通にCRUDのためのメソッド(Selectに該当するfind()、Upsertに該当するsave()、Deleteに該当するdelete()など)を基本的に提供していて、PagingAndSortingRepositoryの場合はページング(ページネーション)に対応したメソッドを提供しているなど、基本的によく使われるメソッドが揃っています。例えば以下のようなものです。\n// レコードを数える long count = repository.count(); // 全レコードを取得 Iterable\u0026lt;Car\u0026gt; cars = repository.findAll(); // レコードを一件取得する Optional\u0026lt;Car\u0026gt; car1 = repository.findById(1); // レコードを一件削除する repository.delete(car2); // レコードを保存する(Upsert) Car saved = repository.save(car3); 最初エンティティを定義して、それぞれのエンティティに合わせてJPAが提供するRepositoryを作成すると(extend)、そのRepositoryを使ってクエリを発行することができるようになります。また、基本的に提供しているメソッドが十分でない場合は、命名規則に従ってRepositoryに直接クエリーメソッドを書くと自動的にクエリーを作成してくれたり、@Queryアノテーションで必要なクエリーを直接書くこともできます。\npublic interface CarRepository extends CrudRepository\u0026lt;Car, Long\u0026gt; { // nameで照会するカスタムクエリーメソッド boolean existsByName(String name); // アノテーションにクエリーを指定する場合 @Query(\u0026#34;select c from Car c where c.name = ?1\u0026#34;) Optional\u0026lt;Car\u0026gt; findByName(String name); } 他に 2018年には、Spring Data JDBCというものも発表されています。Spring Data JPAとJDBCのAPIの間にはJPA + Hibernateという階層を経由することに比べ、Spring Data JDBCは直接JDBCのAPIを呼び出すことで性能面での利点があるらしいですね。他にも実装をより単純化することでJPAの持つ問題を解消しているらしいですね。例えばキャッシュやLazy Loadingをなくしたとか(JPAのメリットとして紹介しましたが…)\nただ、Spring Data JDBCは2018年にリリースされたばかりのものなので、クエリーメソッドだけでクエリーを自動生成してくれるなどの機能には対応してなかったり、テーブルやカラムの命名規則がJPAと違うなど、JPAからマイグレートするにはまだいろいろと考慮しなければならない点が多いように思われます。こういうところは、まるでNode.jsとDenoの関係みたいですね。\nSpring Data JDBCは自分でもあまり詳しくなく、サンプルコードをみてテストしてみただけなのですが、すでにSpring Data JPAの経験がある人ならすぐにでも適応できそうな構成となっていました。パッケージは変わってもアノテーションやインタフェースそのものの使い方が大きく変わっている訳でもないので、感覚的には大差ありません。@Entityのようなアノテーションを使う必要がなくなったというところが目立つくらいでした。なので、個人的なプロジェクトでは一度試してみるのも良いかもしれません。(エンタープライズレベルでは、まだ検証すべきところがあると思うので)\nまた、Spring 5からReactiveなプログラミングのできるWebfluxが紹介されたことと共に、同じくNon-blockingに対応するSpring R2DBCというものも注目されているようです。既存のSpringプロジェクトにWebfluxを今すぐ導入する必要はないらしいのでSpring R2DBCも今すぐ導入する必要はないのかもしれませんが、Non-blockingによって性能を改善できる場合3は確実に存在するので、場合によっては導入を考慮する価値があるのかもしれませんね。\n最後に 自分の場合、Springでアプリケーションを作りながらなるべくXMLを排除しJavaのコードだけで全てを管理したかったため、クエリーを自動生成してくれるというところで惹かれていましたが、実際に使ってみると使い方も簡単で(癖はあると思いますが)、テーブルが途中で変わっても簡単に直せるというところがかなり魅力的なものですね。占有率としては標準ということもあり、JPAが他のORMと比べよく使われているようですが、自分みたいにMyBatisしか扱ったことのない方には一度触れてみて損はないかと思います。\nJPAの他にも紹介したようにSpring Dataだけでもいろいろなライブラリが存在していてそれぞれの思想が違うので、やっと一つを少しわかったくらいではまだまだ先が遠いような気もしていますね。でもこうやって、一つのライブラリが作られた経緯や思想を調べるだけでも得られる知識は多く、どれも無駄にはならないと思われるくらい新鮮なものなので、意味のあることとなっていると思います。またこうやって情報を集めつつ、それが誰かにとっては貴重な情報になるといいですね。今後もいろいろ調べてブログに載せて行きます。では、また！\nサーバの台数を増やすこと。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nサーバのマシンパワーをあげること。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nスレッドプールが小さすぎたり、一つのリクエストに対して処理時間が長すぎてボトルネックになるケースなど\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-07-10T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-data-jpa/","title":"MyBatisよりJPAが使いたい"},{"content":"常に勉強し続けなければならないのがこの業界の定めとは思いますが、変化は日々加速されていて、何に従って何を頼れば良いかわからない時もありますね。新しい言語が続々登場してくるなかでも、昔は性能の問題で注目されてなかった言語がいきなり人気を得たり、当たり前と思っていたパラダイムがひっくり返されたりもします。今はJavaScriptの時代と行っても過言ではない状況に来ていると思いますが、今後はどうなるのでしょうか。いくつか、個人的に興味をもったIT業界の動向についてまとめてみました。\nApple Silicon mac こないだ、Appleの開発者向けイベントである「WWDC2020」が開催されました。毎年必ずライブで見ているわけではないのですが(深夜に始まるので…)、最近はテレワークをしていて通勤時間がなくなっているのもあり、最初から最後までライブで見ることができました。\niOSやiPadOS、macOSの変化ももちろん興味深かったのですが、やはり今回のイベントで最も印象的なものがなんだったかというと、macのメインプロセッサがApple製のものに変わるということでしたね。自分はPowerPC時代のmacは経験したことがありませんが、PPCからIntelへの転換は成功的だったので、今回もそうなると思います。個人的にiPadも使っているのですが、あまりIntelのCPUに比べ性能が劣るという印象は受けたこともありませんでした。\nもちろん、Bootcampを使えなくなることもあり、他にもx86を基盤に作られたサードパーティアプリケーションが使えなくなる可能性もあるので、新しいプロセッサのmacが今年発売したところですぐに使っているmacを買い換えるつもりはありません(高いし…)。でも、以前からLLVMやCatalystのように、違うプラットフォーム(プロセッサ)でも問題なくアプリが起動できるような環境を構築するために何年も投資しているのを見ると、意外と転換は早く、問題なくできるのではと思います。特に自分はWebアプリを書くだけなので、あまりlow levelの技術に触れることもなく、使っている言語のコンパイラが新しいプロセッサに対応してくれれば良いだけですね。実際WWDCの紹介でも、xcodeで作成したプロジェクトは再度ビルドするだけで良いと言ってました。\nただ気になるのは、自分のIntel製CPUを搭載しているmacに対してのOSのサポートがいつまで続くか、くらいですかね。最近はWindowsの発展もすごいのですが、自分の環境だとあまりWindowsにこだわる必要はないので問題なく転換できそうな気もします。そしてプロセッサのみではなく、SoCなのですでにiPadやiPhoneに適用されているさまざまなセンサーやニューラルエンジンなどを使えるのも大きいメリットと思います。\nただ、こういう感想はあくまで開発者としての自分の立場から考えた個人レベルの問題で、エンタプライズレベルだといくらRosetta2やUniversal Binary2が優秀だとしてもどこかで互換性や性能の問題は起こり得ると思うので要注意とは思います。Officeのデモンストレーションは、今よりもパフォーマンスが優れているように見えたのでよかったのですが、事務職の人があえてmacを使う理由はそもそもないので…\n他、すでに$500でA12Zを搭載したmacを借りることができるようなので、意外とすぐに新プロセッサmacの性能や互換性問題は明らかになるかもしれませんね。まずはそこに注目かな、と思っています。性能のみならず、作業時の温度や電力消費量もどれだけ改善されたか、知りたいところですね。来週あたりでベンチマークを見れるのではないかと思っています。\nDeno JavaScriptはあまり触ってないので、Node.jsにも詳しくないわけですが、やはり昨今のWebアプリだとNode.jsなしでは話にならない時代になっていますね。自分の場合はTypeScriptの方でJavaScriptよりも良い印象を受けたので、NodeでもネイティブなTypeScriptサポートができるといいなと思っていました。それが、Nodeの開発者によってDenoという新しいランタイムが登場したらしいです。\n基本的にNodeでの反省点(async/awaitのような？)から作っているもののようで、その他にも特徴としてはTypeScriptのコンパイラを内蔵していて、いちいちコンパイルしなくても(JavaScirptにトランスパイルしなくても)使えるというところが自分にとっては最大のメリットのように思われます。\nただDenoの問題だと、やはり既存のNode.js用モジュールを使えない、というところと、TypeScriptのコンパイルが遅いというところがあげられています。Rustを使って自体のTypeScriptコンパイラを作るという計画があるらしいですが、いつになったら完成できるかわからないのでしばらくDenoを使ったプロジェクトはあまり発券できないかもですね。\nBlazor 5月には「Microsoft Build 2020」にて、Blazor WebAssemblyのリリース発表がありました。これで.NETとC#による、ブラウザから実行可能なWebアプリケーションを作成できるらしいです。\nNode.jsを使ったWebアプリケーションのメリットとして、よく「一つの言語でサーバサイドとクライアントサイドの両方を開発できる」ことがあげられていますが、このようにJavaScriptを使わなくてもそれが可能になるようなフレームワーク＆ランタイムが登場することもあるのですね。JavaScirptも良い言語ですが、そもそもの限界が明確なので、C#のようなコンパイル言語をブラウザ上で使えるのは逆のパターン(JavaScriptをサーバサイドで使う)に比べ良いところが多いのではないかと思います。\nまた、BlazorファミリーとしてPWAの実装ができるBlazor PWA、ElectronとWebViewでのデスクトップアプリが実装できるBlazor Hybrid、HTML要素なしでネイティブアプリが実装できるBlazor Nativeも続々登場する予定だというので、これに刺激を受け他のコンパイル言語でもブラウザで実行できるようになるのではないかと思います。\nWSLやGitHubもありますが、いろいろな意味で最近のMicrosoftの変化と投資はすごいものですね…\nFlutter for web iOSとAndroidの二つのプラットフォームのアプリを同時に開発でき、同じく機能するReact Nativeに比べ性能でもネイティブアプリに劣らないことがメリットというFlutterですが、最近Flutterを利用したWebアプリケーションも続々登場しているようです。そもそもFlutterを書くための言語であるDartが、次世代JavaScirptを目標に開発されたものなのでおかしいことではないのですが、こうやって一つの言語でモバイルとWebアプリケーションの両方を開発できるということはBlazorの場合と同じく魅力的です。\nただ、Googleが開発しているというところから、DartではなくKotlinでよかったのでは？という気持ちはありますね。また、自社のサービスをすぐ見捨てる傾向のあるGoogleなのでいつまでFlutterが生き残ることができるか、ということも疑問として残っています。MicrosoftのXamarinがいまいちになっているので、モバイルだけはFlutterを採用して開発するようなこともあるのではないかと思いますが、Webアプリケーションを開発するとしたらC#を利用できるBlazorの方がより魅力的ではないかと思います。\nRust ポストC、C++として注目されているRustですが、最近はこの言語の人気が恐ろしいほどですね。まだエンタプライズレベルでは、既存に構築しているシステムや熟練されたエンジニアがいないなどの理由でRustが使われているケースはあまりないようですが、CやC++と同等のパフォーマンスを見せながらも安定的という面が最大のメリットと言われているらしいです。\n個人的にはWebアプリケーションレベルであまりシステムと直接関わるようなことをする場合はないのですが、PWAの登場などWebアプリケーションでもデスクトップ並みの性能を求められることがあったり、Javaでは直接扱うことのできないバイナリファイルの扱いなどで限界を感じていたりしているので、Rustのような言語が扱えたらより良いアプリケーションを作れるようになるのでは、と思っています。\n特に、コミュニケーションツールとして有名なDiscordがもともと使っていたGolangをあえてRustに交替したとの記事もあったので、同じくCとC++の代替言語として開発されたGolangさえも圧倒するメリットとは一体何かという疑問が湧いてきて、ますます気になっているところです。最近人気のKotlinの場合も、おそらくJavaと完全互換できるという特徴がなかったら成功できなかったと思いますが、互換性もない言語に転換するほどRustが持っているメリットとは一体なんなのか、知りたいですね。\n最後に いろいろな技術と変化があり、幸せのような、プレッシャーを感じるような複雑な気持ちではありますが、どれも肯定的な変化をもたらしてくれている、と思っています。特に、各言語の変化に関してはJavaのモットーであるWrite once, run everywhereがどの言語でもより発展した形として実現されているところをみているようですね。結局どの言語も似たようなものになりつつあるような感覚もするのですが、逆にいうともうどの言語を使うかはあまり関係ない時代になりつつある気もします。\nなので、いったんはそのような変化についていけられるよう、自分のスキルを磨くことですね。どの言語を使うかがあまり重要ではなくなるということは、つまりその言語で何ができるかがより重要ということになると思うので、今の自分にできる言語を使ってさまざまな経験をしてみるのが優先すべきことではないかという気がします。あまり実装ができない最近ですが、個人的なプロジェクトでも進めながら経験してみたくなりました。\nでは、また！\n","date":"2020-06-29T00:00:00Z","image":"https://retheviper.github.io/images/magic.jpg","permalink":"https://retheviper.github.io/posts/personal-interests-in-this-time/","title":"最近注目しているものたち"},{"content":"前回はREST APIでのログインの方式と、JWTを使った認証認可について説明しました。でも、アプリケーション全体のセキュリティという観点からするとログインができたということだけで全てが終わったわけではないです。例えばエラーハンドリングがありますね。\nSpring SerucityのConfigurationクラス(WebSecurityConfigurerAdapterの継承クラス)を作成しておいて、ロールによってアクセスできるURLをの制限をかけておくことだけで認可されてないリクエストに対してエラーを出してくれますが、カスタムクラスで独自のエラーハンドリングをしたい場合もあリますね。\nなので、今回はログインの以外に、REST APIでの認証認可周りで必要となるSpring Securityの設定を紹介したいと思います。ここで紹介するのは以下のようになります。\nログインしてない使用者が、特定のURLにアクセスする場合のハンドリング ログインしているが、特定のURLにアクセスする場合のハンドリング ログインに失敗した場合のハンドリング ログアウトした場合のハンドリング では、一つづつどうやってカスタムクラスを作るのかをみていきましょう。\nAuthenticationEntryPoint ログインしていない使用者が、特定のURLにアクセスする場合のハンドリングのためには、AuthenticationEntryPointの実装が必要となります。ここではレスポンスとしてUNAUTHORIZEDを表示する例を紹介します。\npublic class JWTAuthenticationEntryPoint implements AuthenticationEntryPoint { @Override public void commence(HttpServletRequest request, HttpServletResponse respose, AuthenticationException exception) throws IOException, ServletException { // レスポンスの設定(401 UNAUTHORIZED) response.sendError(HttpStatus.UNAUTHORIZED.value(), HttpStatus.UNAUTHORIZED.getReasonPhrase()); } } AccessDeniedHandler ログインしているが、特定のURLにアクセスする場合のハンドリングのためには、AccessDeniedHandlerの実装が必要となります。ここではレスポンスとしてFORBIDDENを表示する例を紹介します。\npublic class JWTAccessDeniedHandler implements AccessDeniedHandler { @Override public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException exception) throws IOException, ServletException { // レスポンスの設定(403 FORBIDDEN) response.sendError(HttpStatus.FORBIDDEN.value(), HttpStatus.FORBIDDEN.getReasonPhrase()); } } AuthenticationFailureHandler ログインに失敗した場合のハンドリングのためには、AuthenticationFailureHandlerの実装が必要となります。ここではレスポンスとしてFORBIDDENを表示する例を紹介します。\npublic class JWTAuthenticationFailureHandler implements AuthenticationFailureHandler { @Override public void onAuthenticationFailure(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException, ServletException { // レスポンスの設定(403 FORBIDDEN) response.sendError(HttpStatus.FORBIDDEN.value(), HttpStatus.FORBIDDEN.getReasonPhrase()); } } LogoutSuccessHandler ログアウトした場合のハンドリングのためには、LogoutSuccessHandlerの実装が必要となります。JWTを使ったログインだけでなく、REST APIのログインはSessionではなくトークンのような情報をクライアントが持つので、サーバサイドで処理することはありません。ログアウトの場合はクライアント側で認証に必要な情報を削除するようにして、サーバサイドとしては単純にレスポンスとしてOKを表示します。\npublic class JWTLogoutSuccessHandler implements LogoutSuccessHandler { @Override public void onLogoutSuccess(final HttpServletRequest request, final HttpServletResponse response, final Authentication authentication) throws IOException, ServletException { // レスポンスの設定(200 OK) response.setStatus(HttpStatus.OK.value()); } } Configurationクラスの設定 最後に、Configurationクラスに今まで作成したカスタムHandlerクラスをそれぞれの目的に合わせて登録します。メソッドを覚えるだけで、登録の内容自体は難しくないです。ここでは以前のポストで作成したクラスを基盤に修正しています。\n@Configuration @RequiredArgsConstructor public class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception { return super.authenticationManagerBean(); } @Override protected void configure(final HttpSecurity httpSecurity) throws Exception { httpSecurity .httpBasic().disable() .csrf().disable() .formLogin().disable() .and() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() .authorizeRequests() .anyRequest().hasRole(\u0026#34;USER\u0026#34;) .and() // 例外ハンドリングの設定 .exceptionHandling() // ログインしなかった場合のハンドラーの設定 .accessDeniedHandler(new JWTAccessDeniedHandler()) // 権限がない場合のハンドラーの設定 .authenticationEntryPoint(new JWTAuthenticationEntryPoint()) .and() // ログアウトの設定 .logout() // ログアウトに使うURLを設定する .logoutUrl(\u0026#34;/logout\u0026#34;) // LogoutSuccessHandlerを指定する .logoutSuccessHandler(new JWTLogoutSuccessHandler()) .and() .addFilterBefore(new JWTAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class) .addFilterAt(getJsonUsernamePasswordAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class); } private JsonUsernamePasswordAuthenticationFilter getJsonUsernamePasswordAuthenticationFilter() { final JsonUsernamePasswordAuthenticationFilter jsonFilter = new JsonUsernamePasswordAuthenticationFilter(); try { jsonFilter.setFilterProcessesUrl(\u0026#34;/api/v1/web/login\u0026#34;); jsonFilter.setAuthenticationManager(this.authenticationManagerBean()); jsonFilter.setAuthenticationSuccessHandler(new JWTAuthenticationSuccessHandler()); // 認証に失敗した場合のハンドラーの設定 jsonFilter.setAuthenticationFailureHandler(new JWTAuthenticationFailureHandler()); } catch (Exception e) { throw new RuntimeException(e); } return jsonFilter; } } ここではFilterやHandlerなどのクラスのインスタンスを新しく作成していますが、@ComponentでBeanとして登録しておいても構いません。どっちかとするとBean登録した方が良いかなという気はしますが、ここではコードが長くなるので省略しておきました。\nまた、一般的にDIはフィールドよりもコンストラクタにつけた方が良いといいますが、フィールドをfinalとして宣言している場合ならLombokの@RequiredArgsConstructorを使っても自動でDIされますので今回はそのような書き方にしてみました。\n最後に ハンドリングの場合は、メソッドが一つしかないインタフェースを実装して、Configurationクラスに登録するだけなので難しくないですね。それぞれのHandlerインタフェースは、引数としてリクエストはレスポンス、認証情報をとっているので、場合によってはカスタム例外ビューモデルを作成して返したり、エラー処理用のControllerを作ってリダイレクトさせるなど様々な処理ができます。\nセキュリティという面から考えると、定型化されたSpring Securityのエラーが出されるということはサーバサイドがSpringで作成されていることを知らせるようなものなので、アプリの脆弱性の問題に繋がる場合もあると思われますね。なのでWebアプリケーションを作る場合は、外部に漏出される情報に関しては徹底的に管理しておいた方が良いのではないかと思います。\nでは、また！\n","date":"2020-06-17T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-rest-api-security-handling/","title":"REST APIでのSpring Securityの例外ハンドリングを実装する"},{"content":"最近は、本格的にSpring SecurityによるREST APIでのログインの実装を勉強しています。実際の業務で使うことになるかどうかは分かりませんが、とりあえずREST APIで一般的な認証認可はどのように実装すべきで、Spring Securityはどう動くものかをまず理解しておかないとという気がして(そして最近はあまり実装していないので、感覚を失いたくないという願望もあり)、とりあえず自分で調べたことと、実装したコードの記録を兼ねて書きたいと思います。\n他にもいろいろな方法があるのかもしれませんが、今回自分が選んだのはJWTとSpring Securityを使ったREST APIでの認証認可のところです。初めはSpring SecurityもJWTもREST APIでの認証認可も全く知識がなかったのでかなり苦労しましたが、とりあえず成功したので、わかったことをまとめてみました。\nJWTとは JWT(JSON Web Token)は、署名や暗号化などを含む情報をJSONオブジェクトとして伝達することができる、オープン標準RFC 7519です。JWTのデータは署名されているためこの伝達されるデータの送信元の特定や途中でデータが入れ替えされなかったか(改ざんされてないか)の検証などができるので、多くの場合にユーザの認証で使われます。\nこのようにJWTの中には署名や暗号化などに必要なデータは全て含まれているので、サーバサイドではJWTを受け取ることでデータのバリデーションチェックができるなど自己完結性があって、既存のSessionを使った方法とは別にStatelessな方法としてユーザの認証と認可を可能にします。なので今回紹介する、REST APIでのログインのために適している認証方式といえますね。\nJWTでのログインシナリオ まずJWTを使ったログインの場合、クライアントからログインに必要なクレデンシャル(IDやパスワードなど)をサーバに送ると、サーバからはユーザの情報に基づいてJWTを発行して返します。クライアントはこの情報をリクエスト毎に載せてサーバに伝達し、サーバではJWTのバリデーションを行なってからレスポンスを返すようになります。\nここでJWTとして作られたJWTをクライアント側に送る時やクライアントがリクエストを送る時、レスポンスやリクエストのHTTP Headerに載せることになります。なぜSessionではなくHeaderに載せるかについてはこのポストを参考にしてください。\nログインに成功したあとは、Sessionを使う場合と同じく認証させているかをチェックして各URLへのアクセスを認可するようになりますね。ここはSpring Securityが担当するようになります。\nJWTとSpring Securityによるログイン では、以上のシナリオをどう実現できるかを考えてみます。まず、ログイン要請を受けるコントローラとメソッドを用意する必要がありますね。そして、そのコントローラからはサービスクラスから入力されたクレデンシャルを検証してもらいます。検証できたら、その情報を元にJWTのJWTを作る必要がありますね。\nSpring Securityの設定(JWTを使う前) まずはREST APIでの認証認可を簡単にSpring Securityで実装します。Spring Securityそのものだけでもかなり膨大な量を勉強しなければならないのですが、ここではまず、DBに登録されているユーザの情報を取得し、そのユーザのロールによってアクセスできるURLを制限するという機能だけを実現します。(他にも、ログインとはあまり関係のないクラスのコードは省略しています)\nEntityクラス まずはDBからユーザの情報を取得するためのクラスを作る必要がありますね。UserDetailsをimplementしたクラスを簡単に作っておきます。これは既存にユーザのエンティティとして作っても良いですが、認証のための専用のクラスとなるので、別クラスとして作成しても構わないです。ただ、その場合はちゃんとテーブルで既存のユーザ情報と紐づくように管理する必要がありますね。\n以下のコードは、UserDetailsクラスをSpring Data JPA基準のエンティティとして作成した例です。ここからユーザ名(username)とロール(roles)をJWTに載せて認証と認可に使うことにします。\n@Data @Entity public class User implements UserDetails { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private long id; // ユーザ名(一般的にID) private String username; // ユーザアカウントが満了されているかの設定 private boolean accountNonExpired; // ユーザアカウントがロックされているかの設定 private boolean accountNonLocked; // ユーザアカウントのクレデンシャルが満了されているかの設定 private boolean credentialsNonExpired; // ユーザアカウントが活性化されているかの設定 private boolean enabled; // 認可のためのユーザのロール @ElementCollection(fetch = FetchType.EAGER) private List\u0026lt;String\u0026gt; roles = new ArrayList\u0026lt;\u0026gt;(); // ユーザの認可情報を取得する @Override public Collection\u0026lt;? extends GrantedAuthority\u0026gt; getAuthorities() { return this.roles.stream() .map(SimpleGrantedAuthority::new) .collect(Collectors.toUnmodifiableList()); } } Serviceクラス UserDetailsを取得するためのServiceクラスを作っておきます。このクラスでログイン後のユーザ情報を取得するようになります。UserDetailsServiceのメソッドはユーザ名からUserDetailsを取得するためのloadUserByUsernameしかないので、これを適切にRepositoryから取得できるように実装します。\n@Service public class UserServiceImpl implements UserDetailsService { // UserDetailsを取得できるRepository private final UserRepository repository; @Autowired public UserServiceImpl(UserRepository repository) { this.repository = repository; } // 認証後にユーザ情報を取得するためのメソッド @Override public UserDetails loadUserByUsername(final String username) throws UsernameNotFoundException { return this.repository.findByUsername(username); } } Configurationクラス 認可のためのConfigurationクラスです。ここではUSERというロールが設定されてない場合、どのURLにもアクセスできないようにしておきました。ログイン後、ユーザがURLにアクセスためのリクエストを送ると、この設定によりユーザのロールを確認してアクセスを認可するようになります。このロールはユーザを作成するとき、createUserなどのメソッドでrolesをROLE_USERとして保存するようにしておくと良いです。\nまた、今回作るのはREST APIであり、JWTによる認証と認可を行うことになるので、いくつかのデフォルト設定を変えておきます。例えばBasic AuthとCSRFの設定や、セッション設定などがあります。\n@Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter { @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception { return super.authenticationManagerBean(); } @Override protected void configure(final HttpSecurity httpSecurity) throws Exception { httpSecurity // Basic認証を使わない .httpBasic().disable() // CSRF設定を使わない .csrf().disable() // セッションはStatelessなので使わない .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() // USERではないとどのURLでもアクセスできない .authorizeRequests() .anyRequest().hasRole(\u0026#34;USER\u0026#34;); } } これでSpring Securityを使うための必要最低限の準備は終わりました。他にユーザのCRUDのためのServiceやRepositoryは適宜作成してあるという前提として、次に進めましょう。\nJWTの依存関係の追加 次に、本格的にJWTを使うための設定を行います。Spring BootでJWTを使うためには依存関係の追加が必要です。JWT自体は仕様が決まっていて、規格に合わせて適切なJSONとして作成した後にBase64としてエンコードしても実現はできますが、こういうものを扱う場合はなるべくライブラリを使った方が安全ですね。\nSpring(Java)で使えるJWTのライブラリはいくつかありますが、JWT自体が標準なのでどちらを選んでも基本は同じです。ただ、ライブラリ毎にJWTの仕様にどこまで対応しているのかは違う場合があるので、公式サイトから対応しているライブラリのリストを確認してどれを使うかを選びましょう。このポストではJSON Web Token Support For The JVMを使った場合での実装方法を紹介します。\nMavenの場合は以下のように依存関係を追加します。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.jsonwebtoken\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jjwt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradleの場合の場合は以下となります。\ndependencies { implementation \u0026#39;io.jsonwebtoken:jjwt:0.9.1\u0026#39; } JWTを提供するクラスを作る 依存関係を追加したら、ログイン成功時のResponseとクライアントからのリクエスト時のHeaderに載せるためのJWTを実際に作ってくれるクラスの作成が必要となります。色々な方法があると思いますが、ここではJWTを作成して検証もしてくれるようなクラスを作ります。その後はリクエスト毎に、HeaderのJWTを検証するためのクラスを作って行きます。\nJWTの仕様 JWTを作る前に、まず簡単にJWTがどんな構造を持っているかを見ていきましょう。JWTはHeader、Payload、Signatureという三つの要素で構成されています。それぞれの要素はBase64の文字列としてエンコードして、つなげることで一つのJWTが構成されます。\nHeaderの構成 Headerではこのトークンがどのようなもので、どのアルゴリズムでエンコードされているかを表す情報を載せるようになります。\nid 意味 詳細 typ Token Type JWTのタイプ(=JWT) alg Hashing Algoritym エンコードする時のアルゴリズム Payloadの構成 Payloadはnameとvalueのペアで構成された複数の情報単位で構成されている、いわばBodyのようなもので、ここに載せる個々の情報単位たちをClaimと呼びます。今回のログインではこのClaimにユーザのIDとロール、そしてトークンが発行された期間を載せることにします。\nid 意味 詳細 jti JWT ID JWTの識別子 sub subject JWTのユニークなキー iss issuer JWTを発行者 aud audience JWTの利用者 iat issued at JWTが発行された時間 nbf not before JWTの開始時間 exp expiration time JWTの満了時間 Sigunature SignatureではHeaderとPayloadをエンコードしたあと、更に任意のシークレットキーを持ってエンコードします。これによって、サーバ側ではクライアントが送ってきたJWTをSignatureをデコードしてHeaderとPayloadを取得できるようになります。\nHeader、Payload、Signature順で正しく作成したJWTは、JWTの公式サイトから検証できます。以下のようにJWTの構造と格納しているデータを確認してデバッグができるので、興味のある方はぜひ試してみてください。\nToken Providerクラスの実装 では、JWTがどんな情報により構成されているかがわかったので、必要な情報を載せて実際のJWTを作成するクラスを作りましょう。ここでHeader、Payload、Signatureの全部を埋める必要はなく、最低限の情報だけ使うことにします。\nまずトークンを作成するメソッドを作ります。ここで作成されるJWTのPayloadに載せるClaimは、ユーザ名とロールに限定します。また、JWTが発行された時間と満了時間を設定して、期間外では使えないようにします。そして最後にカスタムシークレットキーを設定してSignatureを作成することにします。\n他には、トークンを読み込んでDBのユーザ情報を取得するメソッド、リクエストのHeaderからトークンを取得するメソッド、トークンの有効期間を検証するメソッド、トークンにのせたユーザ名(ID)を取得するメソッドなども作っておきます。(こちらは別途クラスで切っても良さそうな気はします)\n@Component public class JWTProvider { // Signatureのエンコードに使うシークレットキー private static final String TOKEN_SECRET_KEY = \u0026#34;This is secrect!\u0026#34;; // トークンの有効期間(1時間) private static final long TOKEN_VAILD_DURATION = 1000L * 60L * 60L; // ユーザ情報を取得するためのサービスクラス private final UserDetailsService service; @Autowired public JWTProvider(UserDetailService service) { this.service = service; } // UserオブジェクトからJWTを作成する public String createToken(User user) { // Claimとしてユーザ名とロールを載せる Claims claims = Jwts.claims().setSubject(user.getId()); claims.put(\u0026#34;roles\u0026#34;, user.getRoles()); // トークンの開始時間と満了時間を決める Date iat = new Date(); Date exp = new Date(start.getTime() + TOKEN_VAILD_DURATION); // JWTの作成 return Jwts.builder() .setClaims(claims) .setIssuedAt(iat) .setExpiration(exp) .signWith(SignatureAlgorithm.HS256, TOKEN_SECRET_KEY) .compact(); } // トークンからユーザ情報を取得する public Authentication getAuthentication(final String token) { final UserDetails userDetails = this.service.loadUserByUsername(this.getSubject(token)); return new UsernamePasswordAuthenticationToken(userDetails, \u0026#34;\u0026#34;, userDetails.getAuthorities()); } // リクエストのHeaderからトークンを取得する public String resolveToken(final HttpServletRequest request) { return request.getHeader(\u0026#34;X-AUTH-TOKEN\u0026#34;); } // トークンの有効期間を検証する public boolean validateToken(final String token) { try { final Jws\u0026lt;Claims\u0026gt; claims = Jwts.parser().setSigningKey(this.secretKey).parseClaimsJws(token); return !claims.getBody().getExpiration().before(new Date()); } catch (Exception e) { return false; } } // トークンからユーザ名を取得する pubic String getSubject(final String token) { return Jwts.parser().setSigningKey(this.secretKey).parseClaimsJws(token).getBody().getSubject(); } } カスタムFilterクラスの実装 次に、ログインするときにリクエストを検証するためのFilterクラスを作成します。最初ユーザがログインすると、このクラスでは先に作成したJWTProviderを使って、Headerからトークンを取得、有効期間の検証を行った後、問題なければDBからユーザ情報を取得してSpring Securityの認証情報(Authentication)としてセットするようになります。\n@Component public class JWTAuthenticationFilter extends GenericFilterBean { // トークンを検証するためのProvider private final JWTProvider provider; @Autowired public JWTAuthenticationFilter(JWTProvider provider) { this.provider = provider; } // ログインに対するフィルタリングを行う @Override public void doFilter(final ServletRequest request, final ServletResponse response, final FilterChain filterChain) throws IOException, ServletException { final String token = this.provider.resolveToken((HttpServletRequest) request); if (token != null \u0026amp;\u0026amp; this.provider.validateToken(token)) { final Authentication auth = this.provider.getAuthentication(token); SecurityContextHolder.getContext().setAuthentication(auth); } filterChain.doFilter(request, response); } } Spring SecurityにJWTでのログイン設定を追加する では、JWTを作成して検証するための準備が終わったので、これからはSpring Securityでこのクラスたちを使って認証と認可を行うための設定をする版ですね。どれが定番だとかという訳ではないですが、どちらも好みに合わせて(もしくは要件に合わせて)使えるのではないかと思い、三つの方法を用意しました。\nFormLoginを使う場合 Spring SecurityのFormLoginを利用してログインのためのURLと、ログイン後のJWT作成を全て設定しておきたい場合の実装です。クライアント側ではログインのためのクレデンシャルをPOSTのFormデータとして送るようになります。\nここではログインを処理するためのURLとクレデンシャルのためのパラメータなどを設定して、ログインに成功したらトークンを作成して返すためのAuthenticationSuccessHandlerを設定することになります。\nAuthenticationSuccessHandlerの作成 まずログインに成功した場合にトークンを返すためのSuccessHandlerを作成します。ログインに成功した場合、Authenticationクラスにユーザ情報が保存されるので、それをProviderに渡してトークンを作成してもらった後にレスポンスのHeaderに載せて返すことをやっています。\n@Component public class JWTAuthenticationSuccessHandler implements AuthenticationSuccessHandler { // トークンを作成するためのProvider final private JWTProvider provider; @Autowired public JWTAuthenticationSuccessHandler(JWTProvider provider) { this.provider = provider; } @Override public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, Authentication auth) throws IOException, ServletException { // すでにレスポンスで情報を返した場合は何もしない if (response.isCommitted()) { return; } // ログインに成功したユーザ情報を取得する User user = (User) auth.getPrincipal(); // Headerにトークンを作成して載せる response.setHeader(\u0026#34;X-AUTH-TOKEN\u0026#34;, this.provider.createToken(user)); // HTTP Statusは200 OK response.setStatus(HttpStatus.OK.value()); } } Spring Securityの設定 では、ログインに成功したときのHandlerクラスを作成したので、Spring Securityの設定を変えていきます。必要なのはloginProcessingUrl()(ログインの処理をするためのPOST通信が行われるURLとHanlderの設定で、ユーザ名のパラメータ名やパスワードのパラメタ名はデフォルトと違う場合にだけ必要となります。そしてFilterはデフォルト設定だとUsernamePasswordAuthenticationFilterが先に実行されるので、先ほど作成したJWTAuthenticationFilterを使うための設定をします。\n@Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter { // ログインが成功した場合の処理のためのHandler private final JWTAuthenticationSuccessHandler successHandler; // ログイン以降の認証認可のためのFilter private final JWTAuthenticationFilter filter; @Autowired public SecurityConfig(JWTAuthenticationSuccessHandler successHandler, JWTAuthenticationFilter filter) { this.successHandler = successHandler; this.filter = filter; } @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception { return super.authenticationManagerBean(); } @Override protected void configure(final HttpSecurity httpSecurity) throws Exception { httpSecurity .httpBasic().disable() .csrf().disable() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() // formLoginを使う .formLogin() // POSTでクレデンシャルをもらい、ログイン処理を行うURL(UserDetailsServiceを使うことになる) .loginProcessingUrl(\u0026#34;/api/v1/web/login\u0026#34;) // ログイン処理ようのURLには認証認可なしでアクセスできる .permitAll() // ユーザ名のパラメータ(デフォルトはusername) .usernameParameter(\u0026#34;id\u0026#34;) // ユーザパスワードのパラメータ(デフォルトはpassword) .passwordParameter(\u0026#34;pass\u0026#34;) // ログインに成功したら実行されるsuccessHandlerの指定 .successHandler(this.successHandler) .and() .authorizeRequests() .anyRequest().hasRole(\u0026#34;ROLE_USER\u0026#34;) .and() // デフォルトのFilter設定を変える .addFilterBefore(this.filter, UsernamePasswordAuthenticationFilter.class); } } JSONを使えるFilterを使う場合 Spring SecurityのFormLoginを使う場合、ログインのためのクレデンシャルはFormデータである必要があります。しかし、REST APIならばデータのやりとりはJSONが基本ですね。なのでクレデンシャルもJSONで送るようにしたいのですが、Spring Securityではそのようなメソッドを提供していません。\nこういう場合は、カスタムUsernamePasswordAuthenticationFilterクラスを作ってJSONをパースする必要があります。FormLoginを使う場合に比べて少し複雑になりますが、REST APIらしくやってみましょう。FromLoginでの実装ができると、こちらはFilterをもう一つ追加するだけの感覚で実装ができます。\nJsonUsernamePasswordAuthenticationFilterの作成 クレデンシャルを確認するSpringのデフォルトのFilterクラスはUsernamePasswordAuthenticationFilterというもので、これを継承したカスタムクラスを作り、そこからJSONをパースして使うようにします。以下の実装は、FormデータとJSONの両方に対応している例です。\npublic class JsonUsernamePasswordAuthenticationFilter extends UsernamePasswordAuthenticationFilter { // Headerからコンテントタイプを取得するための定数 private static final String CONTENT_TYPE = \u0026#34;Content-Type\u0026#34;; // JSONデータを保存するためのMap private Map\u0026lt;String, String\u0026gt; jsonRequest; // ユーザ名を取得する @Override protected String obtainUsername(HttpServletRequest request) { return getParameter(request, getUsernameParameter()); } // パスワードを取得する @Override protected String obtainPassword(HttpServletRequest request) { return getParameter(request, getPasswordParameter()); } // JSONもしくはFormデータのクレデンシャルを取得し、Authenticationとして載せる @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException { if (headerContentTypeIsJson(request)) { ObjectMapper mapper = new ObjectMapper(); try { this.jsonRequest = mapper.readValue(request.getReader().lines().collect(Collectors.joining()), new TypeReference\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt;() { }); } catch (IOException e) { throw new RuntimeException(e); } } String username = obtainUsername(request) != null ? obtainUsername(request) : \u0026#34;\u0026#34;; String password = obtainPassword(request) != null ? obtainPassword(request) : \u0026#34;\u0026#34;; UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(username, password); setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); } // リクエストからパラメータ(ユーザ名とパスワード)を取得する private String getParameter(HttpServletRequest request, String parameter) { if (headerContentTypeIsJson(request)) { return jsonRequest.get(parameter); } else { return request.getParameter(parameter); } } // HeaderからコンテントタイプがJSONかどうかを判定する private boolean headerContentTypeIsJson(HttpServletRequest request) { return request.getHeader(CONTENT_TYPE).equals(MediaType.APPLICATION_JSON_VALUE); } } AuthenticationSuccessHandlerの作成 ここはFormLoginの場合と同じです。詳細についてはこちらを参考にしてください。\nSpring Securityの設定 ここでは、作成したカスタムUsernamePasswordAuthenticationFilterにFilterProcessUrlと、AuthenticationManager、AuthenticationSuccessHandlerを設定して、デフォルトのフィルタ設定を変えるようになります。この設定ではFormLoginが要らなくなリます。\n@Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter { // ログインが成功した場合の処理のためのHandler private final JWTAuthenticationSuccessHandler successHandler; // ログイン以降の認証認可のためのFilter private final JWTAuthenticationFilter filter; @Autowired public SecurityConfig(JWTAuthenticationSuccessHandler successHandler, JWTAuthenticationFilter filter) { this.successHandler = successHandler; this.filter = filter; } @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception { return super.authenticationManagerBean(); } @Override protected void configure(final HttpSecurity httpSecurity) throws Exception { httpSecurity .httpBasic().disable() .csrf().disable() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() // FormLoginは使わない .formLogin().disable() .authorizeRequests() .anyRequest().hasRole(\u0026#34;ROLE_USER\u0026#34;) .and() // 認証前にJWTのFilterを設定 .addFilterBefore(this.filter, UsernamePasswordAuthenticationFilter.class) // UsernamePasswordAuthenticationFilterはカスタムクラスに代替 .addFilterAt(getJsonUsernamePasswordAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class); } // カスタムUsernamePasswordAuthenticationFilterの設定 private JsonUsernamePasswordAuthenticationFilter getJsonUsernamePasswordAuthenticationFilter() { JsonUsernamePasswordAuthenticationFilter jsonFilter = new JsonUsernamePasswordAuthenticationFilter(); try { // ログインを処理するURLの設定 jsonFilter.setFilterProcessesUrl(\u0026#34;/api/v1/web/login\u0026#34;); // AuthenticationManagerの設定 jsonFilter.setAuthenticationManager(this.authenticationManagerBean()); // AuthenticationSuccessHandlerの設定 jsonFilter.setAuthenticationSuccessHandler(this.successHandler); jsonFilter.setUsernameParameter(\u0026#34;id\u0026#34;); jsonFilter.setPasswordParameter(\u0026#34;pass\u0026#34;); } catch (Exception e) { throw new RuntimeException(e); } return jsonFilter; } } ログイン用のControllerを使う場合 他のURLをControllerで制御しているのと同じく、ログイン専用のContollerを作る場合の例です。一般的なControllerとあまり使い方は変わらないので、こちらの方がやりやすい感もしますね。\nまた、レスポンスとしてResponseEntityやカスタムクラスも使えるのでHeaderにトークンを載せるだけでなく、Bodyに何かデータを埋めて共に送る必要のある場合はこちらの方が良い選択なのかもしれません。\nControllerの作成 前述した通り、一般的なREST API用のControllerとあまり変わりないものを作ります。ログインようのURLと、それに紐づくメソッドを作り、ログイン時の認証を担当することになります。\n@RestController @RequestMapping(\u0026#34;api/v1/web\u0026#34;) public class SignApiController { // クレデンシャルを検証するためのサービスクラス private final UserService service; // トークンを作成するためのProvider private final JWTProvider provider; @Autowired public SignApiController(MemberService service, JWTProvider provider) { this.service = service; this.provider = provider; } // Formデータでクレデンシャルをもらい、認証を行う @PostMapping(\u0026#34;/login\u0026#34;) public void login(@Validated @RequestBody LoginMemberForm form, HttpServletResponse response) { // クレデンシャルからユーザ情報を取得 User user = this.service.getUser(form.getId(), form.getPassword()); // 取得した情報でトークンを作成 String token = this.provider.createToken(user); // Headerにトークンを作成して載せる response.setHeader(\u0026#34;X-AUTH-TOKEN\u0026#34;, this.provider.createToken(user)); // HTTP Statusは200 OK response.setStatus(HttpStatus.OK.value()); } } Spring Securityの設定 Controllerを使った場合は、認可認可なしでもログイン用のURLにアクセスできる設定と、Filterを使うための設定を追加します。\n@Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter { // ログイン以降の認証認可のためのFilter private final JWTAuthenticationFilter filter; @Autowired public SecurityConfig(JWTAuthenticationFilter filter) { this.provider = provider; } @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception { return super.authenticationManagerBean(); } @Override protected void configure(final HttpSecurity httpSecurity) throws Exception { httpSecurity .httpBasic().disable() .csrf().disable() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() .authorizeRequests() // ログイン処理ようのURLには認証認可なしでアクセスできる .antMatchers(\u0026#34;/api/v1/web/login/\u0026#34;).permitAll() .anyRequest().hasRole(\u0026#34;ROLE_USER\u0026#34;) .and() // デフォルトのFilter設定を変える .addFilterBefore(this.filter, UsernamePasswordAuthenticationFilter.class); } } テスト 今まで実装したログインのテストは、CURLもしくはPostmanなどのツールで簡単にできます。\n例えば、FormLoginを使った場合のログインはこちらになります。\ncurl -i -X POST \u0026#34;http://localhost:8080/api/v1/web/login\u0026#34; -d \u0026#34;id=user\u0026#34; -d \u0026#34;pass=1234\u0026#34; Postmanを使ったJSONでのログインテストはこちらになります。(X-AUTH-TOKENでJWTが帰ってきたのを確認できます!)\n最後に 思ったよりSpring Security周りの設定がいろいろと必要となり、自分の欲しがっていたレクチャはあまりなかったのでかなり苦労しましたが、これでなんとかREST APIでのJWTを使ったログインは実装できました。これだけを別途ライブラリとして作っても良いかと思いますね…\nでも、まだこれで完全な設定ができた訳ではありません。ここではSuccessHandlerのみを作成しましたが、場合によってはログインに失敗した場合のAuthenticationFailureHandlerが必要になる可能性もあります。また、これはあくまでログインに関するポストなので扱ってはなかったのですが、認証認可できてないURLへのアクセスに対するException Handlingも必要です。また、JWTを使った認証の場合、クライアントがトークンを持ってしまうのでサーバ側からログアウトを制御できないという点があり、クライアント側の実装ではそこに対しての対策も考えなければなりません。\nが、今回はとりあえず最小限の目標は達成できたということで、いったんここまでとなります。では、また！\n","date":"2020-06-10T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-rest-api-login-with-jwt/","title":"JWTによるREST APIのログインを実現する"},{"content":"新しい案件が始まって、AngularとSpring bootによるSPA(Single Page Application)を作ることとなりましたが、まだ要件定義の段階で実装まではしばらく時間が残っています。いわゆる上流工程にはあまり経験がないので日々奮闘中ではありますが、仕事が終わった後の時間には少しづつ、練習をかねて自作アプリケーションを作っています。\nSpring BootでREST APIを作ることにはある程度慣れてはいるものの、まだ認証/認可に対してあまり知識がないので、自作アプリではわざとSpring Securityを入れることにしました。しかし、Spring Securityの記事をいくつか読んでロールによってアクセスできるURLに制限をかけたりしているのだなと理解した後に、いざ自作アプリケーションに導入してみようとしたら、問題ができました。いつも通りREST APIで実装していたのですが、自分が参考にしていたSpring Securityのコードはほぼ昔ながらのSpring MVCパターンのためのものだったのです。\nなので、今回のポストはSpring MVCパターンとREST APIのログインするため必要となるのが何かについて簡単に書いてみようと思います。(実際のログインの方法は今後のポストを予定しています)``\nコントローラの実装 まずMVCパターンとREST APIのコントローラをどのように実装するかからみていきましょう。\nMVCパターンの場合 今も多くの研修で行われているSpringの研修は、普通のMVCパターンとJSPによるものが多いのではないか、と思います。例えば@Controllerアノテーションをつけたコントローラクラスに、@RequestMappingアノテーションをつけたメソッドを書いて、そのメソッドではModelもしくはModelAndViewなどのクラスを使ってデータやビュー(主にJSPファイルのパス)を書いていくような形ですね。実際はJSPを使う=MVCパターンというわけでもないですが、多くのSpring MVCパターンのプロジェクトがJSPを使っているのでまずはこのようなレガシーなものをMVCパターンと呼ばせていただきます。\nただ文章で表現するよりは、コードを持って書いた方がわかりやすいと思います。例えば以下のようなものです。/homeというURLに接続すると、サーバの時間を返す簡単な例題ですね。ModelAndViewを使うとしても、Modelにデータとビューを共につめてるだけでやっていることとしてはあまり変わらないと思います。\n@Controller public class HomeController { // homeというJSPファイルとサーバの時間を表示する @RequestMapping(value = \u0026#34;/home\u0026#34;, method = RequestMethod.GET) public String home(Model model) { Date date = new Date(); model.addAttribute(\u0026#34;serverTime\u0026#34;, date); return \u0026#34;home\u0026#34;; } } REST APIの場合 自分の場合も、やはり最初はSpring MVCパターンとJSPから学んだのでこのような書き方に慣れていましたが、入社後にはSpring BootとREST APIというものに出会い、コードの書き方が少し変わりました。最近のトレンドだとやはりJSPよりもAngular/React/VueなどのJavaScriptフレームワークを使って作成する場合が多いので、JSONの形としてデータだけを返したらよくなります。JSONとしてデータをレスポンスボディに載せるには、普通にDTOクラスをビューモデルとして作ります。\n@Data public class HomeViewModel implements Serializble { // サーバの時間を表示する public Date serverTime; } あとは、@RestControllerアノテーションをつけたコントローラクラスを作り、作成したDTOクラスを返すメソッドを作成するだけです。少しアノテーションの種類や使い方も少し変わりましたが、内容としてはビューを担当しているJSPファイルの指定が消えただけで返しているデータは同じです。\n@RestController @RequestMapping(\u0026#34;/api/v1\u0026#34;) public class HomeApiController { // カスタムビューモデルにデータをつめて渡す @GetMapping(\u0026#34;/home\u0026#34;) public HomeViewModel getHome() { HomeViewModel model = new HomeViewModel(); model.setServerTime(new Date()); return model; } } そしてフロントエンドのJavaScriptフレームワークでは、レスポンスのボディから取得して画面に表示するようになりますね。このようにREST APIだと、サーバサイドではあくまでデータモデルとビジネスロジックだけを考えれば良いようになっていて、よりそれぞれの役割の分担がよくなっていますね。\nログインをするには では、とりあえず簡単にSpring MVCパターンとREST APIの比較をしてみたので本題に戻りましょう。先にSpring Securityの話をしましたが、実際はSpring Securityを導入しなくても同じ話になると思います。REST APIはアーキテクチャの一つにすぎないので、フレームワークを導入できるかどうかの問題はすでに別の話になってしまいます。また、ログインというのはSpring Security以前の問題です。なので今回はSpring Securityの話はさておき、二つのアーキテクチャでどのようにログインをするかについて話したいと思います。\nMVCパターン MVCパターンでは、Sessionでログインを実現する場合が多い(と思い)ます。例えばログインを担当するメソッドを作成して、引数としてHttpServletRequestを指定すると、そこからセッションを取得することができますね。あとは、またの引数としてFormのデータ(IDとパスワード)を受けて、これは自分が最初に学んだ方式でもあります。\nまずユーザの観点から話してみましょう。普通のWebアプリケーションでは、画面のほうでログインのためにIDとパスワードを入力するようになります。JSPではそのデータをformとして受け取り、POSTとしてコントローラに送りますね。そうしたらコントローラではIDとパスワードをServiceクラスに送って検証してもらい、問題なかったらログインした情報をSessionに載せます。こういうったシナリオで、コントローラにログインのためのメソッドを作るとおそらく以下のコードのような形のなるはずです。(実際はユーザのIDだけ載せる場合はないので参考までに)\n// URLは/login、メソッドはPOST @RequestMapping(value = \u0026#34;/login\u0026#34;, method = RequestMethod.POST) public String login(User user, HttpServletRequest request) { // Serviceからユーザを取得する User loginedUser = service.getUser(user.getId(), user.getPassword()); // リクエストからセッションを取得する HttpSession session = request.getSession(); // SessionにユーザIDを載せる session.setAttribute(\u0026#34;userId\u0026#34;, loginedUser.getId()); return \u0026#34;/\u0026#34;; } これでログインができたら、あとは他のメソッドでSessionの検証をして、ログインされているかどうかを判断することになります。ログアウトする場合は、Sessionを破棄すればいいです。例えば以下のようになりましょう。\n// URLは/logout、メソッドはGet @RequestMapping(value = \u0026#34;/logout\u0026#34;, method = RequestMethod.GET) public String logout(HttpServletRequest request) { // リクエストからセッションを取得する HttpSession session = request.getSession(); // Sessionを破棄する session.invalidate(); return \u0026#34;/\u0026#34;; } Spring Securityを使った場合はかなりコードが変わってきますが、Sessionを使う場合にこう言った基本的なフローは変わりませんね。これでMVCパターンでの認証/認可は実現できます。\nここで問題が しかし、Sessionを使った方法はREST APIだと使えなくなります。なぜかというと、REST APIの最大の特徴の一つはStateless、つまり状態を持たないというのであるからです。ここでいう状態というのはサーバに取ってのクライアントのステータスのことです。今まで通りだとクライアントはサーバに接続した瞬間からサーバに自分の状態を管理させる必要がありました。こういう状態を管理するためのものがSessionであり、先に説明した通りだとREST APIには適用すべきではないですね。(無理やり適用させるとしたら不可能でもないですが…)\nそしてStatelessである故に、REST APIではクライアントのリクエスト毎にクライアントから必要な全ての情報をサーバに送るようにしています。ここから推論できるのはSessionでログインの情報を載せておくのではなく、クライアントからリクエスト毎に自分はログインしているということを証明するデータを何かの形で送る必要があるということになります。\nではどうしたら？ クライアントがリクエスト毎に、本質的なデータのみでなく、認証のためのデータを送るための手段は、すでにHTTPの中にありました。つまり、Headerです。クライアントのリクエストには操作のためのデータがBodyとして入っていて、そこに認証のための情報をHeaderとして載せればいいですね。\nクライアントがHeaderに認証のための情報を載せるには、まずサーバサイドから認証をしてもらわねばなりません。この認証の詳細については今後のポストで話すことにして、まずはHeaderに載せるデータが完成されたと仮定し、これをサーバのレスポンスでどう返すかについて述べます。\nSpring BootでレスポンスのHeaderにデータを載せるためには二つの方法があります。先に述べたSpring MVCのリクエスト同様、レスポンスを扱うためのクラスであるHttpServletResponseを使う方法と、ResponseEntityでレスポンスデータをラッピングすることです。どれも使える方法ですが、\nHttpServletResponseでHeaderを追加する まずはHttpServletResponseです。シンプルに、「レスポンスにデータを載せたい」という時に使える方法ですね。MVCパターンでのログインと同じく、ログインするメソッドの引数に指定すると、レスポンスのHeaderにデータを載せることができます。\n@PostMapping(\u0026#34;/login\u0026#34;) public User login(User user, HttpServletResponse response) { // Serviceからユーザを取得する User loginedUser = service.getUser(user.getId(), user.getPassword()); // レスポンスのHeaderにユーザのIDを載せる response.addHeader(\u0026#34;userId\u0026#34;, loginedUser.getId()); // Bodyを返す(Headerは自動で含まれる) return loginedMember; } ResponseEntityでHeaderを追加する もう一つの方法であるResponseEntityの場合は、引数はユーザ情報(IDとパスワード)のみで良くなります。名前からもわかると思いますが、ResponseEntityはBodyとHeaderを含め、HTTP Status(200 OKなど)を含めレスポンスに必要な情報は全て扱えるクラスです。使い方は簡単で、ログインメソッドの戻り値となるBodyをResponseEntityで包み、Headerなどの情報も一緒に詰めて返します。\n@PostMapping(\u0026#34;/login\u0026#34;) public ResponseEntity\u0026lt;Member\u0026gt; login(User user) { User loginedUser = service.getUser(user.getId(), user.getPassword()); // レスポンスとしてHeaderとBodyを一緒にセットして返す return ResponseEntity.ok().header(\u0026#34;userId\u0026#34;, loginedUser.getId()).body(loginedUser); } そのあとは？ レスポンスのHeaderに載せた情報は、フロントエンド側でリクエスト毎にHeaderに載せて送ることになります。しかし、リクエスト毎に送るためにはどこかにこの情報を保存する必要がありますね。Cookieを使う方法もありますが、セキュリティ情のリスクがあるため多くの場合ではブラウザのローカルストレージに保存するようになっているようです。これはフロントエンドの領域なので、ここでは深掘りしません。\nまた、ログアウトの場合はどうなるか、という問題もありますが、これは様々な方法で実現しているようです。例えばフロントエンドからできる方法としては、Headerに載せる認証情報を削除したり、間違った情報を送るようにするような方法があります。そしてサーバサイドでは、一定時間がすぎると認証情報を使えなくするなどの対策があります。\n最後に 本当は、REST APIでのログインに関してはまだ考えなければならないことは他にもあります。例えば、HTTP Headerに認証のため必要となる情報を載せるということは理解できても、実際はどのようなデータを載せればいいか、そのデータはどう作るか、リクエスト毎にHeaderで認証情報を扱うということはセキュリティやリソースの面で大丈夫かなど。\nでも、とにかくこれでREST APIでログインするための仕組みの一つはわかりました。次回は、このHeaderとSpring Securityを使ってのログインを実装する方法を述べながら、以上の問題についても扱いたいなと思っています。では、また！\n","date":"2020-05-30T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-rest-api-how-to-login/","title":"REST APIでのログインのためには"},{"content":"このたびは、新しい案件でAngularとSpring BootによるWebアプリケーションの開発を担当することになりました。Spring Bootは今までずっとやってきているのであまり問題はないと思いますが、問題はAngularです。Angularはバージョン2からTypeScriptを使うことになっているらしく、JavaScriptとjQueryを少し触ったくらいの経験しかないのでそもそもTypeScriptがどんなものかすら知らなかったです。ただわかっているのは、JavaやC#みたいに静的言語みたくコードを書いたら、それをいい感じにコンパイルしてJavaScriptに変えてくれる、ということだけでした。\nなのでまずTypeScriptの勉強を始めましたが、言語そのものの特徴に含め、TypeScriptで何かを作る時に必要となる知識は意外と大きいことに気付きました。今回のポストでは正直な感想と、どんなものが必要なのかをJavaしか知らない初心者である自分の観点から述べていきたいと思います。\n驚くほどJavaに似ている 元々C/C++、C#、JavaといったいわゆるC-family言語と似ているとの話は聞いていましたが、実際TypeScriptを触ってみると本当にそうでした。最初、なんの情報もなかった時(型を指定できて、コンパイルしたらJavaScriptに変わるという話だけ聞いている時)、ただ普通の変数の宣言に型指定ができるようになっているだけかと思っていたのですが、それだけでなく意外とオブジェクト指向に合わせて発展、もしくは回帰している印象を受けました。\n型指定はいい感じ 型の指定は、元々動的言語であるJavaScriptを昔ながらの静的言語化するものですね。でも、書き方自体は昔のままでなく、KotlinやSwiftのようなモダンな言語に似ています。型をまず書くのではなく、変数として宣言したあとに型をつける1のですね。例えば以下のようになります。\nJavaでの型指定\nint a = 1000; TypeScriptの型指定\nvar a: number = 1000; また、こういう型指定を、TypeScriptでは「型注釈」とも呼ぶらしいです。TypeScriptはコンパイルされたらランタイムで型が決まるJavaScriptになるので、コンパイラーのために注釈をつけてあげる、という意味に近いようです。\n型以外の類似点 TypeScriptには型指定だけでなく、C#やJavaプログラマーならすでに慣れているAccess修飾子、Interface、Class、Generics、Decorator(Annotation)なども用意されています。これらの一部は最近JavaScriptでも対応している機能ではありますが、実際のアプリを開発する業務ではブラウザの制約から最新のJavaScriptを使えない場合もあり不便ですね。しかし、TypeScriptを使うとコンパイラのオプションを指定することでどのバージョンのJavaScriptとしてソースコードを出力するか選択できるので、ブラウザをあまり気にせず同じコードを書けます。これは本当にありがたいですね。\n他に、依存関係をnpmといったパッケージマネージャで管理できるというのも良いところ。これはJavaScriptでもできる(というか、そちらが先ですが)ことですが、Importと組み合わせたらほぼC-familyと似たような感覚で運用できるという面が良いですね。JavaScriptのモジュールというのも、またES6から対応しているのでそれ以前のバージョンを使う場合だとかなり面倒くなるかも知れない、と思いました。知識不足なだけかもしれませんが…\n気になったところ TypeScriptの良いところは、やはり昔ながらの言語に慣れている人にはかなり快適にコードをかけるような環境を提供していて、コンパイルタイムでエラーを探せるという、その何ふさわしいものとなっている点だと思います。ただ、少し気になったところもあったのでそれについても書きます。\n結局はJavaScript TypeScriptはJavaScriptのSuper setなので、JavaScriptの機能をそのまま利用できるというのが特徴だとも言われています。最初はそれをメリットとして捉えていましたが、勉強をしながらやはりデメリットもあるんだなと感じました。JavaScriptをそのままかけるということは、結局言い方を変えると、ソースコードの中にJavaScript(Vanilla)とTypeScriptが混在していても問題なくなるということですね。こうなった場合にTypeScriptがSuper setという特徴を諦めないかぎりコードが動かなくなることはないと思いますが、人の観点からするとかなり混乱するコードが生まれる可能性もあるのではないかと思いますね。\nJavaScriptでも書けるということの意味 そもそもJavaScriptの歴史の方が長く、ユーザも絶対的にJavaScriptの方が多いです。そして、私みたいにC-family言語ではなくJavaScriptからプログラミングに入門した人もいるので、そのような人にとってTypeScriptはあえて使うメリットのない、ただの不便なものにしか思われない可能性もあると思います。また、JavaScriptの方に慣れている人がTypeScriptに移ったとしても、結局はJavaScriptと変わらない書き方になる可能性もなくはないはずですね。\nTypeScriptがSuper setとして企画された理由は、既存のJavaScriptプログラマーを吸収するための政策だったと思いますが、プログラミング言語は自由が多いほど混乱しやすいと思うので、これはやはりメリットでもデメリットでもあると思いました。\nprototype クラス基盤のオブジェクト指向言語から入門しているからか、自分にはJavaScriptのprototypeという概念があまりわかってないです。ともかくJavaScriptにはこのprototypeというのがあって、オブジェクトを変数として使うこともできるという特徴を持っていますね2。Javaプログラマーとしてはオブジェクトはクラス、クラスはファイル一つ、という風に考えることが一般的なので、こういう自由度に触れるとどうしたらいいか混乱しそうな感覚です。そしてもちろん、これはTypeScriptよりもJavaScriptの特徴ではありますが、TypeScriptがSuper setである以上避けられない問題なのですね。\nModule モジュールがJavaScriptに導入されて、TypeScriptでももちろんそのまま使えます。個別のファイルにクラスを一つづつ作成して、インポートして使っているとまるでJavaと大差ないようにも思われますね。でもこのモジュールというものも実際はややこしいところがあるようで、書き方をどうするか考えなくてはならない場合もあるようです。\n例えばPythonのようにasを入れることでインポートしたモジュールに別名をつけることができるけど、そうしたら問題が怒るケースもあるとか、CORS設定で塞がる場合もあるとかというのが最初はかなり複雑だなと感じました。まだ深堀りしてないので実際は使ってみないとわからないと思いますが、そもそも使ってみないとわからないという部分はやはり気になりますね。\n// 二つの書き方ができるが、一緒ではない import express from \u0026#34;express\u0026#34;; import * as express from \u0026#34;express\u0026#34;; コンパイル TypeScriptをインストールすると、ターミナルからtscというコマンドを使えるようになります。そしてTypeScriptで作成したファイルは.tsというファイルに書いて、tscコマンドで.jsに変わりますね。tsconfig.jsonファイルを定義することでどうコンパイルするか様々なオプションを指定することもできます。例えばJavaScriptにコンパイルする時、どんなバージョンでコンパイルするか、どのファイルをコンパイルするか、.jsファイルはどこに吐き出すかなど。\nただ、コンパイルにかなり時間がかかるのもあり、コンパイルされたファイルは結局JavaScriptになります3。そしてこれは、TypeScriptで作成しても「ランタイムでは弱タイプになる」ということを意味しますね。ちゃんとタイプを明示して(anyを使わず)コードを作成しただけでは不十分なケースが十分あり得るので、ランタイムでも型による問題が起こらないようにする必要性があるかも知れないという印象を受けました。\n最近はnpmの代替ランタイムとしてTypeScriptをそのまま使えるDenoのようなものも登場していますが、やはり内部的にはtscを使っているらしく、このコンパイル速度が遅いのが最大の問題として挙げられていました。Rustで新しいコンパイラを作成しているとのことですが、それがいつ完成されるかもわからないですね。\nそしてtscのみでなく、webpackを使う場合はts-loaderを使ったりするとかなり初期設定が複雑な印象を受けました。他にBabelなどを使う場合もあるらしいですが、コンパイルをするためという理由でこのようなベンドラやコンパイラをまた勉強しなくてはならないというのは少し不便な部分ですね。JavaScriptと紐付けられている言語としての運命みたいなものかもしれませんが…\nすっきりしたところ JavaScriptライブラリを使える 「TypeScriptではJavaScriptのライブラリも使える」という話も聞きましたが、実際それがどうやって可能になったのかが疑問でした。例えばTypeScriptにも型推論があってある程度、型を宣言しなくても良い場合はあるものの、基本的には型注釈で明示的に型を指定してあげないとうまくコンパイルできない部分もありますね。しかし、すでに存在しているJavaScriptのライブラリ全てがTypeScriptを考慮して作られているとは考えられないので、これをTypeScriptではどのような形で対応しているかが一番の疑問でした。\n答えは意外と簡単で、.d.tsという形で「型定義ファイル」を作成すると良い、ということでした。すでにTypeScriptに対応しているライブラリの場合もこの型定義ファイルによってTypeScriptが型を参照できるようにしていて、自作することも可能。そしてメジャーなものの場合、node_modulesにインストールできる形で型定義ファイルを提供している場合もありました。\nすでに作成されている型定義ファイルのインストールも簡単で、例えばnodeのTypeScript型定義が必要な場合はnpm install --save-dev @types/nodeのようにコマンド一つでインストールができて、GitHubのリポジトリを参照するとどんなライブラリの型定義ファイルが提供されているか確認することもできました。\n最後に まだ色々気になるところはありますが、多くはTypeScriptそのものというよりもJavaScriptの特性から起因しているものが多いため、現時点でフロントエンドエンジニアがどの言語を選択すべきかというと、やはりTypeScriptの方が良いのではないかと思いました。もちろん、これは自分のスタートがJavaであることもあるとは思いますが、やはりコンパイルタイムで多くの問題を事前にキャッチできるというメリットは捨てがたいものだと思っています。\n現在はJavaScriptとの互換性のために色々初期設定など煩雑だったりコンパイルに時間がかかるなどの問題もいくつかありますが、これらもAngularみたいに最初からTypeScriptを前提に作られるフレームワークやライブラリが増えたり、ブラウザからTypeScriptに対応したり(これは可能性が低いかな…)したら自然に解決される問題ではないかとも思いますね。\n最初は単純にAngularを使うために勉強したものの、意外としっかりしていて、これからの未来もかなり明るい感じだったのでエンジニアの皆さんにもぜひお勧めしたいと思っているところです。\nああ、Pythonも型指定できるようになるといいな…4\nもちろん、最近はC#もJavaもvarで変数を宣言することもできますが、習慣もあり、元々静的言語では型から書いた方が良いのではないかと個人的には思っています。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nしかも、ES6で登場したClassは結局プロトタイプ基盤のfunctionの書き方をちょっと変えただけなのですね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJavaのようにバイトコードに変わったり、Cのように機械語に変わるようなものとは少し違うのでTypeScriptのコンパイルはトランスパイルとも呼ぶらしいです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n知識不足でした！Pythonも3.6から型の宣言ができるようになっています。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-05-16T00:00:00Z","image":"https://retheviper.github.io/images/typescript.jpg","permalink":"https://retheviper.github.io/posts/typescript-first-impression/","title":"JavaプログラマーがみたTypeScript"},{"content":"この度は、ブログのテーマを新しく変えてみました。新しいテーマはNexTです。一年がたってもあまりJekyllに慣れてないので思ったより苦労していますが(特に消えたページなどが気になります…)、それでも前のテーマと比べてかなりすっきりした感じが良いと思います。検索機能もついていたのでとりあえずアクティベートしてみましたが、ローカルでは機能してなかったのでGithub Pagesにあげてからテストですね。なるべく使いたい。(追記：残念ながら何かのバグのようで使えませんでした…)\nこのブログを作ってからも1年が立ちます。最初のポストが1年前だったのですね。あの頃は入社したばかりの新人だったのでいろいろとわからない物だらけでした。なので書いていたポストの内容もいまいちな物が多かったと思います。最近はJavaを使っていろいろと開発はしているものの、あまり経歴が長いわけでもないので当時書いたポストをみているとなぜこのような恥ずかしいものを書いたんだろう、とも思います。\nそれでもブログの更新を続けているのは、このブログが自分の勉強の記録であって、過去の自分が書いたコードや文面を振り返ってみることで新しい発見ができていたからです。時々過去のポストを振り返ってみることで同じことをしてもより良い方法、より安全な方法を探すことができました。修正したいものもかなり多いのですが、のちにそれを補えるようなポストを書いた方が自分のエンジニアとしての軌跡をちゃんと追える手段となるのではないかと思います。\n誰かに見せるにはかなり恥ずかしいブログですが、それでも訪問してくださる方々も増えました。おそらく私みたいに、何らかの問題と遭遇していたり他の誰かはどんな解決策を見つけたんだろうという好奇心から検索してみたりしている方々ではないかと思います。自分のコードや知識も正解とは言い切れないものなので、このブログを参考にしてくださいとおすすめはできませんが、エンジニアの仲間の一人として、こういう答えを探し出していたのでだなと受け止められれば幸いです。\nでは、また何か勉強ができたら、ポストにてお伺いしましょう。これからもよろしくお願いします。\n","date":"2020-05-03T00:00:00Z","image":"https://retheviper.github.io/images/tools.jpg","permalink":"https://retheviper.github.io/posts/renew-blog-theme/","title":"ブログ刷新"},{"content":"アノテーションは普通のJavaでも使えるもので、様々なライブラリやフレームワークでは積極活用していますね。中でも、最もアノテーションを有効活用しているのはSpringではないかと思います。DIをするためでも、クラスの位置付けにも、なんでもアノテーションをつけることで簡単に定義できるようにしているからです。なのでSpringでWebアプリケーションを実装するときはどんなアノテーションがあるのかを調べてみるのが重要と思います。\nなぜこのような話をするかというと、実は前回のポストでServiceを切り替える方法について説明しましたが、他に方法がないか探していたところ、Springならアノテーションでも条件によってBeanやConfigurationを登録する方法があるということをわかったからです。\nまた、その条件によって様々なアノテーションが存在していたため、それらの勉強をかねて、今回は様々な条件と場合を想定して作られたSpringのアノテーションについて紹介したいと思います。これらのアノテーションはSpringのauto-configurationに属するものらしいです。Spring Bootではすでに多くの設定が自動化されていますが、これをカスタムして使えるようにアノテーションを提供しているということですね。\n※これらを全部試してみたわけではないですが、とりあえずはご紹介まで。\n条件でDIする 以前のポストで紹介したコートから始めます。以下のようなことをすれば、application.ymlに記載した値によりどのリポジトリをDIするか決定できるという話をしました。\n@Configuration public class SomeServiceConfig { // YAMLに設定した値を読み込む @Value(\u0026#34;${settings.TestMode:false}\u0026#34;) private boolean testMode; // YAMLの設定からどのImplクラスを使うかを決定してBean登録 @Bean public SomeService someService(SomeRepository repository, SomeTestRepository testRepository) { return this.testMode ? new SomeTestSerivce(testRepository) : new SomeServiceImple(repository); } } 同じようなことを、これから紹介するSpringのアノテーションで実装するとしたら、以下のように変わります。\n@Configuration public class SomeServiceConfig { // 本番用のサービスクラス @Bean @ConditionalOnProperty(value = \u0026#34;settings.Testmode\u0026#34;, havingValue = \u0026#34;false\u0026#34;) public SomeService someService(SomeRepository repository) { return new SomeServiceImple(repository); } // テスト用のサービスクラス @Bean @ConditionalOnProperty(value = \u0026#34;settings.Testmode\u0026#34;, havingValue = \u0026#34;true\u0026#34;) public SomeService someTestService(SomeTestRepository testRepository) { return new SomeTestService(); } } 以上のコードからわかるように、@ConditionalOnPropertyというアノテーションを使うと、とある条件によりメソッドの内容が実行されるようにすることができるようになります。わざわざ分岐を書いたり、カスタムな@Valueアノテーションを用意するよりかなりすっきりしたコードになりますね。\nまた、このアノテーションはBeanアノテーションとだけ組み合わせができるわけでもないです。他のSpringのクラスアノテーション(Configuration、Component、Service、Repository、Controller)にも使えるので、より自由度が高いですね。\n他にもSpring Bootのorg.springframework.boot.autoconfigure.conditionパッケージにはには@Conditional...といったアノテーションがいくつか用意されていて、これらを簡単に紹介したいと思います。\n定義済みのConditionalアノテーション ConditionalOnProperty application.ymlなど、プロパティを書いたファイルやシステムプロパティに、指定したアノテーションがある場合実行されるアノテーションです。Spring Bootアプリケーションで最も一般的に使われるものらしいですね。括弧の中にはプロパティ名と値、そしてそのプロパティが存在しない場合も実行するかどうかを指定できます。\n以下のコードは、use.testmodeというプロパティが存在していて、かつtrueの場合に実行されるConfigurationクラスの例です。matchIfMissingをtrueに指定すると、use.testmodeというプロパティが存在しなくても実行されるようになります。もちろん指定しなかった場合のデフォルト値はfalseとなります。\n@Configuration @ConditionalOnProperty(value = \u0026#34;use.testmode\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = true) public class TestConfig { // テストモードで使うConfiguration } ConditionalOnExpression プロパティの記述方法による実行というアノテーションです。括弧で条件を指定できます。ここでいう条件(表現式)は、Valueアノテーションなどでも使われるSpELを使います。application.ymlなどに記載したプロパティが、括弧の中の条件と一致する場合に実行されますね。以下のコードはuse.testmodeとuse.submodeのどちらもtrueの場合に実行される例です。\n@Configuration @ConditionalOnExpression(\u0026#34;${use.testmode:true} and ${use.submode:true}\u0026#34;) public class TestSubConfig { // テストモード及びサブモードの両方がtrueの場合に使うConfiguration } ConditionalOnBean 指定したBeanが存在する場合に実行というアノテーションです。括弧の中にBeanとして登録されているかどうか判定したいクラスを指定するだけで使えます。ConditionalOnPropertyでとあるBeanが登録されたら、それに合わせて必要なサブモジュール的なものも登録したい、といった場合に使えるのかなと思います。以下のコードは、TestRepositoryというクラスがBeanとして登録されている場合に実行される例です。\n@Configuration @ConditionalOnBean(TestRepository.class) public class TestBeanConfig { // テスト用のBeanが登録された場合に使うConfiguration } ConditionalOnMissingBean ConditionalOnBeanとは逆のものです。こちらは、指定したクラスがBeanとして登録されてない場合に実行というものとなります。以下のコードは、RepositoryがBeanとして登録されてない場合は自動的にTestRepositoryをBeanとして登録する例です。\n@Configuration public class AlternativeConfigutration { @Bean @ConditionalOnMissingBean public Repository testRepository() { return new TestRepository(); } } ConditionalOnResource リソースファイルが存在する場合に実行するというアノテーションです。例えばLogBackを使う場合、xmlファイルが必要となりますが、そのxmlファイルが存在する場合はConfigurationも実行するという構成にしたい場合に使えますね。以下のコードは、そのような場合の例です。\n@Configuration @ConditionalOnResource(resources = \u0026#34;/logback.xml\u0026#34;) public class LogbackConfig { // リソースフォルダにlogback.xmlが存在する場合に使うConfiguration } ConditionalOnClass 指定したクラスが存在する場合に実行するというアノテーションです。ConditionalOnBeanと似ていますが、こちらはBeanではなくても良いという違いがありますね。例えば依存関係でとあるライブラリがあるかどうかで使えると思います。以下のコードはcom.custom.library.moduleパッケージのSomeClassというクラスが存在する場合に実行される例です。\n@Configuration @ConditionalOnClass(name = \u0026#34;com.custom.library.module.SomeClass\u0026#34;) public class CustomLibraryConfig { // カスタムライブラリのSomeClassがある場合使うConfiguration } ConditionalOnMissingClass ConditionalOnClassの逆の場合のアノテーションです。こちらはConditionalOnMissingBeanと似ていますね。同じく、指定するクラスはBeanでなくても良いです。以下のコードは、上のConditionalOnClassの逆の場合の例です。\n@Configuration @ConditionalOnMissingClass(name = \u0026#34;com.custom.library.module.SomeClass\u0026#34;) public class NoCustomLibraryConfig { // カスタムライブラリのSomeClassがない場合使うConfiguration } ConditionalOnJava アプリケーションがJavaのどのバージョンで実行されているかのによるアノテーションです。JavaのバージョンによってAPIの仕様が変わる場合があるので、複数の環境でアプリケーションを実行する必要がある場合は使うことを考えられますね。以下のコードは、Javaのバージョンが1.8の場合の例です。\n@Configuration @ConditionalOnJava(JavaVersion.EIGHT) public class JavaEightConfig { // Javaのバージョンが1.8の場合に使うConfiguration } カスタムCondition Conditionインタフェースを実装することで、カスタムConditionを作ることもできます。例えば以下のコードのように、アプリケーションが実行されるOSがLinuxの場合のConditionを自作することができます。\n使い方は簡単で、戻り値がbooleanであるmatchesを実装するだけです。\nclass OnUnixCondition implements Condition { // OSがLinuxかどうかを判定するConditionとなる @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { return SystemUtils.IS_OS_LINUX; } } 実装したConditionは、Conditionalアノテーションに指定して使います。\n@Configuration public class OsConfig { // LinuxではBeanが登録される @Bean @Conditional(OnUnixCondition.class) public UnixBean unixBean() { return new UnixBean(); } } AnyNestConditionクラスを継承すると、より複雑な条件の指定ができます。上で実装したOnUnixCondition以外でも、Windowsで実行されているかどうかを判定するOnWindwosConditionクラスを実装したとしましょう。そういった場合は、以下のように実装することができます。\npublic class OnWindowsOrUnixCondition extends AnyNestedCondition { OnWindowsOrUnixCondition() { super(ConfigurationPhase.REGISTER_BEAN); } @Conditional(OnWindowsCondition.class) public static class OnWindows { // Windowsの場合 } @Conditional(OnUnixCondition.class) public static class OnUnix { // Linuxの場合 } } 実装したConditionは、また同じ方法でConditionalアノテーションに指定します。\n@Configuration public class OsConfig { // WindowsかLinuxのどちらだとBeanが登録される @Bean @Conditional(OnWindowsOrUnixCondition.class) public WindowsOrUnixBean windowsOrUnixBean() { return new WindowsOrUnixBean(); } } 最後に こちらで紹介したアノテーション以外でも、org.springframework.boot.autoconfigure.conditionのパッケージの下には様々なクラスが存在しています。例えばWebアプリケーションかどうか、クラウドプラットフォームかどうかのアノテーションが用意されていて、のちにまた様々な条件が追加される可能性もありますね。\nこれらConditionalアノテーション群は、Spring BootのAuto Configurationでも使われているものらしいです。あので以前私が紹介したように自分でプロパティを直接読み込み、if文を書くよりは安定的な方法であると思います。また、様々な条件に対応するアノテーションがそれぞれ存在していて、カスタムCondtionを作ることで共通化できる部分もあると思うので、いろいろ便利ですね。\nJavaそのものもそうですが、Springの世界もまだいろいろと奥深いと感じました。これからも勉強ですね。\n","date":"2020-04-26T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-conditional/","title":"条件で動作するアノテーションを使う"},{"content":"個人的には、関数型プログラミングにあまり詳しくはないですが、Java 1.8のStream APIは好んで使っています。他にもLambdaやOptionalといったAPIも好きですが、自分がJavaの資格を取った理由も、このStream APIについてもっと勉強したかったからと言っても過言ではないです。\nそんな私ですが、Streamについて勉強している中、疑問が出来ました。Streamは確かにいいAPIですが、伝統的なJavaのAPIとはかなり違うものです。これをJavaに導入したことで得られるメリットがあるから導入されたはず、というのは難しくない推論ですが、逆の場合はどうでしょうか？Streamを使った場合のデメリットは？そして自分が使っているStreamの書き方は正しいのか？などなど。\n今回のポストでは、そのような疑問について独自に調査したことを述べていきたいと思います。正解というより、こういう見解があるということとしてご理解ください。\nStreamは万能か？ まず最初の質問です。Streamは万能か？つまり、既存のコードをすべてStreamに書き換えても問題はないか？そしてなるべくこれから書くコードはStreamに変えるべきなのか？という質問ですね。確かに新しいAPIが出て、既存のコードと同じ役割ができるとしたら、それにはなんらかの理由があります。Javaの場合は、NIOがそうでした。一般的なI/OではOSのカーネルの機能を利用できなかったため、それを改善するために登場しましたね。しかし、NIOもまた、全ての場面で既存のI/Oより優れているとは言えない面がありました。そしたらStreamの場合も、その可能性はあると思いました。\n結論からいうと、「全てのコードをStreamに書き換える必要はない」です。その理由を、一つ一つ項目別に説明します。\n性能は劣る場合も Java 1.5では、伝統的なFor文意外にも、いわゆる拡張For文というものが登場しました。そして1.8では、Streamと共にforEach()というメソッドもできましたね。しかし、forEach()もStreamも性能は拡張For文より劣ります。また、とあるベンチマークでは、Streamを使った場合の処理はParallelを使っても拡張For文より性能は劣るという結果が出たらしいです。理由は簡単です。Streamを使うと、より複雑な処理が中に入るからです。特に配列をStreamに変換する時はラッピングが入るので、そこでもう処理が加えられるということを考えられますね。\n特に、オブジェクトを扱う場合の性能の差はそんなに大きくないものの、プリミティブ型を扱う場合は性能の差がより大きいらしいです。なので無理やり配列をStreamに変えて処理をする必要はありません。StreamやforEach()は、それを持って安定したコードを書けられる場合に限定して使う必要があります。そしてStreamを使う場合もプリミティブ型を扱う場合はIntStreamやToIntStreamといった、それぞれの型に合わせたクラスを使った方がより良い性能を出すので、そこもちゃんと考慮すべきですね。\nJVMが長い間伝統的なFor文に最適化されてきて、1.8になってやっと登場したStreamはそれほど最適化されてないので性能が劣るという話もありましたが、これは1.8がリリースされた当時の記事に書いてあったものなので14までバージョンアップがなされた今はどうかという疑問はあります。それでも伝統的なFor文の方がまだ性能では優秀ではないだろうかと思いますが。\n途中でやめられない Streamの処理は一般的なforループとは違って、continueやbreak、returnなどで一部の処理をスキップしたり途中で処理を止めることができません。基本的にStreamは全要素に対して処理をすることを前提にして設計されたからです。なのでそれぞれの目的に合わせて、Streamのメソッドを適切に使い分ける必要があります。例えばFor文での処理は以下のような変えられます。\n条件に合致する要素だけを処理したい場合(if) filter() Collectionにしたい場合(add) collect() 要素を取り出したい場合(return) findAny() / findFirst() ループ変数を使用できない 拡張For文ではなく、伝統的なFor文ではループ変数を使って、現在のループが何回目かを数えることができます。しかし、Streamではループ変数を使うことができません。例えば、以下のようにStreamの外部に変数を置いてもコンパイルエラーとなります。\nint型のループ変数を使いたい場合はIntStreamを、ループ変数で処理をスキップしたい場合はskip()を使いましょう。もちろん、こういう場合は普通に既存のFor文を使った方が正解に違いです。\nそもそもの関数型 実は、Streamの中でも外部変数をループ変数として使う方法がなくはないです。AtomicIntegerでループ変数を使う方法がありますが、そこまでしてStreamを使う理由もなければ、関数型プログラミングの目的に合いません。\n関数型プログラミングのコンセプトの中では不変性(Immutability)というものがあります。以前Immutabilityについて述べたことがありますが、ここで重要なのはデータが変わることはない、ということです。データが変わらないならどうやって処理が行われるかというと、元のデータはそのままで、処理ではそのデータのコピーを作って作業することになります。\nStreaｍを持って処理をする場合に、その結果が新しいインスタンスになるのもそれが理由です。Listをループさせる場合は、元の要素を編集できます。しかし、Streamで処理する場合は変更された要素で構成された新しいListを返すようになりますね。中間操作で元の要素を編集するとしても、Streamは終端操作が終わるとクローズされ再利用できなくなります。これで元のデータは変えずに済みますね。\nなのでStreamを使う場面というのは、まず元のデータをどうするかによります。もちろん、Streamを使わない場合でも、関数型的なコードの作成がより場合も多いです。(ということで、まずは関数型プログラミングを勉強ですね…)\nStreamをより活用する 次に、Streamを使う場合に、どうしたら正しく、より効率的に活用できるかに関する質問です。Streamは最初から再使用できないようになっていますが、場合によっては同じデータに対してそれぞれ違う処理を行う必要があるのでそれがどうやって実現できるか、という疑問がありました。例えば、普通のFor文だとループの中で分岐を置くことで二つのCollectionに要素を分配するような処理ができるのですが、Streamだと同じオブジェクトに対しての処理はできませんね。こういう場合は同じデータに対してどうやったらStream処理を2回以上できるか気になります。\nもう一つは、自分だけなのかもしれませんが、Stream以外でも、メソッドチェーニングを使えるAPIは非効率的な処理が入ってもすごく場合があって、それをどうしたら効率的な書き方にできるかという疑問がありました。例えばCollectionや配列をStreamに変換してからforEach()を使うこともできますが、CollectionだとstreamなしでもforEach()は使えますね。こういう場合は直感的にCollectionのforEach()の方が良さそうだとは思いますが、それ以外の場合はどうなのかよくわかりません。\nなので、この二つの疑問についても調査してみました。\n再使用 Streaｍは何度も繰り返して中間操作が可能ですが、一度でも終端操作が行われるとクローズされ、再利用ができなくなります。なぜなら、Streamの目的はデータの処理であって、データの格納ではないからです。\nしかし、たまには同じデータに対してStreamを利用し、それぞれ違う処理を行たい場合もありますね。そういう時はどうしたらいいでしょう。Javaでデータを格納するためのものは配列やCollectionがありますので、必要なデータを予め定義して、場合によってそれをStreamに転換して使う方法があります。配列の場合はArrays.stream()やStream.of()があり、Collectionだとstream()がありますね。例えば以下のような方法です。\n// Listで必要なデータを集めておく List\u0026lt;String\u0026gt; names = Stream.of(\u0026#34;Eric\u0026#34;, \u0026#34;Elena\u0026#34;, \u0026#34;Java\u0026#34;) .filter(name -\u0026gt; name.contains(\u0026#34;a\u0026#34;)) .collect(Collectors.toList()); // 1番目のStream Optional\u0026lt;String\u0026gt; firstElement = names.stream().findFirst(); // 2番目のStream Optional\u0026lt;String\u0026gt; anyElement = names.stream().findAny(); 予め必要なデータはListとしてCollectし、必要な場合はそれをまたStreamに変換して使う例でした。データがそもそもCollectionや配列の場合は、必要に応じてstream()を呼び出すことでそれぞれ違う処理ができます。また、peek()を挟むことで違うCollectionにデータを追加することもできます。厳密にいうと再使用というよりはどうStreamを作るかに関する話となりますが、これで一つのデータから複数の処理結果を出すことは可能、ということになります。\n短く書く 先に述べましたが、StreamはメソッドチェーニングのできるAPIなので、非効率的なコードを書きやすい傾向がありました。なのでケース別により効率的な書き方を集めてみました。自分はEclipseを主に使っているのですが、Intellijだと、こうした方がいいよとオススメしてくれる部分らしいです。\nCollectionのメソッドを使う // CollectionのForEach collection.stream().forEach() → collection.forEach() // Collectionを配列に collection.stream().toArray() → collection.toArray() Streamを作る // 配列からStreamに Arrays.asList().stream() → Arrays.stream() / Stream.of() // 空のStreamを作成 Collections.emptyList().stream() → Stream.empty() // 範囲指定で配列を作成 IntStream.range(expr1, expr2).mapToObj(x -\u0026gt; array[x]) → Arrays.stream(array, expr1, expr2) // 範囲指定でStreamを作成 Collection.nCopies(count, ...) → Stream.generate().limit(count) 要素の判定 // 条件に一致する要素が存在するかの判定(1) stream.filter().findFirst().isPresent() → stream.anyMatch() // 条件に一致する要素が存在するかの判定(2) stream.map().anyMatch(Boolean::booleanValue) → stream.anyMatch() // 要素が一つでも条件と一致しないかの判定 !stream.anyMatch() → stream.noneMatch() // 全要素が条件と一致するかの判定 !stream.anyMatch(x -\u0026gt; !(...)) → stream.allMatch() // ソートして最も先にある値を探す stream.sorted(comparator).findFirst() → Stream.min(comparator) 要素を集める // 要素の数を数える stream.collect(counting()) → stream.count() // 最も大きい要素を探す stream.collect(maxBy()) → stream.max() // 要素を違うオブジェクトにマッピングする stream.collect(mapping()) → stream.map().collect() // 要素を一つにまとめる stream.collect(reducing()) → stream.reduce() // 要素を数字の合計にする stream.collect(summingInt()) → stream.mapToInt().sum() 要素の処理 // 要素の状態だけを変える stream.map(x -\u0026gt; {...; return x;}) → stream.peek(x -\u0026gt; ...) 最後に 関数型プログラミングに興味がないとしても、Streamそのものはかなり魅力的なAPIですので、皆さんにもぜひ使ってみて欲しいです。Java 1.8がリリースされた当時には性能も劣り読みにくいという批判も多かったのですが、もう時間は経ち、Javaのバージョンはすでに14となっているくらいです。もうそろそろStreamを使ってモダンな書き方を試してみても良いでしょう。\nそしてStreamを通じて、関数型プログラミングを味わえるのも一つのメリットではないかと思います。もちろん、Streamが完璧な関数型プログラミングの例だとは言い切れませんが、少なくとも、オブジェクト指向だけでなく新しいプログラミングのトレンドはどういうものかを経験できるということだけでも十分価値があるのではないでしょうか。もう関数型プログラミングの概念が登場してからも数年が経っています。プログラミングの世界は常に変化と発達が伴うものなので、少なくとも最近のトレンドが何であるかくらいは把握しておきたいものです。\nでは、またあいましょう！\n","date":"2020-04-06T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-stream/","title":"Streamを正しく使う"},{"content":"この度はJava SE 8 Goldを受験しました。Silverを取ったのが去年の9月末だったので、およそ半年ぶりの受験ですね。以前、Silverの方を受験した時はポストでその感想を書いていたので、今回も同じくGold受験後の感想を書こうと思います。\nGenerics Genericsに関する問題は、型が明確ではない場合をとう問題が多かったです。例えばとあるクラスを継承しているクラスのみを引数として受けたい場合はどうするか、とかの問題ですね。\u0026lt;? super X\u0026gt;と\u0026lt;? extends X\u0026gt;の違いがよくわからないと難しくなる、そのような問題です。\nまた、Genericsに関しての問題はComparator\u0026lt;T\u0026gt;とComparable\u0026lt;T\u0026gt;も登場していて、これからどう違うか、またどこで必要となるものかを理解していないと難しい問題になっていました。個人的にはあまり使ってないAPIだったので最初は苦戦した問題でした。理解できればなるほど、となりますが、これからもそんなに使うことあるかどうか…でも確かに知っておけば便利そうな気はしますので覚えておきたいAPIです。\nLambda Java 1.8の重要機能であるLambdaはやはり問題数が多いです。Lambdaはただ単にメソッドの書き方が楽になったということだけではなく、それを活用できる関数型インタフェースやStreamなど、他の重要なAPIと関係がある重要なAPIですので、やはりこれがわからないと本当の意味でJava 1.8を理解していないと言えるでしょう。\nLambdaに関する問題は、関数型インターフェースの条件(SAM)やjava.util.functionパッケージで提供しているインターフェースの種類を理解していないと解けないものが多い感じでした。また、Method Referenceに関する問題も出題されています。Lambdaをしっかり理解していれば書き方が違うだけというのがわかりますが、そうでないとコードすら読めないかもしれません。\nStream Streamは問題の割合が多いです。Java 1.8で追加された重要なAPIの一つでもあり、今までのJavaのAPIとは性格がちょっと違うものでもあるのでなんとなく感だけで使っていた自分には大変勉強となりました。今まではせいぜいstream()やcollect()くらいしか使ってなかったのですが、今回の試験でこれから色々Streamを活用できる箇所が増えていくような気がします。\n個人的には既存のコードが問題なく作動していていればわざわざStreamに変える必要があるくらいではないものの、関数型プログラミングの特徴を少しでも味わえる素敵なAPIと思います。例えば問題では中間操作と終端操作の違いやそれぞれのメソッドの活用方法などに関する質問が出ていて、それらしきメソッド名を並べては「次の中、〇〇APIのメソッドとして正しいものを選びなさい」のような意味のない問題に比べ、APIそのものの特徴と使い方を理解できないと解けない問題となっている感じがして好きでした。特に、peek()やsorted()、Collectors.groupingBy()のような良いメソッドが出てきて嬉しかったです。\nI/O 基本的にはJava 1.7から追加されたNIOに関する問題が多かったです。これもまたStreamと結び付けるためか、lines()やlist()、walk()などのメソッドに関して問う問題が多かったですね。また、同じく1.7から登場したtry-with-resourceに関する問題も出たのでよかったと思います。\n自分の場合はファイルI/Oで主にNIOのFilesクラスを使うことが多いので馴染んでいるのもあって、close()をわざわざ呼び出さなくても安全なAPIを提示してくれるってことはこれからの習慣にもいい影響を与えるだろうと思います。\n個人的には仕事でもAutoCloseableを継承したクラスを作ったこともあったので嬉しかったです。\nThread 並行性に関する問題も割合的には多い感じでした。AtomicIntegerやExecutorServiceのような重要なAPIを学ぶ良い機会となりましたが、せっかくRunnableやCallableまで扱っているので、CompletableFutureまで扱ってくれていたらよかったのでは、と思いました。CyclicBarrierが出ていましたが、最近はあまり使われてないらしいですしね。スレッドはそこまで難しくなく、ForkJoinPoolとCyclicBarrier、RunnableとCallableのシグニチャをちゃんと覚えておいたら解ける問題が多かった気がします。\nTemporal LocalDateTimeなどのAPIに関する問題が少し出ていましたが、割合は低め。Silverでも扱っていたAPIなので、こちらではフォーマッターや例外になるパターンに関する問題が多かったような気がします。ZoneIdなども出てきましたが、そこまで重要ではないような印象でした。\nLocale こちらもまた割合は低め。ただ、プロパティーをどう読み込むかはLocaleだけでなく環境変数などを扱う時も必要な知識となるため、覚えておくかちはあると思います。APIのシグニチャを問う問題の場合はインターネットでも十分情報を得ることができるのでわざわざ覚える必要はあるのかな？といったところですが…\n他に JDBCに関する問題も少々ありましたが、やはり今はあまり使われてないので問題の数は少なかったです。自分の場合はJavaでのDB処理にはMybatisやJPAを使っているのでわざわざJDBCを使うことはないのですが、基本的にJavaではこういう風にDBにアクセスして処理をしているんだな、ということだけ分かれば十分なのではと思っているので、問題の割合としては適切だったかなと。\nまた、SilverまではJavaのAPIをちゃんと覚えているかを問う問題が多いような気がしますが、Goldからはクラス設計に関する問題もいくつか出ていて、カプセル化やシングルトンクラスをどう実装するべきかに関する問題や、Files.walk()による再帰処理などに関する問題もあったのですが、Goldを受験する人がそのような知識がないとは思えないのですね。\n一つ、せっかく同じような使い方(メソッドチェイン)をしていて、Streamで要素を取得する時も使えるOptionalについては言及されてないことは残念に思います。他と同じくJava 1.8で導入されたものなのになぜ…？って感じですね。\n最後に 去年はJava SE 11の資格も登場したので、最初からそちらにした方がよかったかなとも思いましたが、Oracleの公式ホームページの試験の紹介だとJava 9から導入されたモジュールシステム以外はあまり試験の内容が変わってないようです。\n8も11もLTSバージョンなので、次のLTSである17がリリースされたら、おそらく資格も更新される可能性があります。個人的には12から14まで導入された新機能の方が便利だと思うので、私のように勉強の目的で受験していて、元々8の資格を持っている方ならあまり11へのアップグレードはメリットがないかもしれません。もちろん、これから初めて資格を取得する人にとっては11の方が良いでしょう。自分はそんなことも知らずにとりあえず8を受験してしまったのですが…\nとりまとめ、資格取得は嬉しいことですが、何よりも勉強のため始めたJavaの資格が、思っていたより仕事で役立つ知識を提供していたので満足しました。問題自体はパターンをよく覚えだけで解けるものも多いのですが、JavaのAPIに対する理解を深めたい方なら、受験してみて損することはないと思います。受検料は高いですが…\n","date":"2020-03-29T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-se-8-gold/","title":"Oracle JavaSE 8 Goldについて"},{"content":"Springでは、application.propertiesやapplication.ymlファイルに別途設定したい項目を定義することによってデータをアプリケーションの外に出すことができます。こうしてデータとアプリケーションを分離するのは、ハードコードすることによって変更が発生した場合にアプリケーションそのものを修正する不便からの開放されることができます。\nしかし、こうやって別途の外部ファイルに様々な設定値を書くということは、セキュリティ問題と繋がることになる場合もあるでしょう。例えばDBなどのクレデンシャル情報や、企業の業務と関連したコードなどが平文で記載されているとしたら、ハッキングでなくても、セキュリティ問題となる可能性は十分です。こういう場合に、記載したい各種設定値などを暗号化できるといいでしょう。それを可能にするAPIがすでに存在していて、今回紹介したいものはJasyptというものです。\nJasyptを使うと、平文を暗号化したり、暗号文を複合して平文に戻すことができます。Spring Boot Starterも提供しているのですでに作成済みのSpring Bootアプリケーションに暗号化機能を追加するのも簡単。では、Jasyptを使ってどうやって暗号化と複合の機能をSpring Bootアプリケーションに導入できるかを今回のポストで説明しましょう。\nJasyptによる暗号化のフロー Jasyptを使ってのSpring Bootアプリケーションの外部ファイルに記載した設定値の暗号化と複合は、以下のようになります。\nEncryptorクラスをBean登録 Encryptorクラスで平文を暗号化 YAMLに暗号文を記載 アプリケーションを起動時、YAMLの暗号文を複合して使用 Jasyptで提供しているEncryptorは、デフォルトとして提供されるクラスもありますが、カスタマイズもできるので今回はその方法を紹介していきます。\n依存関係を追加 Spring bootを基準に、依存関係は以下のように追加します。\nMavenの場合\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.ulisesbocchio\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jasypt-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradleの場合\ndependencies { implementation \u0026#39;com.github.ulisesbocchio:jasypt-spring-boot-starter:3.0.2\u0026#39; } Jasypt本体だけを普通のJavaアプリケーションで使うこともできますが、今回はSpring Bootアプリケーションを基準にしているので、設定が簡単な方を紹介しています。\nカスタムEncryptorを作る まずはSpringのBeanとしてEncrytorを登録します。Encryptorには様々なオプションが指定できますが、実際重要なのはパスワードとアルゴリズムです。パスワードとアルゴリズムが一致しない場合は複合ができないからです。ここではYAMLファイルにカスタム設定としてEncryptor用のパスワードを記載し、それを持ってEncryptorをBean登録する場合のコードを紹介します。\n@Configuration public class StringEncryptorConfig { // YAMLから読み込むパスワード @Value(\u0026#34;${settings.jasypt.password}\u0026#34;) private String password; // encryptorBeanという名前でEncryptorをBean登録する @Bean(name = \u0026#34;encryptorBean\u0026#34;) public StringEncryptor stringEncryptor() { PooledPBEStringEncryptor encryptor = new PooledPBEStringEncryptor(); SimpleStringPBEConfig config = new SimpleStringPBEConfig(); config.setPassword(\u0026#34;password\u0026#34;); config.setAlgorithm(\u0026#34;PBEWithMD5AndDES\u0026#34;); // 以下は必須項目ではない config.setKeyObtentionIterations(\u0026#34;1000\u0026#34;); config.setPoolSize(\u0026#34;1\u0026#34;); config.setProviderName(\u0026#34;SunJCE\u0026#34;); config.setSaltGeneratorClassName(\u0026#34;org.jasypt.salt.RandomSaltGenerator\u0026#34;); config.setStringOutputType(\u0026#34;base64\u0026#34;); encryptor.setConfig(config); return encryptor; } } これでEncryptorのBean登録の設定は完了です。次は外部設定ファイルの設定ですね。\n外部ファイルの設定 外部設定ファイルでは、Beanとして登録したEncryptorの名前を指定し、暗号化したプロパティを記載します。Encryptorの名前が一致しなかったり、記載されてない場合はアプリケーションの起動時のエラーとなるので注意しましょう。\napplication.propertiesの場合\njasypt.encryptor.bean=encryptorBean properties.password=ENC(askygdq8PHapYFnlX6WsTwZZOxWInq+i) application.ymlの場合\njasypt: encryptor: bean: encryptorBean properties: password: ENC(askygdq8PHapYFnlX6WsTwZZOxWInq+i) ここでお気づきの方もいらっしゃるだろうと思いますが、暗号化した項目は必ずENC()で囲まなければなりません。そうしなかった場合は、JasyptのEncryptorは設定値をそのまま文字列として読み込みますので複合されません。\n暗号化のアルゴリズム Jasyptのパッケージを辿ると、基本的にいくつかのEncryptorが定義されていることがわかります。文字列だけでなく、数字タイプやバイナリーも暗号化できるので必要に応じてはSpringではなく、普通のJavaアプリケーションでもインポートして使うことができます。\n今回は文字列の暗号化だけを紹介しますが、この文字列のEncryptorには以下のようなものが予め定義されています。\n// デフォルトとして使われるEncryptor public BasicTextEncryptor() { super(); this.encryptor = new StandardPBEStringEncryptor(); this.encryptor.setAlgorithm(\u0026#34;PBEWithMD5AndDES\u0026#34;); } // より強いアルゴリズムを使うEncryptor public StrongTextEncryptor() { super(); this.encryptor = new StandardPBEStringEncryptor(); this.encryptor.setAlgorithm(\u0026#34;PBEWithMD5AndTripleDES\u0026#34;); } // AES256を使う最も強力なEncryptor public AES256TextEncryptor() { super(); this.encryptor = new StandardPBEStringEncryptor(); this.encryptor.setAlgorithm(\u0026#34;PBEWithHMACSHA512AndAES_256\u0026#34;); this.encryptor.setIvGenerator(new RandomIvGenerator()); } ここに記載されているアルゴリズムは、そのままBeanとして定義するカスタムEncryptorにも使えます。ただし、アルゴリズムが複雑なものであるとそれだけ暗号化した結果は複合が難しくなるので安全ですが、アプリケーション起動が遅くなる可能性もあるので場合によって適切なものを選びましょう。また、AES256を使う場合はIvGeneratorも指定する必要があるということに注意しましょう。\nコード内で暗号化がしたい場合は、Bean登録したEncryptorを呼ぶか、新しいEncryptorのインスタンスを作成してencrypt()メソッドを呼び出すとできます。当然のことですが、同じパスワードを指定しないと正しく暗号化と複合ができないということに注意しましょう。\nコマンドラインツール Jasyptをダウンロードすると、コマンドラインツールで暗号化や複合ができるようになります。リンクからditributableバーションをダウンロードして解凍すると、binフォルダの中にbatファイルとshファイルが入っています。格ファイルの機能は以下となります。\nencrypt.sh(bat): パスワードベースで平文を暗号化する decrypt.sh(bat): パスワードベースで暗号文を複合する digest.sh(bat): 複合できないHashコードを生成する listAlgorithm.sh(bat): 暗号化に使えるアルゴリズムの種類を羅列する encryptとdecryptでは、パスワードと暗号化・複合したい文をコマンドライン引数として入力するとその結果が標準出力で画面に表示されます。また、オプションとしては使いたいアルゴリズムを指定することもできます。使い方は以下のコマンドになります。\nbin % ./encrypt.sh password=password input=this_is_input このコマンドでの出力結果は以下です。\n----ENVIRONMENT----------------- Runtime: AdoptOpenJDK OpenJDK 64-Bit Server VM 11.0.6+10 ----ARGUMENTS------------------- input: this_is_input password: password ----OUTPUT---------------------- 2lgKlL4gECBBtjch4WZITWDBHWhIxvVz また、listAlgoritymを実行すると、以下のように現在のシステムで使えるアルゴリズムのリストが出力されます。\nPBE ALGORITHMS: [PBEWITHHMACSHA1ANDAES_128, PBEWITHHMACSHA1ANDAES_256, PBEWITHHMACSHA224ANDAES_128, PBEWITHHMACSHA224ANDAES_256, PBEWITHHMACSHA256ANDAES_128, PBEWITHHMACSHA256ANDAES_256, PBEWITHHMACSHA384ANDAES_128, PBEWITHHMACSHA384ANDAES_256, PBEWITHHMACSHA512ANDAES_128, PBEWITHHMACSHA512ANDAES_256, PBEWITHMD5ANDDES, PBEWITHMD5ANDTRIPLEDES, PBEWITHSHA1ANDDESEDE, PBEWITHSHA1ANDRC2_128, PBEWITHSHA1ANDRC2_40, PBEWITHSHA1ANDRC4_128, PBEWITHSHA1ANDRC4_40] このリストの中のアルゴリズムはEncryptorをBean登録する時指定できるもののリストでもあるので、必要に応じて適切なものを選びましょう。強力なアルゴリズムを使うとアプリケーションの起動が遅くなる可能性もあります。(Spring BootアプリケーションのYAMLファイルは起動時に読み込まれますので)\n最後に アプリケーションの作りで、セキュリティの重要性はいうまでもなく高いものですね。先にも述べましたが、Springの設定ファイルでは特に、DBや外部システム連携のための接続情報などの敏感な情報が書かれることが少なくないため、外部設定ファイルがそのまま流出されたら困ることも起こり得ると思います。普段からそのようなことが怒らないように気を付けることももちろん大事ですが、こうやって暗号化によって情報を守るという手段もまた良い方法になるのでは、と思います。\n特に、JasyptのEncryptorは外部設定ファイルだけでなく、コードの中でも使えるので、活用できる範囲が広いですね。敏感な情報を扱っている場合は、アプリケーションの中でも積極的に活用していきたいものです。性能も安定性も大事ですが、何より情報が漏れないように、ですね。\n","date":"2020-03-16T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-settings-encryption/","title":"Jasyptでプロパティを暗号化する"},{"content":"Springではビジネスロジックを書く場合、一般的にServiceというクラスを作成することになります。Serviceは重要な処理が入るため開発やテストでは重要なクラスですが、開発をしていると、状況によっては実装しても動かせない場合もあります。例えばまだ環境が整っていない、他のクラスに依存する設計となっているがそのクラスがまだ存在していないなどの場合ですね。こういう時は実際の処理が行われず、常に同じ結果を返すクラスを書いておく場合もあります。\nこういう場合に、予め複数のServiceクラスを書いておいて、外部の設定ファイル(application.yml)に開発モード、テストモードなど状況に合わせてどちらのServiceクラスを使うかを選択できたら便利でしょう。実際、ServiceクラスはInterfaceと実装クラス(Implという名の)を分けて書く場合が多いので、複数のImplクラスを作って置いて、場合によって違うものがBeanとして登録されるようにすることも不可能ではありません。\nなので今回は、YAMLの設定を読み、場合によってどのServieImplをBeanとして登録するかを決める方法を紹介します。\nServiceの元の構成 一般的なServiceクラスの構成は以下のようになります。Interfaceを作成して、それを具現化するサービスを作ることですね。そして中にはDBなどアプリの外部との連携を担当するクラスをDIして使ったりします。\npublic interface SomeService { public void doSomething(); } @Service public class SomeServiceImpl implements SomeService { private SomeRepository repository; @Autowired public SomeServiceImpl(SomeRepository repository) { this.repository = repository; } @Override public void doSomething() { // ... } } ここで、テスト時に使いたいImplクラスを以下のように作成したとします。\n@Service public class SomeTestServiceImpl implements SomeService { private SomeTestRepository testRepository; @Autowired public SomeServiceImpl(SomeTestRepository testRepository) { this.testRepository = testRepository; } @Override public void doSomething() { // ... } } こういう場合は、アプリケーションを起動するとSpringではInterfaceの実装クラスはどれ？と聞いてくることになります。一つのInterfaceに対して二つの実装クラスが存在していて、両方Beanとして登録しようとしているからです。\nアノテーションを削除 Serviceクラスには一般的に@Serviceをつけることになります。このアノテーションをつけると、Springではこのクラスを自動的にBeanとして登録することになります。なので一つのInterfaceに対して複数の@Serviceのついたクラスを作成すると、どれを使いたいかSpringとしてはわからなくなります。なので、ここでは@Serviceアノテーションは使わないことにします。\nYAMLの作成 YAMLの作成は簡単です。今回はbooleanを使って、trueになっていればテストモード(SomeTestServiceImplを使用)、falseになっていれば通常モード(SomeServiceImplを使用)で動くようにします。例えば以下のようなものです。\nsettings: TestMode: true application.ymlに直接このカスタムプロパティを書いても良いのですが、自作の設定なので、適当な名前をつけて別途のファイルにしても良いです。別途ファイルにした場合は、application.ymlでそのファイルを含むようにすることを忘れないようにしましょう。\nspring: profile: include: settings # ファイル名がapplication-settings.ymlの場合 Configuration設定 アノテーションを外したら、ServiceImplはBeanとして登録できなくなります。しかし、使いたいServiceImplクラスを選ぶということは、状況によって使いたいクラスをBeanとして登録したい、ということです。なのでどこかでクラスを選び、Beanとして登録するようにする必要がありますね。また、YAMLに書いた設定を読み込む必要もあります。これらをまとめて@Configurationのついたクラスとして実装しましょう。\n@Configuration public class SomeServiceConfig { // YAMLに設定した値を読み込む @Value(\u0026#34;${settings.TestMode:false}\u0026#34;) private boolean testMode; // YAMLの設定からどのImplクラスを使うかを決定してBean登録 @Bean public SomeService someService(SomeRepository repository, SomeTestRepository testRepository) { return this.testMode ? new SomeTestSerivce(testRepository) : new SomeServiceImple(repository); } } Springでは@Valueや@ConfigurationPropertiesを使うことでYAMLに指定した値を読み込むことができます。@ConfigurationPropertiesだとクラス全体のフィールドに対してYAMLの値をマッチすることができますが、ここでは一つの値を読み込みたいだけなので、個別フィールドに対して使える@Valueを使います。YAMLファイルがない場合は例外となるため、デフォルト値としてfalseを指定しておきました。\nBean登録は普通に@Beanアノテーションをつけ、新しいインスタンスを作成して返すだけです。今回の例ではSerivceImplで依存しているRepositoryクラスをコンストラクターにAutowiredを使って注入しているため、そのインスタンスも必要となりますね。メソッドの引数にRepositoryを書いておけば、それがBeanとして登録されているクラスだと自動的に引数として入ってきます。なのであとはこれがテストモードであるか、通常モードであるかによってそれぞれのコンストラクターに合わせた引数を渡し、インスタンスをリターンすればBean登録も完了となります。簡単ですね！\nアノテーションを使う場合 Beanを状況により切り替えたいといった場合に、@Profileアノテーションを使う方法もあります。こちらもやり方は難しくありません。まずYAMLファイルを以下のように定義したとします。\nspring: profile: active: dev YAMLの定義ができたら、あとはどのプロファイルを使うかをアノテーションで指定します。以下のコードのようにです。\n@Configuration public class SomeServiceConfig { // devやdebugの場合はこちらをBean登録する @Bean @Profile({\u0026#34;dev\u0026#34;, \u0026#34;debug\u0026#34;}) public SomeService someService(SomeTestRepository testRepository) { return new SomeTestSerivce(testRepository); } // prodの場合はこちらをBean登録する @Bean @Profile(\u0026#34;prod\u0026#34;) public SomeService someService(SomeRepository repository) { return new SomeServiceImple(repository); } } @Profileアノテーションでは指定できるプロファイルを配列で指定できるため、YAMLの記載によってどんなBeanを登録するかを簡単に指定できます。どちらの方法をとっても良いので状況によって適切な方法を選びましょう。\n最後に 最近はDjangoやExpressなど、他の言語のウェブフレームワークにも触れてみたいと思っていますが、日々新しい発見と勉強が続いているので、なかなかSpringから離れることができません。こうやってできること、知らなかったことを発見するたびに、他にも良いフレームワークがありながらSpringがエンタープライズ市場で長い間生き残ることができたのはこのようにできることが多いからなのではないか、という気もします。Springだけでもしばらくブログに載せる記事のネタは尽きないかもですね！\n","date":"2020-02-25T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-switching-service/","title":"ServiceのImplクラスをYAMLで選択する"},{"content":"ウェブアプリケーションを開発していると、一つのRest APIだけで全ての機能を自己完結させる必要はない時もあります。例えば組み込みたい機能がすでに実装されているサーバー(API)が存在している場合もありますね。そういう場合は、素直にそのAPIをコールするだけで簡単に目的を達成できます。実際、仕事で他のRest APIサーバーとの通信が必要となって調べてみましたが、Springではすでにそのような場合に対応できるようなクラスを用意していました。今回のポストの主人公であるRestTemplateです。\nRestTemplateを使うと、簡単にget・post・deleteと言ったHttpメソッドのAPIをコールできます。また、リクエストやレスポンスをカスタムして状況に合わせて使うこともできます。例えばカスタムヘッダーを作ったり、サイズの大きいファイルを転送するリクエストを作ることも可能ですね。なので今回はRestTemplateを利用し、ファイルをアップロードするとそのファイルになんらかの処理をして返してくれるAPIがすでに存在している場合で、そのAPIをコールする部品を作る方法を紹介します。\nサーバー側の例 すでにファイルを処理するサーバーが存在している場合のことですので、まずはコールしたいAPIで利用するリクエストとレスポンスの形を把握する必要がありますね。ファイルをアップロードされたら、ヘッダーからファイル情報を読み込むようになっていて、ファイルデータが書かれているボディを読み込むようなメソッドがあるとしましょう。ヘッダーの情報に問題がなかったらローカルストレージにファイルを書き込み、処理を行います。そして処理の終わったファイルはレスポンスとして返すサーバーです。\n@PostMapping(\u0026#34;/upload\u0026#34;) public ResponseEntity\u0026lt;StreamingResponseBody\u0026gt; fileupload(HttpServletRequest request) { // リクエストヘッダーからファイルサイズを取得 long fileLength = request.getContentLengthLong(); // ファイルサイズが0だとIOException if (fileLength == 0) { throw new IOException(\u0026#34;data size is 0\u0026#34;); } // ファイルを臨時ファイルとして保存 String fileName = request.getHeader(\u0026#34;Content-File-Name\u0026#34;); Path tempFile = Files.createTempFile(\u0026#34;process_target_\u0026#34;, fileName); try (InputStream is = request.getInputStream()) { Files.copy(is, tempFile); } catch (IOException e) { e.printStackTrace(); } // ...ファイルを持ってなんらかの処理を行う // レスポンス用のヘッダーを作る HttpHeaders headers = new Headers(); // 処理が終わったファイルを書き込むボディを作る StreamingResponseBody body = new StreamingResponseBody() { @Override public void writeTo(OutputStream outputStream) throws IOException { byte[] bytes = Files.readAllBytes(path); outputStream.write(bytes); } }; // ヘッダーとボディ、HttpStatusをセットしてレスポンスを返す return new ResponseEntity\u0026lt;StreamingResponseBody\u0026gt;(body, headers, HttpStatus.OK); } サーバーがこういう形になっている場合、APIをコールする側としてはRequestのヘッダーにはファイル情報を書き、ボディにはファイルのデータを書いて転送する必要がありますね。そして処理結果のResponseにもまたファイルデータが入ってあるので、それを受け止める処理が必要となります。そのためにリクエストもレスポンスもカスタムのものを作って、RestTemplateに載せることにしましょう。\nRestTemplateを使う 今回はRestTemplateのexecute()というメソッドを使いますが、このメソッドの引数は以下のようになります。\nAPIのURL(StringもしくはURI) Httpメソッド(ENUM) リクエスト(RequestCallbackの実装クラス) レスポンス(ResponseExtractorの実装クラス) get()・post()・delete()などのメソッドの時と違ってリクエストとレスポンスの方を両方指定する理由は、上に述べた通りリクエストとレスポンスの両方でファイルの転送が必要からです。また、execute()でも引数にURI変数を指定することもできますが、現在はURIが固定なので使いません。では、リクエストとレスポンスのインタフェースをどう実装するかをみていきましょう。\nリクエスト リクエストで使うRequestCallbackの実装クラスを作成します。このインタフェースにはコンストラクターの引数としてファイルを渡すとヘッダーとボディを作るようにしてみましょう。RequestCallbackをimplementsすると、doWithRequest()というメソッドをオーバーライドするようになります。このメソッドの引数であるClientHttpRequestにヘッダーとボディを設定することでリクエスト時のファイルアップロードができます。以下のコードを参照してください。\npublic class FileTransferRequestCallback implements RequestCallback { // アップロードしたいファイル private Path path; // ヘッダーにファイル情報を載せるためのコンストラクター public PdfConvertRequestCallback(File file) { this.path = file.toPath(); } @Override public void doWithRequest(ClientHttpRequest request) throws IOException { // ファイルからヘッダーを作る request.getHeaders().set(\u0026#34;Content-Length\u0026#34;, Files.size(path)); request.getHeaders().set(\u0026#34;Content-File-Name\u0026#34;, path.getFileName().toString()); // ボディにファイルを書き込む try (InputStream is = Files.newInputStream(file); OutputStream os = request.getBody()) { is.transferTo(os); } } } ヘッダーにはサーバーで要求するファイルサイズとファイル名を載せました。そしてアップロードするファイルをInputStreamとして取得して、OutputStreamであるボディに書き込みます。これでリクエストでのファイルアップロード設定は終わりです。次はレスポンスですね。\nレスポンス レスポンスでは、ResponseExctractorをimplementsします。この場合はextractData()というメソッドをオーバーライドするようになります。このメソッドの引数であるClientHttpResponseからはリクエストの時と同じくHttpステータスコード、ヘッダー、ボディを取得できます。このレスポンスの結果からResponseEntityのインスタンスを作成し、レスポンスの結果を載せて返すとRestTemplateからは通信の結果としてResponseEntityを返すようになります。\nResponseEntityを返すためにはそのボディの型を指定する必要があります。InputStreamの型を指定して、レスポンスのボディがファイルであることを指定しましょう。また、ClientHttpResponseのボディをResponseEntityにそのまま載せると、InputSteamがCloseされるのでボディはコピーしておきます。私は一回byte[]に変えて、さらにByteArrayInputStreamを生成することにしました。\npublic class FileTransferResponseExtractor implements ResponseExtractor\u0026lt;ResponseEntity\u0026lt;InputStream\u0026gt;\u0026gt; { @Override public ResponseEntity\u0026lt;InputStream\u0026gt; extractData(ClientHttpResponse response) throws IOException { // レスポンスのボディをコピー byte[] bytes = response.getBody().readAllBytes(); // ステータスコード、ヘッダー、ボディのデータを載せてResponseEntityを返却 return ResponseEntity.status(response.getStatusCode()).headers(response.getHeaders()).body(new ByteArrayInputStream(bytes)); } } これでレスポンスのファイルを取得できるようになりました。次は、RestTemplateでAPIをコールするだけです。\nRest APIのコール 先に述べましたが、RestTemplateのメソッドを実行するのは簡単です。まずはURLと、アップロードしたいファイルのインスタンスを作っておきましょう。そして、先ほど作成したRequestCallbackとResponseExtractorのインスタンスも作成します(ResponseExtractorは、状態を持たないのでBeanとして登録しても良いです)。\nexecute()の引数に、URL・Httpメソッドのタイプ・RequestCallback・ResponseExtractorを指定して実行すると、その結果をResponseEntityとして取得できて、そこからさらにステータスコード、ヘッダー、ボディを取得できます。これでアップロードしたファイルを処理してもらい、処理結果のファイルも即取得可能になりますね。\n// RestTemplateに渡す引数を準備 String url = \u0026#34;http://api/v1/file/upload\u0026#34;; File uploadFile = new File(\u0026#34;path/to/upload_file.txt\u0026#34;); FileTransferRequestCallback requestCallback = new FileTransferRequestCallback(uploadFile); FileTransferResponseExtractor responseExtractor = new FileTransferResponseExtractor(); // RestTemplateでAPIコールし、その結果を取得 ResponseEntity\u0026lt;InputStream\u0026gt; responseEntity = new RestTemplate().execute(url, HttpMethod.POST, requestCallback, responseExtractor); // ResponseEntityからHttpステータスを取得 if (responseEntity.getStatusCode() != HttpStatus.OK) { throw new IOException(); } // ResponseEntityからヘッダーを取得 HttpHeaders headers = responseEntity.getHeaders(); // ResponseEntityからボディを取得 try (InputStream is = responseEntity.getBody()) { File downloadedFile = new File(\u0026#34;path/to/downloaded_file.txt\u0026#34;); Files.copy(is, downloadedFile); } 意外と簡単！これで他のRest APIとのファイルのやりとりができるようになりました。\n最後に クラスやメソッドを機能別に分けるだけでなく、Rest APIもまた機能によっては分離されることもありますね。今回の場合がまさにそのような例でした。もちろんネットを経由するのでこのようなやり方は一つのRest API内に機能をまとめて置くよりは安定性が劣るかもしれませんが、再使用性が確保できるという面では良い方法ではないかと思います。同じサーバー内だと通信失敗の確率も下がるだろうし、色々と活用できる余地はありそうですね。\n最近はなかなかブログに載せられるようなコンテンツがなかったので(勉強は続けているつもりですが…)、次は何を書けばいいかなと悩んでいましたが、ちょうど面白い部品を作ることができてよかったです。Springの世界も本当に広くて奥深いものですね。では、また！\n","date":"2020-02-10T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-rest-template/","title":"Rest APIからRest APIにファイルを送る"},{"content":"Javaは長い間業界で生産性、性能、安定性で評判がよく、最近はバージョンアップも様々な機能が追加されています。仕事では主に11バージョンを使っていますが、次のLTSバージョンである17がリリーズされたらそちらを使うことになるのではないかと思います。なので今も新しいバージョンが発表されると一応更新履歴には目を通していますが、言語仕様そのものが変わる場合は新しいAPIの追加と比べ少ない気がしなくもないです。\nJavaが人気を得た理由の一つの生産性という部分では、今じゃPythonやJavaScriptなどに比べて劣る部分もあり、「コードが読みやすい」というメリットも、いつの間にか「冗長すぎる」という評価と変わってしまいましたね。自分はJavaが好きで、まだ言語そのものの仕様まで見切ってはいないですが、それでも使いながらこれは不便だな、これは他の言語と同じくなって欲しいなと思う時もあります。今回のポストでは仕事でJavaを扱いながら感じた不便なところ、また他の言語と比べて改善したいところについて述べたいと思います。こうして他の言語と比べ、Javaがどんなものであるかを把握していくのもまた一つの勉強になるのでは、と思いますので。\nOptional表記の改善 以前のポスト](../java-optional)で紹介したOptionalですが、これはJavaだけでなく他の言語でも多く採用しているAPIの一つですね。むしろ、JavaのOptionalが他の言語から影響され導入されたものらしいです。今は私自身もよく使っていて、すごい便利だと思っていますが、それでも他の言語と比べたらやはり不便と思うところがあります。\n他の言語と比べてコードが冗長であることがJavaの特徴と先に述べましたが、実際のコードではどうかをまず比較してみましょう。複雑にネストしているオブジェクトのとあるフィールドを読み、Nullだった場合はデフォルト値を返却する例を持って説明します。\nJava Javaのコードでは、最初のオブジェクトをOptionalでラップし、ネストされているフィールドやメソッドに対してmap()をチェーニングしていくことで次から次へとラップの対象を変えていきます。そして最後に、ターゲットのオブジェクトがNullだった場合はorElse()などのメソッドでデフォルト値を設定しますね。\n// 元のオブジェクト SomeClass object; // 複雑にネストされている Optional.ofNullable(object).map(obj::getProp1).map(prop1::getProp2).map(prop2::getProp3).orElse(\u0026#34;default\u0026#34;); C# 言語そのものがJavaと似ているC#ですが、より若いからか、Javaと比べ進んでいる部分がよく見当たるC#です。こちらでもKotlinとコードの書き方は同じです。違うのは、オブジェクトそのものがNullになる可能性を事前に宣言しないということだけですね。\nobject?.prop1?.prop2?.prop3? ?? \u0026#34;default\u0026#34;; JavaScript JavaScriptのOptionalもまた、C#とあまり変わらないです。\nobject?.prop1?.prop2?.prop3? ?? \u0026#34;default\u0026#34; Swift Swiftでもそう変わりません。\nobject?.prop1?.prop2?.prop3 ?? \u0026#34;default\u0026#34; Kotlin Kotlinもデフォルト値指定のためのElvis opertor特有の表現を覗くと、一緒ですね。\nobject?.prop1?.prop2?.prop3 ?: \u0026#34;default\u0026#34; 他の言語の例と比べて見ると、JavaのOptionalは確かに冗長な印象ですね。なので今後Optionalを言語の基本仕様として導入し、ラップをするのではなく?として表現できるようにしたらどうかという気もします。?を導入したところでコードの読みやすさを損ねるわけでもないですので。\nMultiple Return Statements Javaの仕様ではメソッドの戻り値となれるオブジェクトは常に一つのみですが、Pythonのような言語では戻り値を複数指定することができます。もちろん、Javaの戻り値が一つという制約を乗り越えるためによくBeanやCollectionに複数のオブジェクトやデータを入れて返すことはできるのでこれはシンタックスシュガー的なものになるだけですが、それでも便利な方法があったら使いたくもなります。\nJava メソッドの処理結果として複数のデータを受け取りたい場合、Javaだと先に述べたようにBeanやCollectionを使うことになりますね。以下は戻り値が複数の数字である例です。\n// 複数の戻り値を持つメソッド public List\u0026lt;Integer\u0026gt; multipleReturn() { return Arrays.asList(1, 2); } // 戻り値の取得 List\u0026lt;Integer\u0026gt; data = multipleReturn(); C# C#でもJavaと似たようなやり方で複数のデータを取得することができますね。実際はref/outパラメータを使ったり、structやclassを使う方法もあるらしいですが、Javaと比べより便利ではないかと思うのはTupleを使う場合です。C#のバージョンによって書き方が違っていて、昔の書き方ではJavaでCollectionを使うのとあまり変わらないものの、新しい書き方ではかなり便利なものとなっています。以下はその二つの例のコードです。\n// 複数の戻り値を持つメソッド(7以前) public Tuple\u0026lt;int, int\u0026gt; oldMultipleReturn() { return Tuple.Create(1, 2); } // オブジェクトとして取得 var result = oldMultipleReturn(); // 複数の戻り値を持つメソッド(7以後) public (int, int) newMultipleReturn() { return (1, 2); } // 変数として取得 (int one, int two) = newMultipleReturn(); Python Pythonの例では、7以後のC#と似たような感覚でコードを書けます。オブジェクト(tuple)として戻り値を全部取得するか、個別の変数として取得するか両方一つのfunctionでできるのがより便利な気もしますね。\n# 複数の戻り値を持つfunction def multiple_return(): return 1, 2 # 個別の戻り値を取得 a, b = multiple_return() print(a) # 1 # 戻り値をtupleとして全取得 d = multiple_return() print(d) # (1, 2) JavaScript ES6から導入された書き方ではPythonと似たようなコードで複数の戻り値を取得できます。\n// 複数の戻り値を持つfunction funtion multipleReturn() { return { first: 1, second: 2 } } // 個別の戻り値を取得 var (first, second) = multipleReturn() Swift SwiftはやはりOptionalと同じく、JavaScriptとあまり変わりません。\n// 複数の戻り値を持つfunction func multipleReturn() -\u0026gt; (Int, Int) { return (1, 2) } // 個別の戻り値を取得 let (first, second) = multipleReturn() Kotlin KotlinではPairかTripleなどがあり、使い方は簡単です。\n// 複数の戻り値を持つfunction fun multipleReturn(): Pair\u0026lt;Int, Int\u0026gt; { return 1 to 2 } // 個別の戻り値を取得 val (first, second) = multipleReturn() Javaでのコードの書き方の方がメソッドの役割をわかりやすいというメリットはありますが、戻り値のオブジェクトやデータをそのまま変数として使えるという面ではPythonのやり方がより便利ですね。このように複数の戻り値を持つメソッドを定義できるのは現代プログラミング言語ならどれもが持っている機能のようです。Javaにもいつかは導入されるのでしょうか？\n引数の種類をor指定 たまに、一つのメソッドで引数の型を複数指定できたら便利ではないだろうかと思うことがあります。Javaではこれをオーバーロードで実現していますね。\nJava public void doSomething(String value) { // Stringの場合の処理 } public void doSomething(int value) { // intの場合の処理 } もちろん、引数の型をObjectとして宣言し、内部ではinstansofを使って判定することもできます。しかし、前者ならやりたいことに比べコードの量が増えすぎる問題がありますし、後者なら意図した型以外のObjectが渡された場合の挙動がおかしくなる可能性もあります。\nTypeScript TypeScriptでは、これを簡単に引数のタイプを複数指定できるようにすることで解決しています。\nfunction checkString(v: string | number) { if (typeof v === \u0026#34;string\u0026#34;) { return true } return false } これまで引数の種類だけ違う場合はオーバーロードして、共通処理だけprivateメソッドで書いていましたが、これならよりコードを簡単に把握できそうですね。ぜひ導入して欲しい機能の一つです。\nTernary Operator with Throw 結果が二択しかない場合は、なるべくifより三項演算子を使った方がコードも短くなり便利と思います。しかし、Javaの三項演算子では例外を投げることができません。条件式に当てはまらない場合はどうしても以下のようなコードを書くしかないです。\n// xが0だとnumberも0で、0ではなかった場合は例外とする int number = x -\u0026gt; { if (x == 0) { return x; } else { throw new RuntimeException(); } } 無理やり三項演算子で例外を投げようとしたら、以下のような方法はありますね。\n// Genericな戻り値を持っていて、例外を投げるだけのメソッド public \u0026lt;T\u0026gt; T throwSomeException() { throw new RuntimeException(); } // elseでメソッドを呼ぶ int number = x == 0 ? x : throwSomeException(); 個人的にif文は二択しかない結果のために使うのはスペースの無駄遣いと思いますし、無理やりメソッドを作ってまで三項演算子を使う必要はないので、三項演算子でelseの場合には単純に例外を投げられるといいな、と思っていました。そして調べてみると、他の言語ではそれができるようです。\nC# C#では、二つのやり方があります。まず7以前だと、elseの場合に例外を投げるFuncを実行させることで実現できます。そして7以後では普通に三項演算子でthrowできるようです。まさに私が望んでいた形ですね。\n// 7以前 int number = x == 0 ? x : new Func\u0026lt;int\u0026gt;(() =\u0026gt; { throw new Exception(); })(); // 7以後 int number = x == 0 ? x : throw new Exception(); Kotlin Kotlinでは三項演算子がなく、if-elseを使うことになるということだけで、簡単な形になっています。\nval i = if (x == 0) x else throw Exception(\u0026#34;error\u0026#34;) JavaScript JavaScriptでは、7以前のC#と似た形で例外を投げることができます。\nvar number = (x == 0) ? x : (function() { throw \u0026#34;error\u0026#34; }()); わざわざ関数を実行してまで三項演算子で例外を投げたくはないですが、こういうやり方があるということがわかっただけでもかなり興味深いですね。JavaにもC#の7以後のような書き方ができるといいな、と思います。\nアクセス修飾子の拡張 Javaでのアクセス修飾子は、public・private・protectedをよく使っています。しかし、自作ライブラリーを作る場合はpublicとprivateの中間的なものもあって欲しいな、と思う時もあります。例えばJarにまとめた時、Jar以外ではアクセスできないようなアクセス制限をかけられるようなものですね。\nJava 9からモジュールが導入されましたが、自分が経験した問題もあるのでなるべくモジュールを使いたくはないなと思っているので…同じプロジェクトの中ならパッケージが違ってもアクセスできるような修飾子があったらいいな、と思います。また、protectedならサブパッケージでも参照できるなど。同じモジュール内でのみのアクセス修飾子はC#とSwift、KotlinでInternalとして提供しているので、Javaにも導入されるといいですね。\n最後に 最近のJavaの更新履歴をみると、続々と便利な機能が導入され続けています。特に14では、recordでLombokの@Dataと同じ機能を持つクラス宣言ができるようになるらしいです。次のLTS版は17なので、まだ十分色々と改善される余地はありますね。1.8でも便利な機能は多いですが、これからもどんどん他の言語の良い点を吸収して変転できるといいなと思います。\nまた、こうやって他の言語ではどうしているかを調べてみるのも良い勉強となりました。特にTypeScriptは最近注目している言語なので、機会があれば経験してみて、Javaとの比較もしてみたいものですね。では、また！\n","date":"2020-02-03T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-wishes/","title":"Javaはこう進化して欲しい"},{"content":"前回までご紹介したJenkinsでのジョブ生成は、どちらかというと、古いやり方によるものでした。実際、2016年にJenkinsが2.0にアップデートされながら、スクリプトでジョブを作成できるPipelineというものが導入されていました。\nPipelineではgroovyを使って、実行したいコマンドやタスクなどをステージという単位で書きます。書いたスクリプトは上から順次実行され、各ステージごとに実行結果を画面上に表示してくれます。今までのJenkinsジョブと比べ使い方がかなり違うので、今回はそのPipelineジョブを作成する方法をサンプルを持って紹介したいと思います。\nPipeline使うと何が嬉しい？ まずは普通のジョブと比べ、どんなメリットがあるかを知りたいですね。既存のFreestyleジョブでなく、PipelineでJenkinsのジョブを作成すると以下のようなメリットがあります。\nスクリプトなので管理がしやすい ファイルとしても管理ができるので、Gitなどでバージョンコントロールができます。 作成が簡単 Snippetを提供するので簡単にスクリプトを作成できます。 ジョブの成功・失敗履歴を簡単に確認できる ステージ単位でジョブを実行するので、どのステージが成功・失敗したか簡単に確認できます。 Pipelineジョブ内のステージに関する実行履歴はGUIから表示され、簡単に確認できる実行ログを提供しています。以下のような画面です。\nPipeline作成チュートリアル Pipelineジョブの作成 では、まずPipelineジョブを作成する手順を簡単に説明していきましょう。ジョブ作成画面からジョブ名を入力して、Pipelineを選択します。\nPipelineスクリプト Pipelineでジョブを作成すると、ジョブで実行する項目を指定する画面もFreestyleジョブとは違うものになります。ビルドトリガーなどの設定は同じですが、画面を下にスクロールしてみるとPipelineというタブがあることを確認できます。ここに直接スクリプトを書くか、Gitなどで管理しているスクリプトファイルを指定するかで何を実行するか選べられます。\nしかし、いきなりスクリプトを書くのも難しいことです。まず画面の右にあるtry sample Pipeline\u0026hellip;をクリックしてみましょう。まずはHello worldを選んでみます。\nPipelineのスクリプトはgroovyを使っていますが、groovyの文法をまず勉強する必要はありません。サンプルのコードは他にもあるので、それらを参考してどんな書き方をするかを確認しましょう。\nまた、Pipelineスクリプトに慣れてない人のためにJenkinsではSnippet作成機能を提供しています。実行したいタスクをドロップダウンメニューから選び、必要なパラメータなどを入力すると自動的にスクリプトを生成してくれる便利な機能です。Pipelineのスクリプト入力欄の下にあるPipeline Syntaxをクリックすると以下のような画面が表示されます。\n最初からスクリプトを手で書いても良いですが、どう書いたらわからない場合はこちらの機能を使いましょう。\nPipelineの実行結果 完成したPipelineジョブを実行するとステージ別に成功と失敗の結果が表示されます。先ほど作成したHello Worldサンプルの場合の実行結果画面です。\nここでは各ステージをクリックすると、ステージ別に書いたタスクに対して結果を確認できます。Logsをクリックしてみましょう。\nLog画面ではステージで実行したコマンドやタスクの結果がそれぞれ出力され、実行時間と共に詳細を確認することもできます。\nPipelineスクリプトの構造 では、簡単にPipelineスクリプトがどんな構造となっているかもみていきましょう。Pipelineスクリプトはまず以下のようなコードで定義します。\npipeline { // この中に実行するエージェントやステージを書く } 実行エージェントの設定 pipelineブロックを書いたら、次はpipelineを実行する環境を設定します。単純にJenkinsが起動しているインスタンスの中での実行ならagent anyと書くだけですが、最近はジョブを実行するためだけのDockerコンテナを使うことも多いようです。その場合は実行環境としてDockerコンテナを指定する必要がありますね。以下のようなコードでコンテナを指定します。\npipeline { agent { docker { image \u0026#39;実行したいイメージ\u0026#39; args \u0026#39;イメージを実行する時に渡すコマンドライン変数\u0026#39; // 省略可能 } } } ステージを作る 環境まで設定できたら、次は実行したいタスクを書きます。ここで大事なのはステージという概念です。Jenkinsの公式サイトではステージブロックを「Pipeline全体で実行するタスクの明確なサブセット」として定義しています。つまり、ステージ一つが一つのタスクの段階という意味でしょう。ステージの中では一つのタスク単位であるステップを定義し、ステップの中で実行するコマンドを書きます。\npipeline { agent { // Docker環境 } stages { stage(\u0026#39;ステージ名1\u0026#39;) { steps { // 実行したいコマンド } } stage(\u0026#39;ステージ名2\u0026#39;) { // ... } // ... } } Pipelineそのものに対する説明は以上となります。では、次に実際のPipelineジョブを書いたらどんな形になるのかを紹介します。\nPipelineスクリプト例題 以下のジョブをPipelineで作ると仮定して、簡単な例題を作ってみました。\n実行環境はopenjdkコンテナ(rootユーザー) Gitでソースコードをチェックアウト(ディレクトリはspringboot) gradlewタスクを実行してwarファイルを作る 出来上がったwarファイルをAzure Blobにアップロード これを実際のコードで表現すると以下のようになります。\npipeline { agent { docker { image \u0026#39;openjdk\u0026#39; // openjdk公式イメージを使用 args \u0026#39;-u root\u0026#39; // ユーザーをrootに指定 } } stages { stage(\u0026#39;Checkout\u0026#39;) { // Gitチェックアウトステージ steps { checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/ブランチ\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;保存するディレクトリ\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;GitクレデンシャルID\u0026#39;, url: \u0026#39;https://Gitレポジトリ\u0026#39;]]]) } } stage(\u0026#39;Build\u0026#39;) { // ビルドステージ steps { dir(path: \u0026#39;springbootapi\u0026#39;) { // 作業ディレクトリを指定 sh \u0026#39;./gradlew bootWar\u0026#39; } } } stage(\u0026#39;Upload\u0026#39;) { // ビルドしたwarファイルをAzure Blobにアップロードするステージ steps { dir(\u0026#39;springbootapi/web/build/libs\u0026#39;){ azureUpload storageCredentialId: \u0026#39;ストレージクレデンシャルID\u0026#39;, storageType: \u0026#39;blob\u0026#39;, containerName: \u0026#39;コンテナ名\u0026#39;, filesPath: \u0026#39;**/*.war\u0026#39; } } } stage(\u0026#39;Finalize\u0026#39;) { // 作業が終わるとワークスペースを削除するステージ steps { cleanWs() } } } } まだPipelineのコードに慣れてないと難しいと思われるかもしれませんが、Pipeline Syntaxを活用するとすぐかけるものなので皆さんもぜひ挑戦してみてください。\n最後に 最初Jenkinsに接した時もこんなに便利なツールがあるとは！と思いましたが、Pipelineの導入でさらに便利かつ明確にタスクがわかるようになっていて驚きました。最近はAzure Pipelinesを少し触る機会があったのですが、そちらのジョブ作成もこのJenkinsのPipelineを意識した感じです。これからのCI/CDツールは多分言語や文法は違っても、どれもこのような形になるのではないかと思うくらい良い変化です。皆さんもぜひJenkinsのPipelineに触れて、快適なビルド・デプロイを楽しんてみてください。\nでは、また！\n","date":"2020-01-26T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-pipeline/","title":"Jenkins Pipelineを使う"},{"content":"私がそうでしたが、今でも初めてJavaでのI/Oを学ぶとしたら、やはりFileオブジェクトを生成してInputStreamで読み込んだり、OutputStreamで書き込むのが一般的なのではないかと思います。ここで少し発展すると、WriterやReaderなどのクラスを使ったり、StreamをBufferで包んだり、SerializableでオブジェクトのI/Oを行ったりするレベルまで行くのでしょう。\n昔のAPIだとしても、動作や性能に大した問題がなければあえて新しいAPIにコードを全部変える必要はないと思います。むしろ無理やり新しいAPIに書き換えたコードが問題を起こす可能性もあるし、常に優秀とは言えませんので。例えばJava 1.8で追加されたforEach()は便利で、Lambdaが好きな私は多くの場面で使っていますが、実際は今までのJVMは伝統的なforループに最適化されているのでforEach()は性能で劣るらしいです。今後、forEach()の性能がより良くなる可能性もなくはないでしょうが、最近のJavaのバージョンアップ履歴をみると関数型APIの性能改善にどれだけ時間がかかるかは少し謎です。\n新しいAPIを使うということにはこのような問題もあり、慎重ではければならないのですが、それでもAPIが新しくなるのには何らかの理由があるためなので、新しくコードを書いたり簡単なコードに変えたりするなどの理由があれば、積極的に新しいAPIを導入してみるということもそう悪くないと思います。今回紹介したいAPIもまたそのようなものです。ファイルI/Oを扱う新しい方式、NIOです。(と言っても、Java 1.7から導入されたので今はあまり新しくもないですが…)\nNIOって何？ NIOは、Javaの新しいI/Oのことです。Newの略かと思いがちなのですが、実際はNon-blockingの略らしいですね。JavaはCやC++と比べ遅いですが、その理由の一つがI/Oだったらしいです。なのでそれを改善するために出たのがこのNIOですと。\nBlockingかNon-blockingかによる違い、Stream基盤かChannel基盤かという違いなど様々な違いがありますが、一般的には頻繁なI/Oが要求される場合ではNIOを選択した方がより良い性能を期待できると言います。他には以下のようなメリットがあります。\nスレッドのブロックが発生しない コードがより簡潔 コピー、移動、読み込みのオプション指定が簡単 基本的にBufferを使うので、Buffered~でのラッピングが要らなくなる あまりJVMの構造に詳しくないので、私がここで浅い知識を持って説明するようなことはしません。ただ、自分の観点からしてコードがより簡潔になるということは確かなメリットです。なので皆さんにもぜひ使ってみていただきたいと思います。\nそれでは、実際のコードでNIOをどう使うかについて説明して行きます。\nFile → Path NIOではFileオブジェクトよりPathオブジェクトを使います。PathはFileオブジェクトに比べ、ファイルパスをディレクトリとファイル名で分離して指定できるのが最大のメリットです。\n例えばファイルパスが複数のフォルダでネストされているとしましょう。\n// 複数のディレクトリとファイルがそれぞれ文字列として存在(path/to/file.txt) String rootDirectory = \u0026#34;path\u0026#34;; String toDirectory = \u0026#34;to\u0026#34;; String fileName = \u0026#34;file.txt\u0026#34;; この複数の文字列からインスタンスを作成するとしたら、Fileのコンストラクターは引数が一つの文字列なので以下のようになります。ディレクトリの文字列にスラッシュが含まれてないので、文字列を結合しながらスラッシュも一緒にいれる必要があります。\n// Fileオブジェクトの生成 File file = new File(rootDirectory + \u0026#34;/\u0026#34; + toDirectory + \u0026#34;/\u0026#34; fileName); しかし、Pathの場合は指定できる文字列が複数でも構いません。ディレクトリとファイル名の文字列を順番通り指定するだけで良いです。\n// Pathオブジェクトの生成 Path path = Paths.get(rootDirectory, directory, fileName); このように、インスタンスの作成がより便利なのがPathです。また、どうしてもFileオブジェクトが必要な場合があるとしても、FileのメソッドからPathに変換できる機能があるので便利ですね。もちろん、その逆もできます。\n// PathからFile生成 Path path = file.toPath(); // FileからPath生成 File file = path.toFile(); 他にもtoURI()メソッドでURIオブジェクトを生成できるなど、PathにはFileと同じ機能をするようなメソッドが多いので、どちらか便利な方を使いましょう。\nFiles 昔ながらのI/Oでファイルのコピーや削除などの操作を行うためにはInputStream、OutputStream、Writer、Readerなどのクラスを活用してきました。NIOでは主にこれらの作業をFilesクラスを持って行います。また、FilesクラスにはWriterとReader、InputStreamとOutputStreamを生成する機能もあるので使い勝手が良いクラスです。\nファイルのコピー Filesクラスでのファイルコピーは簡単です。以下のコードを見てください。基本的にコピー元とコピー先のファイルをPathオブジェクトとして指定するだけです。\n// PathをPathにコピー Files.copy(source, target); FilesクラスでのコピーにはENUMでコピー時のオプションを指定することもできます。\n// オプションを指定(ファイル属性もコピー) StandardCopyOption option = StandardCopyOption.COPY_ATTRIBUTES; Files.copy(source, target, option); また、実際存在するファイルではなく、InputStreamをコピー元として指定することもできます。この場合、データをファイルに書き込むということもできますね。\n// InputStreamをPathにコピー Files.copy(sourceStream, target); ファイルの削除 Filesクラスでのファイル削除はコピーと同じく、Pathオブジェクトを引数として渡します。\n// 削除 Files.delete(path); 戻り値がbooleanのメソッドも用意されています。ファイルが存在する場合は削除して、その結果をbooleanとして返します。\n// 存在する場合削除 Files.deleteIfExists(path); ファイルの移動 ファイルの移動は、コピーと削除の組み合わせみたいなものですね。また、ファイル名を変える場合にも使えます。基本がコピーだからか、コピーの時と同じオプションを使えます。\n// 移動もしくはリネーム Files.move(path); // オプションを指定(上書きする) StandardCopyOption option = StandardCopyOption.REPLACE_EXISTING; Files.move(path, option); ファイルの書き込み InputStreamをcopy()で使えるのですが、ファイル書き込みの場合のメソッドもあります。\n// Pathにデータを書き込む Files.write(path, content); write()メソッドの引数として渡せるのはbyte[]、List\u0026lt;String\u0026gt;などがあります。また、コピーの場合のようにオプションが指定できます。こちらのオプションではファイルが存在する場合上書きするか、追記するかを選べるので場合によってはcopy()と分けて使えます。\n// オプション指定(追記) StandardOpenOption option = StandardOpenOption.APPEND; Files.write(path, content, option); ファイルの読み込み 書き込みが文字列かbyte[]で分けられているように、読み込みも同じ形でファイルを取得できるメソッドがあります。文字列取得の場合、シンタックスシュガーとして結果物がStreamかListかくらいの違いがあります。\n// 文字列として全行を読み込む Stream\u0026lt;String\u0026gt; lines = Files.lines(path); List\u0026lt;String\u0026gt; liness = Files.readAllLines(path); // byte[]として読み込む byte[] bytes = Files.readAllBytes(path); Fileがそうであるように、Pathもまたファイルではなくディレクトリになれるので、Filesのメソッドもそれに対応しています。list()メソッドではディレクトリないのエントリをPathとして取得してStreamを生成します。\n// ディレクトリ内のエントリを要素として持つStream取得 Stream\u0026lt;Path\u0026gt; files = Files.list(path); I/Oとの組み合わせで使う 先に述べたように、Filesのメソッドの一部は昔ながらのI/Oと組み合わせて使えるものもあります。その一部を紹介します。\n// 読み込みの場合 InputStream is = Files.newInputStream(path); BufferedReader br = Files.newBufferedReader(path); // 書き込みの場合 OutputStream os = Files.newOutputStream(path); BufferedWriter bw = Files.newBufferedWriter(path); もちろんOpenOptionの指定もできます。\n// ファイルがない場合は作成する StandardOpenOption option = StandardOpenOption.CREATE; InputStream is = Files.newInputStream(path, option); 最後に どうでしたか。同じ機能をするだけならあまり使いたくなるメリットはないように見えるかもしれませんが、実際使ってみると、ENUMによるオプション指定でやりたいことが明確となって、コードの量も減らすことができる便利なクラスを提供するのがNIOだと思います。特にFileはそのまま使うとしても、Filesのメソッドは便利かつ強力なので、皆さんにぜひお勧めしたいものです。\n他にもFilesクラスには双方通信ができるというChannelクラスを提供するメソッドや、ファイルの属性、シンボリックリンクを取得したり指定したPathがディレクトリかを確認したり、二つのPathが同じファイルかをチェックするなど便利なメソッドが多いので、ぜひ使ってみてください。\nでは、また！\n","date":"2020-01-20T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-nio/","title":"IOからNIOへ"},{"content":"Javaでアプリケーションを組みながら最も遭遇率の高い例外が何かとしたら、それはNullPointerException(NPE)でしょう。初めてプログラミングを接する人に取っては「空白」と「Null」がどう違うのかを理解することもかなり難しいことではないのかと思いますが、Nullを理解できたとしても予想してなかったところで出てくるNPEで苦労する場合は決して少なくないと思います。ある意味、Javaでのアプリケーション開発はNPEとの戦いであるといっても過言ではないのではないでしょうか。\nなので今回は、NPEに対処するための方法を紹介します。Nullをチェックし、安全なコードを書く方法を探してみましょう。\nNullチェックで十分か？ とある学生の名前を取得するメソッドがあるとしましょう。データオブジェクトを引数として渡すと、そこから順番に学校、学年、組、学生の情報を取得して最後に学生の名前をStringとして返却するようなものです。これをコードで表現したら、例えば以下のように表現できます。\npublic String getStudentName(Data data) { School school = data.getSchool(); Grade grade = school.getGrade(); ClassName className = grade.getClassName(); Student student = className.getStudent(); return student.getName(); } これをより簡潔なコードで表現するとしたら、以下のようになるでしょう。\npublic String getStudentName(Data data) { return data.getSchool().getGrade().getClassName().getStudent().getName(); } このメソッドが意図通りに作動するとしたら、シグネチャーとコードだけで意図と結果が明確なものとなるはずです。しかし、皆さんにもわかるように、このコードにはいつどこでも例外が発生する可能性があります。\n学生の名前フィールドがNullだとしたら？いや、そもそも学生が、もしくは組が、学年が、学校がNullだったら？引数がNullだとしたら？どれもNPEになりうる可能性があるので、極めて危険なコードとなっています。\nここでまず考えられる対策は、事前にNullチェックの処理を入れNullでない場合にだけ次の処理に移行するようなコードを書くことでしょう。そしてNullだった場合にまた適切な処理(もしくはデフォルト値)を書くことで意図した通りに動かすことができます。\nでは、上のコードにNullチェックの処理を入れ、をNull Safeなコードに変えてみましょう。例えば以下のように変えることができるでしょう。\npublic String getStudentName(Data data) { if (data != null) { School school = data.getSchool(); if (school != null) { Grade grade = school.getGrade(); if (grade != null) { ClassRoom classRoom = grade.getClassRoom(); if (classRoom != null) { Student student = classRoom.getStudent(); if (student != null) { String studentName = student.getName(); if (studentName != null) { return studentName; } } } } } } return \u0026#34;Sato\u0026#34;; // Default value } 以上のコードはネストしすぎて、極めて読みづらいコードとなっています。なのでもし一つの項目でもNullチェックが抜けているとしてもわからなくなります。また、コードを直すこともかなり困難になります。メソッドの目的はあくまで、「学生の名前が知りたい」というシンプルな要求に応えるためのものだったのですが、もはやNullチェックが入りすぎてなんのためのロジックなのかわかりづらいですね。\nネストしている処理を避けるためif文をバラバラにしても結果はあまり変わりません。以下のコードをご覧ください。\npublic String getStudentName(Data data) { if (data == null) { return \u0026#34;Sato\u0026#34;; // Default value } School school = data.getSchool(); if (school == null) { return \u0026#34;Sato\u0026#34;; // Default value } Grade grade = school.getGrade(); if (grade == null) { return \u0026#34;Sato\u0026#34;; // Default value } ClassRoom classRoom = grade.getClassRoom(); if (classRoom == null) { return \u0026#34;Sato\u0026#34;; // Default value Student student = classRoom.getStudent(); if (student == null) { return \u0026#34;Sato\u0026#34;; // Default value } String studentName = student.getName(); if (studentName == null) { return \u0026#34;Sato\u0026#34;; // Default value } return studentName; } if文のネストを無くして読みやすくしてみようとしました。でも、このやり方だとむしろreturnが多すぎてこれはこれであまりよくない処理になっています。\nこのようなコードはどう直したらいいのか？という時に、使えるAPIをJavaでは用意しています。今回の主題であるOptionalです。\nOptionalを導入する 現代の言語はこのNullによって起こり得る問題を最初からブロックするため最初からNullを代入することを許さなかったり、Nullになりえるオブジェクトを扱えるAPIを提供したりするようです。例えばKotlinやSwiftではNullableやOptionalを制約の一つとして使っているようで(触ってみたことがないのでこうとしかいえませんが)、Pythonの場合もUnionやOptionaと言ったAPIが用意されているようです。そしてJavaもそう言ったトレンドに答えるべく、Java 1.8でOptionalをAPIとして導入しています。\nOptionalは、Nullになる可能性のあるオブジェクトに対しての新しい(といってもJava 1.8で導入されたのでもうそんなに新くもないですが)方法です。基本的には関数型言語から影響を受けて作られているらしいですね。\n私自身は関数型言語に詳しくないのですが、確かにこのOptionalの使い方をみるとLambda同様、元のJavaの思想とはかなり違うもののような気がします。なぜなら、オブジェクトのNullチェックを比較演算してその後の処理を決めるわけではなく、メソッドの連鎖で決めていくような形になっていて、書き方がかなり異質的だからです。\nなら、そんな異質的なAPIをなぜ使うのか？それはOptionalがどんなものであり、どんな特徴を持っているかをまず見て判断することにしましょう。\n使い方が簡単 map()やfilter()などCollection1やStream2と似たような機能をするメソッドがあり、さらに引数としてLambdaを使えるので、CollectionやStreamに慣れていると簡単に適応できます。\nOptionalを効率的に使うためにはメソッドチェーニング3やLambdaにまずなれる必要があるので、まずはjava.util.functionsになれるとしましょう。以前のポストを参考にしてください。\n見ただけでわかる Optionalはオブジェクトを包み、そのオブジェクトがNullである場合の処理のため作られたAPIです。なのでOptionalで包まれているオブジェクトがあると、そのオブジェクトはNullになる可能性があることを明らかにしているということです。なので戻り値だけでNullになる可能性があるコードを見分けることができるようになります。\n可読性が上がる Nullチェックという本来の目的に充実しながらも、コードが簡潔になるので読みやすいコードになります。取得したいオブジェクトがネストしている場合もOptionalで対応できます。最初のオブジェクトのNullチェックをして、さらにネストしているオブジェクトをNullチェックしていくような形です。\nOptionalでNullチェックを変えてみましょう では、実際のコードを持ってOptionalでのNullチェックがどう可能になるのかをコードを持ってみてみましょう。さっきのメソッドは以下のように変えることができます。\npublic String getStudentName(Data data) { return Optional.ofNullable(data) .map(Data::getSchool) .map(School::getGrade) .map(Grade::getClassRoom) .map(Class::getStudent) .map(Student::getName) .orElse(\u0026#34;Sato\u0026#34;); } Optionalが初めての方にはどんなことをしているか一見わからなくなるのではと思いますが、それでもコードの量が減り、可読性がよくなったのはわかるでしょう。もちろん、Nullチェックが省略されているわけでもありません。このように簡潔で分かり安く、安全なNullチェックを可能にするのがOptionalです。\nOptionalのメソッド OptionalはSingletonのjava.util.Optional\u0026lt;T\u0026gt;をインポートしてオブジェクトを包み、包まれたオブジェクトがNullか否かによってどんな挙動をするかのメソッドを持っています。これからそれらのメソッドを一つづつ見ていきましょう。\nempty() 空のOptionalを作成します。空のOptionalはその名の通り空で、中にラップされたオブジェクトがNullの状態です。\nOptional\u0026lt;String\u0026gt; emtpyName = Optional.empty(); // StringはNull get() Optionalでラップされたオブジェクトを取得する時に使います。\nString value = \u0026#34;Sato\u0026#34;; Optinal\u0026lt;String\u0026gt; name = Optional.of(value); System.out.println(name.get()); // Sato of(T value) 引数として渡したオブジェクトを包むOptionalを生成します。ただ、引数のオブジェクトがNullの場合はget()の結果もNullになります。\nString value = null; Optinal\u0026lt;String\u0026gt; name = Optional.of(value); name.get(); // Null ofNullable(T value) 引数として渡したオブジェクトを包むOptionalを生成するということではof()と同じですが、引数のオブジェクトがNullだった場合はempty()で生成されたOptionalを返却します。\nString value = null; Optinal\u0026lt;String\u0026gt; name = Optional.ofNullable(value); name.get(); // Optional\u0026lt;String\u0026gt; map(Function\u0026lt;? super T, ? extends U\u0026gt; mapper) CollectionやStreamのmap()と似たようなメソッドです。複雑にネストされているフィールドを安全にチェックする時に使います。mapで取り出したオブジェクトは自動的にOptionalでラップされたクラスとなります。\nString name = \u0026#34;Sato\u0026#34;; Student sato = new Student(name); Optional\u0026lt;Student\u0026gt; student = Optional.ofNullable(sato); String nameOfSato = student.map(Student::getName).get(); // Optional\u0026lt;Student\u0026gt; -\u0026gt; Optional\u0026lt;String\u0026gt; ここで使われている::での表現式はMethod Referenceといい、ターゲットレファレンスとメソッドを書くだけで一般的なLambdaと同じ効果を期待できる書き方です。Lambdaで既存のコードをより簡潔に書くことができるようになりましたが、さらに引数の変数名を省略できるようにしたものですね。変数名を書かなくても指している対象が明確でメソッドも一つだけを呼ぶ場合に使います。\n// 引数を標準出力するLambdaの一般的な書き方 Consumer\u0026lt;String\u0026gt; print = name -\u0026gt; System.out.println(name); // Method Referenceに変えた形 Consumer\u0026lt;String\u0026gt; print = System.out::print; // インスタンスの生成 Supplier\u0026lt;String\u0026gt; = String::new; filter(Predicate\u0026lt;? super T\u0026gt; predicate) filter()もまたCollectionやStreamのメソッドに慣れているなら簡単に使えるメソッドの一つです。条件と一致する場合(PredicateによりTrueとなる)にだけ値を返却します。単にNullかどうかの判定だけでなく、何かの処理を付け加えたい時に使います。\n// 伝統的なパターン public String getSato(Student student) { String name = student.getName(); if (name != null \u0026amp;\u0026amp; name.equals(\u0026#34;Sato\u0026#34;)) { return name; } } // filter public String getSato(Student student) { return Optional.ofNullable(student) .filter(s -\u0026gt; s.getName().equals(\u0026#34;Sato\u0026#34;)) .map(Student::getName) .get(); } Optionalの要素は一つしかないのでfilterで指定した条件の結果がfalseの時は以後のメソッドが無視されます。\nisPresent() OptionalでラップしたクラスがNullであるかを判定するためのメソッド。Nullでない場合はTrue、Nullの場合はFalseとなるシンプルなものです。\nString name = \u0026#34;Sato\u0026#34;; Optional\u0026lt;String\u0026gt; studentName = Optional.ofNullable(name); studentName.isPresent(); // true ifPresent(Consumer\u0026lt;? super T\u0026gt; consumer) ラップされたオブジェクトがNullでない場合にだけ実行するメソッドを記述します。\nOptional\u0026lt;String\u0026gt; name = Optional.ofNullable(student.getName()); name.ifPresent(n -\u0026gt; System.out.println(n)); orElse(T other) 引数として渡したオブジェクトがNullの場合にデフォルト値を使います。このメソッドを使った場合はget()は記述しなくてもよくなります。\nString defaultName = \u0026#34;Sato\u0026#34;; Optional\u0026lt;String\u0026gt; name = Optional.ofNullable(student.getName()); String result = name.orElse(defaultName); // student.getName()がNullの場合defaultNameになる orElseGet(Supplier\u0026lt;? extends T\u0026gt; other) 引数として渡したオブジェクトがNullの場合にデフォルト値として指定したLambdaを実行し、その結果を返却します。このメソッドを使った場合はget()は記述しなくてもよくなります。\nOptional\u0026lt;String\u0026gt; name = Optional.ofNullable(student.getName()); String result = name.orElseGet(() -\u0026gt; student.getNumber + \u0026#34;の名前がありません\u0026#34;); // student.getName()がNullの場合Lambdaを実行する orElseThrow(Supplier\u0026lt;? extends X\u0026gt; exceptionSupplier) 引数として渡したオブジェクトがNullの場合に例外を投げます。このメソッドを使った場合はget()は記述しなくてもよくなります。\nOptional\u0026lt;String\u0026gt; name = Optional.ofNullable(student.getName()); String result = name.orElseThrow((BusinessException::new); Optionalで注意すべきこと Nullチェックで便利で安全なOptionalですが、全ての状況でNullに関する処理を全部Optionalに変える必要はありません。Optionalの導入を検討する時、注意すべきことについて説明します。\n性能を意識する すでに気づいている方もいらっしゃると思いますが、Optionalはオブジェクトをラップするものなので必然的に性能の低下と繋がります。なのでNullチェックがいる場面では一旦Optionalを使う、ということはあまり良い考えではありません。簡単なNullチェックはOptionalでなくてもできますし、早いです。\nOptionalを使ってオブジェクトがNullの場合の処理を書く際もorElse()よりはorElseGet()を使った方が良いです。orElse()はNullではない場合も必ず実行されるからです。それに対してorElseGet()の場合はLazy4なメソッドなのでより良い性能を期待できます。\nただ、場合によっては(staticなデフォルト値をフィールドとして持っているなど)、orElse()の方を使った方が良いケースもあるのでその場の判断が重要です。返却したいデフォルト値のインスタンスがどこで作成されるかの時点をよく把握しましょう。\n// よくない例(Nullではない場合捨てられるインスタンス) public Student getStudent(String name) { Student student = this.repository.getStudent(name); return Optional.ofNullable(student).orElse(Student::new); } // 良い例 public Student getStudent(String name) { Student student = this.repository.getStudent(name); return Optional.ofNullable(student).orElseGet(Student::new); } また、戻り値としてNullもしくは決まったデフォルト値を期待する場合はOptionalよりもNullチェックの方が良い場合もあります。\n// よくない例(常に同じデフォルト値が決まっている場合) private static Student defaultStudent; public Student getStudent(String name) { Student student = this.repository.getStudent(name); return Optional.ofNullable(student).orElse(defaultStudent); } // 良い例 public Student getStudent(String name) { Student student = this.repository.getStudent(name); return student != null ? student : defaultStudent; } isPresent()とget()の組み合わせはNG isPresent()でオブジェクトがNullかを確認したあと、get()でオブジェクトを取得するようなコードは結局普通のNullチェックと変わりません。デフォルト値を使いたい場合はorElseGet()を、例外としたい場合はorElseThrow()を活用しましょう。\n// よくない例 public String getStudent(String name) { Optional\u0026lt;Student\u0026gt; student = Optional.ofNullable(this.repository.getStudent(name)); if (student.isPresent()) { // (value != null)の方が良い return student.get(); } else { throw new NullPointerException(); } } // 良い例 public String getStudent(String name) { Optional\u0026lt;Student\u0026gt; student = Optional.ofNullable(this.repository.getStudent(name)); return student.orElseThrow(NullPointerException::new); } また、オブジェクトがNullでない場合にだけ処理を行いたい場合なら、ifPresent()を使いましょう。\n// よくない例 public void adjustScore(String name, int score) { Optional\u0026lt;Student\u0026gt; student = Optional.ofNullable(this.repository.getStudent(name)); if (student.isPresent()) { student.get().setScore(score); } } // 良い例 public void adjustScore(String name, int score) { Optional\u0026lt;Student\u0026gt; student = Optional.ofNullable(this.repository.getStudent(name)); student.ifPresent(s -\u0026gt; s.setScore(score)); } フィールドでは使わない そもそもOptionalはフィールドとして使われる場合を想定していないようです。なぜなら、OptionalはSerializableを継承してないからです。なのでDTOなどでフィールドとしてOptionalを使うとNullチェック以前に問題が起こる可能性があります。\n// よくない例 @Data public class Student implements Serializable{ private Optional\u0026lt;String\u0026gt; name; // 直列化できない } // 良い例 @Data public class Student implements Serializable{ private String name; // 直列化できる } 引数では使わない メソッドやコンストラクターの引数としてOptionalを使うと、それを呼び出すたびに引数としてOptionalを生成する必要があります。また、内部的にOptionalでNullチェックのロジックが入るのでコードも複雑になりますね。こういう場合、内部でどんな処理が行われ、期待通りの処理になっているかわからなくなるので不便です。\nなのでメソッドやコンストラクターの引数は普通のオブジェクトにして、Nullチェックをした方が使いやすく意図した処理を期待できるようになります。\n// よくない例 public class Student { private String name; public Student(Optional\u0026lt;String\u0026gt; name) { // インスタンスを作成するたびOptionalも必要となる this.name = name.orElseThrow(NullPointerException::new); // OptionalでNullチェックおよび代入が必要 } } // 良い例 public class Student { private String name; public Student(String name) { this.name = name; // 期待通りの処理 } } Collectionの中では使わない Collectionの中の要素は無理やり入れない限りNullが入らない場合もあれば、Nullチェックに対応するメソッドを含めている場合もあります。そして中の要素は複数になるので、Optionalを要素として使う場合は性能の低下が必然的に起こります。なので要素ではなるべくOptionalを使わないようにしましょう。また、フィールドや引数と同じく要素を追加したり取得する場合に毎回Optionalを経由しなければならないという不便さがあります。\n// よくない例 List\u0026lt;Optional\u0026lt;String\u0026gt;\u0026gt; names = new ArrayList\u0026lt;\u0026gt;(); names.add(Optional.ofNullable(name1)); // 要素を追加するたびラップが必要 // 良い例 List\u0026lt;String\u0026gt; names = new ArrayList\u0026lt;\u0026gt;(); names.add(name1); CollectionはCollectionで Collectionが戻り値のメソッドの場合、NullだとCollections.emptyList()やCollections.emptyMap()などで空のCollectionを返却した方が良い場合が多いです。Collectionは\nまた、Spring Data JPAを使っている場合はそもそも戻り値がNullだと、自動的に空のListを生成してくれるので尚更Optionalを使う必要がありません。\n// よくない例 public Optional\u0026lt;List\u0026lt;Student\u0026gt;\u0026gt; listStudent() { List\u0026lt;Student\u0026gt; students = this.repository.listStudent(); return Optional.ofNullable(students); } // 良い例 public List\u0026lt;Student\u0026gt; listStudent() { List\u0026lt;Student\u0026gt; students = this.repository.listStudent(); return students != null ? students : Collections.emptyList(); } int/long/doubleはOptionalでラップしない Optionalのバリエーションでは、一部プリミティブ型のためのクラスも用意されています。int/long/doubleの場合がそうです。これらはOptionalInt、OptionalLong、OptionalDoubleで包む方が良いです。\n// よくない例 Optional\u0026lt;Integer\u0026gt; count = Optional.of(100); int countNum = count.get(); // 良い例 OptionalInt count = OptionalInt.of(100); int countNum = count.getAsInt(); 最後に 今は使われているJavaのバージョンが古くても、公式サポートなどの理由でJava 1.8以上にバージョンアップするところも多いと聞きます。ならばJavaプログラマーとして、未来に備えJava 1.8の重要なAPIに慣れて置いた方が良いでしょう。そういう意味でFunctionやOptionalは皆さんにもぜひ使ってみて欲しいAPIでもあります。そもそもJavaがこんなにメジャーな言語になり得たのは、開発しやすいというメリットがあったからなので、さらに開発が楽になるAPIは覚えておいて損はないでしょう。\nJavaもかなり古い言語ですが、最近は急激なバージョンアップと共に関数型言語など最近のトレンドを反映して変化しているところもあります。今は性能も書きやすさも優秀な言語が溢れ出している時代ですが、こんなJavaの変化がどこまで続き、いつまで生き残ることができるか気になります。JVMは依然として強力ですが、LLVMなどより性能が優れた技術も続々と登場していますしね。でも、Javaの変化に適応し、大体のAPIを使うことができたら、他の言語にも適応しやすくなるのではと思います。そういう理由ででも、みなさん、Java 1.8以後のAPIは注目してください。では！\nList, Set, Mapなど複数の要素を持つオブジェクトのことを指します。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nファイルの入出力で使われるInputStreamやOutputStreamではなく、Collectionの要素を一つずつ巡回しながら特定のメソッド(主にLambda)を実行できるようにしてくれるJava 1.8のAPIです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n戻り値が自分自身のため、何度もメソッドをつなげて書くことのできる仕組み。Builderパターンが代表的なメソッドチェーニングの例です。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nプログラミングでLazyということは、とある処理が常にではなく、呼ばれた際に初めて実行される仕組みのことを意味します。必要な時だけ処理が始まるので不要な処理が減ります。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-01-05T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-optional/","title":"Nullチェックの地獄から脱出したい"},{"content":"一般的なJavaプロジェクトなら、外部設定ファイル(YAML)を記載してその値を読み込む場合なら私の以前のポストのようにできます。しかし、今回はSpringプロジェクトとして同じようなことをするようになりました。SpringはYAMLを読み込む時に固有の仕様や使い方がありますね。そしてそうやって読み込んだYAMLの値はBeanに設定することができて、アプリケーションの中ではどこでも@Autowiredを使って呼び出せるというメリットがあります。\nしかし、そんな便利なDIですが、使い方の難点もあります。例えば、普通にnewして使うインスタンスのなかで@Autowiredは使えない問題があるということです。今回もかなりハマっていたことなのですが、Builderでオブジェクトを作成するようにして、使用者が指定してない値はYAMLから取得したBeanを使いたかったです。でもBuilderだと新しいインスタンスを作ってしまうので、Beanを読み込めなくなっていたのでかなりはまりました。\n結果的には違う方法をとると、@AutowiredなしでもBeanを取得することができるということがわかったので、今回のポストではそれに至るまでの過程をコードを持って述べていきたいと思います。YAMLの作成から、newしたインスタンス内でBeanを取得して使う方法を紹介します。\nYAMLからBeanを作る Springではapplication.ymlに以下のように記載して、特定のYAMLを読み込むという指定ができます。\nspring: profiles: active: buildingdefault ここでactiveに記載したものを使って、カスタムYAMLファイルを準備します。ファイル名のプレフィックスとしてはapplication-が入ります。なので今回のファイル名はapplication-buildingdefault.ymlになりますね。\nファイルを作成して、以下のように項目と値を記載します。\nsettings: material: \u0026#34;cement\u0026#34; 作成したYAMLファイルはsrc/main/resourceにおきます。そしてこれからはSpringでYAMLを読み込むためのクラスを作成します。\nSpringでYAMLを読み込み、Beanを作成する方法は二つがあります。一つ目はまず、フィールドにアノテーションをつけてYAMLの項目と紐づくことです。\n@Getter @Component public class DefaultSettings { @Value(\u0026#34;${settings.material}\u0026#34;) private String material; } フィールドに@Valueをつけて、アノテーションの引数としてYAMLの項目名を入力します。こうすることでYAMLから読み込まれた値はString形でBeanに取り込まれます。フィールドは必ずStringである必要はなく、intやdoubleなどのプリミティブ型はもちろん、ENUMにも対応しています。Localeならja_JPなどとYAMLに記載しておくと、ちゃんと取り込まれます。\nYAMLの値をBeanにするもう一つの方法は、フィールドではなくクラスにアノテーションをつけることです。以下のように@ConfigurationPropertiesの引数にprefixを指定すると、指定した項目の配下にあるもの全てがフィールドのマッピング対象となります。\n@Data @Configuration @ConfigurationProperties(prefix = \u0026#34;settings\u0026#34;) public class DefaultSettings { private String material; } YAMLから複数の設定を読み込みたい時 YAMLから設定値を読み込む際に、設定を複数を記載して状況に合わせて使いたい場合もあります。もちろんYAMLでは配列での記載ができますし、Springで読み込む時もこれをListにすることができます。なのでどうやって複数の設定をBeanにするかを説明します。\nYAMLでは以下のように記載します。\nsettings: - preset-name: \u0026#34;default\u0026#34; material: \u0026#34;cement\u0026#34; - preset-name: \u0026#34;cabin\u0026#34; material: \u0026#34;wood\u0026#34; ここでpreset-nameは、実際Javaで設定を使う時にそれぞれの設定セットを区別するためのキー的なものです。なくても値を読み込むには問題がないですが、こうやって名前をつけておくとのちにどれがどれかを分かりやすくなりますね。\nYAMLの記載が終わったら、それぞれの設定セットに合わせてBeanクラスを作成しておきます。\n@Data public class Material { private String presetName; private String material; } 最後に、YAMLを読み込むクラスを作成します。このクラスにBeanのListをフィールドとして記載すると、Springアプリケーションの起動と同時にこれらの設定が読み込まれることを確認できます。\n@Data @Configuration @ConfigurationProperties(prefix = \u0026#34;settings\u0026#34;) public class MultiSettings { private List\u0026lt;Material\u0026gt; presets; } のちにこのクラスからListを取得して、presetNameで各設定値を探すだけで簡単に使えるようになります。\nBuilderからBeanを使う(失敗の例) 今までの設定で、普通のSpringアプリケーション内ではBeanをDIして使うことができるようになります。しかし、今回はDIなしてBeanを取得する方法を説明するためのポストになっていますので、その過程を説明します。\nまず自分がやりたかったことは、先に述べましたが、Builderの中でYAMLの値を読み込んでいるBeanを使うことでした。ここでYAMLに記載した値はデフォルト値として使われて、必要に応じて一部の項目だけbuild()時に上書きしたいです。まず試して、ダメだったコードは以下のようなものです。\npublic class Building { public static BuildingBuilder() { return new BuildingBuilder(); } public static class BuildingBuilder { // DIができない @Autowired private DefaultSettings settings; private String material; public BuildingBuilder() { this.material = this.settings.getMaterial(); } ... } } Builderを使うと、まずBuilderのインスタンスを新しく生成するしかないです。そしてnewしたインスタンスの中では@Autowiredで記載していても、DIがまともにできません。実際上のようなコードを書くと、BeanのフィールドがNullになっていることを確認できます。\nなのでDIのことは忘れて、newしたインスタンスの中でBeanを取得できる方法をとります。\nApplicationContextProviderを作る ApplicationContextは、SpringでBeanの生成やオブジェクト間の関係設定など様々な機能を担当するインタフェースです。ここで重要なのは、ApplicationContextがSpringアプリケーションを起動する時予め登録されたBeanを生成して管理するということです。つまり、このインタフェースにアクセスできればBeanを取得できるということになります。\nただ、ApplicationContextそのものはあくまでインタフェースであるため、インスタンスを取得するためにはその役割をするクラスを作成する必要があります。以下のコードでインスタンスを取れるようになります。\n@Component public class ApplicationContextProvider implements ApplicationContextAware { private static ApplicationContext context = null; public static ApplicationContext getApplicationContext() { return this.context; } public void setApplicationContext(ApplicationContext context) throws BeansException { this.context = context; } } 構造は簡単で、フィールドにApplicationContextがあって、それに対するGetterとSetterがあるだけです。これで動くのも不思議ですが、Springアプリケーションが動作すると自動的にApplicationContextのインスタンスがSetterを通じてフィールドにセットされます。ただ、このクラスのインスタンをnewしては使えなくなるのでフィールドとGetterはstaticにしておきます。\nBuilderからBeanを使う(成功の例) それでは、ApplicationContextのインスタンスを取得できるようになりましたので、Builderを修正します。\npublic class Building { public static BuildingBuilder() { return new BuildingBuilder(); } public static class BuildingBuilder { private DefaultSettings settings; public BuildingBuilder() { this.settings = ApplicationContextProvider.getApplicationContext().getBean(DefaultSettings.class); } ... } } さっき作成したApplicationContextProviderクラスからApplicationContextを取得して、さらにgetBean()を呼び出します。このgetBean()に引数として取得したいBeanのクラスを渡すと、そのBeanのインスタンスを取得することができます。もちろんコンストラクターではなく、フィールドそのものに書くこともできます。そうする場合は以下のようになりますね。\npublic class Building { public static BuildingBuilder() { return new BuildingBuilder(); } public static class BuildingBuilder { private DefaultSettings settings = ApplicationContextProvider.getApplicationContext().getBean(DefaultSettings.class); public BuildingBuilder() { } ... } } 修正したコードを動かしてみると、BeanのフィールドがNullではなくちゃんとYAMLから読み込んだ値が入っていることを確認できます。\n最後に Springを使いながら、恥ずかしくも実際アプリケーションの内部ではどんなことが起きているかを知らなかったので今回は失敗したのではないかと思います。ただ単に動くことを確認するだけでなく、こうして自分の使っている言語やフレームワークの特性をちゃんと理解していないとこのようにハマることはなかったでしょう。なので新しい知識を得た同時に、自分に対する反省もすることになりました。これからはちゃんと自分が使っているものはどう、なぜ動くのかをちゃんと理解してから使わないとですね。\n","date":"2019-12-22T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-bean-with-yaml/","title":"newしたインスタンスの中でBeanを使いたい"},{"content":"Springの代表的な特徴といえば、それは色々ありますが、一つをあげるとしたらやはり@AutowiredによるDI1ではないかと思います。初めてSpringに接したときはオブジェクトがnewなしでも動くと言うことが何ともいえない不思議なことに見えました。これがデザインパターンの一つであるということを知ってからはますますすごいなぁと思いましたね。やはり良いコードを書くためには、様々な方面での工夫が必要なものですね。\nとにかく、こうも重要で便利なDIですが、最近Spring Bootを触りながら気になったことがありました。今までは当たり前のように、@Autowiredはフィールドに宣言していましたが、今回の案件ではコンストラクターにつけている場合がありました。なぜ一部はフィールドにつけ、一部はコンストラクターにつけるんだろう？と思いましたね。結果的には全てのアノテーションをコンストラクターにつけることになりましたが、それが特にフィールドにつける場合との違いを理解させたわけではないので、少し調べてみました。\n結論からいうと、大概の場合に@Autowiredはフィールドよりはコンストラクターにつけた方がいいらしいです。そしてこれを(フィールドやコンストラクターに@Autowiredをつけることを)、それぞれ「フィールドインジェクション」と「コンストラクターインジェクション」と呼ぶらしいです。では、これらをコードを持って説明していきましょう。\nField Injection まずインジェクションのために以下のようなコンフィギュレーションクラス2を定義したとしましょう。\n@Configuration public class Mapper extends ModelMapper { @Bean public Mapper mapper { return new ModelMapper(); } } ここではModelMapperを使ってみます。ModelMapperに関しては、以前のポストにも書きましたが、互いにマッチングするGetter/SetterのあるBean同士のマッピングを自動で行ってくれる便利なライブラリーです。\nこうしてSpringでBeanを登録し、Autowiredアノテーションをフィールドにつけるサービスクラスの例が以下です。これをフィールドインジェクションと呼びます。\n@Service public class ItemServiceImpl implements ItemService { @Autowired private Mapper mapper; } 自分が最初にSpring Frameworkについて学んだ時はこのようなフィールドインジェクトションが一般的でした。しかし、フィールドインジェクションでは致命的な問題があリます。ここでフィールドがNullだった場合もプログラムは動作するということです。クラスの動作に必要な要件が整ってないのにもかかわらず、プログラムが動作してしまうのはバグを呼ぶこととなりますね。なのでフィールドインジェクションはよくないです。\nSetter Injection 実は、インジェクションはSetterを通じても可能らしいです。あまり一般的な方式ではありませんが、これをセッターインジェクションと呼び、コードで表現すると以下のようになります。\n@Service public class ItemServiceImpl implements ItemService { private Mapper mapper; @Autowired public void setMapper(Mapper mapper) { this.mapper = mapper; } } Setterによるインジェクションの問題は、フィールドインジェクションと同じです。Setterで必要なオブジェクトが注入されたかどうかと関係なくプログラムは動く可能性がありますね。この問題を解決できるのが、次に紹介するコンストラクターインジェクションです。\nConstructor Injection コンストラクターによるインジェクションはコンストラクターインジェクションと呼び、コードは以下のようになります。\n@Service public class ItemServiceImpl implements ItemService { private Mapper mapper; @Autowired public ItemServiceImpl(Mapper mapper) { this.mapper = mapper; } } コンストラクターによるインジェクションの良い点は、先に述べたような問題が発生する可能性をブロックできるということです。これはSpringというよりJavaの言語仕様の話ですが、コンストラクターで引数の要件が満たされてないクラスはインスタンスを生成できませんね。そしてNullを注入しない限り、NullPointerExceptionは発生しなくなります。\nまた、コンストラクターインジェクションだと、循環参照3の問題を事前に防ぐことができるというメリットがあります。フィールドインジェクションやセッターインジェクションでは実際のコードが呼ばれるまでは問題を発見することができませんが、コンストラクターインジェクションで循環参照が発生する場合はSpringアプリケーションを起動する時に警告が出力されます。\nまた、フィールドインジェクションの場合はそのクラスの単体テストができないという問題もあります。Autowiredアノテーションがついているフィールドに対してオブジェクトを注入できる方法がないですので。Setterを使うと一旦注入はできるようになりますが、あえてSetterを使う理由はないですね。\nコンストラクターインジェクションが良いもう一つの理由は、フィールドをfinal宣言できるということです。フィールドにfinalをつけることでクラス内でオブジェクトが変更されることを防止できるので、より安全になります。\n最後に 今までは自分も当たり前のことのようにフィールドインジェクションを使っていましたが、フィールドインジェクションの問題を知ってからはなるべくコンストラクターインジェクションとしてコードを書くようにしています。あえてそうしなくても、IntelliJでは常にコンストラクターインジェクションを使うことと警告まで出すみたいで、Springの公式のドキュメントでもそういう言及がありました。これは今までの認識を変えざるを得ません。\nSpringだけでなくJavaコーディングの話をすると、コンストラクターは基本的に書かなくても暗黙的に引数なしのものが生成されるのがJavaの仕様ですね。Singletonクラスや引数の初期化なしで動くと問題になるクラスではこれを防ぐためにわざとコンストラクターを書くこととなっています。なので常にコンストラクターは明示的に書いておく習慣も大事ですね。こういうことも含めて考えると、コンストラクターを記述することの重要性がわかるような気もします。やはり良いコードを書くには、様々な方面での工夫は必要なものですね！\nDependency Injection(依存性の注入)。ネット上に詳しい説明が多いので深くは入りませんが、簡単に概念を説明するとオブジェクトを外部から生成してコードに入れることでオブジェクトの依存性をコードから独立させることを意味します。注入されたオブジェクトはコードに依存してないので、どこで呼ばれても同じものとして機能することができます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nConfigurationアノテーションをつけると自動的にSpring内で設定クラスとして認識されます。ここでオブジェクトをBeanとして定義すると、DIができるようになります。以前はxmlファイルに記入しておく場合もありました。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n循環参照とは、複数のオブジェクトが互いを参照していることを意味します。例えばAクラスのインスタンスを生成する時にBを参照することとなっていて、BクラスもAクラスを参照することとなっていると、どちらかのインスタンスを作成する時に互いの参照を繰り返す無限ループに落ちてしまいます。この無限ループの果てはStackOverflowですね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-12-09T00:00:00Z","image":"https://retheviper.github.io/images/spring.jpg","permalink":"https://retheviper.github.io/posts/spring-dependency-injection/","title":"SpringのDIはコンストラクターでしましょう"},{"content":"実家で使っているルーターは、Asus社のRT-AC58Uです。そして個人的に使っているものは同じくASUS社のRT-AC68U。この二つは単にマシンスペックだけでなく、ファームウェアレベルで提供している機能も少し違います。例えばAiMeshといった機能はRT-AC58Uでは対応していません。\nそしてこないだファームウェアをアップデートすると、管理ページにWANからアクセスするにはhttpsしかできないという制約ができました。RT-AC68Uの場合はLet\u0026rsquo;s encryptでSSL証明書1を作成して更新まで自動で行われる機能を持っているので特に問題はないですが、残念ながらRT-AC58Uはそうではないです。なのでいつもブラウザからRT-AC58Uの管理ページに接続すると証明書がおかしいと怒られます。\nこれから修正される可能性もなくはないと思いますが、最近は802.11axに対応した新型が続々と登場しているので、もう古くなってしまったRT-AC58Uのファームウェアバージョンアップがいつまで続くかわからない状態です。そして毎回証明書がおかしいと怒られるのをみると少し不安になりますね。\n実家と家のルーターの場合、ソフトウェアそのものはそう変わらないだろうと思って調べてみたらやはりそうでしたので、手動でもSSL証明書を入れられる方法がありそうな気がしました。もっと調べてみるとOSはLinuxであって、結論から言うと58Uにも証明書を入れることは成功しました。\n今回はそのSSL証明書をRT-AC58Uに適用させるまでの方法を記載します。\nシステム構成 現在のシステム構成図を絵で表現するとこちら。\nここでやりたいことは、DDNS2を登録したルーターにSSL証明書を入れ、httpsで接続した管理ページで怒られないようにすることです。これを試した理由のもう一つはのちにこのルーターの下にホームサーバーとして機能するLinuxのマシンもおきたいからでもあります。ホームサーバーにはのちに簡単なWebアプリケーションを置いて運用する予定で、今回試したことが成功したら同じ仕組みでそちらにもSSL証明書を適用できると思います。\nそれでは、自分がどうやってSSL証明書を作成してルーターにアップロードし、適用したかを述べていきます。\nルーターの設定(1) ルーターではまず、DDNS設定が必要です。ASUS社のルーターの場合、Chromeなどのブラウザーからhttp://router.asus.comを入力するとローカルのルーターの管理ページにアクセスできます。そして「詳細設定」メニューから「WAN]を選び、さらにDDNSのタブに入って好みのアドレスとして登録します。ASUS社のルーターはasuscomm.comという無料のサーバーを提供しているのでそちらを使いましょう。DDNSの登録ができたら、「管理」メニュー配下の「システム」タブで「WANからの接続を許可」を「はい」にしておきます。私は家から接続するために事前に実家のルーターで予めDDNSの設定をしておきました。\nDDNSでの管理ページ接続設定が終わったら、次はルーターへのSSH接続を設定します。こちらも同じく「管理」ページから設定することができます。SSHを接続設定が終わったらテストをして、なるべく公開鍵でアクセスできるようにして、ポート番号も変えておきましょう。SSHのポートを変えた場合は、ターミナルでは以下のコマンドでアクセスできます。\n# SSHのポートが2022の場合 $ ssh -p 2022 retheviper@javaman.asuscomm.com SSH時のIDとアドレスは管理ページのIDとDDNSで登録したものとなります。ここまでできたらSSL証明書を作成するためのルーター側の準備はまず終わります。\nmacでの設定(1) ルーターのOSはLinuxとなっていますが、やはり重要なコマンドがいくつか足りていないです。代表的にパッケージ管理用のyumやapt、dnfのどちらも搭載されていなく、CPUの性能も怪しいので重要な作業はまずmacで行うことにしました。\nまた、SSL証明書自体はRT-AC68Uで対応しているLet\u0026rsquo;s encryptを使います。こちらは有効期間が90日にすぎないですが、発給も更新も無料となっているのでこういう簡単な作業で使うには最適です。\nまず、ターミナルでLet\u0026rsquo;s encryptをインストールします。\nbrew install letsencrypt インストールが終わったら、certbotコマンドで証明書を作成できます。ただ、証明書を作成する前にDDNSを登録して置く必要があります。私はすでにルーターで提供している機能でドメインを登録してあるので、それをそのまま使います。\nsudo certbot certonly --manual コマンドを入力すると以下のような画面が出力されます。ただ、自分は何回か同じコマンドを実行しているので初めての実行で出力される画面は少し違う可能性はあります。\nSaving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator manual, Installer None Please enter in your domain name(s) (comma and/or space separated) (Enter \u0026#39;c\u0026#39; to cancel): [ドメイン] ルーターで使っているDDNSのドメインを入力してエンターを押下すると次の画面へ。\nObtaining a new certificate Performing the following challenges: http-01 challenge for [ドメイン] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - NOTE: The IP of this machine will be publicly logged as having requested this certificate. If you\u0026#39;re running certbot in manual mode on a machine that is not your server, please ensure you\u0026#39;re okay with that. Are you OK with your IP being logged? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (Y)es/(N)o: Y IPが記録されることに同意しますかという質問が表示されます。同意するしかないのでYを入力。すると以下の画面が現れます。\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Create a file containing just this data: [コード] And make it available on your web server at this URL: [httpアドレス] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue この画面で一旦作業を止め、画面に表示されるコードとURLはコピーしておきましょう。あとでここに戻ってきます。\nPCでの設定 先ほど出力された画面は、「このURLにリクエストを送るので、このコードがレスポンスとして取得できるようにしてください」という意味です。なので一時的にサーバーを立てて、レスポンスできるようにしておく必要があります。\nただ単にサーバー上でアクセスできるファイルを作る方法もありますが、別の方法を試すことにしました。準備するものはルーターに繋がっているPC上でレスポンスの提供ができるサーバーを立てること。ルーターの性能が十分であればルーターでやっても良いのですが、自分のRT-AC58UはPythonをダウンロードして圧縮ファイルを解凍するだけでもしばらく死んでいました。ここではPCにNode.jsを使って簡単サーバーを作ってみます。他にPythonやRubyなどを使っても構いません。これはあくまで自分が最速でサーバーを立てられる方法がNode.jsだっただけです。\n実家のPCはWindowsなので、公式ホームページからNode.jsをダウンロードしてインストールします。また、expressを使ってサーバーを構築することにします。インストールが終わったらコマンドラインからnpmを使えるようになリます。以下のコードでexpressのスタータープロジェクトを作ることができます。\n\u0026gt; mkdir node \u0026gt; cd node \u0026gt; npm install express このあとはVSCodeなどのテキストエディタを使って、以下のコードを作成します。ファイル名はapp.jsにして、先ほどexpressをインストールしたフォルダに保存します。先ほどコピーしておいたURLとコードはは忘れずに入力しておきましょう。\nvar express = require(\u0026#39;express\u0026#39;) , http = require(\u0026#39;http\u0026#39;) , app = express() , server = http.createServer(app); app.get(\u0026#39;/[コピーしておいたhttpアドレス]\u0026#39;, function (req, res) { res.send(\u0026#39;コピーしておいたコード\u0026#39;); }); server.listen(80, function() { console.log(\u0026#39;Express server listening on port \u0026#39; + server.address().port); }); ファイルを保存したらコマンドラインから実行して、サーバーを起動します。以下のコマンドで実行できます。\n\u0026gt; node app.js サーバーが起動したら、ローカルからアクセスできるか確認します。ブラウザーでURLを入力してみて、ちゃんとコードが表示されるかを確認できたらPCでの設定は終わりです。\nmacでの設定(2) PCでサーバーを起動している状態でmacに戻ります。エンターを押下するとサーバーとの通信が始まって、結果として以下の画面が出力されます。\nWaiting for verification... Cleaning up challenges IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/javaman.asuscomm.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/javaman.asuscomm.com/privkey.pem Your cert will expire on 2020-02-14. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \u0026#34;certbot renew\u0026#34; - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let\u0026#39;s Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le SSL認証書の作成が無事完了しました。この画面からは認証書が保存された位置と、満了日を確認できます。また、cerbot renewを入力すると更新ができるということを教えてくれます。\nSSL証明書を作成したので、あとはルーターにコピーして適用するだけです。まず画面に表示されたパスに入って、以下のファイルをコピーしておきましょう。\ncert.pem key.pem コピーができたら、ルーターに接続します。\nルーターでの設定(2) SSHでルーターに接続して以下のパスに移動します。\ncd /tmp/etc ディレクトリー内のファイルをみると、先ほどコピーしておいたファイルと同じものが置かれてあるのを確認できます。viでファイルをあけ、先ほどコピーしておいたもので上書きします。\ncert.pemとkey.pemの上書きが終わったら、次にルーター内でのプロセス目録を確認します。すでに管理ページがhttpdsのサービスとして実行されているため、新しい証明書を適用するためには一回サービスを終了して再実行する必要があります。\nps 以上のコマンドを入力すると、現在実行中のプロセスの目録が出力されます。そのうち、httpds -s -i br0 -p 8443があればそれを終了させます。8443は管理ページで指定したデフォルトのポート番号です。プロセスの左に出力されるのがプロセスのID(PID)なので覚えておきましょう。その後は以下のコマンドを入力します。\n# PIDが562の場合 $ kill 562 # プロセスの再実行 $ /usr/sbin/httpds -s -i br0 -p 8443 \u0026amp; \u0026amp;を入力しないと他のコマンドを入力できなくなるので注意しましょう。入力が終わり、もう一度psを入力してちゃんとプロセスが起動していたらこちらでの設定は終わり。exitを押してsshから抜け出した後は、ブラウザからルーターの管理ページに接続して証明書で怒られるかを確認します。今までの過程で特に問題なかったら、問題ないはずです。\nただ一つ、注意しなければならないことはルーターの再起動です。自分は週に一回は再起動するようにしていますが、こういう場合はせっかく入れたSSL証明書の値が初期化されるみたいです。なので再起動はなるべくしないか、再起動後には証明書を入れ直す必要があります。\n最後に 以上のことで特に問題がなかったら、WANからルーターの管理ページにアクセスしても証明書がおかしいと怒られるようなことはなくなります。これで安心して外からの管理ができますね！\nただ、これで全てが完璧になったわけではないです。残りのタスクは以下です。\n証明書の更新はどうするか ルーターの再起動にどう対応するか Let\u0026rsquo;s encryptで作成してもらった証明書は有効期間が90日なので、のちに更新する必要があります。更新自体はcertbotのコマンドを打つだけで簡単に終わりますが、更新後の処理(ルーターにアップロードする、アプリケーションを再起動する)が必要となりますね。こちらはcrontabでスケジューリングすると何んとかなりそうですが、残念ながらルーターにはコマンドとして入ってなかったです。\nルーターが再起動した時もどう対応できるかは検証対象ですね。最初はscpでファイルを上書き、httpsdプロセスも再起動するようなシェルスクリプトを作ることで対応できるかと思ったら、権限問題があったりするのでより簡単にできる方法はないか考えています。\nま、結果的にLinuxでサーバー構築ができたらWANからルーターの管理ページに接続するようなことはなくなる可能性もなくはないですが。とにかく何かわかったらまたポストとして書くことにしましょう。\nそれでは、皆さんもぜひ、SSL証明書で安全かつ快適なWeb生活を！\nSSL証明書とは、このサーバーは信頼できるかを証明してくれる電子文書のことです。SSL証明書を適用することによりhttpsでの通信は第三者の攻撃より守られます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDynamic Domain Name Systemの略で、家庭用のルーターはIPアドレスが動的に変わることが多いですが、これを文字列のホスト名とつなげてくれる便利なサービスです。ルーターのIPアドレスがどうかわろうが、DDNSの設定ができていたらいつでも同じURLからルーターにアクセスできます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-12-01T00:00:00Z","image":"https://retheviper.github.io/images/linux_terminal.jpg","permalink":"https://retheviper.github.io/posts/linux-implementation-ssl-in-router/","title":"ルーターにSSL証明書を入れる"},{"content":"アプリケーションを作る場合、考慮しなければならないことの一つは「設定ファイルを作る」ことです。設定値のデータがアプリケーションの内部にあると、ディコンパイルしない限りそれを変えられる方法がなくからですね。なので一般的にはアプリケーションの柔軟性のためにも動的に変える必要のある設定値はアプリケーションの外に別途のファイルとして置く場合が多いです。ゲームでいえばセーブファイルみたいなものですね。\n外部のファイルを読み込むことに関してはすでに様々な方法があるとは思いますが、今回は自分で使っている方法を共有します。簡単に、外部設定ファイルはYAMLで記載してプログラムの起動時に読み込んでシングルトンクラスのフィールドとして持す方法です。I/Oを一回だけにできて、どこからも参照できるというメリットがあります。設定ファイルのフォーマットとしてYAMLを選定したのは、書きやすく読みやすいというメリットもあって、自分が主に扱っているSpringで使っているためでもあります。JSONでも問題はないですが、項目が増えるほどJSONは読みづらくなるので…\nとにかく、準備するものはYAMLを読み込むためのライブラリーです。ここではSnakeYamlを使います。ファイルを読み込んでオブジェクト化するだけなので他のライブラリーを使っても構いません。\nでは、まず以下のようなYAMLファイルがあるとしましょう。\nDevelopment: id: \u0026#34;develop\u0026#34; version: 8 use_local_storage: true Debugging: id: \u0026#34;debug\u0026#34; version: 11 use_local_storage: false DevelopmentとDebuggingという二つのケースでそれぞれ違う値を使いたい、というシナリオで簡単に書いてみました。これを利用するコードは以下です。\n// 共用の設定情報クラス public final class Settings { // シングルトンクラスなので自分のインスタンスを持っている private static final Settings UNIQUE_INSTANCE = new Settings(); // 設定ファイルから読み込んだ値を一次的に入れておくためのフィールド private static final Map\u0026lt;String, Map\u0026lt;String, Object\u0026gt;\u0026gt; SETTINGS_FROM_FILE = new HashMap\u0026lt;\u0026gt;(); // 設定ファイル名 private static final String SETTINGS_FILENAME = \u0026#34;application-settings.yml\u0026#34;; // Developmentの設定情報が入るMap @Getter(lazy = true) private final Map\u0026lt;String, Object\u0026gt; developmentSettings = setDevelopmentSettings(); // Debuggingの設定が入るMap @Getter(lazy = true) private final Map\u0026lt;String, Object\u0026gt; debuggingSettings = setDebuggingSettings(); // イニシャライザーブロックで最初からファイルを読み込む処理 static { // ファイルを指定して読み込む ClassLoader classloader = UNIQUE_INSTANCE.getClassLoader(); URL resource = classloader.getResource(SETTINGS_FILENAME); try (InputStreamReader reader = new InputStramReader(resource.openStream())) { // 読み込んだYAMLファイルをパースしてMapに値を取り込む Yaml yaml = new Yaml(); Map\u0026lt;String, Map\u0026lt;String, Object\u0026gt;\u0026gt; importedMap = autoCast(yaml.load(reader)); // 読み込んだ値をフィールドのMapに移す for (Map.Entry\u0026lt;String, Map\u0026lt;String, Object\u0026gt;\u0026gt; entry : importedMap.entrySet()) { SETTINGS_FROM_FILE.put(entry.getKey(), entry.getValue()); } } catch (IOException e) { // 例外処理 } } // コンストラクターは外部からアクセスできない private Settings() { } // Lazy Getterで要請が入った時点でインスタンスを作るためのメソッド private Map\u0026lt;String, Object\u0026gt; setDevelopmentSettings() { return Collections.unmodifiableMap(SETTINGS_FROM_FILE.get(\u0026#34;Development\u0026#34;)); } // Lazy Getterで要請が入った時点でインスタンスを作るためのメソッド private Map\u0026lt;String, Object\u0026gt; setDebuggingSettings() { return Collections.unmodifiableMap(SETTINGS_FROM_FILE.get(\u0026#34;Debugging\u0026#34;)); } // 外部からインスタンスを取得するためのGetter public static Settings getInstance() { return UNIQUE_INSTANCE; } // オブジェクトのキャストをより簡単にするためのメソッド @SuppressWarnings(\u0026#34;unchecked\u0026#34;) private static \u0026lt;T\u0026gt; T autoCast(final Object object) { return (T)object; } } ここではLazy Getterを使ってDevelopmentとDebuggingの設定のフィールドが、get要請が入った時点でインスタンスが作られるようにしています。こうしている理由は、イニシャライザーブロックでファイルを読み込んだ後から個別フィールドに値を入れたい + フィールドはprivate finalにしたい + Settingsクラスはシングルトンとしてプログラムの起動時にインスタンスが生成されるようにしたいからです。Lazy Getterを設定しておくとprivate finalを維持しつつ、フィールドの初期化は後に担保できて、一度インスタンスが生成されるとキャッシュとして残りますので便利です。もしこうでなく、コンストラクターやフィールドで初期化しようとするとその時点がファイルを読み込む前となってしまうので注意しましょう。\n最後に 今回紹介したコードを使うと、設定クラスはシングルトンクラスとしてアプリケーションの起動時にインスタンスが生成され、その時ファイルを読み込んでMapとしてメモリー上に載せ、どこからでも固定された値をGetterで取得できます。もしYAMLの設定がより深くネストしたり、項目が増えたり、ファイル名が変わったりする場合はMapとフィールドを調整するだけで簡単に変更ができますね。\n単純に動くだけでなく、維持補修が簡単で無駄のないコードを書くことこそが重要なので、何か頻繁に変わるデータを扱うためにはこう言った仕組みを(自分が紹介したものと同じではなくても)考える必要があるのでは、と思います。これからもどんなものが良いコードなのかを常に意識しないと、ですね。\n","date":"2019-11-24T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-yaml-for-configuration/","title":"外部設定ファイルを扱うクラスを作る"},{"content":"今回のポストでは、スキルといっても大したものはないですが、自分がコードを書きながらこれは便利だな(もしくは単に格好いいな)と思ったコーディングスキル的なものをいくつか集めました。\nStreamでListの変換 以下のような二つのクラスがあるとしましょう。コードの量を減らすためLombokを使っていると仮定します。\n@Getter public class Item { private String value; } @Setter public class Product { private String value; } 業務的にこの二つのクラスのフィールドであるvalueが同一なものと仮定します。そうすると、Itemクラスからvalueを取得してProductクラスに取り込むことが必要な状況もあるでしょう。このような場合、オブジェクトがそれぞれ一つだけだとあまり問題にならないですね。\npublic Product setProductValueFromItemValue(Item item) { Product product = new Product(); product.setValue(item.getValue()); return product; } 複数の項目をマッピングする必要があるなら、ModelMapperのようなライブラリを使う方法もあります。同じような名前のGetter/Setterがあると自動でマッピングしてくれるので便利ですね。\npublic Product setProductValueFromItemValue(Item item) { ModelMapper mapper = new ModelMapper(); Product product = mapper.map(item, Product.class); return product; } しかし、これらがListやMapに入っているとどうするのでしょうか。普通にFor文のなかで同じくマッピングをさせる方法がありますね。\npublic List\u0026lt;Product\u0026gt; itemListToProductList(List\u0026lt;Item\u0026gt; itemList) { List\u0026lt;Product\u0026gt; productList = new ArrayList\u0026lt;\u0026gt;(); for (Item item : itemList) { productList.add(mapper.map(item, Product.class)); } return productList; } これをStreamとLambdaを利用して、より簡単なコードにすることができます。\npublic List\u0026lt;Product\u0026gt; itemListToProductList(List\u0026lt;Item\u0026gt; itemList) { List\u0026lt;Product\u0026gt; productList = itemList.stream().map(item -\u0026gt; mapper.map(item, Product.class)).collect(Collectors.toList()); return productList; } やっていることはFor文とあまり変わりません。元のリストから要素を一つづつ取り出し、マッピングして新しいオブジェクトを作る。そしてそれを取り集めて新しいListを生成していますね。ただ、map()の引数はLambdaなのでただのマッピングだけでなく、より複雑な処理を入れることもできます。同じことをするとしてもより簡単で短いコードが完成されます。\nCollectionでImmutable Immutable、すなわち不変クラスについては前回のポストでも扱いました。今回はCollectionを使って、そのクラスのListやMapもImmutableにする方法について述べます。下のコードは、Listの例です。\npublic List\u0026lt;Item\u0026gt; returnAsUnmodifiableList(List\u0026lt;Item\u0026gt; list) { return Collentions.unmodifiableList(list); } 同じ方法で、Collentions.unmodifiableMap()でラッピングするとMapもImmutableにできます。こう変換されたListやMapは変更が不可能になるため設定系などのデータを持っている場合に有効活用できます。ただ、Nullが入るとNullPointerExceptionが発生するため注意しましょう。包みたいListがNullになる可能性がある場合はCollection.emptyList()を代わりに入れることができます。\n逆に、ImmutableになったListやMapを変更したい場合は新しいオブジェクトに複製します。\npublic List\u0026lt;Item\u0026gt; returnAsModifiableList(List\u0026lt;Item\u0026gt; list) { return new ArrayList\u0026lt;\u0026gt;(list); } ただ、こうしてオブジェクトを複製してデータを変更する場合、元のImmutableなListにも反映されるので注意する必要があります。\nカスタムクラスをIterableにする とあるクラスの中に、子要素のクラスがListとして入っているとします。例えば、以下のようなイメージです。\npublic class Container { private List\u0026lt;Baggage\u0026gt; baggages = new ArrayList\u0026lt;\u0026gt;(); } 場合によっては、このクラスの中の子要素を全部取り出してFor文を書きたい場合もあるはずです。そういう場合は普通、こんな形で使うのではないかと思います。\npublic void printBaggageNames(Container container) { List\u0026lt;Baggage\u0026gt; baggages = container.getBaggages(); for (Baggage baggage : baggages) { System.out.println(baggage.getName()); } } でも、このクラス自体を拡張For文のなかで使えるとしたら、より便利になりますね。つまり、以下のように使えるとしたいということです。\npublic void printBaggageNames(Container container) { for (Baggage baggage : container) { System.out.println(baggage.getName()); } } もしこれができたら、Getterは要らなくなって、よりシンプルなコードがかけますね。また、Listそのものを取得させるわけではないので、Immutableにする必要もなくなります。\nこれはIterableを使うことで具現化できます。\npublic class Container implements Iterable\u0026lt;Baggage\u0026gt; { private final List\u0026lt;Baggage\u0026gt; baggages = new ArrayList\u0026lt;\u0026gt;(); @Override public Iterator\u0026lt;Baggage\u0026gt; iterator() { return baggages.iterator(); } } こうすることで簡単に、親クラスから子要素を拡張For文の中で使えるようになります。簡単ですね！\n最後に そこまで高級スキル的なものはなかったのですが、覚えておくとどこかで必ず役に立ちそうなスキルをいくつか集めてみました。これらは実際の仕事でも使っているものであって、とりあえず「動けばいい」レベルを超えて行きたい時に有効活用できるようなものではないかと思いました。こういう細かいところでのスキルの差が、プログラマーとしての実力に繋がるものではないでしょうか。そう思って、今後からも何かわかったらまたポストとして作成したいと思います。\nでは、また！\n","date":"2019-11-17T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-skills/","title":"Javaの色々なコーディングスキル"},{"content":"現代のプログラミングは、半分インターネットでなされていると言っても過言ではないでしょう。言語やMWの公式ガイドラインだけではなく、ウェブ上の数多くのコミュニティで情報を得られる時代ですからね。そしてMavenやGradleのように、依存関係の管理自体がネットに繋がっていることを前提としているものもあります。私もそういうトレンドから離れてはなく、自分が書いているコードで問題が発生するととりあえず検索で調べてみる方です。時間はかかっても、大概はそうすることで解決できますね。\nしかし、そんな便利ながらもネットで知識を求める行動にはリスクもあります。果たしてその情報が正しいかという問題ですね。まずコーディングに関する情報だと、私の基準では2年以上立っているものなら信じがたいものとなります。当時はそれが正解だったとしても、今はそうではない可能性がありますので。例えば同じライブラリーを使っているとしても、バージョンアップによりパッケージの構成が変わったり、メソッドのシグニチャーが変わったりしますが、ネットにある全ての情報がそのような変更まで全部反映しているとは思えません。実際動いたというコードが書かれているとしても、そのコードはあくまで普通のテキストであって、現在コンパイルして動かすことのできるものでもないですしね。\n今回の主題であるGradleのdependenciesをどう書くべきかというのも、そういう意味でのものです。私自身も今まで依存関係を書くときは、公式で提案しているコードやブログなどを参照してコピペしていました。しかし、そうしていると同じライブラリーでもcomplieだったりimplementationだったりruntimeだったりしていて、かなり混乱するものでした。結局どんな書き方をとるとしてもその結果は同じように見えるのに、こうして区分している理由は何かと思いました。\nそして結局、その疑問の答えは自分がとある問題に直面することで得られました。なので今回はただ単に理論の話ばかりではなく、問題が起こり得る場所とその解決法についてのものでもあります。\ncompile? implementation? ネットでGradleで依存関係を書く方法を調べると、同じライブラリーでもその書き方がcompileだったりimplementationだったりしますね。現時点でそのうちどれを使ってもまず動きはするので、一見何の問題も内容に感じされます。\nしかし、問題はcompileです。このキーワードは多分、依存関係を表現する最も古い記述法です。実際の検索結果が一番多いような気もしますね。意味的にも、このライブラリーをコンパイル時に使うのような感覚なのでわかりやすいと思います。\nただ、Gradleの4.7バージョンのJava Library PluginのDependency managementタブを参照すると、compileはDeprecated1と書いてあります。このポストを書く時点での安定化バージョンは5.6で、今後6.0が予定されているのでこちらはなるべく使わない方が良さそうです。\n公式の文書を参照すると、compileはimplementationとapiの二つに分けられたらしいです。つまり、これからはcompileの代わりにそのどちらかを選ぶのが望ましいということですね。\nimplementationとapi 既存のcompileだと、必ず「依存関係の伝播」が発生していたらしいです。つまり、Aというライブラリーを使って新しくBというライブラリーを作成したとしましょう。そしてまた、Bに依存するCを作成します。こういった場合、CではBを依存することだけでAにも触れられるようになります。このような状況は、場合によってはまあり望ましくないことになる可能性もありますね。Aをラッピングして、仕様を絞る目的としてBを作成したとしてもCからAを直接扱うことができますから。これはJavaのカプセル化の観点からしてもあまり望ましくないです。\nimplementationでは、この依存関係の伝播に制約をかけています。つまり、BからAに依存するとき、compileではなくimplementationで記述するとCからAを直接参照できなくなるということです。この理由から、最近は多くの場合にcompileの代わりにimpelementationを使うことを推奨しているらしいですね。\nそれに対してapiでは従来通り依存関係の伝播が発生します。BがAに依存しているとき、完全なラッピングではなく、CからAも参照させたいならこちらを使うべきですね。実際、業務でライブラリーをいくつか作っていましたが、一部では大元のライブラリーを参照させる必要があるものもありました。この場合にimplementationを使うとCからのAに対する直接的な参照がGradleとしては認識できなくなったらしく、コンパイルでのエラーが発生することもありました。なのでdependenciesの記述では、自分が作成しているものの性質を正しく理解し記述方法を決めるということも大事ですね。\nこれらの関係を簡単な図として表現すると、このようになります。\n最後に これでシンプルに、Gradleのdependenciesはどう書くべきかについて述べてみました。実際はimplementationとapi意外にもランタイムのみ参照のruntimeOnlyやテスト用のtestImplementationなど、様々な記述方法があるので、状況と場合によっては柔軟な対応が必要かと思います。ただ、大抵の場合は依存関係を整理し、implementationとapiの使い分けを確かにすることが最も重要なことなのではないでしょうか。\nまた、先に述べたように、ネットから得られた情報がAPIの更新事項を確かに反映しているかのチェックも重要ですね。古い情報だと今のコードでは問題を起こす可能性がありますので。そういう意味では、このポストも時間がたてばいつか正しくない情報となる可能性はあります。いや、このポストだけでなく、もしかしたらこのブログ全体で私が書いている情報の全てがそうなのかもしれません。\nコードを書きながら、ネットの情報を参照するときは常にそれが書かれた日付を確認し、公式のドキュメントと見比べる必要があるのではないかと思います。勉強も最新化の方針で！\nDeprecatedは、「オススメしない」という意味です。プログラミングの世界では、何らかの問題があったり必要が無くなったりするなどの理由でこれからなくなる可能性の高い関数を指す言葉となっています。実際、EclipseではJavaの関数に@Deprecatedをつけると関数名に打ち消し線が現れることを確認できます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-11-04T00:00:00Z","image":"https://retheviper.github.io/images/gradle.jpg","permalink":"https://retheviper.github.io/posts/gradle-dependencies/","title":"Gradleのdependenciesはどう書くべきか"},{"content":"JavaのEnumは興味深いものです。読みやすく、同時に複数の値を持たせるということが魅力的です。なので複数のクラスで共通のコード値を扱う必要があったり、DBに連携する場合に使うと便利ですね。私も現在の仕事では積極的に活用しています。また、完全なクラスとして機能しているので単に定数のため使うだけでなく、処理を記述することも可能なので活用できる範囲はかなり広いのでは、と思います。\nなので今回はJavaのEnumの活用法やメリットについて、自分の経験と調べたことをまとめ紹介したいと思います。\n読みやすい コードが読みやすいということは、つまり維持補修に有利ということですね。個人的にはプログラミングの段階は「とにかく動く物を作る」→「共通(重複)処理をメソッドやクラスで分離する」→「他人がみてもわかりやすいコードにする」順に進むべきだと思います。そして全ての段階が最初の設計からできていればなおさらですね。\nテーブルにとあるコード値が項目として存在するとしましょう。DBの種類によってはその項目をbooleanにすることもできますが、char(1)になる場合もあります。そしてこういった場合はそのコード値は二つの状態だけでなく、3つ以上の状態を持つ場合もありますね。サンプルとしてJava内での状態をStringとして持ち、DBにはchar(1)として記録するとしましょう。そういう場合は以下のようなコードが必要となるはずです。\n// Stringで記述されている状態をDBのコード値に変換するメソッドの例 public String toCodeValue(String status) { if (status.equals(\u0026#34;TRUE\u0026#34;)) { return \u0026#34;0\u0026#34;; } else { return \u0026#34;1\u0026#34;; } } このメソッドを使うコードは、以下のようになります。\n// itemオブジェクトにステータスを指定する public void setStatusTrue(Item item) { item.setStatus(\u0026#34;TRUE\u0026#34;); } ハードコーディングされたstatusを定数かすることは可能です。定数は普通のクラス内でも持たせますね。しかし、そういう場合は定数がどのクラスで定義されているかをまず知る必要があります。とこで定数が定義されているかわからない場合は修正も難しくなるし、重複して同じ定数をそれぞれ違うクラスに作成することになる可能性もありますね。\nそして、このコードでは処理の結果を見る前まで状態がどうなっているか確認できなくなる可能性があるという欠点があります。もしどこかで指定されたstatusがTRUEでもFALSEでもない第三の文字列だったら？そういう場合はifの分岐を増やすしかないでしょう。\nこれをEnumを使うコードに変えてみましょう。コードは以下のようになります。\npublic enum StatusEnum { TRUE(\u0026#34;0\u0026#34;), FALSE(\u0026#34;1\u0026#34;); private Integer codeValue; StringEnum(Integer codeValue) { this.codeValue = codeValue; } public Integer getCodeValue() { return codeValue; } } 定数名にそれぞれのコード値を設定し、フィールド・コンストラクタ・Getterを用意するだけです。構造からわかるように、このようなEnumクラスはLombokのアノテーションでも簡単に作ることができます。\n@Getter @AllArgsConstructor public enum StatusEnum { TRUE(\u0026#34;0\u0026#34;), FALSE(\u0026#34;1\u0026#34;); private Integer codeValue; } 作成したEnumを実際活用する場合のコードは、以下のようになります。\n// itemオブジェクトにステータスを指定する Item item = new Item(); item.setStatus(StatusEnum.TRUE.getCodeValue()); // \u0026#34;0\u0026#34;となる Enumを作成することで、入れたい値が明確になります。また、Enumは独立したクラスなのでパッケージを分けて保存することで管理がより簡単になります。あとでコード値が増える場合にもそちらのEnumを修正するだけで住みますね。また、Enum自体をフィールドとして宣言することも可能です。その場合は以下のようになりますね。\n// フィールドにEnumがある例 @Data public Item { private StatusEnum status; } フィールドがEnumの場合は、値の指定がより簡単になります。\nItem item = new Item(); item.setStatus(StatusEnum.TRUE); // TRUEとして保存 // コード値を抽出したいときのコード String itemCodeValue = item.getStatus().getCodeValue(); 複数の値を持つことが可能 そこまで読み易くはならないのでは？と思われる可能性もありますね。確かに、一つの定数につき値が一つだけの場合はそうかもしれません。しかし、Enumの良いところは、定数に複数の値を持たせることも可能ということです。\n例えば二つ以上のDBを使っていて、同じ項目を片方のテーブルはchar(1)、またの方はbooleanで管理しているとしましょう。Enumでは両方を一つの定数として管理することができます。それがどういうことか、以下のコードで確認してみましょう。\n@Getter @AllArgsConstructor public enum MultiValueEnum { Y(\u0026#34;0\u0026#34;, true), N(\u0026#34;1\u0026#34;, false); private Integer charValue; private boolean booleanValue; } コードを見ると簡単に理解できると思いますが、どちらのGetterを使うかによって同じ定数でも違うデータ型のコード値を返却します。これを実際のコードで使うとしたら、以下のようになります。\n// DBの項目がchar(1) Item item = new Item(); item.setStatus(MultiValueEnum.Y.getCharValue()); // \u0026#34;0\u0026#34; // DBの項目がboolean item.setStatus(MultiValueEnum.Y.getBooleanValue()); // true 複数の値を指定可能ということは、配列やListでも可能ということではないか？と思う方もいるかもしれません。結論からいうとYesです。自分も最初からわかっていたわけではありませんが、調べてみるとStreamを使うと値としてListを持たせ、さらにそのListの中の値との比較もできるようです。この場合のコードは以下のようになります。\n@Getter @AllArgsConstructor public enum ListValueEnum { Y(Arrays.asList(\u0026#34;Good\u0026#34;,\u0026#34;Excellent\u0026#34;)), N(Arrays.asList(\u0026#34;Bad\u0026#34;, \u0026#34;Unavailable\u0026#34;)), NULL(null); private List\u0026lt;String\u0026gt; codeValueList; // Listの中に値があるかチェック public boolean hasCodeValue(String codeValue){ return codeValueList.stream().anyMatch(code -\u0026gt; code.equals(codeValue)); } // Listの中の値と一致する場合、その定数を返す(\u0026#34;Good\u0026#34;ならListValueEnum.Yを返却) public ListValueEnum findByValue(String codeValue) { return Arrays.stream(ListValueEnum.getCodeValueList()) .filter(listValueEnum -\u0026gt; listValueEnum.hasCodeValue(codeValue)) .findAny().orElse(NULL); // 当てはまる値がない場合はNullを返す・ } } まだStreamは一部でしか使ってみたことがないのでこういう活用方法は考えてみたことがないですが、どこかで使えそうなコードです。\nまた、フィールドとして使うEnumに特に値がない場合は以下のようなアノテーションを使うことも可能です。\n@Data public class Item { // StringEnum.YESのまま使う場合 @Enumerated(EnumType.STRING) private StringEnum codeValue1; } Classらしき活用 Enumはクラスなので、もちろん処理を持たせる方法もあります。処理を持たせるというのは難しく感じるかもしれませんが、処理をメソッドと思えば良いだけの話です。メソッドのフィールド化する方法についてはLambdaに関する以前のポストを参照してください。\n@AllArgsConstructor public enum CalculateEnum { // Lambdaでフィールドを指定 TYPE_1(num -\u0026gt; num), TYPE_2(num -\u0026gt; num + 10); private Function\u0026lt;Integer\u0026gt; calculate; // 値を入れると処理結果が帰ってくる pulbic Integer calculate(Integer number) { return calculate.apply(number); } } このような形のEnumを使う場合のコードは以下のようになります。\n// 処理結果を取得 Integer type_1 = CalculateEnum.TYPE_1.calculate(10); // 10 Integer type_2 = CalculateEnum.TYPE_2.calculate(10); // 20 最後に 共通の部品として使われるものが重複されていると管理もその値の理解も難しくなり、余計なコードが増えるますね。これをEnumで克服できるということを伝えたかったのですが、いかがでしょうか。ただ、定数を必ずEnumにする必要があるかは、よく考えてみる問題だと思います。場合によってはテーブルとして管理した方が良いのかもしれません。そしてある特定のクラス内でしか使われていないなら、あえてEnumを作る必要もないでしょう。\nそれでもこのようにAPIの活用方法を理解し、覚えておくと、どこかで使っていい場合が現れるのではないかと思います。最初はLambdaをただの「読みづらいコード」としか認識していなかった私も、Functionの存在を知ってからは積極的に使っています。知識のみでなく、その知識を適材適所で活かしていけるようになることが真のプログラマーと思います。まずその判断は難しいかもしれませんが、知識を先に持つことで見えてくるものもあるのではないでしょうか。なので、これからも新しく得られた知識があれば、このブログで紹介していきたいと思います。\n","date":"2019-10-27T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-enum/","title":"Enumを使いましょう"},{"content":"もともと自分が学んでいたのはMavenを利用したSpring MVCのプロジェクトを作る方法でしたが、最近はGradleで管理されるSpring Bootプロジェクトを触っています。最初はGradleを単純にMavenと似たような依存関係用のツールだとしか思ってなかったですが、直接タスクを作ったりJarをビルドしたり色々挑戦してみてからそう単純なものではないということに気付きました。\nその中でも最も良いと思ったのは、それぞれ違う機能を持つ個別のプロジェクトをマルチプロジェクトという概念で管理できることでした。OOPの観点からしてクラスやパッケージだけを分けるのではなく、プロジェクト単位の分け方によってより維持保守に長けているアプリケーションを作られるのではないか、と思います。\nなのでここではまず簡単にGradleのマルチプロジェクトが何であり、その作り方について簡単に説明したいと思います。\nGradle Multi Project Graldeのマルチプロジェクトは、その名の通り複数のプロジェクトで構成されているプロジェクトのことを意味します。具体的には全ての起点となるルートプロジェクトがあり、その配下に複数のGradleプロジェクトが入っている形になります。ルートプロジェクトの配下で管理されるプロジェクトたちはサブプロジェクトといいます。\nこういうマルチプロジェクトの作り方は今回紹介する方法とはまた違うものもあるらしく、プロジェクトのフォルダ構成にも違いがあるようです。\nただ、マルチプロジェクトは複数プロジェクトの管理を簡単にさせるということにその目的があると思いますので、一つのリポジトリ単位で管理できるようにルートプロジェクトの中にサブプロジェクトが入っている形を採用します。これは実際自分が仕事で書いているSpring Bootプロジェクトの構成でもあります。\nまた、ここで紹介しているのはEclipseを基準にした説明となります。(他のIDEはまだ詳しくないので…)\nRoot Project 全てのサブプロジェクトを含む大元となるルートプロジェクトを作ります。一般的なGradleプロジェクトをまず作成します。ただbuild.gradleはこれからサブプロジェクトに使う予定なので、コピーしておきましょう。あとはいらなくなったファイルとフォルダを削除します。ここで残すファイルとフォルダは以下だけです。\ngradle(フォルダ) build.gradle gradlew gradlew.bat 以上のファイルとフォルダを残したら、次はbuild.gradleファイルを修正します。ルートプロジェクトはサブプロジェクトの管理だけをしていて、必要なタスクやプラグイン、依存関係の設定はサブプロジェクトで記述します。なのでなるべくシンプルにしましょう。\nsubprojects { repositories { mavenCentral() } } ここでやっていることは、サブプロジェクトのリポジトリ指定だけですが、同じやり方でサブプロジェクトのbuild.gradleに共通記述しなければならないことはなんでも入れていいです。例えばプラグインとかがあるでしょう。\n次に、settings.gradleファイルを作成します。このファイルはルートプロジェクトとサブプロジェクトを紐付け、プロジェクト名を決める役割をします。\ninclude \u0026#39;core\u0026#39; include \u0026#39;web\u0026#39; rootProject.name = \u0026#39;TestProject\u0026#39; rootProject.children.each { it.name = rootProject.name + \u0026#39;-\u0026#39; + it.name } includeと書いたものはサブプロジェクトで、rootProject.nameはその名の通りルートプロジェクト名を意味します。他にも命名できる方法はありますが、あくまでルートプロジェクト名がこのプロジェクト全体を意味するもので、サブプロジェクトはそのうちの一つの機能単位という感覚て命名しています。\nrootProject.children.each { it.name = rootProject.name + '-' + it.name }では、ルートプロジェクト名にダッシュをつけ、その下にサブプロジェクト名を付けています。これは以下の方法でも表現できます。\nrootProject.children.each { p -\u0026gt; p.name = \u0026#34;${rootProject.name}-${p.name}\u0026#34;} この命名法によりそれぞれのサブプロジェクトはTestProject-coreやTestProject-webのように命名されます。\nこう構成したプロジェクトをEclipseのパッケージエクスプローラからみると、以下のように見えるようになります。\nTestProject TestProject-core TestProject-web あとでもしサブプロジェクトを追加する必要があるとしたら、ルートプロジェクトのincludeを記述することで自動的にTestProject-...という風にサブプロジェクト名が付けられることとなります。\nSub Project ルートプロジェクトの準備が終わったら次は、子要素のサブプロジェクトを作ります。サブプロジェクトは普通のGradleプロジェクトでも良く、ルートプロジェクトの配下に置くだけで使えるようになりますが、ここでは空のフォルダを作成して作る方法を紹介します。\nまずルートプロジェクトの配下にサブプロジェクトとして使うフォルダを作ります。フォルダ名をサブプロジェクト名として使う予定なので、適したフォルダ名を選びます。ここでフォルダ名はルートプロジェクトのsettings.gradleで記述したincludeと一致しなければならないです。例えば上のルートプロジェクトではcoreとwebをすでに書いているので、それに合わせてcoreとwebのフォルダを作成します。\nフォルダを作成したら、その中にルートプロジェクトを作成した時コピーしておいたbuild.gradleを置いて編集します。今回はEclipseを基準にJavaプロジェクトを生成するので以下のようなプラグインが必要です。\nplugins { id \u0026#39;eclipse\u0026#39; } ファイルを保存してルートプロジェクトからgradlew tasksを実行すると、ide節にeclipseがあります。こちらのタスクを実行します。コマンドラインだとgradlew eclipseを入力することで実行できます。もちろんEclipse上でも実行できますが、更新に時間がかかる場合があるのでコマンドラインの方をお勧めします。\nこのタスクを実行することで、作成したフォルダの中にEclipseプロジェクトとして機能するための準備ができます。あとは普通のJavaプロジェクトのようにソースフォルダ(src/main/java)を生成し、パッケージを作成してサブプロジェクトの中を埋めていきます。\nもしマルチプロジェクトがEclipse上で正しく認識されない場合は、プロジェクトを右クリックしてリフレッシュしたり、GradleメニューからGradleプロジェクトのリフレッシュを実行してください。\nSub Project Dependency この方法で作成したマルチプロジェクトでは、サブプロジェクト同士で依存することもできます。例えばwebプロジェクトの起動にcoreプロジェクトで作成したクラスを読み込む必要があるときは、webのbuild.graldeで以下のように記述するとcoreのクラスをインポートできるようになります。\ndependencies { implementation project(\u0026#39;:TestProject-core\u0026#39;) } ただこの場合、webの起動時に一度coreプロジェクトをコンパイルすることになるのでテスト時にcore側の修正事項は即反映されないので注意しましょう。\n最後に まだEclipse上でGradleのマルチプロジェクトの生成を簡単にできる方法がなく、このような多少不便な方法でしかプロジェクトの作成ができないというところが不便ですね。のちに追加されるかもしれませんが…\nでもまずはマルチプロジェクトを構成する方法とその構造がどんなものかが理解できれば、これから活用できる道が増えていくのはでないかと思います。特にプロジェクトの規模が大きくなればなるほど、こういう管理の仕方だ必要になるのではないでしょうか。\nそれでは皆さんもぜひ、Gradleのマルチプロジェクトを試してみてください！\n","date":"2019-10-20T00:00:00Z","image":"https://retheviper.github.io/images/gradle.jpg","permalink":"https://retheviper.github.io/posts/gradle-multi-project/","title":"Gradleでマルチプロジェクトを作る"},{"content":"今回、Oracle認定JavaSE 8 Silverを受験しました。仕事でしばらくJavaを使うことになって、自分の実力がどのレベルなのかを確かめたかったので受けてみましたが、運よくも合格。Java Silverで検索すると合格した人たちはどう勉強したか、難易度はどうなのかについてのブログが多いのでここでは書きません。\nそれでもあえてポストとして資格試験のことを書く理由は、どうしても資格とは実際の業務で必要となる知識とは乖離があるものだからです。資格をまず取得して業務に入る人もいれば、私のように業務でのお作法を経験してから受験する人もいるだろうと思います。なので、私と全く同じ状況ではないとしてもテストで困る問題は多分ある程度その領域が重なるのでは、と思います。その結果として他の多くの方が書いているものとあまり変わらないことを述べているのではないか、とも思いますが…\nもしこれからJavaSE Silverを受験しようとしている方がこのポストを見つけた場合に、少しでも役に立てるような情報になれるといいなと思い、自分の経験として意外とハマったところについて述べます。\n全体的な印象 業務では使わない知識が要求される場合が少なくないです。ただそれが全部無駄な知識とは言えない印象でした。一旦覚えておくと業務でも活かせるのでは、と思う問題もありました。大きく分けてAPIに対する単なる知識を聞く問題と、コードを読んで理解する能力を要求するような問題があります。\n意外と難しいなと思ったのは、IDEに依存できない問題でした。代表的にimportの書き方、コンパイルエラーの見つけだしなどがあります。実際の業務でIDEを使わず、単なるテキストエディタでJavaを書くことはないと思いますが、試験としてはそのような問題の頻度が決して低くないので、問題で提示されるコードはちゃんと読む必要があります。\nそれでは、以下ではテストの個別項目について感じたことを述べていきます。\nデータ型 データ型で最も多く使われるのは、int/double/Stringの三つだと思います。テストでもbyte型やfloat型の範囲などを聞く問題はなかったですが、一部の問題でこれらが出てくる場合があります。例えばswitch文の条件文で使えないデータ型は何かを選ぶ問題など。Bronzeならもっとデータ型について詳しく聞く問題が出てくるのでBronzeの次にSilverを受けるならともかく、私みたいにSilverから挑戦する人なら少し困るかもしれません。\n他にもデータ型と関連する問題の種類は、型変換(暗黙的な型変換とそうではない場合を選ぶ)があります。こちらもbyte, short, long, floatの特徴を確かに覚えないと、意外と正解がわからなくなる問題でした。\nString / Stringbuilder StringやStringBuilderのメソッドに関する問題が多かったです。ただ普段はあまり意識してなかった部分ですが、StringはImmutableなオブジェクトになるためreplaceやsubstringなどのメソッドを使うと新しいインスタンスが返却されるということをちゃんと覚えておく必要があります。例えば以下のような問題があります。\n// 出力される文字列を当てる問題 String s = \u0026#34;this is string\u0026#34;; s.substring(0, 1); System.out.println(s); 元のString自体は変わってないので最初に宣言した通り出力されるのが当たり前ですが、注意深くコードを読まないとsubstringで切り取った文字が出力されるように勘違いしやすいです。(自分だけかもしれませんが…)\n配列 配列をあえて使う場合があまりなく、ほとんどListを使っていたのでこちらの問題でも苦戦しました。配列の宣言の仕方から、配列の中の要素を処理する方法などの問題がよく出てきましたが、配列の宣言には様々な方法があるので少し選択肢が怪しく思われます。\nもちろんListは基本が可変・スレッドセーフではないので場合によっては業務でも配列を導入する必要はあるかもしれません。覚えておいて損ではないと思いますので、配列については確実に覚えておきたいと思いました。\nアクセス修飾子 / 継承 defaultとprotectedの違いを問う問題があります。普段からパッケージ構成や継承を考慮した実装をしていたならともかく、大抵の場合はpublicとprivateのみを使っていて他を忘れやすい場合もあると思うので、ちゃんと覚えておきたいものでした。\nまた、継承の問題でアクセス修飾子が重要なキーワードとなる問題もありました。例えばスーパークラスからオーバーライドーしているメソッドをより狭い範囲にするようなコードを提示して、そのコードを実行したらどうなるかを聞く問題など。正常実行されると思ってその結果を答えとして選んでも、「コンパイルエラー」が正解だったりします。\n継承に関してはかなり問題の量が多いような印象を受けました。Interfaceとabstractの違い、継承の仕方、インスタンスと参照の違い、キャストなどバリエーションも多く、どれも実務では使われているので慣れてはいるものの、問題自体もそのためか知識だけよりは少し複雑な方法でミスを招こうとしている印象でした。コードを注意深く読む必要があります。\nラベル ラベルについてはこの度初めて接したので全然知らなかったですね。しかし二重ループで使うとかなりパフォーマンスの改善を図れると思いますので、これはテストだけではなく覚えておくと良いこつと思います。ただ、ループ以外で使うことはそんなにないかも…とも思います。\n問題としてはラベルがついたループ文ないでif文を書き、結果がどうなるかを聞く問題が少しありました。例えば以下のようなものです。\n// 出力がどうなるかを問う問題 int num = 0; x: for (int i = 0; i \u0026lt; 10; i++) { if (i % 5 == 0) { continue x; } System.out.println(num); num++; } ラベル自体よりは条件文とcontinue / breakの違いをちゃんと理解しているかを聞くような問題とも言えると思いますが、とにかくラベルとはなんぞや？となると、一旦答えることができなくなります。もちろん知識としてラベルをつけることができるのはどこか、という問題もありました。\n例外 Javaの例外の特徴や、try-catch文での挙動に関する問題が出てきます。業務ではカスタム例外を作ることもあるのでExceptionとRuntimeExceptionの違いについて理解しているならそこまで問題となることはないと思いますが、提示されるコードで見逃す可能性があるのは「throws宣言があるかどうか」、「catchでスーパークラスの例外を宣言しておいてその次にサブクラスをまた書いてコンパイルエラーになる場合であるか」のような部分です。上位の例外をcatchすると、下位の例外は書く必要がないので注意しましょう。\nIDEを使える環境だったらあまり気にする必要ない部分なので、意外と見逃せやすい部分ではないかと思います。\nLambda JavaSE SilverではJava8ならではのAPIについて聞く場合が少なくないです。Lambdaもそうですね。ただ、大抵のLambda関連の問題は正しい書き方やFunctionの種類に関する知識を問う問題です。慣れてなくても、覚えておくと大丈夫でした。\n書き方については括弧とreturnの省略が正しく書いてあるのかについての問題で、例外同様IDEだとすぐコンパイルエラーになるため見逃しやすい部分でした。\nLocalDate / LocalTime / LocalDateTime 今まで自分が使っていた日付関連のAPIはjava.util.Dateとjava.sql.Dateしかなかったので、苦戦した問題でした。こちらもStringと同じくImmutableなので、似たような問題が出てきます。例えば以下のような問題です。\n// 出力がどうなるかを問う問題 LocalDate date = LocalDate.now(); date.plusDays(5); System.out.println(date); Stringと同じく、Local〜類のAPIは値を操作するメソッドが新しいインスタンスを返却することを覚えておくと良いだけです。ただ、時間や日付の出力フォーマットに関する問題が出るのでこちらは覚えておく必要がありました。APIを見るとjava.sql.Dataにも変換ができるのですが、変換に関する問題は出てませんでした。\nその他 問題集だけだと複数の選択肢を選ぶ問題なのに一つしか選んでなかったりしていて、本場でこうなると困るな…と思っていましたが、実際のテストでは単一選択肢だとラジオボタン、複数だとチェックボックスで選ぶようになっているので心配ありませんでした。チェックスボックスなのに一つだけ選んでいたりすると次の問題に移る前に警告も出ました。\n一部ループでの処理などを直接計算する必要がある場合もありますが、多くは時間がかからなく即答できるものです。時間が足りないということはほぼないと思いますので、問われているAPIに関してその特徴をよく覚えると簡単に合格できる資格なのでは、と思います。\nただ逆に、私みたいに業務に慣れていて基本を忘れていたり知らなかったりするとハマるところも少なくないと思います。先に述べたように時間が足りなくなることはあまりないので、コードが提示される問題は注意深く読みましょう。そしてコードが提示される問題の答えは実質的に「処理の結果が出力される」、「コンパイルエラーになる」、「実行時に例外がスローされる」の三つだけなので、まずどれに当て嵌まるかを考えてみると良いと思います。\nそれでは、これから受験する皆さんも合格できますように！\n","date":"2019-10-06T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-se-8-silver/","title":"Oracle JavaSE 8 Silverについて"},{"content":"最近仕事で作っているのは、固有ライブラリーです。ただ思っていたことと違ったのは、まず完全自作ではなく既存のライブラリーを改良するような物であるという点。そして二つ目はSpring Bootで作られているということです。Spring bootは今回初めて触れましたが、以前Spring MVCでWebサイトを作ってみた経験はありました。ただSpringはWebアプリケーションを作るためのフレームワークなので、これを持ってライブラリーを作ったり、Webページのないアプリケーションを作るということは想像もしたことがなかったです。\nそれが仕事で両方の使い道に触れることがあって、まだ世の中自分の知らないものばかりだなと思いました。プログラミングとは言語やフレームワークを使えるかどうかの問題だけでなく、どう活用してどんな物を作るかの問題も含めて考えなければならない物ですね。そして画面はなくてもMainクラスを作ることでJarから独立実行ができて、インポートしてはライブラリーとして使える独特？なしようとなりました。そしてもちろんそうするためには少し準備は必要となりました。\n表題では短くなりましたが、今回の主題であり仕事での要件は実際はこういう物でした。\nGradleのtaskで実行するSpring bootアプリケーションにして、起動時にはコマンドライン引数を渡す こうして書いてみると複雑のようですが、実際はそこまで難しい概念ではないと思います。まずGradleはMavenのようなライブラリー管理ツールとして知られていて、gradlewに様々なオプションをつけることでJarのビルド、テストなどの行為ができますね。ここでしたいのは、そんなGradleでできるタスク(オプション)を追加することです。そして追加されたタスクを実行すると、Spring Bootプロジェクトとして作成されているライブラリーのMainクラスを起動させることです。\nただ、皆さんにもわかるように、Sping BootプロジェクトでMainクラスが用意されていると、普通にjava -jar project.jarという風にコマンドラインから起動させることができます。なのになぜあえてGraleのタスクに入れようとしたかというと、以下のような理由があります。\nマルチプロジェクト構造となっている 現在の設計で、ライブラリー全体のプロジェクトは複数のサブプロジェクトを含むマルチプロジェクトとなっています。これでちゃんと伝わるか分かりませんが、とにかく表で表現すると以下のようになっています。\nrootproject ┣ target ┣ generator ┗ runtime 簡単に説明しますと、現在のライブラリーはgeneratorというサブプロジェクトを利用して(runtimeはその時generatorから参照します)targetプロジェクトのコードを操作します。ここで最終的にgeneratorとruntimeはJarとして提供され、使用者はこのライブラリーでの処理を適用したいプロジェクトをtargetの位置に置いて使うことになります。こうなった場合に、コマンドラインからruntimeに依存しているgeneratorのJarを起動してtargetのソースファイルを操作するように指定するのはかなり面倒臭い作業となりますね。これを自動化したかったです。\n起動時に渡したい引数が多い Gradleのタスクとして実行したかったもう一つの理由は、ライブラリーが処理を行うために必要な引数の種類が7つくらいがあって、これを全部覚えるのは難しいからです。それにタイポにより処理に失敗する可能性も上がりますね。ここで思ったのが.batファイルを用意していて、使用者がそれを修正したらいいだけの話ではないかと一瞬思いましたが、やはりあまり良い方法ではなかったです。Gradle自体も今はそうなっていますが、別途のファイルを用意するということは、OSごとにそのファイルを作成する必要があるということですね。使用者がどんな環境で実行するかわからないのでとりあえず.batと.shの両方を準備する必要がありますね。\nそれに、このライブラリーがWindowsやLinuxでしか使用されないだろうと言い切れないので、そういう場合はより変数は多くなります。そうなるといちいちOSや環境に合わせて、コマンドライン引数を渡すための方法を作らなければならないですね。わざわざそんなことをするよりは、Gradleのタスクとして用意し(もちろん引数はファイルに記載して読み込まれるように)、環境のことはGradleにお任せした方がコードの管理や便宜性という面からして良さげな気がしました。統一感もあって、使用者にも良い印象になりそうですしね。\n[target] build.gradle まずはタスクを実行したいtargetプロジェクトの方から始めましょう。Gradleプロジェクトは基本的にbuild.gradleというファイルを持ち、このファイルを修正することで依存関係やプラグインなど様々な設定ができますね。同じく、カスタムタスクを追加したい場合もこちらに追加したいタスクの内容を記載します。\napply from: \u0026#39;default.gradle\u0026#39; // このファイルを読み込むという意味 task taskname(type: GradleBuild) { // タスク名とタイプの設定 group = \u0026#39;application\u0026#39; // gradlew tasks コマンドから、applicationタブにこのタスクが追加される description = \u0026#39;run program\u0026#39; // このタスクの説明 dir = \u0026#39;../generator\u0026#39; // タスクの実行基準位置 tasks = [\u0026#39;bootRun\u0026#39;] // 実行する内容 startParameter.projectProperties = [args: \u0026#39;${defaultArgs}\u0026#39;] // コマンドライン引数としてdefaultArgsを読み込む } ここでdefault.gradleを読み込んでいる理由は、そのファイルにデフォルト値を記載しておき、タスクを実行する時に読み込んだ値をコマンドライン引数として使うためです。最後の行でdefaultArgsと書いてありますが、これがデフォルト値を変数にしたものです。こうやってファイルを分離することで実際の使用者がこのタスクを実行するときは、default.gradleを修正して引数として渡される値だけを調節することになります。\n[target] default.gradle 次に、build.gradleでタスクを実行する時に読み込まれるファイルの設定は以下のようになります。ここでは読み込まれる対象としての設定と、変数の形で宣言したコマンドライン引数を記載するだけです。\next { // 読み込まれる対象と表記 defaultArgs = \u0026#39;-arg1 value -arg2 value\u0026#39; // arg1とarg2の二つの引数がある場合 } こちらは簡単ですね。引数名が書いてあるので順番は関係なく、あとはそれぞれの値を変えるだけでよくなります。\n[generator] build.gradle それでは続いて、タスクで実行される側の設定です。 targetのタスクでgeneratorをbootRunすると指定していたので、それに合わせてbootRun時の挙動を設定します。例えば引数をどんな形で受け取るか、メインクラスはどれかという設定ですね。\nbootRun { // bootRun時の挙動 if (project.hasProperty(\u0026#39;args\u0026#39;)) { args project.args.split(\u0026#39;\\\\s+\u0026#39;) // コマンドライン引数がある場合、空白を基準に分割する } } jar { // Jarとしての設定 manifest { attributes \u0026#39;Main-Class\u0026#39;: \u0026#39;com.package.Main\u0026#39; // メインクラスのクラスパス(パッケージとクラス名)の指定 } } コマンドライン引数を分割する理由は、皆さんも予想しているとは思いますが、Javaのメインメソッドは普通文字列の配列として引数を受け入れるからです。こうやって分割しておくと実際の処理で引数のパースが簡単になりますね。\nここまでこればGradleの設定は終わりです。あとはJavaでのメインクラスの設定です。\n[generator] Main.java generatorのJarを実行した時に呼ばれるメインクラスを作ります。ここでは一般的なJavaのメインメソッドと、Spring bootとしてのメインクラスの作法、JCommanderでコマンドライン引数をパースするための作法、Lombokが混在していますのでそれぞれに対する知識のない方には少し難しいコードになっているかもしれません。\nただ、実行されている時の動作としては単純なものになっているので、Springのアノテーションにある程度慣れている方ならすぐに理解できると思います。まずコードは以下のようになります。\n@SpringBootApplication // Spring bootとしてのメインクラスにつける @EnableAutoConfiguration(exclude = { DataSourceAutoConfiguration.class }) // H2関連エラーが出たので付けました public class Main implements CommandLineRunner { @Autowired private CoreProcessClass coreProcessClass; // @Componentとなっている実際の処理クラス @Override public void run(final String... args) throws Exception { // CommandLineRunnerを継承するして実行時の動作をオーバーライドする final CommandLineOptions options = CommandLineOptions.parse(args); // パースと同時にBeanを生成 coreProcessClass.startProcess(options.getArg1, options.getArg2); // 本処理開始 } public static void main(final String[] args) { SpringApplication.run(Main.class, args); // メインメソッドとしては引数を渡すだけ } @Data public static class CommandLineOptions { // コマンドライン引数をパースするクラス // JCommanderを使用した引数の設定 @Parameter(names = \u0026#34;-arg1\u0026#34;, description = \u0026#34;File\u0026#34;, required = true, converter = ArgumentsToFileConverter.class) // 引数は文字列なので、コンバータクラスを使う private File arg1; @Parameter(names = \u0026#34;-arg2\u0026#34;, description = \u0026#34;String\u0026#34;, required = true) // 普通の文字列の場合 private String arg2; private CommandLineOptions() {} public static CommandLineOptions parse(final String... args) { // 実際のパースを行うメソッド try { final CommandLineOptions options = new CommandLineOptions(); JCommander.newBuilder() .addObject(options) .acceptUnknownOptions(true) .build() .parse(args); return options; } catch (ParameterException e) { e.getJCommander().usage(); throw e; } } } public class ArgumentsToFileConverter implements IStringConverter\u0026lt;File\u0026gt; { // JCommanderで引数をオブジェクトに変えるためのクラス @Override public File convert(final String argument) { return new File(argument); } } } JCommanderを使うことでコマンドラインのパースは簡単にできます。ここでいうパースは単純に文字列だけを意味することではなく、必須項目としての指定(ない場合は例外となる)やオブジェクト変換などの様々なことができるという意味です。例えば引数として渡した文字列があるものはファイルパスだとしたら、それを読み込んでFileオブジェクト化したり複数の引数をListとして取得することもできます。\nそしてパースしたオブジェクトを、本処理で使われるオブジェクトに渡すだけで終わり。意外と簡単に終わりますね。\n最後に 実は、今まで説明した内容は自分で考えて作り出したものではないです。最初はGradleタスクを作るために導入したライブラリーの問題があってなかなか仕事が進まなく、すでに出来上がっていたものを参考にしたものにすぎません。でもここであえて紹介するのは、皆さんに共有する価値があると思ったからです。\n特にサーバーで動くアプリケーションを作る場合はやはりコマンドラインで起動させる場合が多いですし、環境によっては引数で違う値を渡す必要があるかもしれません。そのような時に、このようにファイルから引数を渡して実行するタスクを作成して環境ごとに設定を変えたり違うファイルを読むようにするとかなり便利そうですね。なので皆さんにもぜひ紹介したいと思いました。\nそして個人的には、今は自分も先輩方の成したものから学ぶばかりですが、いつかはこんな考え方もあるんだと後輩に伝えられたらいいなと思わせる、大事な経験でした。これから先はまだまだ遠いですね！\n","date":"2019-09-17T00:00:00Z","image":"https://retheviper.github.io/images/gradle.jpg","permalink":"https://retheviper.github.io/posts/gradle-command-line/","title":"Gradleからコマンドライン引数を渡す"},{"content":"最近のJavaはバージョンアップが早いですね。自分が初めて学んだものは1.8でしたが、すぐ9がでて今は13もリリースを目前としています。バージョンアップはバグ修正や性能の向上など良い面が多いためなるべく自分が使っているプログラムは常に最新のバージョンを維持したいですが、言語のバージョンが上がる度、何が変わったかを確認しすでに存在するコードを見直すのは簡単な問題ではいですね。\nJavaはかなり歴史が長いので、現在のトレンドと比べてみると不便(パラダイムが変わったからという理由が多いと思いますが)な面が多いです。そして1.8が維持された期間が長かったのですが、そのため流行には遅れていますね。10になって型推論が導入されるなどトレンドを追いかけているような面もありますが、Kotlinのように同じくJVMを使う言語と比べてみるとまだ先が遠い印象はあります。\nもちろん変化は肯定的なものであって、元の特徴を維持しながらもトレンドに合う書き方ができるようになったというのは、その言語を使えるユーザーのプールが広くなったとも評価できるでしょう。でも、全ての要素において「古いものと新しいもの共存」ができるわけではなさそうです。そういう場合はどちらを使うかを選択する必要がありますね。\n今回のポストで話たいModuleがその代表的なものです。昔からの問題を改善するために導入されたものですが、結局は既存のコードに影響を与えてしまい、対応が必要となる部分です。最初は自分が書くコードでは考慮する必要がないものだろうと思っていたのですが、どうもそうはいかなかったです。なのでここでは、JavaのModuleが何であり、どんな問題を経験したかを述べたいと思います。\nProject Jigsaw ModuleはProject Jigsawという名で、1.7から導入を検討していたものらしいです。Moduleという名からわかるように、アプリケーションを起動する時読み込むライブラリー(Java内臓の)を選択することができるシステムです。1.8まではコマンドラインで起動するアプリケーションを作るとしても、基本的なシステムライブラリーであるSwingなどが含まれていたのですが、それを調整できるようになりました。要らないシステムライブラリーを除去するとアプリケーションのサイズも小さくなりますし、メモリーを節約できるというメリットもありますね。また、Javaの特徴でもあった「完璧にロードされるまでには時間がかかる」という問題も、このModuleの設定である程度解消できるようになりました。\nそのほかにも、パッケージの「Publicすぎる問題」も、Moduleで解消できるようです。JavaのクラスはProtected宣言で同じパッケージでアクセスできるようにできますが、パッケージが細かく分けられた場合は同じライブラリーの中でもアクセスできませんでした。そういう場合はPublicで宣言するしかなかったですね。Publicで宣言されたクラスはライブラリーの中だけではなく、どこでもアクセスできるようになるため問題が生じる可能性もあります。ライブラリーを作りながらクライアントに使って欲しいクラスと使って欲しくないクラスを分けることが難しいことだったということです。これをModule設定により外部へ公開するクラスと、ライブラリー内部に向けて公開されるクラスで分けられるようになりました。\nModuleの実例 では、Public問題をModuleでどう解消できたかを、コードを持って説明します。まだ自分もModuleを積極的に使っているわけではないので基本となる部分だけですが、重要なポイントは以下の三要素だそうです。\nName Exports Requires まずNameは、Moduleそのものの名称を意味します。パッケージ名と同じ命名規則で書きます。次にExportsは、このModuleから外へ公開するパッケージのことを意味します。ModuleではPublicであっても、Exportsと明示されていないパッケージは外部からアクセスすることができません。そして最後にRequiresは他のModuleに対する依存関係を表します。\nこれらを実際のコードで書くと、以下のようになります。デフォルト・パッケージにmodule-info.javaとして記述されます。(Java9以後のシステムライブラリーから確認できます)\n// module-info.javaの書き方 module com.module.mylibrary { exports com.module.mylibrary.api; requires com.module.exlibrary; } Exportsの場合、公開対象を指定することができます。つまり、アクセスできるMobuleを指定することができるということです。\n// exlibrary限定のPublic設定 module com.module.mylibrary { exports com.module.mylibrary.api to com.module.exlibrary; } Moduleはもちろん外部ライブラリーに対しても使うことができます。module-info.javaを作成する方法もありますが、Java9以前に作られたライブラリーの場合にはそれがない可能性が高いですね。このようにModuleかされていないライブラリーを含ませる必要がある場合はAutomatic ModuleかUnnamed Moduleの二つの方法からライブラリーを分けて使うことになります。両方自動的にModuleとして扱われるという面では同じで、全てのパッケージにアクセスできるという面では同じですが、前者はmodulepathに属するものとして名前がある(Jarファイル名となります)ことに対して、後者はclasspathに属するもので名称がないためRequiresで指定することができません。\nModuleでハマったところ 自分がModuleであった問題は、同じパッケージをもつ二つのライブラリーの競合によるものでした。問題が生じたのは、既存のプロジェクトにGradleのタスクを追加しようとしていたので原因でした。Gradleのタスクを作成する方法はbuild.gradleに直接taskを作成することでもできますが、最初自分が参考にしていた方法(Gradleの公式文書に従いました)ではjava-gradle-pluginというプラグインを含ませる方法でした。こうすると自動的にJavaのライブラリーが追加されて、Javaでプラグインを書けられるようになりますが、ここに含まれているライブラリーがJavaのシステムライブラリーと競合を起こしました。\n元のプロジェクト(Java11を使っています)ではjavax.xmlをインポートしていて、これがJava9からはDeprecatedになり、最終的にJava11から除去されたらしいです。それがEclipse上ではUnnnamed Moduleとして読み込まれていたらしく、ちょうどjava-gradle-pluginのパッケージにも同一名のパッケージが含まれていたので競合が起こったのです。そもそも除去されている扱いなので競合が生じるのがおかしいですが…エラ〜メッセージではThe package javax.xml.transform is accessible from more than one module: \u0026lt;unnamed\u0026gt;, javax.xmlと出力されていました。\n似たような事例を参考にすると、二つの解決法が提示されていましたがどちらも自分のプロジェクトでは使えませんでした。module-info.javaを作成するとマルチプロジェクトとなっていてサブプロジェクト間のパッケージ依存関係まで考慮するには複雑な手続きが必要でしたし、Eclipseのモジュール依存関係設定からシステムライブラリーのjavax.xmlを除去すると、他にインポートしているjava.sqlがjavax.xmlに依存しているのでこちらも使えなくなるという問題がありました。\nそしてリンクの文を読んでみると、最新のJava13までこの問題(自分のケースと完璧に一致しているとは言えませんが)は解決されてないというので、どうしようもない状態でした。java-gradle-pluginはGradleで管理されているライブラリーなのでこちらからうかつに手を出すこともできませんでした。\n結局どうしたらいいか 現時点では、外部ライブラリーを維持したまま競合だけを避ける方法はなさそうです。自分のModuleに対する理解がまだ足りてないことも原因かとは思いますが、結局はこのような事態が発生するとなるべく競合の原因となるライブラリーを除外するしか他の道はなさそうですね。便利さのために導入された新しい機能が、思わぬところで問題を起こしてしまうのはそう珍しいことでもないですが…3日ほど悩んだ私の選択は結局、そのライブラリーを使わないということしかなかったです。\nもちろん、Moduleの問題なのでバージョンに対するこだわりがなければJavaを1.８に下げるという方法もあります。ただ1.8はいずれサポートが終わるはずで、これからもJavaのバージョンはどんどん上がっていくはずなのでいつかは直面することになるかもしれない問題ですね。どうかJava14ではこのような問題が起こらないことを祈ります。\n","date":"2019-09-08T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-conflict-of-module/","title":"Moduleの問題に会った話"},{"content":"Pythonのような本格的なオブジェクト指向言語ではあまり見かけられないことですが、Javaではいわゆる参照型の以外にもプリミティブ型というものがありますね。どうもJavaが初めて世に出た時代はまだオブジェクト指向という概念が生まれたばかりだったのでそうなったのではないかと思います。このプリミティブ型が存在するという点から、Javaは完全なオブジェクト指向言語ではないという話もあるようです。\nプリミティブ型に対する定義は言語ごとに少し違うようですが、私が知っているのはJavaだけなのでJavaの基準からいうと、プリミティブ型はオブジェクトではないデータ型を指す言葉です。そしてそれは、メモリー上に載せたデータをどう持つかの観点がオブジェクトとは違うということです。オブジェクトはメモリー上のデータが位置する「アドレス」を指すことに対して、プリミティブ型はそれぞれ独立したメモリー領域にデータを載せます。\nこれを証明するのが、条件文での演算子の違いです。プリミティブ型で、二つの変数が同一なデータを持っているかを比較する演算子は==ですね。しかし、同じ方法でオブジェクト、よく挙げられている例としてStringだと、同じ方法を使えません。equals()を使わないとStringでの正確な値の比較はできなくなりますね。なぜならオブジェクトが持っている値そのものはメモリーのアドレスなので、「同じ値を入れた」つもりでもそれぞれのオブジェクトが指しているメモリーのアドレスは違う可能性があるからです。\nこのように、メモリー問題はプログラミングに対してはかなり重要なものです。いくらメモリーの絶対値が増えても、メモリーに載せられたデータをどう参照するか、どう扱うかを間違えたら思い通りにプログラムは動かない可能性があるからです。そしてまた重要なのは、メモリー上に載せたデータを参照する方法だけではなく、どう管理するかということです。正確なデータを入れたつもりが、途中で変わったりすると、参照の仕方が正しくてもプログラムが正常動作しない可能性がありますからね。\nそういう意味で、今回はImmutableなクラスについて述べたいと思います。\nImmutable Objectとは Immutavle Objectとは、簡単にいうと「一度インスタンスが生成されたら、そのインスタンスが持つデータが変わらない」オブジェクトのことです。逆に、インスタンスの生成以後もデータが変わる可能性のあるオブジェクトはMutableと言います。代表的なMutableクラスとしてはBeanを挙げられますね。Setterを通じて自由にデータを変えることができます。そしてImmutableなクラスとして代表的なのは、Stringと言います。Stringは値を代入しても、元のデータはGarbage Collectorの対象になるまでメモリーに残り、そのStringオブジェクトが指すメモリーのアドレスだけ変わるからです。\n先に述べたように、プログラムの中でオブジェクトが持っているデータが途中で変わると、安定的な動作を保証できなくなります。そしてマルチスレッド環境では、二つのオブジェクトが同じメモリーアドレスを参照していると、スレッドそのものが止まってしまう可能性もあります。もしそのような場合が生じるとどこで問題が起きたか調べることも難しいですね。このような問題を回避するため、Immutableなクラスを作成するときは「インスタンスの生成以後はデータが変わらない」ことと、「クラスの持つデータが同じメモリーアドレスを参照しないように」します。\nそれでは、Immutableなクラスを作成する方法にはどんなものがあるか、見ていきましょう。\nsetterは使わない 最近はかなりLombokを使う場合が多く、ある程度定型的なコードを生成してくれるので、Lombokのアノテーションを使って生成されるコードを持って説明したいと思います。Lombok自体の紹介は、以前のポストを参照してください。\nLombokでは、アノテーションをつけることで簡単にBeanを生成できます。クラスの上に@Dataをつけることで、簡単にSetterとGetterができますね。これを使った場合、直接メソッドを手で書くより安定的でコードの量も減るため積極的に使えます。\nしかし、Setterが生成されるということはImmutableなオブジェクトにならないことを意味します。次は実際、@Dataをつけることで生成されるコードの例です。\n// @Dataの場合 public class Car { // フィールドだけを定義 private String name; private String color; // 以下のメソッドたちが自動生成 public void setName(String name) { this.name = name; } public String getName() { return this.name; } public void setColor(String color) { this.color = color; } public String getColor() { return this.color; } } こうなった場合、フィールドがfinalで守られていないといつでもSetterにより値が変わる可能性があります。そして一部Setterメソッドが使われていない場合は全フィールドに値が設定されてないままnullになってしまう可能性もありますね。これはImmutableの定義にふさわしくないコードとなっています。\n幸い、Lombokで提供するアノテーションの中にはImmutableなクラスを生成するためのアノテーションもあります。@Valueというものです。これを使うと、@Dataと同じ機能をしながら(インスタンスの生成とフィールドの値を指定する方法は変わりますが)もImmutableなクラスを生成することができます。こちらのアノテーションを使ったコードは、以下のようになります。\n// @Valueの場合 public class Car { // フィールドだけを定義 private String name; private String color; // 以下のメソッドたちが自動生成 public Car(String name, String color) { this.name = name; this.color = color; } public String getName() { return this.name; } public String getColor() { return this.color; } } 最初、インスタンスを生成するときはコンストラクターで全フィールドを引数として指定します。フィールドが多いとどれがどれかわからなくなりますが、フィールドの値指定が漏れる可能性は無くなります。そしてSetterがないため一度生成されたインスタンスに対してはフィールドを変更できなくなります。\nそしてこの@Valueアノテーションの良いところは、Builderパターンと両立できるというところです。@Builderをつけることで、インスタンスの生成時にそれぞれのフィールドがどんなものであるかを明確に確認できますね。ただ、Builderパターンでは全フィールドに値を指定する義務はないので注意が必要です。この問題は、手でbuild()メソッドを書くことで回避できます。ある意味、オーバーライドに近いことだと言えますね。コードで表現すると、以下のようになります。\n@Value @Builder public class Car { private String name; private String color; // 以下のコードだけを作成 public static CarBuilder { // nullのフィールドがあったらNPEを発生させる public Car build() { if (this.name == null) { throw new NullPointException(); } if (this.color == null) { throw new NullPointException(); } return new Car(this.name, this.color); } } } この場合はやはりコードが複雑になり、フィールドが増えるとまたそれに対応しなければならなくなりますね。フィールドがまたListだったりすると、ループでnull検査をする必要もあるはずです。このように@Valueだけを使う場合に比べ、コードが複雑になっているので、便宜生と安全性でどちらを選ぶかを考える必要がありますね。\nfinal宣言 Beanを使うしかない場合もありますね。例えばフィールドがnullになっても良い場合もあるはずです。それともそのBeanを持って処理をするメソッドでnullを検査するなど、何かの措置をしといたら良い場合もあるはずでしょう。\nそしてBeanを使う場合、フィールドをprivateに宣言して外部からの直接的なアクセスを防ぐということは常識となっています。publicで宣言されたフィールドだと、どこでもアクセスできるようになり知らないうちに値が変更される可能性がありますからね。それを防ぐために、Beanのフィールドに直接的なアクセスを許容しなく、Setterメソッドで値を指定してGetterメソッドで値を参照することは暗黙のルールとなっています。\nSetterメソッドによってフィールドに直接アクセスせずに値を指定することで最低限の安全は確保したと言いたいところですが、実はそうでもありません。なぜなら、依然としてフィールドの値は何度でも変わる可能性があるからです。一度Setterで値を入れて、その後にまたどこかでSetterを読んでいたら、Beanの持つフィールドの値は上書きされます。\nこれを防ぐためには、フィールドにfinalを使うべきです。final宣言されたフィールドは、初期化以後にその値が代入されないため、安定性が上がります。final宣言されたフィールドに値を代入しようとするとコンパイルエラーとなるため、エラーを見つけやすいというところも良いですね。\nまた、全フィールドがfinalで宣言されている場合、@Dataアノテーションは実質的に@Valueアノテーションと同じコードを生成します。もちろん、場合によってはfinalではないフィールドを持たせることもできます。そういう場合のコードは以下のようになります。場合によってはこれも必要かもですね！\n// @Dataの場合 public class Car { // colorだけがfinal private String name; private final String color; // 以下のメソッドたちが自動生成 public Car(String color) { this.color = color; } public void setName(String name) { this.name = name; } public String getName() { return this.name; } public String getColor() { return this.color; } } ただ、finalとなっているのはあくまでもこのクラスのフィールドのみということに気をつけなければならないです。Carオブジェクトを生成時に使われたデータが、以後も固定されて更新ができなくなります。\n浅いコピーと深いコピー Setterメソッドのもう一つの問題は、オブジェクトをコピーした場合に、コピー先のオブジェクトの値を変えるためにSetterを使うとそれがコピー元に影響するということです。簡単に以下のようなコードを作成したとしましょう。\n// @DataのCarクラス Car car1 = new Car(); car1.setName(\u0026#34;My car\u0026#34;); car1.setColor(\u0026#34;red\u0026#34;); // 友達が私と同じ車を買った Car car2 = car1; car2.setName(\u0026#34;Your car\u0026#34;); // 出力してみる System.out.println(car1); System.out.println(car2); このコマンドをコンパイルして実行してみると、どちらのCarもnameがYour carになっていることを確認できます。なぜこうなったのでしょう？先はStringがImmutableと言っていましたけどね。\nこれは、car1の値だけをcar2にコピーするという意図から書かれたコードが、実はcar1とcar2は「同じメモリーアドレスを参照する」というコードになってしまったからです。同じアドレスから値を参照するため、そのアドレスの値が変わると両方に影響するわけです。\nこのようにオブジェクトは参照が変わるだけなので、代入だけではそれぞれが独立していると言えなくなります。プリミティブ型が値そのものを保存するので代入でも安全なのとはまた違うところですね。このようにオブジェクトの「参照」だけが変わる状況を「浅いコピー」と言います。\n今までの展開から推測できるように、オブジェクトの参照を分離する必要があるでしょう。参照が独立していると、片方の値が変わっても他には影響ないはずですからね。参照がオブジェクトごとに違うというのは、同じ値を持ってメモリーに新しいオブジェクトを生成するということと同じ意味です。そしてこれを「深いコピー」と呼びます。\n深いコピーには様々な方法があります。まずはフィールドのオブジェクトを新しく生成することです。例えば、以下のような方法がありますね。\n// インスタンスを生成して値を入れてみる Car car2 = new Car(); car2.setName(new String(car1.getName())); car2.setName(new String(car1.getColor())); // 値を変えてみる car2.setName(new String(\u0026#34;Your car\u0026#34;)); 同じく出力してみると、今回はちゃんとcar2の値だけが変わったことを確認できます。しかし、全てのフィールドに対してこうするのはあまり便利ではないですね。とこかでメソッド化することはできないのでしょうか？\n簡単なのは、インタフェースを利用することです。Cloneableを継承することで簡単にオブジェクトをクローンできるようになります。ただ少し、メソッドを作成する必要はありますがね。\n@Data public class Car implements Cloneable { private String name; private String color; // cloneメソッドを作る public Car clone() throws CloneNotSupportedException { return (Car) super.clone(); } } こうすると、以下のような方法で深いコピーができるようになります。\ntry { car2 = car1.clone(); car2.setName(\u0026#34;Your car\u0026#34;); } catch (CloneNotSupportedException e) { e.printStackTrace(); } ただ、この方法を使うときに注意すべきことがあります。クラスにclone()を定義したからって、全てのフィールドに対して深いコピーを保証するわけではないということです。例えばCarクラス内にさらにEngineのような、Beanクラスがフィールドとして定義されていると、そのフィールドは浅いコピーになる可能性があるということです。これを回避するためには、EngineクラスにもCloneableを継承させる必要があります。以下のようにです。\n@Data public class Car implements Cloneable { private String name; private String color; // 追加したフィールド private Engine engine; // フィールドも深いコピーをさせる public Car clone() throws CloneNotSupportedException { Car car = (Car) super.clone(); car.engine = this.engine.clone(); return car; } } @Data public class Engine implements Cloneable { private String name; private String cylinders; // cloneメソッドを作る public Engine clone() throws CloneNotSupportedException { return (Engine) super.clone(); } } ListやMapの場合はどうクローンしたら良いでしょうか？同じくオブジェクトをクローンする観点からすると、両方とも方式は似ています。ループによるクローンですね。例えば以下のような方法でクローンができます。\n// Listのコピー List\u0026lt;Car\u0026gt; carList1 = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Car\u0026gt; carList2 = new ArrayList\u0026lt;\u0026gt;(); for (Car car : catList1) { carList2.add(car.clone()); } // Mapのコピー Map\u0026lt;String, Car\u0026gt; carMap1 = new HashMap\u0026lt;\u0026gt;(); Map\u0026lt;String, Car\u0026gt; carMap2 = new HashMap\u0026lt;\u0026gt;(); for (Entry\u0026lt;String, Car\u0026gt; entry : carMap1.entrySet()) { carMap2.put(entry.getKey(), entry.getValue().clone()); } これでImmutableなクラスを作り、コピーもできるようになりますね。\nImmutableなクラスで注意すること Reflectionに関するポストでも紹介したように、Reflectionを使うとフィールドに直接アクセスができ、privateで宣言されていてもアクセスを可能にすることもできます。つまりいくらImmutableなクラスを作ったとしても、Reflectionを使うとフィールドの値を変えることはできるということですね。\nそしてImmutableなクラスを作るということは、結局メモリーの使用量が上がるということでもあります。常に新しくオブジェクトを生成し、それぞれがメモリーを占有することになりますからね。もちろん現代のマシンのメモリーは多少のオブジェクトが作られても耐えられるメモリーを持っていますし、GCも活発に動いてくれるので一般的には心配するようなことではないですがね。\nSingletonクラスの変数には注意 Immutableとも関係があることですが、以前紹介したSingletonクラスを作成する場合にも、そのクラスが持つフィールドには注意しなければならないです。このクラスはインスタンスが生成されるとアプリケーションが終了するまで一つのインスタンスが使われるため、フィールドの値が変更される可能性があったら致命的です。処理ごとに結果が変わる可能性があるからです。なのでSingletonクラスにはなるべくフィールドを持たせないようにするか、final宣言をしておくなど、フィールドの値が変わる可能性を最初から封鎖しておく必要があります。\n最後に 個人的に、プログラミングの始まりが「どう実現するか」だとすると、プログラミングの完成は「どう安定させるか」であると思います。もちろん、性能を改善させたり、維持補修を簡単にさせたりするテクニックも大事ですが、安定的に動作するプログラムを作るということがもっとも難しいものですからね。要件が増えれば増えるほど、コードは複雑になり、例外が発生する可能性も高くなります。Immutableなクラスを作るということは、そのような例外を回避するための一歩であるゆえ、信頼できるプログラムを作り出せるという面で大事な知識なのではないかと思います。\nこれからもこのような知識に触れ、身につけていきたいですね！\n","date":"2019-08-25T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-thoughts-of-immutable/","title":"インスタンスをImmutableにするための工夫"},{"content":"昔からPCを使うといつも問題となるのはメモリーだった覚えがあります。私が初めてPCに触れたのは父が仕事て使っていたもので、当時はOSとしてDOSを採用していてゲームでもしたいときはいつもメモリーの設定を変える必要がありました。そのときはそれが不便だとも思わず、ただゲームができればいいと思っていました。\nしかし時間が経ち、大学でのプレゼンテーションを準備しながら感じたのは、やはりメモリーが十分でないとマルチタスクがきついということでした。今はPCのパーツの中でもっともアップグレードした時に性能向上を感じられるのはSSDと言いますが、それはあくまでCPUとメモリーを安定的に確保できる時代になったおかげと思います。まずメモリーが足りないととにかく遅いとしか思えない時代もありましたからね。\nそしてプログラムを作る立場となってからは、メモリー問題はより現実的な問題となりました。例えばとあるシステムを構築し、複数の使用者がそのシステムを利用するとしたら、限定された資源であるメモリーが足りなくなる可能性はハードウェアが飛躍的な発展を成している現在でも存在しています。オブジェクトを作るたび、残りのメモリーは減り続けるので。\nならば最適化という面で、メモリーを節約するには、無駄なオブジェクトの生成は抑えるべきでしょう。そのためにできる方法がないかと思っていたら、すでに存在していました。今回のポストの主題となる、Singletonパターンです。\nSingletonパターンとは Singletonパターンは、アプリケーション内でインスタンスが一度だけ生成され、そのアプリケーションが終了するまで使われるクラスを作るためのデザインパターンです。Beanの場合はそれぞれ違うデータを持つインスタンスをたくさん生成して使いますが、こちらはインスタンスが一つしかないため動的なフィールドを持たせないですね。なのでどこからでもアクセスできるような不変のデータを持ったり、特定の処理を繰り返す必要がある時このSingletonパターンでのクラスを作る場合があります。\nこのようなクラスがあって何が良いかというと、先に述べたメモリー問題です。例えばグローバル変数の場合、どんなクラスからもアクセスできるのであまりSingletonと変わらないようにも見えます。しかしグローバル変数の場合は、それが使われるが使われないが常にメモリーのなかにあって、無駄になってしまう可能性もあります。しかしSingletonの場合は、必要であれば生成し、必要でなければ生成しないこともできます。なのでメモリーを節約できますね。\n仕事では主に、ユーティリティークラスとしてSingletonクラスを作ることが多かったです。データの処理を繰り返して行う必要がある時、毎回インスタンスを生成することはメモリー問題もあって、コードが無駄に冗長になる傾向がありました。これをデータはそれぞれ違うインスタンスのBeanに持たせ、Singletonクラスに処理を任せることでコードの量も減らし、メモリーも節約することができました。\n古典的Singletonパターン それではSingletonクラスをどうやって作るのかを紹介します。デザインパターンでは様々なパターンがあって、その中の一つであるSingletonもまた様々な方法で具現できます。まずは古典的な方式を紹介します。\nここで目的はインスタンスを一つだけにすることなので、外部からすでに生成されているインスタンスにアクセスはできても、そのインスタンスを勝手に作れないようにします。そうするにはコンストラクターにアクセスの制限が必要ですね。まずコードで紹介しましょう。\n// クラスはpublicにして外部からアクセスできるようにする public class SingletonClass { // コンストラクターはprivateにして、外部からはアクセスできないようにする private SingletonClass() {} } しかしこれだけでは十分ではありません。どこかでインスタンスを生成する必要がありますね。また、先に述べたようにインスタンスの生成の時点は外部で制御できるようにしなければなりません。なのでprivateのコンストラクターにアクセスできるメソッドを用意する必要があります。\npublic class SingletonClass { // インスタンスを保存するための静的フィールド private static SingletonClass uniqueInstance; private SingletonClass() {} // インスタンスの返却(インスタンスが生成されたない場合は生成してから返却する) public static SingletonClass getInstance() { if (uniqueInstance == null) { uniqueInstance = new SingletoneClass(); } return uniqueInstance; } public void doSomething() { // ... 普通のメソッド } } まずstaticで、自分のインスタンスを保存できるフィールドを宣言します。外部からSingletonクラスのインスタンスを取得するためにはこれを使うようになります。宣言だけで、この段階でインスタンスを生成しないのはグローバル変数と区別するためです。\n次に、インスタンスが生成されてない場合でもアクセスできるstaticメソッドを作成します。ここからこのSingletonクラスのインスタンスを取得するようになります。メソッドの中はでは戻り値としてインスタンスのフィールドをセットし、もしインスタンスが生成されてない場合にだけnewをするようにします。\nこれで外部からは以下のように使えるようになります。\n// インスタンスの取得 SingletonClass singletonInstance = SingletonClass.getInstance(); // インスタンスのメソッドを使用 singletonInstance.doSomething(); これでどこからでも同一なインスタンスでつかけるSingletonクラスができました。\n古典的Singletonパターンの問題 マルチスレッドを考える必要がない場合なら気にすることはないですが、現代のプログラミングはそうでもないですね。特に何かのシステムを作り、サービスとして提供する場合は複数の使用者によって同じクラスが要請される場合があります。\nそしてクラスの中が複雑でインスタンスの生成に時間がかかったり、ほぼ同時のタイミングでインスタンスが要請されると古典的なSingletonパターンでは複数のインスタンスが生成されることを塞げられない場合があります。この場合は元の設計通り動かなくなり予想できない例外が発生する可能性がありますね。\nもちろんこれらを解決するためにいくつかの方法が提示されてはいますが、それらの解決策にもデメリットはあります。まずどんな方法があり、それぞれのデメリットには何があるか見ていきましょう。\nマルチスレッド問題を開所するために 他にも方法はありそうですが、スレッドセーフなSingletonクラスを生成する方法は以下のようなものがあります。\nインスタンスの生成をシンクロさせる Double-Checked Lockingを使う JVMのクラスローダーにお任せ まずインスタンスの生成をシンクロさせる方法は簡単です。インスタンスを取得するためのgetInstance()メソッドにsynchronizedを追加することです。コードで見るとあまり変わらないので、古典的Singletonパターンのクラスがあればもっとも簡単に適用できる方法ですね。\npublic class SingletonClass { private static SingletonClass uniqueInstance; private SingletonClass() {} // インスタンスを提供するメソッドをシンクロさせる public static synchronized SingletonClass getInstance() { if (uniqueInstance == null) { uniqueInstance = new SingletoneClass(); } return uniqueInstance; } } ただ、synchronizedの問題は性能です。100倍以上も処理の速度が遅くなる場合もあるらしいので、マルチスレッドを性能のために使う場合があれば、あまり望ましくないですね。\n次の方法は、二重チェックです。インスタンスがnullであればシンクロさせます。この方法だと毎回シンクロさせる必要がないので(2回目からはインスタンスがnullでないため)最初の一回以外は性能が低下しません。\npublic class SingletonClass { // volatile宣言で安定性を確保 private volatile static SingletonClass uniqueInstance; private SingletonClass() {} // インスタンスの二回確認 public static SingletonClass getInstance() { if (uniqueInstance == null) { synchronized (SingletonClass.class){ if (uniqueInstance == null) { uniqueInstance = new SingletoneClass(); } } } return uniqueInstance; } } volatile宣言を使う理由は、変数がCPUのキャッシュメモリーに入ることを防止するためだそうです。プログラムのデータは最初ハードディスクから読み込まれシステムメモリーに載せられますが、そのあとCPUでの処理が行われる時にはさらにCPUのキャッシュメモリーに載せられることがあります。\n最近は複数のCPUを搭載しているシステムも少なくないので、それぞれ違うCPUのキャッシュメモリーにインスタンスが入ってしまうとインスタンスが生成されているかどうかわからなくなりますね。volatile宣言でシステムメモリーにフィールドを乗せることで、インスタンスの生成がより安定的に行われます。でも依然として、同期化による性能低下を一回は経験しなければならないという問題があります。\n最後は、JVMが起動する時にインスタンスを生成させる方法です。この方法では外部からは確実にインスタンスの制御ができなく、常にインスタンスが生成されるためマルチスレッド問題を回避できますね。\npublic class SingletonClass { // フィールドにインスタンスの生成を宣言 private static SingletonClass uniqueInstance = new SingletonClass(); private SingletonClass() {} // インスタンスのチェックも要らなくなる public static SingletonClass getInstance() { return uniqueInstance; } } クラスが読み込まれる時点でJVMからインスタンスを生成してしまうので、どんなスレッドからも静的フィールドにはアクセスできなくなります。ただ、これならグローバル変数で宣言することとあまり変わらないので、使われなくてもメモリー上にはインスタンスが生成されたままであるという問題は残ります。もちろん、グローバル変数宣言とは違ってインスタンスは唯一であることが違うところです。グローバル変数だとstaticでフィールドを宣言しても、違うクラスでまたを宣言できますので。そもそもグローバル変数には何が入っているかわからなくなる場合が少なくないので、乱発しないほうがいいですね。\nメソッドとフィールドを全部staticに宣言して良いのでは？ もちろんその方法もありです。しかし、初期化の過程が極めて簡単な場合にだけ有効(フィールドが何もないなど)な方法と言えます。クラス自体が単純な構造をしていて、メソッドは単純に外部から入れられたデータを処理して返すだけならできる方法ですね。実際使えない方法ではないですが、後の機能拡張などを考えると良い方法ではなくなりますね。\n最後に Singletonパターンは幅広く使われていて、確かに魅力的なクラスの設計の方法ではあります。しかしマルチスレッド問題を回避するため工夫しなければならない問題があり、唯一なインスタンスなためフィールドの処理にも気をつけなければならない面があります。とあるスレッドでインスタンスが使われていて、フィールドにデータを入れたのをまた違うスレッドでアクセスしようとすると問題が起こり得る可能性がありますので。\n他にもOOPの原則である、「一つのクラスは一つだけの責任を持つ」ということからしても、Singletonクラスは問題を持っています。何かの処理を担当していながらも、自分自身でインスタンスを管理するという二つの責任を持っていますからね。そしてコンストラクターがprivateであるため、サブクラスを作られなくなるという問題があります。サブクラスを生成するためにコンストラクターをpublicやprotectedに変えるとSingletonではなくなるジレンマもできますね。\nそれでもSingletonパターンで作られたクラスは、確かな魅力を持っています。ちゃんとインスタンスの管理さえできていれば、どこでも呼びたして使うことができますからね。常にメモリーに載せる必要があるクラスができたら、検討したくなるパターンです。\n","date":"2019-08-19T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-design-pattern-singleton/","title":"デザインパターン、Singleton"},{"content":"前回のポストで、BuilderパターンとともにLombokを紹介しました。Beanだけでなく、Immutableなクラス1やBuilderを作れるなど便利な機能が集まっていて、さらにアノテーションで様々なオプションが指定できたり(フィールドのアクセスレベルを指定できるなど)、必要なメソッドは追記しても良いなど使い道が多いですね。ただ、今回はそのLombokを使いながらバグらしき現象を発見したのでポストします。\nバグの発生したところ どこでバグが発生したかを述べる前に、まずどんなLombokのアノテーションを使っていて、それらがそれぞれどんな機能をしているかを紹介したいと思います。なぜなら今回の場合は二つ以上のLombokアノテーションを組み合わせて使っていて、ほかにも自分と同じような組み合わせでLombokを使って同じバグを経験される方がいるかも知れないからです。\nLombokの@Builderには、(toBuilder=true)のオプションをつけられます。これを使うと、基本的に新しいインスタンスの生成はstaticメソッドを呼ぶ事でできるようになって、すでに存在しているインスタンスから一部の値を変えて違うインスタンスに再生成する事ができます。コードで表現すると以下のようになります。\n// @Builderだけを使って新しいインスタンスを生成 House house2 = House.builder().type(\u0026#34;wooden\u0026#34;).build(); // toBuilderオプションで既存のインスタンスから値を変えて再生成 House house3 = house2.tobuilder().type(\u0026#34;block\u0026#34;).build(); そしてBuilderを使いながらも、元のクラスのフィールドの中でBuilderにデフォルト値として渡したいものもあります。つまり、Nullにしたくない場合ですね。自分の場合は、Listでした。インスタンスが生成される場合にはとにかくListを初期化して、Builderを使ってListそのものを代入したり、addList()のようなメソッドを作って個別の要素を追加できるようにしたかったです。\nこれを実現するためには、元のクラスでListを初期化して、その状態でBuilderに渡す必要はありました。Lombokのアノテーションではフィールドに@Builder.Defaultをつけることでできるようになります。そしてBuilderのアノテーションでは生成されない、addList()的なメソッドだけを実装することですね。コードで表現すると以下のようになります。\n@Builder(toBuilder=true) public class Wallet { // Builderでインスタンスが生成されるときはNullにしたくない @Builder.Default List\u0026lt;String\u0026gt; cards = new ArrayList\u0026lt;\u0026gt;(); // カスタムメソッドをBuilderに追加するための宣言 public class WalletBuilder { // 元のBuilderではListそのものを代入する方法しかないので、個別要素を追加できるメソッドを書く public WalletBuilder addCard(String card) { this.cards.add(card); return this; } } } こうした実装で期待していた動きは以下のようなものでした。\n// インスタンスを生成しながらListにAddする Wallet myWallet = Wallet.builder().addCard(\u0026#34;Apple Card\u0026#34;).build(); // 既存のインスタンスにAddする Wallet newWallet = myWallet.toBuilder().addCard(\u0026#34;American Express Card\u0026#34;).build(); しかし、実際テストをしてみるとこの二つのアノテーションによりバグが発生したのです。\nそれでどんなバグが？ 既存のインスタンス(Listの操作を一切していない)ではaddが思い通りになったのですが、インスタンスを生成すると同時にaddした場合はNPE2が発生していました。場所を調べてみるとthis.cards.add(card);のところで例外が発生していたので、生成されてないオブジェクトに要素を追加しようとしていたとのことですね。つまり、Listがちゃんと初期化されてないという事です。\n少し調べてみると、Githubでのイシューがありました。2017年に書かれたもので今となってはずいぶん古い感じもしましたが、読んでみると今の自分が経験している現象と似ていましたね。しかも、Lombokの1.18.2バージョンで解消されたという話もありましたが、今の自分が使っているバージョンは1.18.8でした。解消されているはずがちゃんと想定通りならなかったのか、バージョンアップにより再発したのかわかりませんが、ともかく同じ現象が起きていましたね。\n解決策 それではどう解決したらいいか？他にも方法があるのかもしれませんが、自分の場合はtoBuilder=trueと@Builder.Defaultの両方を使わない事で解決できました。Builderにちゃんとフィールドが渡らない時点で後者は意味がなくなりました。そしてtoBuilderの場合も、二つのメソッドが追加されるだけなのでそれを手書きで確実に値を渡せるようにしました。上で提示したWalletクラスをこのやり方で直すと、以下のようなコードになります。\n// toBuilderオプションを使わない @Builder public class Wallet { // @Builder.Defaultを使わない List\u0026lt;String\u0026gt; cards = new ArrayList\u0026lt;\u0026gt;(); // toBuilderメソッドも手書きする public WalletBuilder toBuilder() { return new WalletBuilder().cards(this.cards); } } これで元の想定通り、インスタンスを生成する時もちゃんと初期化されたListが渡るようになりました。めでたしめでたし。\n教訓？ ライブラリーを使ってコードの量を減らし、自動化することは生産性の向上という面では大事な事ではありますが、人間の書いたコードはどこでバグが発生するかわからないので(想定していない使い方をする場合もありますし)、たまには手間がかかっても確実なコードを書くのが安全な場合もありますね。特にこの場合、アノテーションを諦めずコードを直そうとしていたらいつまでたってもバグは回避できなかったのかもしれません。そういう意味で、よい勉強になったと思える事件ではなかったのだろうかと思います。\nもしこのポストを読まれる方の中、私と同じような実装を考えている方がいたら、こんなこともあるんだなと参考できるようなことになっていると嬉しいです。それではまた会いましょう！\n一度インスタンスを生成すると、途中で値を変える事が出来ないクラス。Stringの場合がそうです。値を代入すると、メモリーに載せてある値を捨てて新しく生成します。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNull Pointer Exception、ヌルポとも呼ぶ例外です。参照しようとしているオブジェクトがメモリー上にありませんよーとのことで、Javaで最も遭遇しやすい例外ですね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-08-12T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-lombok-annotation-bug/","title":"Lombokのバグにあった話"},{"content":"今回も、いつもと同じく仕事で得られた知識です。とあるIterableなクラスを作り、Forループをさせる必要がありました。これはそんなに難しいことではなかったです。まずフィールドにリストを持たせる。またIterableをインプリメントし、戻り値がIteratorであるメソッドを作るだけでしたね。しかし問題は、そのクラスのループの途中で、「使用者が指定したルールによりループを終了させる機能のあるクラスを作ること」でした。つまり、ループを終了させるには引数でループ中の要素を受け入れる引数を持つメソッドが内臓されていて、そのメソッドはインスタンスごとの基準となるデータを持っているということですね。\nメソッドをフィールドとして使う？ メソッドとして機能しながらフィールドみたいにデータを持つ？それをさらに使用者が指定できるようにする？難しい注文だったので一瞬迷いましたが、「functionを使うといい」というアドバイスを受け、調べてみました。なるほど、これを使ったらフィールドとして宣言しながらもメソッドの機能を期待でき流みたいです。早速適用してみて、それがどう機能するかをまず紹介したいと思います。\nまずはIterableなクラスを用意します。ループの対象となるのはこちらです。\n// Lineというクラスをリストとして持ち、Iterate可能なクラス public class Factory implements Iterable\u0026lt;Line\u0026gt; { private List\u0026lt;Line\u0026gt; lines = new ArrayList\u0026lt;\u0026gt;(); public Iterator\u0026lt;Line\u0026gt; iterator() { return this.lines.iterator(); } } 次はこのFactoryクラスを持ってループをさせる例です。ループの途中でRuleクラスのisEnd(Line)での判定が入ります。Lineインスタンスの中で何か条件に当たるものがあったら、戻り値がTrueとなってループから抜ける構造です。\n// Iterableクラス Factory factory = new Factory(); // forループでFactoryの中のLineオブジェクトを処理 for (Line line : factory) { // Ruleクラスのループ終了判定メソッドを使う if (Rule.isEnd(line)) { break; } // ... 何らかの処理 } ここで判定を行うRuleクラスの場合は、以下のようになっています。\npublic class Rule { // 判定のルールをフィールドとして持っている private Predicate\u0026lt;Line\u0026gt; endRule = line -\u0026gt; line.isBroken(); // 引数のLineがPredicateの条件に当たるかを判定するメソッド public boolean isEnd(Line line) { return endRule.test(line); } public class RuleBuilder { // 中身は普通のBuilder private Predicate\u0026lt;Line\u0026gt; endRule; public RuleBuilder endRule(Predicate\u0026lt;Line\u0026gt; endRule) { this.endRule = endRule; return this; } } } まずPredicateが何であり、Lambdaだけであれができる？と思われそうなコードです。しかしちゃんと動いてくれています。それはなぜか？今までよくしらなかったですが、Java8から追加されたjava.util.functionによるマジックでした。フィールドはデータを持つためのものである、とは思っていましたが、そのデータというのがメソッドそのものにも慣れるとは知らなかったですね。\nそれでは、そのjava.util.functionが一体どんなものかを、それに含まれている様々なインタフェースたちを持って紹介したいと思います。\nFunctional Interface java.util.functionに含まれている様々なインタフェースたちを、関数型インタフェースと呼ぶらしいです。Java8で追加されたLambdaを「実装すべき抽象メソッドが一つしかないインタフェースを具現化したもの」と言いますが、ここでいう「実装すべき抽象メソッドが一つしかないインタフェース」のことが関数型インタフェースです。\n言葉として表現すると難しいですが、要は一つです。中身をLambdaで満たせて完成するインタフェース。様々なタイプのものがあって、ぞれぞれの特徴は少しづつ違いますが、どんなことがして欲しいかによって選択するものが違うだけで、実際はそんな難しくもないです。むしろ難しいと言えばLambdaの方かな…\nとにかくこれらの関数型インタフェースを、一つづつ紹介しましょう。\nFunction Functionは、そのなの通り典型的な関数です。引数と戻り値を指定して宣言します。実行はapply(適用)となります。コードで見ると以下のようになります。\n// Integerが引数で、Stringが戻り値となる例 Function\u0026lt;Integer, String\u0026gt; function = number -\u0026gt; String.valueof(number); // Functionの実行 String result = function.apply(12); BiFunction Function以外にも「Bi」が付くいくつかの関数型インタフェースがあります。何が違うかというと、そのなのとおり引数が二つ。他は元のものとほぼ一緒です。\n// 二つのStringが引数で、Integerが戻り値となる例 BiFunction\u0026lt;String, String, Integer\u0026gt; biFunction = (string1, string2) -\u0026gt; Integer.parseInt(string1) + Integer.parseInt(string2); // BiFunctionの実行 int result = biFunction.apply(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;); Predicate 先に紹介したものですね。Predicateは「述語」の意味を持っています。その名の通り、「引数がTrueかFalseかを述べる」ようなものです。引数は一つです、戻り値がBooleanです。実行はtestです。\n// 引数がStringの例 Predicate\u0026lt;String\u0026gt; predicate = string -\u0026gt; string.isEmpty(); // Predicateの実行 boolean result = predicate.test(\u0026#34;空じゃない！\u0026#34;); BiPredicate 引数が二つのPredicateです。\n// 引数がStringの例 BiPredicate\u0026lt;String, Integer\u0026gt; biPredicate = (string, number) -\u0026gt; string.equals(Integer.toString(number)); // BiPredicateの実行 boolean result = biPredicate.test(\u0026#34;1\u0026#34;, 1); Consumer Consumeは消費するという意味がありますね。引数を受けて戻り値はない(voidとなる)ものです。実行するときはaccept(受納)となります。\n// 引数がStringの例 Consumer\u0026lt;String\u0026gt; consumer = string -\u0026gt; System.out.println(string); // Consumerの実行 consumer.accept(\u0026#34;吸収！\u0026#34;); BiConsumer 引数が二つのConsumerです。\n// 引数がStringとIntegerの例 BiConsumer\u0026lt;String, Integer\u0026gt; biConsumer = (string, number) -\u0026gt; System.out.println(string + \u0026#34;：\u0026#34; + number); // BiConsumerの実行 biConsumer.accept(\u0026#34;今年儲かる確率は\u0026#34;, 0); UnaryOperator Unaryは「単項」の意味。Operateは作用するという意味を持っていますね。引数と戻り値が同じもので、引数に何かの操作をしてから返すという印象です。\nUnaryOperator\u0026lt;String\u0026gt; uOperator = string -\u0026gt; string + \u0026#34;完成されます\u0026#34;; // UnaryOperatorの実行 String result = uOperator.apply(\u0026#34;この文字を入れると\u0026#34;); BinaryOperator 引数が二つのOperatorです。\nBinaryOperator\u0026lt;String\u0026gt; biOperator = (string1, string2) -\u0026gt; string1 + string2 + \u0026#34;ではないです\u0026#34;; // BinaryOperatorの実行 String result = biOperator.apply(\u0026#34;私は\u0026#34;, \u0026#34;大丈夫\u0026#34;); Supplier Supplyは「補給」の意味。Consumerとは真逆のもので、引数がなく戻り値だけがあるものです。実行はgetとなります。引数がないためこちらはBiSupplierのようなインタフェースがないです。\nSupplier\u0026lt;String\u0026gt; supplier = () -\u0026gt; \u0026#34;例えば引数なしで文字列が帰ってくる！\u0026#34;; // Supplierの実行 String result = supplier.get(); 最後に Java8が出てから数年、もうJavaも12までバージョンアップしています。でもまだJava8が使われている場面は多く、なるべくJava8の機能を最大限に活かしたコードを書きたいものですね。LambdaもStreamも難しいですが、Functionみたいにどこかで使うことになってまた今まではできなかったことをできるようになりたいです。\n今回も色々と勉強になりました。Javaの世界はまだまだ広くて奥深いものですね！\n","date":"2019-08-06T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-functional-interface/","title":"関数型インタフェースを使う"},{"content":"今回の仕事で学んだことは、自分のコードを他人がライブラリーとして使うときはどのように実装していくかの方法です。とある機能をするスクリプトを書くこと、エンドユーザーが使うUIやそコンテンツを処理するロジックなどだけに関わったことのない自分にとってはとても新しい経験になりました。今までだと自分が担当した機能を具現し、それを最適化していいだけでした。でも、ライブラリーは基本的にコードを扱える人が使うものなので設計が全く違いますね。\nそして、難しかった部分の一つは、柔軟性を持たせることでした。例えばデータを受け取り、処理していく中で、今までは自分が実装したBeanにデータがマッピングされていることを第一の前提条件としていました。それが今の仕事では、「どんなBeanが入ってくるかわからないから、それに対応する」ようにする必要がありました。\nまずクラスやインスタンスを引数として受け取る方法を知る必要がありますね。最初はObjectそのものを使おうと思いましたが、調べてみるとGenericというものがあったので、そちらを使うことにしました。\n次にそのGenericを使って、引数として受け取ったBeanです。自分が設計したBeanだけを使うなら、Beanが持つフィールドのデータ型も知っていいて、Getter/Setterからデータのやりとりができますね。しかし自分が作ったものではないと、フィールドのデータ型もGetter/Setterメソッドもどう呼ぶかもわからなくなります。ここでどう対応したらいいだろうか…と悩み、探し出した答えがReflectionでした。\n今回のポストはその二つを使い、どう「自分が作っていないBeanを処理」したかについて述べ地と思います。\nGeneric Genericとは、「データのタイプを一般化」するということです。総称型とも呼ぶらしいです。クラスの中で使うタイプを、クラスの外部から設定するとき用いられると言いますね。今までは主に複数のデータ型やオブジェクトを処理したい場合はObjectを使ってきました。Javaでは一部のPrimitive(参照型)を覗くと、ほとんどのデータががオブジェクトとして扱われているので、もっとも上位のタイプであるObjectに変えられらからです。\nしかし、Objectを引数として使うと、メソッドの中のフィールドが何があるかわからなくなります。また、Getter/Setterによる値の指定・取得などの操作もできませんね。なのでオブジェクトそのものを受け取るだけでなく、インスタンスの中を直接覗く必要があります。\nここでGenericを使うと、Objectと同じ機能(どんなインスタンスやクラスでも受け取れる)をしながらも、最初に具体的なタイプが決まるのでキャストがいらなくなります。これなら性能的にもよく、より安全な挙動を期待できますね。あとは実際どんな構造をしているかわからないObjectに対し、Reflectionを活用して、クラスやインスタンスの中を覗いたらメソッドもフィールドも取得できるというメリットもあります。\nまずインスタンスを引数として受け取る方法から見てみましょう。引数としてT1を指定します。これでGenericタイプの引数を受け入れられます。つまり、インスタンス自体が引数となるということです。ただ、引数がTの場合はメソッドの戻り値の前でも\u0026lt;T\u0026gt;を宣言する必要があります。コードで表現すると以下のようになります。\n// Beanのインスタンスを受け取るメソッド public \u0026lt;T\u0026gt; boolean isBean (T parameter) { // ... 何かの処理 } // 使用例 BeanObject beanObject = new BeanObject(); if (isBean(beanObject)) { // ... 何かの処理 } Listの中にもGenericを使うことができます。たとえば以下のように書くと、どんなタイプも受け入れられるようになりますね。\nList\u0026lt;T\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); インスタンスではなく、クラスそのものをGenericで受け入れるには以下のように書きます。\n// Beanのクラスを受け取るメソッド public boolean isBean (Class\u0026lt;?\u0026gt; parameter) { // ... 何かの処理 } // 継承もできる(限定) public boolean isStringBean (Class\u0026lt;? extends String\u0026gt; parameter) { // ... 何かの処理 } // 使用例 if (isBean(BeanObject.class)) { // ... 何かの処理 } こうやってGenericの引数を渡してもらう準備は完了です。次は、そうやって受け取った引数を扱うReflectionについて調べてみましょう。\nReflection クラスの具体的なタイプをわからなくても、メソッド・コンストラクター・フィールドなどににアクセスできるようにしてくれるAPIをReflectionと言います。取得したメソッド・コンストラクター・フィールドはいずれもそのまま使ったり、値(戻り値)を取得したり、つけられているアノテーションを取得するなど、普通のクラスでできることは全部できます。\nでは、このReflectionを実際どう使うかを以下のコードで紹介します。まずはインスタンスからクラスを取得する方法と、さらに取得したクラスからコンストラクターを取得するコードです。\npublic \u0026lt;T\u0026gt; boolean isBean (T object) { // インスタンスからクラスを取得 Class\u0026lt;?\u0026gt; objectClass = object.getClass(); // クラスからさらにインスタンスを生成する Object instance = objectClass.newInstance(); // クラスのパッケージ名を取得 String package = objectClass.getPackage(); // パッケージを含むクラス名を取得 Stirng classNamePackageInvolved = objectClass.getName(); // クラス名だけを取得 String className = objectClass.getSimpleName(); // 配列でpublicコンストラクターたちをを取得 Constructor[] constructors = objectClass.getConstructors(); // 特定のpublicコンストラクターを取得 Constructor constructor = objectClass.getConstructor(parameter1, parameter2, ...); // 取得したコンストラクターからインスタンスを生成 Object instance2 = constructor.newInstance(); } クラス自体を扱うことができるので、その中身がわかれば新しいインスタンスを生成して使うこともできますね。それでは次に、フィールドを取得する方法を紹介します。\nFieldの取得 public boolean isBean (T object) { Class\u0026lt;?\u0026gt; objectClass = object.getClass(); // 配列でpublicフィールドたちを取得 Field[] fields = objectClass.getFields(); // 特定のpublicフィールドを取得 Field field = objectClass.getField(\u0026#34;フィールド名\u0026#34;); // 配列で全フィールドたちを取得(public以外も) Field[] declaredFields = objectClass.getDeclardFields(); // 特定のフィールドを取得(public以外も) Field declaredFiled = objectClass.getDeclaredField(\u0026#34;フィールド名\u0026#34;); // Fieldでできること // フィールドに値を設定 field.set(object, parameter); // フィールドの値を取得 String fieldValue = field.get(object); // フィールドのアノテーションを取得 Annotation[] annotations = field.getAnnotations(); } ここで注意すべき部分は、フィールドを取得するときはクラスからであって、実際値を代入したり取得するときはその対象としてインスタンスを使うということです。クラスが設計図であり、インスタンスがその設計図で生成されたものだということが明確になる瞬間ですね。Reflectionを使うことのメリットはここにもあるのかもしれません。\n次には、メソッドをみていきます。\nMethodの取得 メソッドもフィールドの場合とそう変わりません。クラスからメソッドを取得し、そのメソッドからさらに色々できるようになります。またコードを以下に用意しました。\npublic boolean isBean (T objectClass) { Class\u0026lt;?\u0026gt; objectClass = object.getClass(); // 配列でpublicメソッドたちを取得 Method[] methods = objectClass.getMethods(); // 特定のpublicメソッドを取得 Method method = objectClass.getMethod(\u0026#34;メソッド名\u0026#34;, parameter1, parameter2, ...); // 配列でメソッドたちを取得(public以外も) Method[] declaredMethods = objectClass.getDeclaredMethods(); // 特定のメソッドを取得(public以外も) Method declaredMethod = objectClass.getDeclaredMethod(\u0026#34;メソッド名\u0026#34;, parameter1, parameter2, ...); // Methodでできること // メソッド名の取得 String methodName = method.getName(); // メソッドの実行 Object methodInvoked = method.invoke(); // 引数のアノテーションを取得 Annotation[] parameterAnnotations = method.getParameterAnnotations(); // 引数を取得 Parameter[] parameters = method.getParameters(); } アノテーションの場合はクラスでもフィールドでも取得できますが、メソッドの場合はそれに加えて引数のアノテーションも取得できるということが特徴です。また、引数そのものを取得することもできますね。もちろんParameterクラスでも引数名を取得するなど色々な操作ができます。\n結論 結局、道が見えてくると、解決の方法も見えてくるものです。メソッドをやコンストラクターに触れる必要もなく、引数で受け取ったインスタンスからフィールドを取得して、その値をObjectに代入する。そしてinstanceofを活用して分岐させ、キャストすることでデータを扱う、というシンプルな構造で自分のタスクは完成されました。簡単なコードで表現すると以下のような形ですね。\n// クラスをわからないBeanを引数とするメソッド public \u0026lt;T\u0026gt; processSomething(T bean) { // 複数のタイプのオブジェクトがある String stringObject; Integer intObject; // クラスとフィールドの取得 Class\u0026lt;?\u0026gt; beanClass = bean.getClass(); Field[] beanFields = beanClass.getDeclardFields(); // ループで個別フィールドを取得し、一致するタイプのオブジェクトにフィールドの値を入れる for (Field field : beanFields) { // privateフィールドの場合はアクセスできない場合があるためアクセス可能にする if (field.canAccess(bean)) { field.setAccessible(true); } // フィールドの値を取得し、型を判断 Object value = field.get(bean); if (value instanceof String) { stringObject = (String)value; } else if (value instanceof Integer) { intObject = (int)value(); } } } どうですか。方法がわかれば簡単にできるものなのでは。さらに応用して、インスタンスの一部の値だけを修正して返すなどの挙動もできそうです。色々活用できそうな面が多いですね。\n最後に このようにだいたいフィールドとメソッドがわかれば、これでどんなインスタンスやクラスが入ってきても、対応できそうな気がします。実際どうかはコンパイルして実行してみない限りわからないものですが…でも一つ、賢くなったような気はします。皆さんもぜひ、Reflectionを通じてクラスとインスタンスに対する理解を深めてみてください。\nそれでは、また！\nTypeのこと。ただ、E(Element)やK(Key)、N(Number)、V(Value)も使えるらしいです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-07-30T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-reflection/","title":"ReflectionとGenericを活用する"},{"content":"今回は、Jenkinsで生成したJobを実行する方法について述べたいと思います。基本的にJenkinsの管理コンソールでJobを実行することは可能ですが、いちいちログインしてJobを起動することは便利ではないです。なので「とある条件でJobを自動的に実行したい」と思うようになりますね。例えば簡単に考えられるものとしてまず「定期的実行」があるでしょう。特定の時間、もしくは頻度などを指定して自動的に実行してくれるバッチ的な処理にはこのような設定が必要ですね。また、とある条件が満たされた時にだけ実行したい場合もあります。たとえばGitのリポジトリにコミットが発生したときは、その時だけビルドしたいですね。\nJenkinsのJobではこのような、条件によるJobの自動実行を設定することが可能です。これをBuild Triggerと呼びます。基本設定でも必要なパターンの最低限は揃えていますし、プラグインの組み合わせによってはより多彩な設定でJobを実行するように設定できます。今回のポストでは、そのビルドトリガーにどんなものがあるか、またどのように使えるかということを述べたいと思います。\nBuild Triggerの種類 JenkinsでJobを生成すると、GeneralとSource Code Managementのタブの後にBuild Triggerというタブがあることを確認できます。プラグインの構成が違うと増えたり減ったりもするので一度確認してみてください。私はJenkinsをインストールするときオススメのプラグイン構成を選んでいるので、以下の構成になっています。\nTrigger builds remotely Build after other projects are built Build periodically GitHub hook trigger for GITScm polling Poll SCM では、これらの機能がそれぞれどんな意味を持ち、どう活用できるかを一つづつ見てみましょう。\nTrigger builds remotely これは、「遠隔」という表現通り、Jenkinsの管理コンソールに接続しなくてもJobを外部から実行できるようにするという意味です。このメニューを選ぶと、URLでJobをコールできるようになります。Authentication Tokenに認証として使うトークン名を入力すると、以下のようなURLでJobをコールできます。\n(JenkinsのURLが192.168.0.2:8080、Job名がTriggerTest、トークン名がbuildTokenであると仮定)\nhttp://192.168.0.2:8080/job/TriggerTest/build?token=buildToken http://192.168.0.2:8080/job/TriggerTest/buildWithParameters?token=buildToken １番の設定は、パラメーターなしでただJobを実行するときに用います。２番目の場合は、パラメーターを渡したいときに使えますね。ここで受け取るパラメーターは、GeneralタブでThis project is parameterizedをチェックし、パラメーターを追加すると設定できます。パラメーターで受け取った値はJobの中のシェルスクリプトなどで外部変数として使うことができます。\nLinuxのCurlなど、外部からのコールが必要である時使うと便利な機能です。\nBuild after other projects are built JenkinsではJobを複数生成することができます。そしてJobを複数生成することはそれぞれ違う機能をするJobをそれぞれの目的に合わせて分けるという意味でしょう。ただ場合によってはそれらのJobを連携する必要もあるかもしれません。同じプログラムの中でもメソッドやクラスは分けてもそれらを繋いで使うことになりますので。\nこちらのオプションをチェックすると、どんなJobの後にビルドするかを入力できます。複数のJobを登録することもできますね。Projects to watchに先にビルドしたいJob名を入力し、下の三つのオプションを選びます。JenkinsのJobの実行結果にはstable(成功)unstable(一部成功)fail(失敗)の三段階があって、ここで選ぶのは前段階のJobがその三つのうちどのような結果になったかという判定の条件です。\nTrigger only if build is stable Trigger even if the build is unstable Trigger even if the build fails １番目は、前段階のJobが成功的にビルドできた場合。成果物が必要だったりするとこちらのオプションですね。２番目はビルドが不安定でも実行する。また３番目は失敗しても実行する。各自目的と用途が違うので、必要に合わせて適合したものを選びましょう。\nBuild periodically 時間による定期実行が欲しい場合はこちらのオプションを使います。Scheduleの欄に、Jenkins固有のルールに合わせて周期を書くと自動的に条件に合わせてJobのビルドが繰り返されます。かなり複雑ではありますが、一旦紹介します。\n基本フォーマット\n時間単位は空白を基準に分ける 記入順は分(minute)、時間(hour)、日付(day of month)、月(month)、週(week) 分(minute): 0から59までの数字 時間(hour): 0から23までの数字 日付(day of month): 1から31までの数字 月(month): 1から12までの数字 週(week): 0から7前での数字(0と７は日、1は月、2は火、3は水、4は木、5は金、6は土) *は入力できる全範囲の値を許容するという意味(分の位置に入力すると0~59までの全数字) M-Nのように-を入れると範囲の指定が可能(2-4は2から4までの値) M-N/Xまたは*/Xは合間の意味(時間の位置に*/2と入力すると、２時間置きという意味になる) A,B,Cのように複数の値を列挙できる そのほかに、周期的に実行されるJobが複数ある場合はHを使うことがオススメです。例えばJobが10種類あって、それぞれを0 0 * * *にすると、全てのJobが毎日0時に実行されることになりますね。もしサーバーのスペックがあまりよくなかったり、Jobの数がそれ以上になると負荷は無視できないレベルになるはずです。\nそこでH H * * *に指定すると、Jenkinsで自動判断を行い、サーバーへの負荷があまり発生しない時間帯にJobが実行されるようになります。もちろんHでも範囲の指定ができますのでH H(0-1) * * *のように指定すると、毎日0時から1時までの間、負荷の低い時間帯にビルドが行われます。\nこのフォーマットに従って書かれたいくつかの例を見てみましょう。\n15分置きでビルド実行: H/15 * * * * 毎時間の30分までに、10分置きでビルド実行(1時から1時半まで、2時から2時半までなど): H(0-29)/10 * * * * 平日の9時から16時の間、２時間置きにビルド実行: H 9-16/2 * * 1-5 1月から11月の間、1日と15日だけビルド実行: H H 1,15 1-11 * フォーマットになれるまでは少し時間がかかりそうですが、かなり複雑なルールまで対応できるので魅力的なプションですね。\nGitHub hook trigger for GITScm polling このオプションはGitHubとの連動で、リポジトリにPushが発生するとビルドが行われるということです。Jenkins内ではこのオプションを選ぶだけであまり複雑な設定はないですが、GitHubのリポジトリでPushが発生したということをJenkins側に知らせるための設定が必要となります。\nGitHubのリポジトリに入ると、上段にSettingsというタブがあります。それをクリックし、さらにWebhooksというメニューに入ります。そうすると以下のような画面が現れます。\nここでAdd Webhooksボタンを押すと、また以下のような画面になります。\nここでも複雑な設定はいらなく、Payload URLにJenkinsのURL/github-webhookと書くだけです。例えばJenkinsのURLが192.168.0.2:8080だとすると、http://192.168.0.2:8080/github-webhook/と書きます。これでリポジトリにPushが発生したら、Jobは自動ビルドされます。WebサーバーでのGit管理もこれで簡単にできますね。\nPoll SCM '\nSCMとは、Source Control Management(ソース管理構成)のことです。1GitやSVNなどのツールによる意味します。Poll SCMを選択すると、Build periodically同様Scheduleの入力欄が現れます。ここに同じく周期を入力します。ただ何が違うかというと、設定した時間になっても必ずしもビルドが行われるのではないということです。設定した時間になるとソースの確認をして、そこで変更があった場合のみビルドが行われるらしいですね。GitやSVNの連動で使える設定みたいです。\n最後に 様々なビルドトリガーの指定ができるゆえ、活用できる方法が多いという面もまたJenkinsの魅力ではないかと思います。Jenkins自体のアップデート対応も、周期的にyum updateするようなJobを構成するとできそうな気がしますね。そのほか、Git連動とsystemctlでのサービス管理機能を組み合わせてTomcatやNode.jsの管理もローカルでできますので便利です。\nそれではまた、便利な使い方を見つけたらJenkinsのポストを載せましょう！\nソフトウェアエンジニアリングの観点からは、Software Configuration Management(ソフトウェア構成管理)の略でもあります。簡単にいうとソースコードだけでなく、開発環境やビルド構造などの全般的な環境情報を定義・管理することを構成管理というらしいです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-07-20T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-build-trigger/","title":"JenkinsのJobを実行する"},{"content":"以前、自分より開発者として日本就職が早かった大学の後輩がいて、どんな言語やフレームワークを勉強した方がいいかを聞いたことがあります。周りではC#をやるといい、ReactやNode.jsのような流行りのライブラリーを学んだ方がいいという人もいましたが、現場で使われている人の意見が聞きたかったからです。そして彼は、言語やフレームワークはメインとしている言語をマスターしたらいつでも変えられるもので、実質的に必要となるスキルはデザインパターンと言っていました。\nそれから本を買い、いくつかのデザインパターンを見たことはありますが、そのパターンたちをどう使ったらいいか一人で考えるのは難しいことでした。一人でコードを書く時は、自分が理解できるコードを書けばいいだけなのでそこまで考える余裕が無くなりますね。また、エンドユーザーだけを意識したコードになりがちだったのであまりパターンを含むコードの書き方をする必要もなかったです。\nそれが今はJavaでのフレームワーク開発に関わることとなり、自分の買いたコードを違う人が使えるように書けとの指示を受けましたが、いつも通りDTOを元とするWrapperクラスを生成して行けばいいのかなと思っていたら、実装してみてからはこれはダメだなと感じました。なぜなら数十から数百に至る変数の設定値があって、引数として渡すのも単純な数字や文字などではなかったからです。自分の作ったコードが使う側として接した場合は、これはダメと思うはず。\nどう改善したらいいかと悩んでいたら、指示を出した方からBuilderパターンを使うといいだろうというアドバイうをもらいました。これなら引数は最低限にして、直観的に使えるらしいです。なので実際使ってみました。そして伝統的なDTOと比べてみると、確かになと思いましたね。何が違ってなぜ違うのかを、DTOとBuilderパターンの比べでこれから述べたいと思います。\nTelescoping Constructor Pattern テレスコープとは望遠鏡のこと。どこが望遠鏡的かというと、だんだん伸びていくコンストラクターの形が望遠鏡の伸縮みたいでこんな名前になったようです。最近はJava Beanの中でもよく使われるパターンなのですね。オブジェクトを生成する時に引数の数によって値を入れる変数の数を調節できます。Method Overloading1で複数のコンストラクターを用意するだけです。\n例えばカフェでコーヒーを注文する過程を、Javaのクラスで具現化するとしましょう。カップのサイズ、ホットかアイスか、シロップは入れるか、クリームは入れるか…様々なオプションがありますね。これをTelescoping Constructor Patternで書くと、以下のようになります。\npublic class CoffeeOrder{ private final String size; private final boolean hot; private final boolean addCream; private final boolean addSugar; private final boolean takeout; public CoffeeOrder(Stirng size, boolean hot){ this(size, hot, false, false, false); } public CoffeeOrder(Stirng size, boolean hot, boolean addCream){ this(size, hot, addCream, false, false); } public CoffeeOrder(Stirng size, boolean hot, boolean addCream, boolean addSugar){ this(size, hot, addCream, addSugar, false); } public CoffeeOrder(Stirng size, boolean hot, boolean addCream, boolean addSugar, boolean takeout){ this.size = size; this.hot = hot; this.addCream = addCream; this.addSugar = addSugar; this.takeout = takeout; } } これでオーダを定義するクラスが一つ、できました。実際使ってみましょう。\npublic class Cafe{ public static void main(String[] args){ // オーダーごとのオブジェクトを生成する CoffeeOrder order_1 = new CoffeeOrder(\u0026#34;tall\u0026#34;, false); CoffeeOrder order_2 = new CoffeeOrder(\u0026#34;grande\u0026#34;, false, true); CoffeeOrder order_3 = new CoffeeOrder(\u0026#34;venti\u0026#34;, true, true, false); CoffeeOrder order_4 = new CoffeeOrder(\u0026#34;tall\u0026#34;, false, false, true, false); } } このパターンのよくない点は、オブジェクトを生成する時、引数の意味を分かりにくいという点です。実際クラスの中身をみないと、連続しているfalseやtrueの意味が分かりませんね。そして例えば、サイズとシロップだけを引数として入れたい場合は、それに合わせてまたコンストラクターを作成しなければならないです。変数が増えれば増えるほど、それに合わせコンストラクターを用意する必要があるという問題もあります。あとでオーダーのオプションが増えたり減ったりするとそれに対応するのが難しいですね。\nJava Bean, DTO(VO) JavaでOOP2の概念を学ぶ時、初めて接したのがこのJava Beanです。これも一つのパターンと言えますね。みなさんがよく知っているよう、GetterとSetterで値を渡すパターンです。同じくオーダーのクラスを作ってみましょう。\npublic class CoffeeOrder{ private final String size; private final boolean hot; private final boolean addCream; private final boolean addSugar; private final boolean takeout; public CoffeeOrder(){} public setSize(String size){ this.size = size; } public getSize(){ return this.size; } public setHot(boolean hot){ this.hot = hot; } public getHot(){ return this.hot; } public setAddCream(boolean addCream){ this.addCream = addCream; } public getAddCream(){ return this.addCream; } public setAddSugar(boolean addSugar){ this.addSugar = addSugar; } public getAddSugar(){ return this.addSugar; } public setTakeout(boolean takeout){ this.takeout = takeout; } public getTakeout(){ return this.takeout; } } コンストラクターとして引数を受け取るパターンも含める場合はありますが、Java Beanとしての特徴はこのGetterとSetterにあるので、ここでは省略。では同じく、これでオーダーを生成してみましょう。\npublic class Cafe{ public static void main(String[] args){ // オーダーの内容はSetterで設定する CoffeeOrder order_1 = new CoffeeOrder(); order_1.setSize(\u0026#34;tall\u0026#34;); order_1.setHot(false); CoffeeOrder order_4 = new CoffeeOrder(); order_2.setSize(\u0026#34;grande\u0026#34;); order_2.setHot(false); order_2.setAddCream(false); } } さっきよりは個別項目ごとに値を設定することができ、それぞれのSetterをみてどんなオーダーを出しているのかがより明確になりますね。また変数が増えてもそれに合わせてGetterとSetterを用意するだけで良いです。\nただ、一つの注文を完成する時、オプションの数が増えると無題に長いコードになってしまうという問題がありますね。今は5つのフィールドを使っているだけですが、もし20、30のオプションがあったら？それをいちいち書くのはかなり時間もかかることですね。私が失敗したのはこの部分でした。なのでBuilderを使い、この問題を解決してみます。\nBuilder Pattern public class CoffeeOrder{ private final String size; private final boolean hot; private final boolean addCream; private final boolean addSugar; private final boolean takeout; public CoffeeOrder(Stirng size, boolean hot, boolean addCream, boolean addSugar, boolean takeout){ this.size = size; this.hot = hot; this.addCream = addCream; this.addSugar = addSugar; this.takeout = takeout; } public static Builder builder() { return new Builder(); } public static class Builder{ private final String size; private final boolean hot; private final boolean addCream; private final boolean addSugar; private final boolean takeout; public Builder(){} public CoffeeOrder size(String size){ this.size = size; return this; } public CoffeeOrder hot(boolean hot){ this.hot = hot; return this; } public CoffeeOrder addCream(boolean addCream){ this.addCream = addCream; return this; } public CoffeeOrder addSugar(boolean addSugar){ this.addSugar = addSugar; return this; } public CoffeeOrder takeout(boolean takeout){ this.takeout = takeout; return this; } public CoffeeOrder build() { return new CoffeeOrder(size, hot, addCream, addSugar, takeOut); } } } Inner Classも入り、何か複雑になったように見えますが、実際使ってみるとそうでもないです。このようなBuilderクラスを使うとどうなるのか、また確認してみましょう。\npublic class Cafe{ public static void main(String[] args){ // オーダーを生成してBuilderを使う CoffeeOrder order_1 = new CoffeeOrder(); order_1.Builder().size(\u0026#34;tall\u0026#34;).hot(false).build(); CoffeeOrder order_2 = new CoffeeOrder().Builder() .size(\u0026#34;grande\u0026#34;) .hot(false) .addCream(true) .takeout(true) .build(); } } Setterと似たような使い方で、一回だけで複雑なオプションを全部処理できます。また、生成と同時にもオーダーを完成できます。Builderが戻り値として自分自身を使っていて、連続してメソッドを呼び出すことができるからです。これならいくら変数が増えても対応できますね！\nLombokを使う 以上のパターンはLombokを使うとアノテーションだけで設定できるらしいです。例えばコンストラクターは@NoArgsConstructorや@RequiredArgsConstructor、@AllArgsConstructorでできます。Java Beanなら@DataをつけることでGetterとSetterができるらしいですね。また、Builderの場合は@Builderでできると言います。以下はLombokを使った場合の例です。\nimport lombok.Builder; @Builder public class CoffeeOrder{ private final String size; private final boolean hot; private final boolean addCream; private final boolean addSugar; private final boolean takeout; } ちなみに@Builder(toBuilder = true)にすると、インスタンスの新規生成ではCoffeOrder.builder()から直接Builderにアクセスできるようになります。また、既存のインスタンスの値を受け継ぐ場合はorder_1.toBuilder()を使えるようになります。実際は以下のコードになるようなものとなります。\nimport lombok.Builder; @Builder(toBuilder = true) public class CoffeeOrder{ private final String size; private final boolean hot; private final boolean addCream; private final boolean addSugar; private final boolean takeout; // ... 基本的なBuilder public CoffeeOrderBuilder toBuilder() { return new CoffeeOrderBuilder().size(this.size).hot(this.hot).addCream(this.addCream) .addSugar(this.addSugar).takeout(this.takeout); } } そしてBuilderを使うとき、親クラスのフィールドをそのまま継承したい場合はフィールドに@Builder.Defaultをつけることでそのまま受け継がれます。他にもフィールドにつけてアクセスレベルを指定できるなど便利な機能が多いので、ぜひ使いたいものですね。\n最後に デザインパターンの種類に何があって、どんな構造をしているかを把握することも大事ですが、何より大事なことは適材適所に使えることではないかと思いました。最初から自分がBuilderパターンを知っていたとしても、それを使ったらいいと言われなかったら果たして使おうとしていただろうかと思うと、そうでもないような気がしますね。なのでこれからはデザインパターン自体の研究とともに、それをどの場合に使えるかという面から考察していきたいと思います。\nそれでは、また会いましょう！\n引数の数や種類を変えることで、同名のメソッドを複数作成する記法。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nObject Oriented Programing.オブジェクト指向プログラミングとも言いますね。コードをひたすら上から下まで流れる処理として扱うのではなく(手続き型プログラミング)、隔離されたオブジェクト間のデータ交換として成立するプログラミングのパラダイムです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-07-14T00:00:00Z","image":"https://retheviper.github.io/images/java.jpg","permalink":"https://retheviper.github.io/posts/java-design-pattern-builder/","title":"デザインパターン、Builder"},{"content":"サーバーで動くプログラムなら、一般的なエンドユーザー用のプログラムとはその挙動が違うということは明らかでしょう。どんなデータを持ち、どんな処理をするかの問題でもありますが、今回は簡単に「実行」の面で書いてみたいと思います。簡単にいうと、誰かが止めるまではずっと実行され続けるようなプログラムをLinuxではどう実現したらいいかの話です。\nCentOSやRHELでは、serviceというコマンドがあります。yumのようなパッケージ管理プログラムを使って、様々なプログラムをインストールしていくとその中には一回だけの実行で終わらず、メモリー常に常住させる必要のあるものもありますね。以前このブログで紹介したことのあるJenkinsもまたそのようなプログラムの一つです。\nそのようなプログラムは一回インストールしたら自動的にserviceコマンドで動かしたり止めたりすることができるようになりますが、例えば自作したプログラムをそのように動作させることはできるのでしょうか？ユーザーがなんでもできるLinuxならできそうですが。\nそう思っていたら、ちょうど仕事でJavaアプリケーションをサービスとして登録し、管理する必要があることになりました。私はJenkinsのタスクを管理していたため、実際はJavaアプリケーションがGitにコミットされたらPull→ビルド→実行されているJavaアプリケーションを止めて新しくビルとした物を実行するというタスクが必要となったのです。実際はサービスに触れる必要はなく、ビルドの後にサービスを止めたり再開させたりするシェルコマンドを実行するように仕組んだだけだったのですが、一体これはどう動いてるのか気になりましたので調べてみました。\nService(Daemon)を作ること サービスになるものは、Demonとも呼ぶらしいです。少し調べてみると「システムに常住しながら、とある状態になると自動動作するプログラム」「周期的なサービス要請を処理するために実行され続けるプログラム」「バックグラウンドで動いている」などという定義がありました。これでだいたいどんな性格のプログラムのことを指しているかがわかりますね。SpringやNode.jsなどで作られたWebサーバプログラムとかがこの定義に該当するでしょう。\nそれでは早速、そのサービス(もしくはデーモン)を実現するためにはどうしたらいいかを説明していきたいと思います。準備するものは大きくサービスとして登録したいプログラム、そのプログラムをどんなサービスにするかを書いたサービスの設定ファイル、コマンドでのサービス登録と実行などがあります。仕事ではSpring Bootで作られたプログラムをサービスにしていますが、これの場合は実行するためのシェルスクリプトの設定や外部ファイルの参照なども必要となるので比較的簡単な、Node.jsを使ったプログラムを例として使います。\nNode.jsの場合、$ node /node/index.jsみたいな簡単なコマンドで実行できますね。これをサービスに登録する場合は以下のようになります。\nserviceファイルを作成する まずはサービスとしてどんな動きをしてほしいか、どんな名前でサービス化するかなどを記述したファイルを作ります。\nvi /etc/systemd/system/NodeServer.service それでは以下のような形で中を書いていきます。\n# サービスとしての設定 [Unit] Description = NodeServer # この名称でサービスが登録される After = syslog.target network.target # システム起動時の実行の優先順位(syslogとnetworkの後に実行する) # 実行するプログラムの設定 [Service] Type = simple # 動作の様子を決める・デフォルトはsimple ExecStart = /usr/bin/node /node/index.js # node /node/index.jsと同じだがシンボリックリンクなしで記述する Restart = on-failure # 起動に失敗すると再実行 User = nodeservice # 実行するユーザー、権限に注意！ # シンボリックリンクや別名などの設定 [Install] WantedBy = multi-user.target # どこのフォルダにシンボリックリンクを作るかを指定、これが一般的らしい 他にも様々なオプションがありますが、基礎的な情報はこのくらいかと。テストのために作成しておいたNode.jsでのWebサーバーはこれで動きました。(と言ってもHello Node.js!を出力するだけの簡単なものですが…)\n実際仕事で扱っていたJavaアプリケーションは、PID1を管理するスクリプトの指定や、止める時実行するシェルスクリプトの指定もありました。例えばExecStopを使うとサービスを修理するために実行したいコマンドを、ExecReloadを使うとリロードする時に実行したいコマンドを書くことができるらしいです。シェルスクリプトから実行する場合や状態が変わった時何かの措置が必要な場合に考えられるオプションですね。\nこのようにserviceファイルを作成したら、次はプロセスをシステムサービスに登録して実行します。\nEnable \u0026amp; Start Service 先にserviceというコマンドを言及しましたが、CentOS7からはsystemctlを使うらしいです。2実際、CentOS7ではserviceだけではシステムサービスの登録は不可能なのでsystemctlを使うことになります。このコマンドでできることはサービスの登録や解除、実行と停止、再実行などがあります。まずサービスをシステムサービスに登録すると、システムが起動する時に実行されるようになります。\nWindowsに例えると、「スタートアップ」にプログラムを登録したり解除することと、タスクマネージャーによってプロセスを管理するような機能が一緒になっている感じですね。\nこちらはコマンドだけなので、簡単に紹介します。\n# システムサービスとして登録 $ systemctl enable NodeServer $ systemctl start NodeServer # 実行されているサービスの中で検索する $ systemctl list-units | grep NodeServer # serviceファイルを修正した場合はリロードする $ systemctl daemon-reload # 停止と再起動 $ systemctl stop NodeServer $ systemctl restart NodeServer # サービスの状態を確認する $ systemctl status NodeServer # システムサービスから解除 $ systemctl enable NodeServer # 実行に問題があった場合は以下のコマンドでサービスのログを参照できる $ journalctl -xe サービスの登録や解除はできなくても、serviceコマンドでの制御もできます。ただいずれは消えるかもしれないコマンドなので、なるべくsystemctlに慣れた方が良いかもしれませんね。\n# serviceでできること $ service NodeServer start $ service NodeServer status $ service NodeServer stop $ service NodeServer restart これで私の作った簡単なNode.jsのWebサーバーがシステムサービスとなって、止まることなく動くようになりました。めでたしめでたし。他にも何か自動化のプログラムを作ることになったら有効活用できそうな知識ではないのかと思います。\nそれでは今回のポストはここまで。また会いましょう！\nProcess IDの略で、実行中のプロセスに与えられるIDです。こちらはpsコマンドで確認できて、PIDからプロセス名を参照したりプロセス名からPIDを参照することもできます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCentOS7でもserviceは健在ですが、systemctlにリダイレクトされるみたいですね。CentOS6までは/etc/rc.d/init.dからサービスを管理していて、７からはサービスユニットという名称になったらしいです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-07-11T00:00:00Z","image":"https://retheviper.github.io/images/linux_terminal.jpg","permalink":"https://retheviper.github.io/posts/linux-systemctl-service/","title":"Linuxのシステムサービスを作る"},{"content":"この度はAnsibleを少し、使ってみる機会がありました。Ansibleもまた一つの自動化ツールで、あらかじめタスクを指定することで複数の環境で適用できるという意味ではJenkinsと似たようなものでした。ただ違う点は、Jenkinsは主にデプロイとリリース、テストなどの自動化に特化しているのに対し、Ansibleはサーバーの自動構築にその目的があるということです。\nつまり、Ansibleを使えば複数の環境を同じくセッティングできます。例えば私の経験した範囲では「開発」→「内結」→「外結」という流れになっていて、それぞれで使う環境が変わっていました。段階が変わるたびに同じくサーバーを構築するのは時間の無駄で、セッティングすべき項目が増えると人の手では様々な面でミスが起こり得るので、Ansibleで自動化したいというのが今回のタスク。\nちょっと調べてみると、AnsibleではPlaybookと呼ばれるYAMLファイル1を使ってサーバーの設定を行います。深く入るとより複雑な動きもできるようですが、基本は接続先の情報(hostsファイル)とセッティングの値を保存したYAMLで起動します。できることはフォルダーやユーザーの作成、yumによるプログラムのインストール、シェルコマンド実行、ファイル転送などができて、うまく設定ができたら複数のサーバーを分散して使う時などに有効活用できそうな印象でした。\nただYAMLファイルとフォルダー構成などが少し複雑なので、ここでは一つ一つのファイルの設定を述べてみようと思います。以下のYAML設定は、AMAZON Linuxを基準に書かれています。\nAnsibleのインストール Ansibleはyum、brew、pip、apt-getでインストールできます。簡単ですね！ただ場合によってはPython2やPIPが必要となるので、事前にインストールしておきましょう。\nyum install ansible $ brew install ansible $ pip install ansible $ sudo apt-get install ansible 使っている環境に適した方法でインストールしましょう。\nbatchserver.yml まずはサーバーごとのYAMLファイルを生成します。Ansible実行時に使われるのはこっちで、どのサーバーでどんな動作をするかを記述します。サーバーごとにしたいことに関しては別のYAMLファイルを用意し、共通、バッチサーバー、WEBAPサーバー用のYAMLファイルをそれぞれ作るというイメージです。\n以下のコードは、バッチサーバーを想定した場合の例です。\n- hosts: batchserver become: true roles: - common - batch hostsはhostsファイルに記述されたホストのことを意味します。ここでbatchserverと書くと、Ansibleの実行時は自動的にhostsファイルからbatchserverというグルーブに属している全サーバーに接続し同じ動きをします。全グループに対して実行したい場合はallと書きましょう。\nそしてbecomeをtrueに設定すると、接続先での全ての命令ががsudoとして実行されます。rolesはサーバーで行う行動を指定したYAMLファイルのことを記述していて、私の場合はどんなサーバーでも共通的に実行したことを書いたcommonとバッチサーバーでだけ実行したいbatchを区分しておいたので両方を書いています。\nhosts このファイルは上述した通り、Ansibleで自動設定を行いたい接続先のことを書きます。\n[batchserver] 192.168.0.1 ansible_ssh_user=batchuser1 192.168.0.2 ansible_ssh_user=batchuser2 [webapserver] 192.168.10.1 ansible_ssh_user=webapuser1 192.168.10.2 ansible_ssh_user=webapuser2 基本はグルーブを指定して、ホストの情報を書いておくと自動的に分類されます。上に書いたYAMLファイルではまずbatchserverだけを指定しているので実行時にwebapserverのグループは無視されます。そしてansible_ssh_userはその名の通りAnsibleでSSH接続する時に使われるユーザー名を指定しています。もちろんこうしなくても実行時にユーザー名やパスワードを入れることもできます。\nroles / batch / tasks / main.yml ここはbatchserver.ymlで指定したrolesで、実際どんなことがしたいかを記述するYAMLファイルが入ります。rolesは同盟のフォルダーの下に作っておいたものしか実行できませんので注意してください。\n本格的にやりたいことを書くファイルなので、どんな行動が指定できるかまず参考にしてください。\n- name: Create user groups group: name: \u0026#34;{ { item.group_name } }\u0026#34; # マークダウンの仕様のためスペースを入れているが、実際はスペースなし gid: \u0026#34;{ { item.group_id } }\u0026#34; with_items: - { group_name: \u0026#39;group01\u0026#39;, group_id: \u0026#39;101\u0026#39; } - { group_name: \u0026#39;group02\u0026#39;, group_id: \u0026#39;201\u0026#39; } - name: Create users user: name: \u0026#34;{ { item.user_name } }\u0026#34; password: \u0026#34;{ { item.user_passwd } }\u0026#34; uid: \u0026#34;{ { item.user_id } }\u0026#34; group: \u0026#34;{ { item.user_group } }\u0026#34; shell: /bin/bash with_items: - { user_name: \u0026#39;user01\u0026#39;, user_group: \u0026#39;group01\u0026#39;, user_id: \u0026#39;101\u0026#39;, user_passwd: \u0026#39;user01\u0026#39; } - { user_name: \u0026#39;user02\u0026#39;, user_group: \u0026#39;group02\u0026#39;, user_id: \u0026#39;201\u0026#39;, user_passwd: \u0026#39;user02\u0026#39; } - name : Create folders file: path={ { item } } owner=user01 group=user01 mode=0755 state=directory with_items: - /user01 - name: Package install by yum yum: name: \u0026#34;{ { packages } }\u0026#34; vars: packages: - python2-pip - postgresql - postgresql-devel - name: Upgrade pip by shell command shell: bash -lc \u0026#34;pip install --upgrade pip\u0026#34; - name: Install python modules pip: name: \u0026#34;{ { item } }\u0026#34; executable: pip with_items: - cx_Oracle - psycopg2 - boto3 - paramiko - name: Copy files copy: src= { { item.source } } dest= { { item.dest } } owner=root group=root mode=0755 with_items: - { source: etc/somefile.zip, dest: /etc/somefile.zip } 上から順番に、ユーザーグループ作成、ユーザー作成、フォルダー作成、yumでパッケージインストール、シェルコマンドの実行、ファイル転送となります。結局はSSH接続してシェルスクリプトを実行するようなものですね。でもYAMLファイルにより簡単に設定できるということが良いところではないかと思います。なんども繰り返して実行しても良いですしね。\nただSSH接続したあとはYAMLファイルの上から一行づつ読みコマンドを実行していくので、実行したいことの順番には気をつける必要があります。例えば当たり前なことなんですが、ユーザーグループを作成する前に特定のグループにユーザーを作成するとかはできないので注意してください。\nroles / batch / files このフォルダーには転送に使いたいファイルをおきます。例えば上のmain.ymlに書いたetc/somfile.zipを転送したい場合は、このフォルダーの配下に同じパスのファイルを置きます。もちろん複数のファイルを転送することも、それぞれ違うフォルダーに分けておくことも可能です。\nroles / common / tasks / main.yml このファイルはどんなサーバーでも共通的に実行したいコマンドを集めています。\n- name: Upgrade all packages by yum yum: name=* state=latest - name: Install openjdk 11 shell: bash -lc \u0026#34;amazon-linux-extras install java-openjdk11\u0026#34; - name: Correct java version selected alternatives: name: java path: /usr/lib/jvm/java-11-openjdk-11.0.2.7-0.amzn2.x86_64/bin/java このファイルでやっていることは、yumによる全パッケージのアップデートと、OpenJDKのインストールです。JDKをインストールするだけではサーバーでJava実行時の基本バージョンがOpenJDK11にならないのでAlternativeからJavaのバージョンを選択するところまで入れています。同じやり方でPython3をインストールしてAlternativeで基本実行のバージョンを指定するなどのこともできます。\nここまでくるとAnsibleによる基本設定は終わり。難しくないですね！(深く入れば難しくなりそうですが)\n実行する それではPlaybookが用意されたので実行します。以下のコマンドで実行できます。\n# 一般的な実行 $ ansible-playbook server.yml -i hosts # Dry runの場合(Playbookの文法チェック用) $ ansible-playbook server.yml -i hosts -C # SSH接続ユーザー名を入れる場合 $ ansible-playbook server.yml -i hosts -u hostuser こちらも簡単ですね。実行時にSSH接続するユーザーのパスワードを要求される場合がありますが、これはあらかじめSSH接続するユーザーの公開鍵を登録しておくことで回避できます。sudoの場合は接続先でvisudoからNOPASSWDを設定しておくと便利です。\n最後に どうでしたか。最近はなんでも自動化が進んでいて、JenkinsとAnsibleがあればサーバー構築から作成物のデプロイまで簡単にできる環境を構築できるので、ますます生産性が上がりそうな気がします。まだ手動でサーバーの構築をやっている方にはぜひ一度使ってみてくださいとオススメしたいですね。\nそれでは今回のポストはここで終わり。また会いましょう！\nデータフォーマットの一種で、その構造がJSONとかなり似ています。ただJSONと比べてみるとよりマークアップ言語に近い感じですね。多少癖はあるものの、JSONよりは直観的な表現ができます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-07-05T00:00:00Z","image":"https://retheviper.github.io/images/ansible.jpg","permalink":"https://retheviper.github.io/posts/ansible-server-automation/","title":"Ansibleでサーバーを構築する"},{"content":"あまり詳しくない上に、何か新しいプログラムを作り出すこととは少し距離があるなと思ってDBにはあまり興味を持ってなかった私ですが、ITの業界で働きながらDBと接しないのは難しいことです。そしてそうやって接するDBはまた優しくない課題になってきますね。この度の仕事でも主にPythonを使ってスクリプトを書いたものの、そのタスクのメインとなるものはDBとの連携でした。なので今回のポストではどうやってPythonでDBに繋ぎ、SQL文を発行し、そのデータを扱ったかについて述べたいと思います。\n今回私に与えられたタスクは、DBからバイナリーのデータを抽出して、それをAWSのS31にアップロードするスクリプトを作ることでした。また、アップロードが終わった時点でテーブルを更新する必要がありますが、抽出の対象となるDBと更新の対象となるDBがそれぞれ違うものでした。片方はOracleで片方はPostgreSQL。そしてロガーを入れ各DBやS3の接続に失敗するなどの時にはログで分かるようにしたり、DBの接続情報を外出しにして外部ファイルから読み込むようにするなどの条件がありました。ロガーについては[前回のポスト](../../../06/15/python-logger/で述べましたので、今回は省略とさせていただきます。他にバイナリーデータを抽出するには引数を受け、最終更新日を確認した上でそれより後のファイルを保存する、S3にアップロードする際はテーブルのカラムからパス名を決める、画像のアップロードの後には最終更新日を更新する、などの条件がありました。\n要略すると、今回作るスクリプトの仕様は以下となります。\nコマンドラインで引数を受ける 引数からPostgreSQLのテーブル(1)を参照し、引数に当たる作業の最終更新日を取得する OcaleDBのテーブルから最終更新日以後のバイナリーデータをSELECTしてJPGファイルとして保存する PostgreSQLのテーブル(2)からファイルの保存先のパスとなるカラムを取得する。 S3へJPGファイルごとのフォルダーに画像をアップロードする PostgreSQLのテーブル(3)にアップロードされたJPGファイルの情報を記録する PostgreSQLのテーブル(1)最終更新日を更新する 正常終了したらexitコード0となり、例外が発生するとexitコード9を出力して終了する PostgreSQLの場合は参照するテーブルが多く、処理の順番があるので少し複雑にも感じられますが、まずはDBとの連動を試しみスクリプトを組むことにしました。\nPythonでDBを連動する 案外、PythonでDBに接続することはそんな難しいことではなかったです。それぞれ違うモジュールを使っていてコマンドにも違いがあるわけですが、検索してみると例題が多かったですね。Oracleの場合はホスト名、ポート、サービス名、ユーザー名、パスワードを用意します。使うモジュールはcx_Oracleです。\nPostgreSQLの場合は、psycopg2を使います。接続に必要となる情報はホスト名、ポート、DB名、ユーザー名、パスワードです。ただし、モジュールは両方pipからインストールできますが、psycopg2の場合はライブラリー依存性があるのでAmazon Linuxを基準にpostgresql-developをyumからインストールする必要があります。他は触れたことがないのでよくわかりませんが、CentOSも同様かと。psycopg2のインストールに問題が生じたらエラーメッセージを確認して、必要なPosgreSQLのライブラリーをインストールしましょう。\n二つのDBに接続するための手順には少し違いがありますが、接続した後の処理は同じです(本当はSQL文の文法も少し違うようですが…)。それではOracleとPostgreSQLの接続方法と接続以後の処理を分けて説明しましょう。\n接続(Oracleの場合) import cx_Oracle tns = cx_Oracle.makedsn(\u0026#39;ホスト名\u0026#39;, \u0026#39;ポート\u0026#39;, \u0026#39;サービス名\u0026#39;) connect = cx_Oracle.connect(\u0026#39;ユーザー名\u0026#39;, \u0026#39;パスワード\u0026#39;, tns) cx_Oracleのmakedsnにホスト名、ポート、サービス名を入れます。そしてその情報をconnectを使う際にユーザー名、パスワードと共に入れます。これでOracleの接続設定は終わりです。\n接続(PostgreSQLの場合) import psycopg2 connect = psycopg2.connect(\u0026#39;host=\u0026#39; + \u0026#39;ホスト名\u0026#39; + \u0026#39; port=\u0026#39; + \u0026#39;ポート\u0026#39; + \u0026#39; dbname=\u0026#39; + \u0026#39;DB名\u0026#39; + \u0026#39; user=\u0026#39; + \u0026#39;ユーザー名\u0026#39; + \u0026#39; password=\u0026#39; + \u0026#39;パスワード\u0026#39;) PostgreSQLの場合はより簡単です。Linuxで使うコマンドラインツールのpsqlみたいに、接続に必要な情報を文字列として並びconnectすれば終了です。\nDB処理の共通部分 # 実際DBに接続してカーソルを取得 cursor = connect.cursor() # カーソルからSQL文の実行 cursor.execute(\u0026#39;発行したいSQL文\u0026#39;) # SQL文を実行した結果を取得 # 一件だけの結果が必要な時 result = cursor.fetchone() # 結果を1000件づつフィッチして処理したい場合 result = cursor.fetchmany(1000) # 結果を全部フェッチしたい場合 result = cursor.fetchall() # カーソルをクローズする cursor.close() # SQL文をコミットする(INSERT/UPDATEなどの変動) connect.commit() # コネクションを切る connect.close() 接続したあとはカーソルを取得し、そのカーソルでSQL文を発行します。SQL文の実行結果を取得するにはfecthを使いますが、処理したい結果の規模によって三つの選択肢があります。あと、大量のデータを処理したい場合はなるべくfetchallよりはfetchmanyでフェッチサイズを指定して使う方が良いらしいです。データが多すぎると性能に影響が出ますので。フェッチまで終わって取得した結果は、Pythonでは一つのレコードを配列にして、それらを集めたリストになります。コードで言いますと以下のような形となります。\n# \u0026#39;SELECT column_1, column2 FROM TABLE\u0026#39;の場合 result = cursor.fetchmany(1000) for row in result: # 結果は1000件のリストとなる print row[0] # column_1の内容が出力される print row # column_2の内容が出力される これでDBの連動の基礎は終わりです。それでは早速、スクリプトのコードをいかに公開します。\n画像ファイルをS3へアップロードするコード # -*- coding: UTF-8 -*- import os, sys, cx_Oracle, psycopg2, json, boto3, datetime, shutil function_code = \u0026#39;引数で受け取る\u0026#39; # 以下、DB環境情報(DBConnection.confから読み込む) HOST_POST = \u0026#39;PostgreSQLの接続ホスト名\u0026#39; PORT_POST = \u0026#39;PostgreSQLの接続ポート\u0026#39; DB_NAME_POST = \u0026#39;PostgreSQLのDM名\u0026#39; USER_POST = \u0026#39;PostgreSQLのユーザ名\u0026#39; PWD_POST = \u0026#39;PostgreSQLのパスワード\u0026#39; HOST_ORAC = \u0026#39;Oracleの接続ホスト名\u0026#39; PORT_ORAC = \u0026#39;Oracleの接続ポート\u0026#39; SERVICE_NAME = \u0026#39;Oracleのサービス名\u0026#39; USER_ORAC = \u0026#39;Oracleのスキーマ名\u0026#39; PWD_ORAC = \u0026#39;Oracleのパスワード\u0026#39; # コマンドライン引数 args = sys.argv # 環境関連変数 imageFolder = \u0026#39;/tmp/images\u0026#39; # 差分連携のための前処理 def GetProcdate(args): global function_code function_code = args[1] try: # PostgreSQL接続 print (\u0026#39;\u0026gt;\u0026gt; Starting Job. The function Code is: \u0026#39; + function_code) connect = psycopg2.connect(\u0026#39;host=\u0026#39; + HOST_POST + \u0026#39; port=\u0026#39; + PORT_POST + \u0026#39; dbname=\u0026#39; + DB_NAME_POST + \u0026#39; user=\u0026#39; + USER_POST + \u0026#39; password=\u0026#39; + PWD_POST) cursor = connect.cursor() except: print (\u0026#39;\u0026gt;\u0026gt; Unable to connect PostgreSQL. quitting.\u0026#39;) sys.exit(9) selectSQL = \u0026#34;SELECT last_date FROM date_table WHERE function_code=\u0026#39;%s\u0026#39;\u0026#34; % (function_code,) cursor.execute(selectSQL) result = cursor.fetchone() # 結果があった場合最終更新日を変数として保存 if (result is not None): last_date = result[0] else: print (\u0026#39;\u0026gt;\u0026gt; No data matches with \u0026#39; + function_code + \u0026#39;. quitting.\u0026#39;) sys.exit(1) cursor.close() connect.close() # 次の処理へ移行 GetImageFromTable(last_date) # OracleにSQL文を発行してファイルを読み込み保存 def GetImageFromTable(last_date): global imageFolder try: # Oracle接続開始 tns = cx_Oracle.makedsn(HOST_ORAC, PORT_ORAC, SERVICE_NAME) connect = cx_Oracle.connect(USER_ORAC, PWD_ORAC, tns) cursor = connect.cursor() except: print (\u0026#39;\u0026gt;\u0026gt; Unable to connect Oracle DB. quitting.\u0026#39;) sys.exit(9) # Oracleのテーブルから商品画像ファイル取得 cursor.execute(\u0026#34;SELECT image_data, image_name FROM WHERE \u0026gt;=(:last_date)\u0026#34;, {\u0026#39;last_date\u0026#39;: last_date}) # フォルダがなかったら作成・あったら削除 if os.path.exists(imageFolder): shutil.rmtree(imageFolder) else: os.mkdir(imageFolder) # 1000件づつ取り出し while True: rows = cursor.fetchmany(1000) # 結果がない場合はループから出る if len(rows) == 0: break # 画像ファイル置き場に画像ファイル保存(イメージネーム.webp) for image in rows: fileNameS = imageFolder + \u0026#39;/\u0026#39; + str(image[1]) + \u0026#39;.jpg\u0026#39; imageFile = open(fileNameS, \u0026#39;wb+\u0026#39;) imageFile.write(image[0].read()) imageFile.close() counter = counter + 1 # 保存されたファイルの数をカウント fileCount = len([name for name in os.listdir(imageFolder) if os.path.isfile(os.path.join(imageFolder, name))]) print (\u0026#39;\u0026gt;\u0026gt; As result: \u0026#39; + str(fileCount) + \u0026#39; files were written.\u0026#39;) print (\u0026#39;\u0026gt;\u0026gt; Finished job. Closing connection.\u0026#39;) cursor.close() connect.close() FileUploadToS3() # S3へ画像ファイルを格納 def FileUploadToS3(): # グローバル変数取得 global imageFolder global function_code # ファイル名リスト(イメージネーム)取得 files = os.listdir(imageFolder) item_cd_list = [os.path.splitext(f)[0] for f in files if os.path.isfile(os.path.join(imageFolder, f))] try: # PostgreSQL接続 connect = psycopg2.connect(\u0026#39;host=\u0026#39; + HOST_POST + \u0026#39; port=\u0026#39; + PORT_POST + \u0026#39; dbname=\u0026#39; + DB_NAME_POST + \u0026#39; user=\u0026#39; + USER_POST + \u0026#39; password=\u0026#39; + PWD_POST) cursor = connect.cursor() except: print (\u0026#39;\u0026gt;\u0026gt; Unable to connect PostgreSQL. quitting.\u0026#39;) sys.exit(9) # 結果確認用のファイル数カウンター fileCount = 0 for item_cd in item_cd_list: # アイテム管理用テーブルからコードと一致するレコード取得⇒ファイル格納パス用 selectSQL1 = \u0026#34;SELECT file_path1, file_path2, file_path3 FROM item_table WHERE item_cd=\u0026#39;%s\u0026#39;\u0026#34; % (str(item_cd),) cursor.execute(selectSQL1) resultMaster = cursor.fetchone() # 一致するレコードがなかった場合でも処理続行 if resultMaster is None: print (\u0026#39;\u0026gt;\u0026gt; Result was 0. continue job.\u0026#39;) continue # 結果から必要な情報を取得し変数に保存 image_id = str(resultMaster[2]) path = str(resultMaster[0]) + \u0026#39;/\u0026#39; + str(resultMaster[1]) + \u0026#39;/\u0026#39; + str(resultMaster[2]) + \u0026#39;/\u0026#39; # アイテムイメージ管理用テーブルからコードと一致するレコード取得⇒分岐処理 selectSQL2 = \u0026#34;SELECT * FROM WHERE image_table image_id=\u0026#39;%s\u0026#39;;\u0026#34; % (image_id,) cursor.execute(selectSQL2) result = cursor.fetchone() uploadToS3(item_cd, path) fileCount = fileCount + 1 print (\u0026#39;\u0026gt;\u0026gt; Processing file upload.\u0026#39;) # 作業完了日時を更新 updateProcSQL = \u0026#34;UPDATE date_table SET last_date = CURRENT_TIMESTAMP WHERE function_code=\u0026#39;%s\u0026#39;\u0026#34; % (function_code,) cursor.execute(updateProcSQL) # 作業終了 print (\u0026#39;\u0026gt;\u0026gt; As result: \u0026#39; + str(fileCount) + \u0026#39; files were uploaded.\u0026#39;) print (\u0026#39;\u0026gt;\u0026gt; Finished job. Closing connection and quit.\u0026#39;) cursor.close() connect.commit() connect.close() sys.exit(0) # S3へ画像ファイルをアップロード def uploadToS3(item_cd, path): global imageFolder print (\u0026#39;\u0026gt;\u0026gt; Upload image files to AWS S3.\u0026#39;) bucket_name = \u0026#39;image\u0026#39; s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: # サーバーに保存されたファイルを指定したパスに保存(file_path1/file_path2/file_path3/1_1_YYYYMMDD.webp) s3.Bucket(bucket_name).upload_file(imageFolder + \u0026#39;/\u0026#39; + item_cd + \u0026#39;.jpg\u0026#39;, path + \u0026#39;/1_1_\u0026#39; + datetime.datetime.today().strftime(\u0026#39;%Y%m%d%H%M%S\u0026#39;) + \u0026#39;.jpg\u0026#39;) print (\u0026#39;\u0026gt;\u0026gt; Image file uploaded. (Item code: \u0026#39; + item_cd + \u0026#39;)\u0026#39;) except: print (\u0026#39;\u0026gt;\u0026gt; Unable to upload files. quitting.\u0026#39;) sys.exit(9) # コネクション情報を入力ファイルから読み込む def getDBConnection(): # DBConnection接続情報ファイルのパスを取得 pwd = os.path.dirname(os.path.abspath(__file__)) dbConnection = pwd.rsplit(\u0026#34;/\u0026#34;, 1)[0] + \u0026#39;/env/DBConnection.conf\u0026#39; # ファイルがない場合異常終了 if (not os.path.exists(dbConnection)): print (\u0026#39;\u0026gt;\u0026gt; Please check DBConnection.conf. quitting.\u0026#39;) sys.exit(9) # ファイルを読み込む connectionInfo = open(dbConnection, \u0026#39;r\u0026#39;) lines = connectionInfo.readlines() # グローバル変数として保存するための宣言 global HOST_POST global PORT_POST global DB_NAME_POST global USER_POST global PWD_POST global HOST_ORAC global PORT_ORAC global SERVICE_NAME global USER_ORAC global PWD_ORAC # 条件と一致するデータがある場合変数に保存 for line in lines: if (len(line) \u0026gt; 1 and \u0026#39;POSTGRESQL\u0026#39; not in line and \u0026#39;ORACLE\u0026#39; not in line): result = line.split(\u0026#39;=\u0026#39;) if (\u0026#39;HOST_POST\u0026#39; in result[0]): HOST_POST = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;PORT_POST\u0026#39; in result[0]): PORT_POST = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;DB_NAME_POST\u0026#39; in result[0]): DB_NAME_POST = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;USER_POST\u0026#39; in result[0]): USER_POST = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;PWD_POST\u0026#39; in result[0]): PWD_POST = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;HOST_ORAC\u0026#39; in result[0]): HOST_ORAC = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;PORT_ORAC\u0026#39; in result[0]): PORT_ORAC = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;SERVICE_NAME\u0026#39; in result[0]): SERVICE_NAME = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;USER_ORAC\u0026#39; in result[0]): USER_ORAC = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) elif (\u0026#39;PWD_ORAC\u0026#39; in result[0]): PWD_ORAC = result[1].replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) # 起動部 if __name__ == \u0026#39;__main__\u0026#39;: getDBConnection() # 引数がない場合は異常終了処理分岐 if (len(args) \u0026lt; 2): print (\u0026#39;\u0026gt;\u0026gt; Please check arguement. quitting.\u0026#39;) sys.exit(9) else: GetProcdate(args) DBConnectionファイルを読み込む処理に関してはより良い書き方があるのではないかと思います…が、Python特有のGetter/Setterの書き方にあまり慣れてないゆえ、自分が理解できるコードとして書いた結果がこうです。グローバル変数がメソッド内でglobal宣言をしないと使えないというところがJavaに慣れていた自分にはかなり新鮮でした。逆にグローバル変数をそれぞれ違うメソッドではなるべく使うなという意味もあるような気もします。\nあとはファイル書き込みがテキストだけでなく、バイナリーデータの場合も簡単にかけるということが魅力的ですね。Javaだったらストリームやバッファーをはじめとしてかなり複雑な書き方になっていただろうと思いますが(しかも私はテキストしか扱ったことがないので、同じやり方でバイナリーまでかけるかどうかわかりません)、Pythonではモジュールのインポートもなしで簡単にできますね。もちろん性能を考えるとJavaでかいた方がよかったかもしれませんが、こういう簡単さんがやっぱり生産性の向上に役立つのではないかと思います。\nあと、やはりPythonはLinux親和的な構造が魅力的だと思います。処理の結果を判別するためにsys.exit()を設定すると終了コードを簡単に確認できますし、sys.argsを通じてコマンドラインから引数を受け入れることもできます。簡単かつLinux親和的なので、Linuxでのスクリプト処理が必要な場合はシェルスクリプトよりもPythonを用いる方が良いのでは、と思うようになりました。性能もシェルより良いところもあるという話もありますしね。\n結果的にPythonを褒めるようなポストとなりましたが、私としてはプログラミングの初心者もしくはLinuxサーバーからスクリプトを組むことの多い開発者に魅力的な言語と思うので、皆さんにもぜひPythonを使って欲しいです。楽しく、楽に開発しましょう！\nSimple Storage Serviceの略で、その名の通りOSにマウントして普通のディスクのように使えるサービスだそうです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-06-30T00:00:00Z","image":"https://retheviper.github.io/images/python.jpg","permalink":"https://retheviper.github.io/posts/python-blob-to-binary/","title":"PythonでDBの処理がしたい"},{"content":"PuttyやMacのターミナルは使ってみたことがありますが(CLIの範囲まで行くとMS-DOSも)、Tera termを使ったことはなかったです。でも仕事ではAWSでLinuxのサーバーを立てて使っているのでSSH接続が必要となります。ここで私が初めてしたことはそのSSH接続ができるようにTera termのマクロを作ることでした。それまでは主に文書作業をしていたので、やっとコーディングのようなことができて嬉しく思いましたね。\n他にも色々良いツールはあるのではないかと思いますが、マクロで簡単にシェルのコマンドが発行できたり、画面のカスタマイズが簡単だということが便利ですね。最近はWindows 10でもターミナルが使えるようになったりWSLというサービスができましたが、まだ会社で支給されるパソコンがWindows 10ではない場合にはこちらの方を使うのが便利だろうと思います。\nそれでは、設計と実際のコードでどうマクロを作ったかを紹介します。\nマクロの設計 全てのAWS EC2がそうなのかわかりませんが、今回の案件ではいわゆる「踏み台サーバー」を経由してからEC2サーバーに接続することができています。最初はその概念すらなかったのでなぜこんな構造になるのか疑問でしたが、それぞれのEC2サーバーが直接インターネットに繋がっている訳ではないのでこういう手続きが必要となるらしいです。なのでまず踏み台サーバーに接続し、そこからSSHコマンドを発行しそれぞれのEC2サーバーに接続できるようなマクロを作ります。\nそして今回はバッチサーバー、WebAPサーバー、CIサーバーという三つの構成となるので踏み台からそれぞれのサーバーに接続できるようにしたいです。また、同時に複数のサーバーで作業する場合を踏まえてサーバーごとのターミナル画面を少しカスタマイズしたいです。少し調べてみると、マクロには接続の手続きを書き、マクロからあらかじめ画面の設定をしておいたINIファイルを読み込むようにすればいいらしいです。またCIサーバーではJenkinsのようなサービスもポートフォワードで使いたいのですが、その設定もこのINIファイルに記録されるみたいなのでマクロからCIサーバーに接続する場合に限定してポートフォワード設定が入ったINIファイルを呼ぶようにしたいです。\n以上の要件を整理すると、マクロの仕組みは以下となります。\nリストから接続先を選択する(選択肢は3つ) まず踏み台サーバーに接続し、そこからさらに選択したサーバーへSSH接続する 接続が終わると選択したサーバーごとのINIファイルを読み込む INIファイルの設定はUIからターミナルとウィンドウ、フォント、ポートフォワードを予め設定し、それぞれの接続先に合わせ3つを用意しておきました。あとはマクロから読み込むだけです。では肝心のマクロのコードはどうなかったかというと、以下のようになります。\nマクロのコード ;============================================== ;; 踏み台ユーザーID／PW ;============================================== USERNAME = \u0026#39;踏み台ユーサーID\u0026#39; PASSWORD = \u0026#39;踏み台ユーザーPW\u0026#39; ;============================================== ;; 踏み台サーバーIPアドレス ;============================================== HOSTIP = \u0026#39;ここにIP\u0026#39; ;============================================== ;; 接続先別作業用ユーザーID ;============================================== strdim WORKUSERLIST 3 WORKUSERLIST[0] = \u0026#39;バッチサーバーのユーザーID\u0026#39; WORKUSERLIST[1] = \u0026#39;WebAPサーバーのユーザーID\u0026#39; WORKUSERLIST[2] = \u0026#39;CIサーバーのユーザーID\u0026#39; ;============================================== ;; 接続先別作業用ユーザPW ;============================================== strdim WORKPWLIST 3 WORKPWLIST[0] = \u0026#39;バッチサーバーのユーザーPW\u0026#39; WORKPWLIST[1] = \u0026#39;WebAPサーバーのユーザーPW\u0026#39; WORKPWLIST[2] = \u0026#39;CIサーバーのユーザーPW\u0026#39; ;============================================== ;; サーバーIP ;============================================== strdim SERVERipLIST 3 SERVERipLIST[0] = \u0026#39;バッチサーバーのIP\u0026#39; SERVERipLIST[1] = \u0026#39;WebAPサーバーのIP\u0026#39; SERVERipLIST[2] = \u0026#39;CIサーバーのIP\u0026#39; ;============================================== ;; リストに表示されるサーバー名称設定 ;============================================== strdim SERVERnameLIST 3 SERVERnameLIST[0] = \u0026#39;バッチサーバー\u0026#39; SERVERnameLIST[1] = \u0026#39;WebAPサーバー\u0026#39; SERVERnameLIST[2] = \u0026#39;CIサーバー\u0026#39; ;============================================== ;; サーバー別INIファイル ;============================================== strdim INILIST 3 INILIST[0] = \u0026#39;/BatchServer.INI\u0026#39; INILIST[1] = \u0026#39;/WebAPServer.INI\u0026#39; INILIST[2] = \u0026#39;/CIServer.INI\u0026#39; ;============================================== ;; 接続先ホスト選択画面 ;============================================== listbox \u0026#39;サーバーを選択して下さい\u0026#39; \u0026#39;決定\u0026#39; SERVERnameLIST if result \u0026gt;= 0 then SERVERIP = SERVERipLIST[result] WORKUSER = WORKUSERLIST[result] WORKPASSWORD = WORKPWLIST[result] INIFILE = INILIST[result] else end endif ;============================================== ;; INIファイルのパスの読み込み ;============================================== getdir INIPATH strconcat INIPATH INIFILE ;============================================== ;; 踏み台サーバーへの接続用コマンド組立て + 接続コマンド実行 ;============================================== PROXY = \u0026#39;-proxy=http://proxy.server.com:6000\u0026#39; COMMAND = PROXY strconcat COMMAND \u0026#39; \u0026#39; strconcat COMMAND HOSTIP strconcat COMMAND \u0026#39;:22 /ssh /auth=password /user=\u0026#39; strconcat COMMAND USERNAME strconcat COMMAND \u0026#39; /passwd=\u0026#39; strconcat COMMAND PASSWORD connect COMMAND wait \u0026#39;$\u0026#39; ;============================================== ;; 接続先別SSH接続処理 ;============================================== SSHCOMMAND = \u0026#39;ssh \u0026#39; strconcat SSHCOMMAND WORKUSER strconcat SSHCOMMAND \u0026#39;@\u0026#39; strconcat SSHCOMMAND SERVERIP sendln SSHCOMMAND ;============================================== ;; 初SSHログイン処理 ;============================================== wait \u0026#39;Are you sure you want to continue connecting (yes/no)?\u0026#39; \u0026#34;\u0026#39;s password: \u0026#34; if result = 1 then sendln \u0026#39;yes\u0026#39; wait \u0026#34;\u0026#39;s password: \u0026#34; elseif result = 2 then goto INPUTPWD endif :INPUTPWD sendln WORKPASSWORD wait \u0026#39;$\u0026#39; sendln \u0026#39;sudo su -\u0026#39; wait \u0026#39;sudo\u0026#39; sendln WORKPASSWORD wait \u0026#39;#\u0026#39; restoresetup INIPATH ;============================================== ;; マクロ終了 ;============================================== end コードの説明 最初は踏み台サーバーに指定しといたユーザーID、PW、IPからコマンドを組み立ててへSSH接続する構造となっています。具現化したものをみるとstconcatで文字列を繋ぎコマンドを叩いています。また仕事ではプロキシサーバーも使っているので踏み台サーバー接続のコマンドに入れていますが、ない場合はそこだけを取り除くことになります。ただ踏み台サーバー接続の後に違うサーバーに接続するのがメインなので、これの前にマクロを起動した時点ではまずリスト画面から接続先の3つのサーバーを選ぶようになっていますね。\nまず接続先のそれぞれのユーザーID、PW、IP、画面に表示するサーバー名を配列(strdim)として用意します。そして画面にはlistboxで選択できるリストを表示します。listboxで接続先を選択して決定ボタンを押すと、その結果がresultという変数に数字として入力されるのでそれを配列のインデックスとして踏み台サーバー接続のコマンドのように文字列を繋ぎSSH接続を行います。また、同じく配列で接続先ごとのINIファイル名を用意し、getdirからマクロの相対パスを取得、INIファイル名と結合したあとrestoresetupでINIファイルを読み込むようにします。(マクロファイルとINIファイルは同じフォルダーにある前提です)\nコマンドの発行では、waitを使ってターミナルの反応を待ちます。これはシェルでのexpectと同じ機能をします。いきなりコマンドを連続で発行してもサーバー側の反応とずれると想定通りにならないことを防ぐためです。waitでサーバーに次のコマンドを発行できるかどうかを判断してからsendlnでコマンドを発行するようになっています。このwaitとifを組み立てることで初接続時に「本当に接続しますか？」という文が出力されても対応できるようにマクロを作ることができます。他にはそんなに難しい部分はないと思うので説明はここまでにします。\nこのマクロを.ttlという拡張子で保存し、Tera termから読み込むかマクロから直接実行するか(Tera termのインストール時にオプションとしてマクロを直接実行できるように設定できます)で終わりです。思ったより簡単ですね！\n最後に ifとwaitを活用してだいたいなんでもコマンドを発行できるので、ある意味シェルのようなこともできるのがこのTera termのマクロの魅力かと思います。ここのコードではマクロを起動することでrootまで行くようにしていますが、他に作業パスを変えたりとあるシェルスクリプトを実行させたりもできますね。構造的にはターミナルでコマンドを発行するようになっているだけですので。\nそれでは今回のポストはここまで。楽なSSH生活のため、みなさんもぜひTera termを使ってください。\n","date":"2019-06-24T00:00:00Z","image":"https://retheviper.github.io/images/linux_terminal.jpg","permalink":"https://retheviper.github.io/posts/linux-teraterm-macro/","title":"Tera Termを使う"},{"content":"前回のポストで、xmlファイルを操作するスクリプトを紹介しました。しかし、仕事でそのスクリプトをしばらく使わない方針となったため実際使うことがなく放置していましたが、方針が変わり作成しておいたスクリプトを実際適用してみるととある問題が出てきました。最初はスクリプトが問題だろうとは思わず、原因究明にだいぶ時間がかかりましたが、なんとか解決できた今はホッとしています。やはりコーディングという行為は設計通りの実装よりも変な挙動をしていないか確認するのが大事なのではないかと、今回も思いました。\nそれでは具体的にどんな問題があり、どうコードを改善したかを今回のポストで述べていきたいと思います。厳密にいうとバグというよりは、詳細設計の段階でミスを起こしたという表現が正しいのかもしれませんが、理由はどうあれ当初の計画通り動かないコードを書いたことは反省しないといけないなと思うきっかけとなりました。これでまた成長できたと言えたらいいですね。\n旧スクリプトの問題 前回のポストで紹介したスクリプトは、一見思い通りに動いているように見えました。実際テストをしてみたときも、指定したエレメントのテキスト部はちゃんと変えてくれてましたね。しかし問題は、処理すべきxmlファイルのエレメント数が想定よりも多かったというところから発生しました。つまり、旧スクリプトではSELECT文を発行するDBのエレメントが一つ、INSERT文を発行するDBのエレメントが一つというシンプルな構成になっていましたが、今回は複数のエレメントを処理しなければならない状況となりました。\nそれを旧スクリプトで処理しようとすると、一つのエレメントを書き換えたあと、残りのエレメントは無視して次のファイルに処理が移行していったのです。それがわからないまま処理が終わった時点で設計通りに動作してくれていると信じ、処理の終わったファイルを使うとエラーが発生した、というのがこのスクリプトを改修するきっかけとなりました。\nまず原因をわかったので、スクリプトの目標を修正します。今回の目標は「条件と一致する全エレメントの修正」です。\nコードを改修する 目標に合わせてコードを修正すると同時に、些細な問題も改善することにします。前回のスクリプトではフォルダー内のファイルをリストとして取得するためにglobのモジュールを使いました。わずかのコードで再帰的に下位フォルダー内のファイルまで収集してくれるので便利だったのですが、globのrecursiveオプションはPython3.5以上でしか使えないという問題があります。普段からPython3を使っていたなら問題はあまりないはずですが、他に使っているPythonのスクリプトは全てPython2を基準に作成されています。なのでこれをPython2でも使えるオプションに変えることにします。\nそしてメインとなる改善点としては、findをfindallに変えることにします。まずDBコネクションのエレメントを全部取得し、ここでif文を使えばDBのコネクション名によって分岐処理ができるはずです。また、今回変えたいDBコネクション名はFrom_PostgreSQL_01のようにアンダースコアで連番が付いているものをFrom_PostgreSQLのように連番だけ外すということなので、その仕組みも考えておきます。replaceを使う方法もありますが、これなら全てのケースに対して条件を書かなければならないですし、条件指定の例によっては重複の可能性もあります。なのでrsplit1を使い、アンダースコアを基準に元の文字列を分割した後元のテキストを代替することにします。\nこれらの要件定義から変わったことは以下となります。\n# -*- coding: UTF-8 -*- import xml.etree.ElementTree as ET import os # 名前空間（prefix）をマップで宣言 ns = {\u0026#39;fb\u0026#39;: \u0026#39;http://builder\u0026#39;, \u0026#39;fe\u0026#39;: \u0026#39;http://engine\u0026#39;, \u0026#39;mp\u0026#39;: \u0026#39;http://mapper\u0026#39;} # xmlファイル名を再帰的に取得(Python2向け) fileList = [] base_dir = os.path.normpath(\u0026#39;./baseFolder\u0026#39;) # 検索するディレクトリの起点を設定 for (path, dir, files) in os.walk(base_dir): # xml2というファイルは消し、xmlファイルだけ書き換えしたいので分岐をかけ、xmlファイルだけをリスト化する for fname in files: if (\u0026#39;.xml2\u0026#39; in fname): fullfname = path + \u0026#34;/\u0026#34; + fname os.remove(fullfname) elif (\u0026#39;.xml\u0026#39; in fname and not \u0026#39;.xml2\u0026#39; in fname): fullfname = path + \u0026#34;/\u0026#34; + fname fileList.append(fullfname) # 取得したファイルを巡回しながらコネクション名の書き換え処理 for fileName in fileList: # ファイルをパーシング開始 tree = ET.parse(fileName) # INSERTのコンポーネントのコネクション名に\u0026#39;_01\u0026#39;などの文字がついていると取る PutConnections = tree.findall(\u0026#34;fe:Flow/fe:Component[@type=\u0026#39;RDB(Put)\u0026#39;]/fe:Property[@name=\u0026#39;Connection\u0026#39;]\u0026#34;, ns) for PutConnection in PutConnections: if (\u0026#39;_0\u0026#39; in PutConnection.text): PutConnection.text = PutConnection.text.rsplit(\u0026#39;_\u0026#39;, 1)[0] # rsplitで分割し、その結果物を元のテキストに入れる # SELECTのコンポーネントのコネクション名に\u0026#39;_01\u0026#39;などの文字がついていると取る GetConnections = tree.findall(\u0026#34;fe:Flow/fe:Component[@type=\u0026#39;RDB(Get)\u0026#39;]/fe:Property[@name=\u0026#39;Connection\u0026#39;]\u0026#34;, ns) for GetConnection in GetConnections: if (\u0026#39;_0\u0026#39; in GetConnection.text): GetConnection.text = GetConnection.text.rsplit(\u0026#39;_\u0026#39;, 1)[0] # rsplitで分割し、その結果物を元のテキストに入れる # prefixが変わることを防止 ET.register_namespace(\u0026#39;fb\u0026#39;, \u0026#39;http://builder\u0026#39;) ET.register_namespace(\u0026#39;fe\u0026#39;, \u0026#39;http://engine\u0026#39;) ET.register_namespace(\u0026#39;mp\u0026#39;, \u0026#39;http://mapper\u0026#39;) # 書き換え処理 tree.write(fileName, \u0026#39;UTF-8\u0026#39;, True) 最後に ファイル取得部はglobオプションで簡単にできたことに比べ少し複雑になっています。os.walk()で起点のディレクトリを指定してパスとファイル名を取得します。ただos.walk()だとファイル名とパスは分離されるのでそれをつなぐ作業が必要ですね。そこで処理するファイルのリストに入れたり消したりする処理を加えます。これで以前のglobと似たような挙動ができます。\nそしてrsplit('_', 1)で、まずFrom_PostgreSQL_01という文字列はFrom_PostgreSQLと01に1回だけ分割されます。そして分割された文字列は配列になるので[0]を指定すると意図通りFrom_PostgreSQL_01がFrom_PostgreSQLに代替されます。またfindをfindallに変えただけで条件に一致する全エレメントをファイル内で探してリストにしてくれます。その中でループ処理するだけですね。これがわからなかった時は以前のコードにさらにループをかけたりして失敗していましたが、意外と簡単な解決策があったものです。\nこれで完成されたコードは意図通りに動いてくれました。あとで変動があっても少しだけ変えればいいので個人的には満足しています。より綺麗な書き方はあるかもしれませんがね。そして教訓として、いつもテストは大事だなということを改めて覚えられました。常に確認と確認です。\nrsplitとsplitの違いは、方向です。前者が文字列の右側を基準に分割するなら、後者は左側からです。今回は文字列の末尾の連番を取りたいので、rsplitを選びました。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-06-20T00:00:00Z","image":"https://retheviper.github.io/images/python.jpg","permalink":"https://retheviper.github.io/posts/python-xml-modifier-2/","title":"Pythonでxmlファイルを操作する(2)"},{"content":"コーディングにおいて、ロジックを組むこと異常に重要なことがあるとしたらそれは自分の書いているコードが正しいかどうかを確認することだと思います。まず仕様書などを書き、要件を満たしているかどうかをチェックしながら、コードを書いていきます。しかし、実行してみない限り思い通りにコードが動いてくれるかどうかは確認できませんね。そしてその動作が正しいかどうかを判別するためにはコードの中で扱っている変数やオブジェクトに正しいデータが入っているかどうかをみます。EclipseなどのIDEにはデバッグ機能があるので、チェックポイントを設定してコードの流れをモニタリングしながらそれを追うことができますが、それを利用できない場合もありますね。では標準出力という方法もありますが、実行後に記録を追うことができないという面で不安です。\nそういう時にカスタムログを利用するのも一つの方法ですね。ちゃんとしたログを吐くように最初からロガーを作っておくと、自分の作ったプログラムが設計通りに動いているかどうかを判断できます。そしてプログラムが完成した後もネットワークの障害やデータの問題を確認するために有効活用できるという面もありますね。といっても、カスタムログを吐くロガーを書くということは難しいことのように思われます。最初ログを吐くようにしろという指示を受けて、私はどうしたらいいか？標準と言えるひな形はないのか？と悩みましたね。でもそんな難しく思うようなことはなくて、状況による分岐(メッセージやコードなど)を決め、それをテキストファイルとして出力するようにすれば良いだけの話でした。\nそれでは、実際のロガーをどう作ったかをこれからコードを通じて見せましょう。\nロガーを作る 今回私た作りたいロガーの構造は簡単です。メインプログラムで呼び出された時に、引数としてログコード(分類番号的な)やジョブコード(作業の処理番号など)を受け、ログコードから出力するメッセージを決めます。ジョブコードは必須ではないが、ログコードは必須卯にしたいです。また、ログの書かれた時点を知りたいため日時もメッセージに入れます。この要件から出力されるログの形は以下となります。\n2019/06/15 12:00:00 LOG0001: Job code 1234は正常終了しました。\nまた、ログファイルの最上段には---- Job Log ----を入れ、これがログファイルであることを示したいです。このように基本要件を決め、実装していきます。私が実装したコードの簡略化したバージョンは以下となります。\n# ログファイルの存在有無を確認するためのosと日付を残すためのdatetimeをインポート import os import datetime # ログをかく関数(job_codeは与えられない場合があるのでデフォルト値を設定) def writeLog(log_code, job_code=\u0026#39;\u0026#39;): # ログファイルのパスを指定(絶対経路) log_path = \u0026#39;/log/program_log.log\u0026#39; # ログコード判定によりメッセージを決める if (log_code == \u0026#39;LOGI001\u0026#39;): log_message = \u0026#39;Job code \u0026#39; + job_code + \u0026#39;を実行します。\u0026#39; elif (log_code == \u0026#39;LOGI002\u0026#39;): log_message = \u0026#39;Job code \u0026#39; + job_code + \u0026#39;は正常終了しました。\u0026#39; elif (log_code == \u0026#39;LOGW001\u0026#39;): log_message = \u0026#39;Job codeがありません。プログラムの起動に失敗しました。\u0026#39; elif (log_code == \u0026#39;LOGE001\u0026#39;): log_message = \u0026#39;Job code \u0026#39; + job_code + \u0026#39;は異常終了しました。\u0026#39; else: log_message = \u0026#39;原因を特定できないエラーが発生しました。\u0026#39; # ログファイルが存在しない場合は先端にログであることを示すコードを含むファイルを生成 if (not os.path.exists(log_path)): with open(log_path, mode=\u0026#39;a\u0026#39;) as log: log.write(\u0026#39;----- Job Log -----\\n\u0026#39;) else # ログファイルがすでに存在する場合は追記する with open(log_path, mode=\u0026#39;a\u0026#39;) as log: log.write(datetime.datetime.today().strftime(\u0026#39;%Y/%m/%d %H:%M:%S\u0026#39;) + \u0026#39; \u0026#39; + log_code + \u0026#39;: \u0026#39; + log_message + \u0026#39;\\n\u0026#39;) これでメインプログラムでロガーを呼び、そのタイミングでログコードやジョブコードを渡すだけでログは残されます。ログの種類を減らしたい、もしくは増やしたい場合はif文を修正するだけで対応できます。\nメインプログラムでロガーを呼び出す メインプログラムでまずログを書く(ロガーを呼ぶ)タイミングを決めます。例えば今回はDBに接続するタスクがあったので、DBの接続に失敗した場合にロガーを書くようにしました。そのほかにも処理の開始や終了時のステータスなどに関してもログを残したいですね。\n同じく簡略化したメインプログラムのコードは以下のような形です。先に作成して置いたロガーをインポートし、ログを出したい場合に呼び出すだけでいいです。ただロガーファイルの位置が実行するメインプログラムと違うパスにある場合はインポートの形が変わるので注意が必要です。ここではメインプログラムとロガーは同じパスから実行される場合を想定して実装しています。それでは下のコードをどうぞ。\n# 簡略化したメインプログラム # 終了時のリターンコードと起動時の引数のためsysを、ログを書くためにロガーのwriteLog関数をインポート import sys from logger import writeLog # 起動時にコマンドラインから引数を受けるための変数宣言 args = sys.argv # 関数の例 def program(job_code): # 処理開始の時点でログを書く writeLog(\u0026#39;LOGI001\u0026#39;, job_code) # 正常処理時の挙動を書く try: print (\u0026#39;何かの処理を行う\u0026#39;) # 正常終了のコードでログを書く writeLog(\u0026#39;LOGI002\u0026#39;) # 正常終了の場合は終了時のリターンコードが0 sys.exit(0) # 処理に問題が起きた場合の処理を書く except: # 異常終了のコードでログを書く writeLog(\u0026#39;LOGE001\u0026#39;) # 異常終了の場合は終了時のリターンコードが9 sys.exit(9) # プログラムの起動部 if __name__ == \u0026#39;__main__\u0026#39;: # 引数(ジョブコード)が入力されなかった場合のチェック if (args[1] in None): # 警告終了のコードでログを書く writeLog(\u0026#39;LOGW001\u0026#39;) # 警告終了の場合は終了時のリターンコードが1 sys.exit(1) else: program(args[1]) 最後に どんなログを吐き出して欲しいかによってロガーを呼び出す位置を決めます。そして引数を渡すことでどんな場合にどんなログを出力するかを決められます。実装が難しい作業ではないですが、こうやってロガーを分離して必要な時だけ呼び出すことでメインプログラムのコードがいくら長くなっても関数を呼び出すだけで済むという効率的な構造となっています。そもそも関数というものは、反復を減らすために書くものでもありますしね。\nあと少し調べたところ、Pythonではモジュールとしてloggerをすでに提供しているとのことです。標準出力としてログのレベルを指定して文字列を出力できるらしいですね。これを応用してファイルに描くようにコードを書くこともまた良い方法になるのではないかと思います。\n仕事でコードを書きながら、大事なのはどんなコマンドと関数名を覚えているかではなく、どんな構造で実装していくかなのではないかと思うようになりました。最近は本でもインターネットでも基礎文法に関する講座やTipは簡単に得られますが、こういう設計の仕方に関しては、やはり実戦ではないとなかなか得られない分野の知識ではないかとも思います。またこれからどんなコードを書くようになるだろうか、色々楽しみです。\n","date":"2019-06-15T00:00:00Z","image":"https://retheviper.github.io/images/python.jpg","permalink":"https://retheviper.github.io/posts/python-logger/","title":"Pythonでログを出したい"},{"content":"お知らせ macOSがCatalina(10.15)になってから、ターミナルの基本シェルがbashからzshに変わりました。なのでこのポストをそのまま適用すると、追加したコマンドを使えない場合があります。\nすでにBashにコマンドを追加した場合は以下の手順で移行ができます。\ncat ~/.bash_profile \u0026gt;\u0026gt; ~/.zprofile 新規で追加する場合は、下の「プロファイルを作る」節のコマンドの代わりに以下を入力します。そのあとは同じ手順で進めてください。\nvi ~/.zprofile 以下、前のポストです。\nパソコンではMS-DOSとWindowsしか使ってみたことのない私がmacを使ってかれこれ一年となります。macといえばやはり高く、Windowsとは全く違う環境なので使いづらいのではないかなと思っていました。動画や音楽の編集には最適だという話もありましたが、自分とは関係のない世界の話でした。\nそんな私がmacを使うきっかけとなったのは、様々な理由がありますが、開発の仕事をしている今は何よりもUnix系のOSということが最大のメリットとなっているのではないかと思います。仕事ではLinuxを触ることが多いのですが、macのターミナルでのほぼ同じことができますので。\nただ、同じくBashを使っているといっても、macとLinuxは完全に同一ではありません。例えばショートカットがそうです。Linuxの便利なショートカットがmacにはないですね。でも調べてみると、やはり方法は存在していました。\n今回のポストはそのLinuxのショートカットをmacで使うための設定の話です。といっても、自分が研究した訳ではなく、インターネットで拾ってきた情報にすぎませんがね。でもこの設定をみるだけでも様々なコマンドやAliasを勉強できて、かなり貴重な資料ではないかと思います。誰か知りませんが、作った方には敬意を。\nプロファイルを作る まずはBashのプロファイルを作ります。ターミナルで以下のように入力します。\nvi ~/.bash_profile bash_profileとbashrcの違いは、前者がターミナルを開くときすぐ適用される設定なら、後者はターミナルで改めてBashを実行した時に適用される設定という違いがあるらしいです。ただコマンドでわかりますが、現在ユーザーのホームにプロファイルを作るのでユーザーが変わる場合はまた同じことをする必要があります。\nプロファイルの内容 以下のコードをプロファイルに入れます。\n#.bash_profile if [ -f ~/.bashrc ]; then . ~/.bashrc fi #============================================================ # # ALIASES AND FUNCTIONS # Arguably, some functions defined here are quite big. # If you want to make this file smaller, these functions can #+ be converted into scripts and removed from here. # #============================================================ #------------------- # Personnal Aliases #------------------- alias rm=\u0026#39;rm -i\u0026#39; alias cp=\u0026#39;cp -i\u0026#39; alias mv=\u0026#39;mv -i\u0026#39; alias cl=\u0026#39;clear\u0026#39; # -\u0026gt; Prevents accidentally clobbering files. alias mkdir=\u0026#39;mkdir -p\u0026#39; alias h=\u0026#39;history\u0026#39; alias j=\u0026#39;jobs -l\u0026#39; alias which=\u0026#39;type -a\u0026#39; alias ..=\u0026#39;cd ..\u0026#39; # Pretty-print of some PATH variables: alias path=\u0026#39;echo -e ${PATH//:/\\\\n}\u0026#39; alias libpath=\u0026#39;echo -e ${LD_LIBRARY_PATH//:/\\\\n}\u0026#39; alias du=\u0026#39;du -kh\u0026#39; # Makes a more readable output. alias df=\u0026#39;df -kTh\u0026#39; #------------------------------------------------------------- # The \u0026#39;ls\u0026#39; family (this assumes you use a recent GNU ls). #------------------------------------------------------------- # Add colors for filetype and human-readable sizes by default on \u0026#39;ls\u0026#39;: alias ls=\u0026#39;ls -h\u0026#39; alias lx=\u0026#39;ls -lXB\u0026#39; # Sort by extension. alias lk=\u0026#39;ls -lSr\u0026#39; # Sort by size, biggest last. alias lt=\u0026#39;ls -ltr\u0026#39; # Sort by date, most recent last. alias lc=\u0026#39;ls -ltcr\u0026#39; # Sort by/show change time,most recent last. alias lu=\u0026#39;ls -ltur\u0026#39; # Sort by/show access time,most recent last. # The ubiquitous \u0026#39;ll\u0026#39;: directories first, with alphanumeric sorting: alias ll=\u0026#34;ls -alv\u0026#34; alias lm=\u0026#39;ll |more\u0026#39; # Pipe through \u0026#39;more\u0026#39; alias lr=\u0026#39;ll -R\u0026#39; # Recursive ls. alias la=\u0026#39;ll -A\u0026#39; # Show hidden files. alias tree=\u0026#39;tree -Csuh\u0026#39; # Nice alternative to \u0026#39;recursive ls\u0026#39; ... #------------------------------------------------------------- # Tailoring \u0026#39;less\u0026#39; #------------------------------------------------------------- alias more=\u0026#39;less\u0026#39; export PAGER=less export LESSCHARSET=\u0026#39;latin1\u0026#39; export LESSOPEN=\u0026#39;|/usr/bin/lesspipe.sh %s 2\u0026gt;\u0026amp;-\u0026#39; # Use this if lesspipe.sh exists. export LESS=\u0026#39;-i -N -w -z-4 -g -e -M -X -F -R -P%t?f%f \\ :stdin .?pb%pb\\%:?lbLine %lb:?bbByte %bb:-...\u0026#39; # LESS man page colors (makes Man pages more readable). export LESS_TERMCAP_mb=$\u0026#39;\\E[01;31m\u0026#39; export LESS_TERMCAP_md=$\u0026#39;\\E[01;31m\u0026#39; export LESS_TERMCAP_me=$\u0026#39;\\E[0m\u0026#39; export LESS_TERMCAP_se=$\u0026#39;\\E[0m\u0026#39; export LESS_TERMCAP_so=$\u0026#39;\\E[01;44;33m\u0026#39; export LESS_TERMCAP_ue=$\u0026#39;\\E[0m\u0026#39; export LESS_TERMCAP_us=$\u0026#39;\\E[01;32m\u0026#39; #------------------------------------------------------------- # Spelling typos - highly personnal and keyboard-dependent :-) #------------------------------------------------------------- alias xs=\u0026#39;cd\u0026#39; alias vf=\u0026#39;cd\u0026#39; alias moer=\u0026#39;more\u0026#39; alias moew=\u0026#39;more\u0026#39; alias kk=\u0026#39;ll\u0026#39; #------------------------------------------------------------- # Using in MAC #------------------------------------------------------------- alias desk=\u0026#39;cd ~/Desktop\u0026#39; alias cl=\u0026#39;clear\u0026#39; そしてsource ~/.bash_profileを叩き、現在のユーザーに適用すれば終わりです。ユーザーのプロファイルに保存されるので、複数のユーザーを使う場合(rootを含め)はまた同じやり方で適用することを忘れずにしましょう。\n最後に llをmacでも使えないかなーという軽い気持ちで調べて得られた情報ですが、まさかll | moreみたいなものまでAliasの指定ができるとは知らなかったです。またよくcdをxsに間違えたりしていますが、それもあえでコマンドとして登録するという発想も斬新ですね。まだまだ知らないコマンドもたくさんありますし、軽い気持ちから始まった作業にしては貴重な情報が得られ他ので嬉しい限りです。これがインターネット時代の恩義というものではないでしょうか。\n一部はLinuxでも登録すると便利そうなショートカットが多いように見えるので、Linuxを使われる肩がいらっしゃるのならぜひ一度は試してみてくださいとオススメしたいです。では、楽なBashライフをお楽しみください。\n","date":"2019-06-11T00:00:00Z","image":"https://retheviper.github.io/images/linux_terminal.jpg","permalink":"https://retheviper.github.io/posts/linux-mac-shortcut/","title":"macOSでもショートカットが使いたい"},{"content":"IT関係の仕事を選び、最も良いと思うところはやはり色々な状況に置かれることによって得られる経験が多いということではないかと思います。なぜかというと、独学だけでは言語の基礎文法はわかっても実際のコーディングではどう設計したらいいか、どんなモジュール(ライブラリーを含め)を使ったらいいかわからない場合があるからです。そもそも何を作ればいいかわからない場合が多いですね。大分類としてはウェブアプリケーションか、バッチで動くコードかということなどがあり、細かくはどんなDBを使って、どんな作業をしたいか(結合する対象など)の詳細を一人で全部想定することはかなり難しい作業ではないかと思います。しかし仕事ではある程度要件が決まっているので、その結合の方法と必要な作業がわかればあとは頑張るだけですから。目標設定が何よりも重要だということはまさにこれのことかもしれません。\nなので今回も仕事で任されたことです。要件はこうです。とあるツールを使って開発をしています。このツールではGUIでマップ上にアイコンを配置し、そのアイコンが一つの作業単位となっています。そしてアイコンとアイコンを結び、各アイコンにとある行動を設定することで全体なワークフローが出来上がる構造となっています。例えばDBのアイコンに接続先のDBの情報と発行するSQL文を入力し、ファイル出力のアイコンに繋ぐとそれを実行した時にDBからSQL文を実行した結果がファイルとして保存されます。このツールで作られたワークフローはxmlファイルとして保存されます。\n問題は、開発環境と本場環境での設定が違うところがあるということです。主にこのツールを通してやっている作業はDB関係のものですが、開発環境と本場環境でそれぞれDBの接続先情報が違います。そしてそのDBの接続先の設定はxmlファイルに保存されているので、開発の終わったxmlファイルを本場環境にデプロイするときはただのコピーでは不完全なので「DBの接続先情報を書き換える」作業が必要です。この作業をどう実装したから今回のポストのテーマとなります。\n事前準備 デプロイ先と、デプロイ元のサーバーは両方Linuxを使っています。そしてxmlファイルのソースはGitで管理することになっています。なのでまずデプロイ元からはGit pullし、DB情報を書き換えたあとはrsyncなどのコマンドでコピーすれば簡単に終わります。こちらの作業はJenkinsで自動化することにしました。ならば、残る問題はDB情報を書き換える作業をどう実装するかですね。\nxmlを分析してみたら、各アイコンのタグの下にはそのアイコンの詳細設定情報がありました。DBの処理を行うアイコンは二つあって、一つ目はSELECTを発行する(以下、From)もので、二つ目はINSERTを発行する(以下、To)ものでした。FromとToで接続先のDBは種類も違って(片方はPostgreSQLで、片方はSQLServer)スキーマやテーブル名も違うので分けて処理する必要があります。\nそして環境から考えると、Linuxで使える言語を選んだほうがいいでしょう。まずはシェルスクリプトを使ってコードを書いてみようと思いました。これが私の初のシェルスクリプトとなります。Javaよりは簡単ではないだろうか、という根拠のない自信からシェルを選びました。LinuxにはPythonも入っていましたが、そちらも触ったことがなかったのでまずシェルでやってみて、ダメだったらPythonに挑戦してみようかなと軽く思っていました。それが結果的には最初からPythonで書けばよかった…ってことになりましたが。\nともかくやりたいこと、環境、道具が揃ったので早速実装に入りました。\nシェルスクリプトでコーディング シェルは初めてだったので、試行錯誤が多かったです。最初に学んだのがJavaだったので、同じ感覚で書こうとしたら全然動きません。何回か失敗を重ねながら得られた結論は、関数を使うという考え方を捨てて、どうコマンドを組み合わせるかが重要だということでした。それに気づくにはだいぶ時間がかかりましたが、まず大事なことはわかったのであとはどんなことをするかですね。\nまずはファイルを読み込むことからです。xmlファイルも結局はテキスト基盤なので、シェルでも読み込みはできます。For文一つで特定の拡張子をもつファイルを巡回しながら一行づつ読むことができるらしいです。そして既存のアイコン(このファイルを使うツールの表現を借りると、コンポーネント)のDB接続先の情報の行を把握し、書き換えれば完了。\nただ、前述したようにFromとToのコンポーネントを区別する必要があります。xmlファイルを覗くとどうやらコンポーネントの構造(タグの種類)はほぼ同じみたいなので、どう判定するかが問題でした。シェルではxmlをパーシングできるモジュールなどはないみたいですからね。それでまずは行数を比較して、Fromのコンポーネントがより上にあったら１番目に引っかかったDB設定がFromのやつだ、という風に判定することにしました。以下はその実装のコードです。\nコードの例(シェルスクリプト) #!/bin/bash # 下のフォルダを巡回しながらxmlの拡張子を持つファイルを変数のfileNameに入れる for fileName in `\\find . -name \u0026#39;*.xml\u0026#39;`; do # コンポーネントの行数をつかめる(grepでコンポーネントのタグかを確認し、sedで行数を確保) getComponentLine=$(grep -n RDBGet ${fileName} | grep Component | sed -e \u0026#39;s/:.*//g\u0026#39;) putComponentLine=$(grep -n RDBPut ${fileName} | grep Component | sed -e \u0026#39;s/:.*//g\u0026#39;) # コネクションの行数をつかめる ConnectionOneLine=$(grep -n Connection ${fileName} | sed -e \u0026#39;s/:.*//g\u0026#39; | awk \u0026#39;NR==1\u0026#39;) ConnectionTwoLine=$(grep -n Connection ${fileName} | sed -e \u0026#39;s/:.*//g\u0026#39; | awk \u0026#39;NR==2\u0026#39;) # コネクション名をつかめる(cutでDB設定名だけを取り、awkで2種類以上の結果からどちらかを取る) ConnectionOneName=$(grep Connection ${fileName} | cut -d \u0026#34;\u0026gt;\u0026#34; -f 2 | cut -d \u0026#34;\u0026lt;\u0026#34; -f 1 | awk \u0026#39;NR==1\u0026#39;) ConnectionTwoName=$(grep Connection ${fileName} | cut -d \u0026#34;\u0026gt;\u0026#34; -f 2 | cut -d \u0026#34;\u0026lt;\u0026#34; -f 1 | awk \u0026#39;NR==2\u0026#39;) if [ \u0026#34;${getComponentLine}\u0026#34; -lt \u0026#34;${putComponentLine}\u0026#34; ]; then # get \u0026lt; putの場合ConnectionOneLineはgetのコネクション sed -i -e \u0026#34;${ConnectionOneLine} s/${ConnectionOneName}/HONBADBFROM/g\u0026#34; ${fileName} sed -i -e \u0026#34;${ConnectionTwoLine} s/${ConnectionTwoName}/HONBADBTO/g\u0026#34; ${fileName} else # get \u0026gt; putの場合ConnectionOneLineはputのコネクション sed -i -e \u0026#34;${ConnectionTwoLine} s/${ConnectionTwoName}/HONBADBFROM/g\u0026#34; ${fileName} sed -i -e \u0026#34;${ConnectionOneLine} s/${ConnectionOneName}/HONBADBTO/g\u0026#34; ${fileName} fi done 問題点 ファイルの形式がxmlであり、タグのパーシングで確実にコンポーネントを分けて処理していない現行の方式ではあまり安全だとは言えない処理です。そしてこの方式だとFromとToのコンポーネントがそれぞれ一つづつある場合は大丈夫かもしれませんが、どちらかのコンポーネントが一つでも増えたら処理の方法を変えるしかないです。もしかしたらそんなケースが増えると、そのケースに合わせてそれぞれ違うコードを書く必要があるかもしれません。そしてそれはいちいちファイルをチェックしてそれにあうコードとマッチさせる必要がありますね。これなら手書きで変えるのとあまり変わらないのでは…と思いますね。\n結果的に汎用性もなく、安全でもないコードとなってしまいました。こんなコードは本場では使えません。なので方法を変えることにしました。\nPythonで書き直す 次の方法として、Pythonを使ってちゃんとパーシングを行うことにしました。挑戦してみてからわかったのですが、こんな簡単な作業をするときはPythonが正解なのではないかと思うくらい簡単でした。それにLinuxの環境では基本的にPythonが入っている場合も多いようなので(yumがPythonを使う代表的な例です)、インストールしなくてもいいというのがメリットでもあります。それにBashがLinuxの基本機能であるのでPythonよりは速度が早いのではないかと思っていましたが、必ずしもそうでもないらしいですね。ならばますますシェルスクリプトにこだわる理由は無くなります。\nただLinuxに内蔵されているPythonは2が多いらしく(確認してみると、私の使うMacでもPython2が入っていました)、Pythonは2と3で文法が違うところも多くて特定の機能を使うには注意がいるらしいです。実際私の書いたコードでは、Python3でしか使えない部分があります。alternativeのようなコマンドでPythonのリンクを3に指定するという方法もありますが、それならPython2を使うプログラムで問題が起こる可能性があります。1なので最初からPython2のコードに書くか、実行するコードをPython3として実行するようにするか、Python2を使うプログラムの実行環境を変えるかの方法を工夫する必要がありました。\nここで私は、自分が書いたコードをPython3で実行するよう(Jenkinsに埋め込むので、そちらの設定を通しています)にしました。コードは以下となります。\nコードの例(Python) # -*- coding: UTF-8 -*- # 日本語のコメントのために最初にエンコードを指定する # xmlパーサーとフォルダからファイルを取得するモジュールをインポート import xml.etree.ElementTree as ET import glob # 名前空間（prefix）をマップで宣言 ns = {\u0026#39;fb\u0026#39;:\u0026#39;http://foo.com/builder\u0026#39;, \u0026#39;fe\u0026#39;:\u0026#39;http://foo.com/engine\u0026#39;, \u0026#39;mp\u0026#39;:\u0026#39;http://foo.com/mapper\u0026#39;} # ファイル名を再帰的に取得(recursiveオプションはPython3専用) fileList = glob.glob(\u0026#34;**/HOGE*.xml\u0026#34;, recursive=True) # 取得したファイルを巡回しながらコネクション名の書き換え処理 for fileName in fileList: # ファイルをパーシング開始 tree = ET.parse(fileName) # Toのコンポーネントで子要素であるコネクション名を取得(prefix内) putCon = tree.find(\u0026#34;fe:WorkFlow/fe:Component[@type=\u0026#39;RDB(Put)\u0026#39;]/fe:Property[@name=\u0026#39;Connection\u0026#39;]\u0026#34;, ns) putCon.text = putCon.text + \u0026#39;x\u0026#39; # Fromのコンポーネントで子要素であるコネクション名を取得(prefix内) getCon = tree.find(\u0026#34;fe:WorkFlow/fe:Component[@type=\u0026#39;RDB(Get)\u0026#39;]/fe:Property[@name=\u0026#39;Connection\u0026#39;]\u0026#34;, ns) getCon.text = getCon.text + \u0026#39;y\u0026#39; # 書き込みの時prefixが変わることを防止 ET.register_namespace(\u0026#39;fb\u0026#39;, \u0026#39;http://foo.com/builder\u0026#39;) ET.register_namespace(\u0026#39;fe\u0026#39;, \u0026#39;http://foo.com/engine\u0026#39;) ET.register_namespace(\u0026#39;mp\u0026#39;, \u0026#39;http://foo.com/mapper\u0026#39;) # 書き換え処理 tree.write(fileName, \u0026#39;UTF-8\u0026#39;, True) 最後に コードの量もそんなに長くないし、ちゃんとパーシングで要素を捉えているのでシェルスクリプトに比べ安全な書き方になっています。それにタグによってコンポーネントを区別しているので、コンポーネントを数に変動があってもそのまま使えるという長所がありますね。タグに名前空間があると初期設定と書き込み直前にその処理が必要となるので少し面倒な部分はありますが2、確かにシェルスクリプトに比べ維持補修の面で手間がかからなくなったので満足できるコードを書けたと思います。速度も直接測定してみた訳ではないですが、相当早かったです(ただ単にPythonは遅いだろうという自分の偏見が問題だったかもしれません)。いやーPythonいいですね。\n何よりもメインの関数やクラスを省略しても、本当にスクリプトぽい書き方でもちゃんと意図通りに動くということが素晴らしいですね。これからもLinux環境で簡単な反復作業を自動化したいという場合には、皆さんもぜひPythonを使ってみてくださいとオススメしたいくらいです。とても簡単な言語なので、これからもどんどん使って色々やってみたいなと思います。\n例えば、yumで問題が発生しています。これはyumの実行環境を変える方法(/usr/bin/yumの設定を参照してください)もありますが、どれがPython2を使うかいちいち確認はできないのであまりおすすめしたくはない方法です。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n特に最後の方のET.register_namespace()がないと、名前空間が勝手に変わってしまいます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-06-09T00:00:00Z","image":"https://retheviper.github.io/images/python.jpg","permalink":"https://retheviper.github.io/posts/python-xml-modifier-1/","title":"Pythonでxmlファイルを操作する(1)"},{"content":"今回のポストではビルドしたJarファイルをデプロイするJobを作りたいと思います。デプロイといっても、実際このポストを書くきっかけとなったタスクは単純です。前回のポストのようなJobで生成したJarファイルを違うサーバに転送して実行するだけです。これを繋いで一連の作業にした方が良いのではと思ったのですが、どうやらプロジェクトの設計はそうではなかったみたいです。理由は各サーバの利用目的が違うから、らしいです。\nともかく、JarファイルをSSHで転送するJobを作ります。もうすでにJarファイルをビルドするJobを作りましたので、そちらのソースを利用します。\n成果物の確保 まずJarファイルをどうやって新規Jobのワークスペースに持ってくるかを考えたいですね。単純には、前回生成したJobにビルドの後、シェルコマンドでファイルをコピーするようなタスクを追加することを考えられます。実際仕事でも一番最初に試した方法でもあります。\nでもファイルのパスやJarファイルの名称などが変わったりするとコマンドの修正が必要となりますし、あまりスマートな方法だとは思えません。なので今回はプラグインによる方法を使いたいと思います。Jenkinsのメイン画面から順番にManage Jenkins、Manage Pluginsをクリックしてプラグインのインストール画面に移動します。そしてAvailaleをクリックした後、右上のFilterにCopyを入力します。\nCopy Artifactというプラグインがリストに出ることを確認できます。これをチェックし、インストールします。なるべくプラグインのインストールやアップデートをした後にはJenkinsを再起動するようにしましょう。再起動中には以下のような画面が現れます。\nターミナルから直接service jenkins restartというコマンドを入力することでも再起動はできますが、プラグインのインストールとアップデートの後のオプションでもできます。再起動の後には自動的に元の画面に戻ります。\n再起動が終わり、無事プラグインがインストールされたらちゃんとJobからプラグインを使えるようになったか確認してみます。まずこのプラグインはJobが終わったあと成果物を指定して保存するように設定できます。またそうやって保存された成果物は他のJobから利用できるようにワークスペースにコピーすることができます。なのでまずはコピー元となるJobに成果物を保存するための工程を追加しましょう。\n前回作成したJavaBuildのJobの設定に入り、Post-build actionsタブからArchive the artifactsを選択します。そして保存したい成果物の経路を入力します。\nJobに変更が発生すると保存してチェックです。ビルドしてみないと思い通りに動くかどうかわからないのがJenkinsの数少ない短所の一つではないかと思いますが、それでもチェックは大事ですので。\nちゃんとビルドできました。保存された成果物に関してはJobのメイン画面から確認できます。どんなファイルが保存できたか確認しましょう。\n狙い通り、ビルドしたファイルだけ保存できました。1*.jarと指定していますが、それでもビルドされるファイルは一つしかなったので当たり前な結果ですね。ともかくこれでこちらのJobでの設定は終了です。次の作業に移行しましょう。\n成果物を引き継ぐ 今回はデプロイ専用としてJavaDeployというJobを生成してみました。こちらではまず保存した成果物をワークスペースに持ってくる設定が必要ですね。\nJobの設定画面からBuildのタブに移動し、Add Build StepをクリックしてみるとCopy artifacts from another projectという項目がドロップダウンメニューに現れたことを確認できます。\nProject nameで、他のJob名を選びます。私は前回生成したJob名にしました。Which buildでは、指定したJobのどんなビルドから成果物を持ってくるかを指定します。様々なオプションがありますが、Lastest successful buildが良さげではないかと思います。Stable build onlyオプションは念のためチェックします。あとはコピー元のファイルパスと、コピー先のパスを指定すればオッケーです。\nコピーしたくないファイルがあればArtifacts not to copyに書くといいです。私はビルドしたJarファイルだけをこのJobのワークスペースにコピーするので、以下のように設定しました。\nではまたJobをビルドして思い通りになるか試してみましう。\n無事ビルドが終わり、成果物をコピーされました。\n次にはこの成果物を他の環境に転送することですね。\n成果物を転送する(1) sshによるファイル転送をするには、Publish over SSHというプラグインが必要です。このプラグインを通じでSSH接続を行い、ファイル転送やリモートでのシェルコマンドが実行できます。プラグインのインストールメニューに移動し、sshでフィルターを指定すると目録からこのプラグインを見ることができます。\nプラグインをインストールして、Jenkinsの再起動まで終わったあとは接続先の設定が必要です。Jenkinsの設定からConfigure Systemに入ると、Publish over SSHの設定項目ができたことを確認できます。\nKeyに公開鍵を入力することでも接続できますが、まだその設定はしてないため普通にIDとパスワードで接続設定を進めます。SSH ServersのAddボタンを押すと、接続先の情報を入力できるフィールドができます。Nameには接続先の自由な名称を書き、Hostnameには実際のIPアドレスやホスト名を書きます。今回、私は自分のmacに接続してみるので(SSH接続できるようなサーバーを持ってないからですが)ルーターでの内部IPとmacのアカウントをそのまま使います。2\nUsernameにはIDを入力します。また、パスワード入力で接続するためパスワードを書くフィールドも必要ですね。Advancedボタンをクリックし、Use password authentication, or use a different keyをチェックしたあとPassphrase / Passwordにパスワードを入力します。またSSH用の基本設定のポートは22となっていますが、こちらもちゃんとポートが開放されているか確認しましょう。必要な情報を全部入力したらTest configurationボタンを押すことで接続できるかかチェックできます。\n入力した情報に間違いがないと、Test configurationを押した後にSuccessが出力されます。設定を保存してJobに戻ります。\n成果物を転送する(2) Jobdの設定に入ってBuild Environmentのタブにいくと、Send files or execute commands over SSH before the build startsとSend files or execute commands over SSH after the build runsの項目ができています。今は成果物を格納した後からSSHを開始したいので後者を選択します。もちろんBuildのタブにもSend files or execute commands over SSHというメニューができるので、こちらで設定しても良いです。\nNameではJenkinsの設定から入力したSSH接続先のサーバーを選択します。そしてSource filesでは転送したいファイルのパスを入力します。あとはオプションですが、Remove prefixでファイルパスを入力すると入力したところまでのパスが消えます。また、Remote directoryではどのフォルダにファイルを転送するかを指定できます。3\n私の設定はこうです。フォルダを同じく作成したくはないのでファイルのみの設定としました。思い通りになったら、ユーザーのホームフォルダ配下のfromJenkinsというフォルダに転送されるはずです。念の為、転送先のフォルダの権限や所有者もチェックしておきましょう。そしてJobのビルドです。\nビルドは無事成功しました。コンソールを見ると転送に成功したファイルの個数が表示されます。では本当に転送に成功したか、macの方から確認してみます。\nこちらでも確認できました。これでファイル転送というタスクは成功です。せっかくですのでmacの方からJarファイルを実行してみます。\nテスト用のデモなのでtestという字を出力するだけにしていますが、とにかく見事実行は成功です。今回のポストでのタスクもこれで終わりました。\n最後に 今回はただ単にファイルをコピーする作業を二つ繋げただけですが、ここからは既存のJarファイルの実行を終了し、新しくデプロイされたファイルを起動するなどのJobを作っていくなど連携の方法は色々ありそうです。何もかも応用次第ですからね！このポストを読まれる皆さんもJenkinsで自動化を試し、私以上に応用できることになるといいなと思います。\nそれでは、今回はここまで。また初心者向けの情報が集まったら、新しいポストでお目にかかりましょう。\nこの画面からはまるでzipファイルになっているようにも見えますが、実際はzipファイルでダウンロードできるという意味です。成果物はちゃんとフォルダの中に元の形で保存されています。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nただ、スクショにはもしものことなので実際の情報は書いていません。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n基本的にはSSH接続したユーザーのホームディレクトリが基準となります。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-30T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-java-deploy/","title":"JenkinsでJarファイルをデプロイする"},{"content":"今回はJenkinsで一つのJobを生成することまでやりたいと思います。まず前回も書きましたが、私に要求されたタスクは次のようなことです。「GitにSpring Bootのアプリケーションがあるので、それをビルドするJobを作って欲しい」とのこと。リポジトリを覗き、JavaとGradleはインストールしたので早速Jobの生成に取り掛かることにしました。\n今回は当時と似たような環境を作るためにあらかじめSpring Bootプロジェクトを作成してGitにアップロードしています。\nそれでJobとは？ Jenkinsを使う理由は、このJobのためです。Jobは自動化されたタスクで、実行の条件から頻度、タスクの内容を組み込むことができます。例えばGitのリポジトリに誰かがPushしたらJobを実行するように設定できるし、実行したら自動的にGitをPullするように設定できるということです。\nなのでJenkinsをサーバ上で起動しておいて、必要に応じてJobを組んでおけば大変手間が省けるという話は全てこのJobのおかげだと言えます。今回はこのJobの生成の手順から説明しましょう。\nJobを生成する Jenkinsのメインページに入ります。何もJobがない状態なので、メイン画面にcreate new jobsというリンクがあることを確認できます。Jobが無い場合はこちらでもJobを生成することができます。しかし何かしらのJobがある場合は、左のメニューでNew itemをクリックすれば新しいJobの生成画面に移行します。どちらかをクリックします。\nEnter an item nameでJobの名前を指定します。また、下にはどんなJobを生成するかに対するいくつかのテンプレートがあります。ここではFreestyle projectを選びます。ちなみに、Job生成時に指定した名前と同じ名のフォルダがJenkinsのWorkspaceというフォルダの下に生成されるので、なるべく半角英文字・スペースなしで生成することをお勧めします。\nJobの名前とテンプレートを選んだなら、Okを押してJobを生成します。\nここがJob設定のメイン画面です。画面の最上段にある各タブの機能は以下のようになります。\nGeneral: Job全般の設定。Jobの説明を記述したり（どんなタスクなのかなど）、ビルド1時以前のビルドを削除するかなどの設定ができます。 Source Code Management: バージョン管理に関する設定です。GitとSubversionに対応しています。今回はGitを使うことになりますね。 Build Triggers: Jobをどう実行するかに対するトリガーの設定です。リモートからURLを通じた実行になるか、定期的に実行するかなどの設定ができます。 Build Environment: Jobのビルド時に使われる環境に関しての設定です。基本的にJobはWorkspaceという空間を占め、ビルド時に使われたファイルなどは全てそのフォルダに保存されます。基本パスは/var/lib/jenkins/workspace/[生成したJobの名前]となります。 Build: Jobのビルド時のアクション（タスク）を指定します。ここで主な処理が行われます。 Post-build Actions: Jobのビルドが終わった後に遂行するアクションを指定します。他のJobをビルドするなどの行動が指定できます。 各タブはどんなプラグインをインストールしたかによって指定できる動作の項目が増えることがあります。また違うポストで扱うかもしれませんが、例えばPublish over SSHという\nGitのレポジトリを設定 それでは本格的に今回のタスクを作ってみましょう。まずGitからJavaのコードを持ってくる必要があるので、Source Code ManagementからGitを選択します。\nRepository URLにはGitのリポジトリを入力します。最初何も入力されてない時は接続エラーが出ていますが、URLを入力すると自動的にリポジトリへの接続を試し、問題がないとエラーは消えます。また、Credentialsから接続するIDなどの認証情報を選択します。最初は何もないので、Addをクリックし新しい認証情報を入力しましょう。\n今の所、私の作ったリポジトリはブランチが一つしかないのですが、もし特定のブランチだけをPullしたい場合はBranches to Buildのフィールドにブランチ名を入力することで指定できます。\n他はデフォルト値で、GitのIDとパスワードだけを入力してAddを押せば終了。こちらで入力した認証情報は全域設定なので、他のJobでも使えます。\n認証情報まで無事入力ができたら、Saveを押して一旦作業を保存しましょう。そうすると、以下のような画面に戻ります。\nビルドしてみる(1) これまででGitからPullするというタスクの設定は終わっています。ちゃんとタスクが意図通りに動作するかを検証するため、ビルドしてみましょう。左のメニューからBuild nowをクリックすると、しばらくして左下のBuild Historyという領域に初めてのビルドが表示されることを確認できます。#1の所に青い丸が付いていたらビルドが成功したという意味です。不安定だったら黄色、失敗だと赤の丸がつくのでビルドの状態を確認できます。\nビルドに成功したら、#1をクリックしてビルドの詳細を確認します。以下のような画面が現れます。\n実際のビルドでどんな処理が行われたかを確認するには、左のメニューからConsole Outputをクリックします。Linuxのコンソールのような画面で表示されます。\nちゃんと指定した通り、Gitのリポジトリ（ブランチはマスター）からPullしてきたことを確認できます。コミットメッセージまで出力してくれてますね。\nLinuxのコンソールから/var/lib/jenkins/workspace/配下のフォルダを直接確認することもできますが、JenkinsのWebコンソールからもJobのワークスペースを確認することができます。左のメニューでBack to Projectをクリックし、Workspaceをクリックすると以下のような画面が現れます。\n実際GitのPullが正しく行われ、フォルダとファイルが生成されたことをこちらで確認できます。\nGradleの設定 それでは次に、Gradleによるビルドの設定です。歯車のアイコンの付いたConfigureをクリックし、Jobの設定画面に戻ります。GradleでJarファイルをビルドするのが目的なので、GitでPullした後のタスクになりますね。\nBuildのタブに移動し、Add build stepをクリック、ドロップダウンメニューからInvoke Gradle scriptを選択します。そしてAdvanced...をクリックしましょう。\nここでInvoke Gradleには前回のポストで入れといたGraldeをドロップダウンメニューで選択します。\nそして次に、Use Gradle Wrapperの下のTasksにbootJarを入力します。このコマンドで実行可能なJarファイルが生成されます。また、bootJarコマンドではbuild.gradleのパスを確保する必要があるので（GraldeがWorkspaceのJob名のフォルダで実行されるので、その同じパスにbuild.gradleがある場合でないと必須です）下のBuild Fileにちゃんとパスを書きます。\nもちろんGradleのコマンドではビルドファイルのパスを指定するオプションもあるので、bootJarだけでなく-b SpringBootDemo/build.gradle bootJarのようにコマンドを書いても同じくビルドはできます。\nこれでGradleでのビルド設定は終わり。次にGitの時と同じく、検証してみます。\nビルドしてみる(2) ビルドの手順もGitの時と変わりません。Build Now → #2 → Console Outputの手順でビルドのコンソール出力を確認します。\n無事Gradleでのビルドが終わりました。それではJarファイルがちゃんとできたか確認しましょう。\nちゃんとbuild/libsフォルダにJarファイルができたことを確認できます。\n最後に Jenkinsでできることはこれだけではありません。ビルド終了後にデプロイのJobを作り、そのJobを実行するように連携もできれば、JUnitとの連携でテストを行えうように設定も可能です。なので今後のポストでは出来上がったJarファイルをテストしデプロイする方法について書きたいと思います。\nまた、Jobの設定でBuild EnvironmentタブにあるDelete workspace before build startsというオプションをチェックしておくのも良いと思います。Jobの実行時にWorkspaceフォルダをまず掃除してからタスクを実行していきますが、これによって前回のビルドで生成されたファイルによる異常動作を防げられますので。\n仕事で触れた部分が少ないのでまだ私も全部の機能を試した訳ではありませんが、Jenkinsは各種プラグインとその組み合わせによって可能性は無限大に違いのではないかと思うくらい良いツールです。また色々研究したいものですね。\nJenkinsのJobにおいてのビルドは、Jobのバージョンに近いイメージです。特定時点のJobを設定〜実行までの単位をビルドと言います。Javaのビルドとは少し違う概念なので混同しないようにしましょう。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-26T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-java-build/","title":"JenkinsでJavaプロジェクトをビルドする"},{"content":"小学校の頃初めてMS-DOSに触れ、その後ずっとWindowsのPCを使っていたので私はLinuxにそう詳しくないです。CLI1にはGUIとは違う感性があると思っていたくらいです。初めてCentOSを仮想マシンでインストールして見たときも、「あ、DOSに似ている」くらいの印象しかなかったです。でも仕事で使いながら思ったより便利ではやいと思いました。さすがサーバーで人気ナンバーワンのマケットシェアを誇るだけはあるんだなと思うくらいです。\nそれで、今回はcdやmkdirのような基礎の中の基礎しか知らない私が仕事で肩ごしで学んだコツをいくつか書こうと思います。これも基礎ですが、実際使ってみないとわからないだろうなと思っていたことが多く、コマンドやショートカットは本当に使い方次第だなと思ったのでその整理のためでもあります。\ntabキーで自動補完 GUIに比べCLIが難しいと思われる理由は、何よりもコマンドの入力がベースになっているからと思います。ファイルを元のフォルダから違うフォルダに移動するという単純な動作も、GUIではマウスでドラッグすればいいだけのものをCLIではいちいちコマンドを打ちます。そもそもmvのようなコマンドを知っていなければ成立できないことで、コマンドのオプションでまったく違う結果になったりもしますね。そして何よるも面倒なことは、ファイルやフォルダのパスをいちいち入力しなければならないということです。\nそしてそれが不便と思ったのは多分私だけじゃなかったみたいです。いつから存在していたのかわからないショートカットですが、なんとtabを押すことで自動補完ができるのでした。これなら長いアルファベットでも、スペースが含まれていても大丈夫です。tabでの自動補完は以下のように動作します。\nファイルやフォルダ名を入力する 書いている途中でtabキーを押す 自動補完でファイルやフォルダ名が完成 ２つ以上のファイルやフォルダがある場合はリストを出力 例えば /home/retheviper/task01というフォルダに移動するとします。\ncd h # ここでtab $ cd home/ # 自動補完される $ cd home/r # ここでtab $ cd home/retheviper/ # 自動補完される $ cd home/retheviper/t # ここでtab task01 task02 task03 # tから始まるフォルダが複数存在しているのでリストを出力する かなり便利なショートカットなので覚えておきたいコツです。\nフォルダ移動と目録の出力関連コマンド 現在のフォルダに何が入っているかをみるコマンドはlsやllなどがありますね。このコマンドにもう少しコマンドを付け加えることで違うフォルダの内容も出力できます。当たり前なことですが、同じような動作をするために私が主に使っていた方式はこうです。\n# 現在のフォルダの内容を確認 $ ll # /varフォルダがあったため、その中を覗く $ cd var $ ll # スタート時点より上のフォルダがみたい $ cd .. $ cd .. $ ll しかし少しの応用でフォルダを変更せずにも違うフォルダを覗くことができます。\n# 絶対経路を使う $ ll /var/lib # 下のフォルダを覗く $ ll ./lib $ ll lib # 上のフォルダを覗く $ ll ../ # さらに上のフォルダを覗く $ ll ../../ 他には、パスが長くて以前の位置に戻るのに時間がかかる場合は、簡単に戻ることもできます。\n# 長いパスのフォルダの中にいる状態 $ pwd /var/lib/jenkins/workspace/job01/git_repository/git # また長いパスで全く違う経路のフォルダに入る $ cd /home/retheviper/todo/task01/awesomeblog $ pwd /home/retheviper/todo/task01/awesomeblog # 以前のフォルダに戻りたい $ cd - $ pwd /var/lib/jenkins/workspace/job01/git_repository/git ただ、cd -を使ってフォルダを移動する場合は直前の経路だけに戻れないので注意してください。2回連続で入力すると２つのパスだけを繰り返して往復することになります。\nファイル同期ができるrsyncコマンド Linuxでのファイルコピーは、scpがもっとも多く使われているようです。でもrsyncコマンドの方がより効率的であるので私はこちらをオススメしたいです。rsyncは単純にファイルをコピーするだけでなく、フォルダやファイルの同期ができます。リモートのフォルダーを同期したい場合も公開鍵による認証を入れておけば、パスワードなしでコピーできるというところがまたいいです。\nまた、同期ができるということは差分を見つけ出すことも可能ということです。すでにコピー先にファイルがあるときはコピー元のファイルを比較し、差分だけを転送するので性能がよりいいです。また、コピー元でファイルが消されている場合はオプションでコピー先のファイルも消すことができます。\n# フォルダを同期する $ rsync origin destination # コピー元から消されたファイルはコピー先でも消す $ rsync --delete origin destination ただ、他のコマンドでもそうですが、rsyncでコピーの対象を指定するときは/の有無がかなり重要です。\n例えばコピー元のフォルダを指定するときfolderのように書くとそのフォルダごとコピーされますが、folder/のように指定するとフォルダの下が対象となります。そしてコピー先のフォルダに権限がなかったり所有者名が違うとコピーできないどでご注意を。権限はchmod、所有者はchownで変えられます。\nまた、同期したいフォルダに消してはならないフォルダーがある場合は、オプションで指定することもできます。--exclude='folder'というオプションを入れることで、指定したフォルダーは同期の対象から外されます。このオプションはコピー元からでもコピー先からでも適用されるので、便利ですね。\nシステムのスペックと状態を見る 仕事で性能テストがあって、特定作業でサーバーのシステム状態をモニタリングする必要がありました。まずサーバーのマシンスペックから確認したいですね。/procフォルダにカーネルで使われる情報が集まっているらしいです。\n# CPUの詳細情報を見る $ less /proc/cpuinfo # メモリーの詳細情報を見る $ less /proc/meminfo リアルタイムでCPUやメモリーの使用量をみたい場合はvmstatを活用します。\n# 現在のステータスを出力する $ vmstat # メモリーの使用量を出力する $ vmstat -s # ディスクの活動を出力する $ vmstat -d # 1秒単位で更新しながら出力する $ vmstat 1 メモリーとディスク、CPUの使用量をモニタリングできるため覚えておきたいコマンドですね。\n長い出力を少しづつ見たい ls -al2などのコマンドでとある内容を出力する場合、リストが多すぎて画面に全部表示されない場合があります。こういうときはmoreコマンドで出力がターミナルを全部満たした場合、エンターキーを押すことで次のリストを表示することになります。\nls -al | more この他にも色々と覚えておくと便利なコマンドやショートカットはたくさんありそうですね。こういうのがLinuxの魅力ではないかとも思います。使えば使うほどLinuxが好きになるかも。\nそれでは、今回のポストはこれにて。私のようなLinux初心者の方が実戦で応用できるような知識になればなと思います。\n私はどっちかというとCUIという言葉に慣れていますが（Character User Interfaceとかの略だと思っていたので）、正確な名称はCommand Line Interfaceの略であるCLIらしいですね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nllはls -lのショートカットらしいです。同じ文字を2回入力するだけでも大体の内容を出力してくれるので便利ですね。ただ、ls -alでは隠されているファイルやフォルダも表示してくれます。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-19T00:00:00Z","image":"https://retheviper.github.io/images/linux_terminal.jpg","permalink":"https://retheviper.github.io/posts/linux-command-tips/","title":"知っていれば便利なLinuxのコツ"},{"content":"仕事でJenkinsを使って、自動化したいと言われたのはいくつかのタスクがあります。そしてそのタスクを実行するにもまたいくつかの手順が入りますね。こんなことがしたいという想像力ももちろん大事なものですが、何かをするにはそれに必要な環境を整えることが何よりも大事なのではないかと思います。今回のポストでは仕事で与えられた作業と、それを準備した過程を述べたいと思います。\nまずGitからSpring bootアプリケーションをPullしてきて、ビルドすること。まず私はSpring Framework1は扱ってみたことがありますが、Spring Bootは今回が初めてでした。そしてMavenは使ったことがあるものの、今回のようにGradleを使ったプロジェクトには触れたことがなかったです。Spring Bootが（そしてGradleが）以前と比べ初期設定が簡単とは言われていますが、Eclipeの上でしかアプリケーションを実行した経験しかないので、最初はどうやったらいいかイメージすらなかったです。\nそしてJenkinsでどのようにJobを構成すればいいかもわからなかったので、まずJenkinsでどのようにJobを構成し自動化ができるかを調べることから始めました。\nJenkinsでは、Jobという名称のタスクを作り、とある行動を中に仕組み、それを一つの単位として実行することができます。全てのプログラムがそうでありますが、Linuxでのシェルスクリプトみたいに、繰り返す必要のある行為を自動化するようなものです。そして出来がったJobは手動実行するか、条件（トリガー）を指定して実行するようになります。\n今回のタスクを振り返ってみましょう。JavaアプリケーションがGitにあります。それをPullし（Jenkinsが実行されているサーバにソースコードを持ってきて）、ビルド（実行可能なパッケージにする）する。これでJenkinsは最新のソースコードを確保しつつ、実行可能なアプリケーションをデプロイできる状態になります。それではまず必要な道具はJDKとGradleです。\nOpenJDKをインストールしましょう まずJenkinsはJava８でも実行できますが、今回のアプリケーションはなんとJava11を使っていました。OpenJDK112をインストールします。yum install javaをするとOracleのJavaがインストールされるので、OpenJDKをインストールしたい場合はまた少しの手順が必要となります。今回はwgetではなく、curlを使ってみます。\ncurl -O https://download.java.net/java/GA/jdk11/13/GPL/openjdk-11.0.1_linux-x64_bin.tar.gz curl -OはURLからファイルをダウンロードして保存するということです。ダウンロードしたファイルは圧縮されているので解凍します。\ntar zxvf openjdk-11.0.1_linux-x64_bin.tar.gz tarは圧縮したり解凍するとき使うコマンド。zは.gzファイルを、xは圧縮ファイルの展開、vは処理したファイルを表示、fはこのファイルを指定するという意味らしいです。解凍が終わったらファイルを適切な場所に格納しましょう。\nmv jdk-11.0.1 /usr/local/ 次に簡単なシェルスクリプトを書きます。Linuxでの環境変数を設定するためです。まずviやvimでファイルを作りましょう。\nvi /etc/profile.d/jdk11.sh iを押して以下の内容を書きます。Javaを格納したパスを確認してください。\nexport JAVA_HOME=/usr/local/jdk-11.0.1 export PATH=$PATH:$JAVA_HOME/bin esc⇨:wqで保存と終了。そしてすぐシェルスクリプトを今の状態に適用させます。sourceコマンドを使います。\nsource /etc/profile.d/jdk11.sh これでOpenJDK11は準備されました。私はCentOS7を使っているのですが、ubuntuなどの違うLinuxではまた手順も色々あるようです。とにかくインストールがちゃんと終わっているかjava -versionで確認してみましょう。\nあれ？Javaのバージョンが8です。すでにインストールされていたんですね。\n使いたいJavaのバージュヨンを切り替える すでに違うバージョンのJDKがインストールされた場合は、次の手順で使いたいJavaのバージョンを選択できます。 まず新しくインストールしたJavaを選択できるよう登録します。\nalternatives --install /usr/bin/java java $JAVA_HOME/bin/java 2 登録が終わったら、以下のコマンドで現在登録されているJavaを表示します。\nalternatives --config java CentOSインストール時にすでに二つのバージョンのJavaがインストールされてましたね。Java11が3番目になったので3を入力します。これでJavaは準備オッケーです。\nちなみに、JenkinsもまたJavaで作られているので場合によってはJava11のJVMで起動できます。Java11のJVMをJenkinsに登録するには以下の手順になります。\nvi /etc/init.d/jenkins Jenkinsの設定ファイルです。candidatesという部分に様々なバージョンのJVMの経路が機材されていますので、ここにOpenJDK11のbinフォルダのパスを追加します。esc⇨:wqで終了！\nGradleもインストールしよう 次にGradleをインストールします。macOSではbrew install gradleで簡単にインストールできましたが、LinuxではどうもまたOpenJDKと同じ手順が必要なようです。wgetをまた使ってみます。\nwget https://services.gradle.org/distributions/gradle-5.4.1-bin.zip -P /tmp -Pオプションをつけると指定したフォルダにファイルを保存します。\nsudo unzip -d /opt/gradle /tmp/gradle-5.4.1-bin.zip zipファイルンなので、unzipコマンドで解凍します。-dもまたフォルダ（ディレクトリ）を指定するオプションです。\nsudo nano /etc/profile.d/gradle.sh 今回はnanoを使ってスクリプトを作ってみます。よりGUIに近い感じがしますね。nanoがなければyum install nanoでインストールしてもいいし、viを使っても良いです。\n私はどこでも使えるということ（環境によってはsudoの権限があるとしても、勝手に色々インストールできない場合もあるので)からviを使うことが多いですが、nanoはより直観的で使いやすいと思います。\n添付の画像のように、以下の内容を入力します。\nexport GRADLE_HOME=/opt/gradle/gradle-5.4.1 export PATH=${GRADLE_HOME}/bin:${PATH} nanoではctrl+xを押すと編集した内容を保存するかを聞いてきます。Yのあとエンターを押すと保存。US標準のキーボードを使う私にとっては:wqより入力が簡単で好きです。\nあとは作られたスクリプトに実行の権限を与え、sourceコマンドで環境変数として登録。\nsudo chmod +x /etc/profile.d/gradle.sh $ source /etc/profile.d/gradle.sh chmod 755のようなコマンドを書いたことが多いですが、+xで実行の権限だけ与える方が習慣的には良さそうですね。\ngradle -v インストールに成功したかを確認するにはやはりバージョンの確認ですね。以下のような画面が表示されたらGradleのインストールも成功です。\n最新のバージョンではSwift5をサポートするようですね。こちらもいつか扱いたいと思います。\nそれではいよいよJenkinsに戻り、JenkinsにJDKとGradleを環境として登録します。\nJenkinsにJDKとGradleをつなげる やっとJenkinsに戻りました。もうこれがJenkinsのポストかLinuxのポストカわからないくらいになっていましたが、とにかく戻ってきました。\nウェブブラウザにlocalhost:Jenkinsのポート番号を入力してJenkinsのメイン画面に入ります。その後は、左のメニューからManage Jenkinsをクリックします。そうすると以下のような画面が現れます。\nGlobal Tool Configurationを押します。ここがJenkinsで使われる環境変数的なものを設定する画面です。\nJDK項目のADD JDKを押します。install automaticallyというオプジョンが基本的にチェックされていますが、これはOracleのJavaしかインストールできないオプションです。またバージョンに制限があるので（私の場合はJava9までしか設定できませんでした）、チェックを外してJAVA_HOMEにインストールしたJava11のパスをいれます。以下のような感じです。Nameも必要なので適当な文句をいれます。\n次にGradleです。こちらもJavaと同様、自動インストールのオプションがあります。でも我々がインストールしたのよりはバージョンが低いですね。なのでこちらも自動インストールのオプションを外し、パスをいれます。以下のようになります。\nSaveを押して保存することを忘れずに！\nこれでJenkinsでのJDKとGradleの設定は終わりです。これからはJavaアプリケーションをビルドできるようになりました。次のポストで実際Spring Bootで作られたアプリケーションをGitでPullし、ビルドするタスクを作ってみたいと思います。\n厳密には、Spring Frameworkというよりは旧バージョンと言えるでしょう。Spring BootはSpring Frameworkに含まれるものなんですからね。いつかSpring Bootに関しても勉強したいと思うので、機会があればポストしたいと思います。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n現在はJava12まで出ていますが、JenkinsでJava11をサポートし始めたのも最近のことなので（2019年３月）、まずJava 11を選びました。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-16T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-automation-3/","title":"Jenkinsで何もかも楽にしたい(3)"},{"content":"前回から続きます。Jenkinのインストールが終わったら、初期設定の番です。どんなことでも始めが一番面倒なものですが、それだけ初期設定をちゃんとやっていると後の作業が楽になるものですね。なので今回はJenkinsの初期設定に関してまず述べたいと思います。\nJenkinsの初期設定はだいたい以下の順になります。他にも色々しておいたら便利な設定があるかもしれませんが、あくまで初心者の私の観点が基準なので参考までにしてください。\nポートの設定 Jenkinsの基本ポートは8080です。このままJenkinsを起動すると、WebブラウザからJenkinsを起動中のシステムのIPアドレス:8080でJenkinsに接続できます。\nしかし、Webアプリケーションを開発してみた経験のある方には8080というポートはあまり良い選択肢ではないことがわかるはずです。同じポートに設定されている二つ以上のサービスが問題を起こす可能性がありますからね。1\n私はのちにTomcatを使うことがあるかもしれないと思い、Jenkinのポートは8088に変えました。viやvim2でJenkinsのシステムコンフィグファイルを開けます。\nsudo vim /etc/sysconfig/jenkins そうすると、以下のような画面が現れます。少し下にスクロールしたらJENKINS_PORTと、親切に書いてあるのが見えますね！Iを押してインサートモードに切り替え、好きなポートに変えましょう。\n書き換えが終わったらESC ⇨ :wqで保存後終了を忘れずにしましょう。\n起動〜初期パスワードの設定 Jenkinsは初起動で初期パスワードを要求します。この初期パスワードが格納されてある位置は、基本的に/var/lib/jenkins/secrets/initialAdminPasswordというパスに保存されるようですが、OSなどの環境によってパスが変わる場合もありますからね。3でも、一度Jenkinsを起動したら初期設定のページからパスを確認できるので心配いりません。\nポート設定が終わったら（8080をそのまま使いたいならそのままでもいいです）、まずJenkinsを起動します。\nservice jenkins start [OK]というメッセージが出力されるはずですが（ポート設定に問題がある場合もあるので）念の為起動状況を確認します。\nservice jenkins status 実は、仕事で１日前は元気だったJenkins先生が、いきなり接続できなくなっていたことがったのです。やはり人は何かよくないことを経験すると、慎重になるものです。Active: active (running)というメッセージを確認できたら、いよいよJenkinsのページに接続です。\nJenkinsを起動中のシステムのIPアドレス:ポーと番号をWebブラウザに入力してJenkinsのページへ接続します。もちろん、起動中のシステムからはlocalhost:8080などでも接続できます。\nやはりパスは/var/lib/jenkins/secrets/initialAdminPasswordでした。ポート設定と同じく、viやvimで中を覗き、そのパスワードを入力します。\nsudo vim /var/lib/jenkins/secrets/initialAdminPassword プラグインと管理者アカウントの設定 パスワードの入力に成功するとしばらくして、プラグインの設定画面が現れます。自分でプラグインを選んでも良さそうですが、私は自信がないのでオススメのボタンを押します。当たり前なことですが、プラグインは後ででもインストールできます。\nオススメのボタンを押すと自動的にプラグインのインストールを進めてくれるので、待ちましょう。\nプラグインのインストールが終わったら次は管理者アカウントの設定です。一つでも満たしてない項目があったら怒られるので全部書きます。\n管理者アカウント設定の次は接続アドレスの設定。私は今のままでいいと思うので（仮想マシンでCentOSをインストールしてJenkinsを動かしています）そのままにします。\nそして、Jenkinsが用意されたという画面が出ます。長かったですね…早速使うというボタンを押します。\nジャジャーン。ようやくたどり着きました。Jenkinsのメイン画面です。ここまでの旅も本当に長かったですね。それでもJenkinsは強力なツールなので、ここまでする価値があると私は思います。\nそれでは、次からは具体的にJenkins先生と共にどんなタスク（Job）を作り、実行したかを述べたいと思います。また会いましょう！\n（続きます）\n特にSpring FrameworkなどでWebアプリケーションを実装する場合にそうですね。Tomcatの基本ポートも8080になっていますから。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nviとvimのうち、どれを選ぶかはいつも悩ましいことです。vimの方がよりカラフルでコードを読みやすいという点では良いですが、システムによってはインストールされてないですからね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmacOSでインストールしたら、初期パスワードのパスがユーザホームディレクトリの直下だった場合もありました。rootではないユーザでインストールしたからかもしれませんが。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-14T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-automation-2/","title":"Jenkinsで何もかも楽にしたい(2)"},{"content":"世の中、なんでも自動化がトレンドのようです。AIの話も究極的には、そういう自動化の範疇に入るようなものではないかと思います。まだ人間がAIに仕事を奪われるような時代になるまではもう少し時間がいるのでは、と思いますがね。\nそういう意味で、Jenkinsについて調べました。もちろん、まだ何が何だかわからないひよっ子みたいな自分に「今回はJenkinsで面倒なことは全部自動化して手間を減らそう」みたいな考えがあったわけではなく、あくまで仕事で使われているから、というシンプルな理由です。\nそれで、Jenkinsとは？ そもそもJenkinsがどんなものか知りたい。仕事で使うということは、なんらかの理由があるはずです。また、この業界で使われるミドルウェアは、結局何か便利（手間を省ける）な面があるから使われるのでしょうね。ならJenkinsは何が便利で使いたくなるのか？という観点から調べてみました。\n継続的インテグレーション と言われましても…な気がしました。つまりビルドからテスト、検証などをしてくれるツールらしいです。最初はGitに連動して使うと言われましたので、もうバージョン管理をGitでしているのに、また何かツールを連携して使うメリットがあるのか？と思いましたが、この「継続的インテグレーション」という行動が自動化できるからいいということです。\nGitやSubversionでのバージョン管理をしても、誰がいつPushしたのか、ログを見るまではわからない。また、Pushしたことをわかったら人の手でテストをしてまたMergeするしかない。これがなんとJenkinsで解決できるということです。例えば誰かがPushしたらJenkinsに通知が送られ（これには別作業が要るらしいですが）、Pull・ビルド・JUnitでのテストまでしてくれて、結果を通知してくれるとか。また設定次第ではテストが無事終わったらデプロイ1もしてくれるすごいツールらしいです。\nでも使ってみないとわからない それでは早速Jenkinsのすごい自動化を経験するために、インストールから始めます。仕事で使う環境はLinux。すぐyumでインストールできるのではないかなと思いましたが、どうもそうではないようです。\nJenkinsのホームページに接続してみると、Linuxでのインストールの手順がありました。幸い、仕事で使うLinuxはAWSなのですが、RedHat Linux2と同じ手順でインストールできました。\nそれではインストールしましょう まず、Jenkinsのリポジトリを持ってきます。\nsudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo ここでwget -Oはフォルダを読み、ファイルとして出力するオプションらしいですね。ここでも勉強になります。\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key rpmはパッケージをインストールする時使うコマンドなのですが、ここでは\u0026ndash;importオプションでキーを持って検証を行いますね。\nここまで終わったら、普通のパッケージと同じくyumでインストールできます。\nyum install jenkins これでインストールは終わり。問題なければ、ポートの設定をやって起動するまでです。\n（続きます）\n他のサーバなどに配置すること。例えば開発環境と本場環境は分離する必要があるので、開発環境で動作の検証が終わったものを本場環境に置いて実行することになりますね。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFedoraとCentOSでも同じくできるらしいです。うちではCentOS7で検証してみました。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-13T00:00:00Z","image":"https://retheviper.github.io/images/jenkins.jpg","permalink":"https://retheviper.github.io/posts/jenkins-automation-1/","title":"Jenkinsで何もかも楽にしたい(1)"},{"content":"まだSEになってからの日が浅いので、業界の動向や流行りの言語はおろか、プログラミングの基礎もまだわからないことが多いです。それでも仕事で学び、覚えられるようなことは多いので、それらの知識をただ頭に入れておくということは無理なのではないかと思います。日々新しい言語、フレームワーク、ライブラリーが登場している中、自分は今までの技術もまともに習得していないのです。それならばせめて、どこかに自分が触れたことのある知識をまとめ参考にしたい。そういう考えからブログを始めることにしました。\n人間の文明の発達の歴史は、記録によるものという話がありました。人類の持つ知識の総量が一人の脳内に収められないくらい膨大になってから、人間が考案した方法は外部媒体による記録であるということです。そのおかげで我々は代々受け継がれてきた知識に触れることができ、最初から知識を持ってなくても臨機応変できるようになったと私は思います。例えば、携帯が生まれてからは誰も人の電話番号を覚えなくなりました。それでも連絡できる人の数はむしろ増えましたね。つまり、知識の外部記録できるようになって、個人が持つ知識の総量よりは、時と場合に適合な知識を探し出す能力が大事な時代になったのではないかと思います。\nプログラミングの世界もそうではないかと思います。bash1は依然として強力なツールであり、GUIより便利な時もあります。しかし全てのコマンドを覚え、パスを入力することはあまり効率的ではないです。むしろ必要な時に適切なコマンドを入力できるかどうかの方がより重要なのではないでしょうか。そのためにコマンどを集めたスクリプトが現れ、今の時代は簡単にコピペするだけでその時必要な操作ができたらいいのではないかと思います。\n自分がブログに勉強したことをまとめる時もなるべくそのような使い方ができるようなポストになったらいいなと思います。例えばAというサーバからBというサーバにフォルダをコピーしたいというなら、ただSCPやRSYNCのようなコマンドを書くだけでなくssh認証をはじめとしてSCPとRSYNCの違いまで比べるという一連の流れとしての知識を残す。そのような情報が得られるようなブログになったらいいなと思っています。\nLinuxだけに限らず、macOSのterminalやWindowsのPowershellなどを含め\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-05-12T00:00:00Z","image":"https://retheviper.github.io/images/tools.jpg","permalink":"https://retheviper.github.io/posts/new-start/","title":"はじめに"}]